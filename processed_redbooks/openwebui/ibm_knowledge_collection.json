{
  "name": "IBM Z Knowledge Base",
  "documents": [
    {
      "id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
      "url": "",
      "title": "sg248573.pdf",
      "content_chunks": [
        {
          "id": "0c1df9f1-8eea-40d6-941d-828e26bb3886",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "RedbooksArtificial Intelligence\nData and AIFront cover\nSimplify Your AI Journey:\nEnsuring Trustworthy AI with \nIBM watsonx.governance\nDeepak Rangarao\nUpasana BhattacharyaSavitha Chinnappareddy PhDLarry CoyneDavid CruzShuvanker GhoshPrem Piyush GoyalVasfi GucerAmna Jamal PhDWarren LucasKaren MedhatBob RenoMohit Sharma\nMark SimmondsJasmeet SinghMartijn Wiertz\n\n\n\nIBM Redbooks\nEnsuring Trustworthy AI wi th IBM watsonx.governance\nJanuary 2025\nSG24-8573-00\n\nii Ensuring Trustworthy AI with IBM watsonx.governance\n\n© Copyright IBM Corp. 2025. iiiContents\nNotices  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\nTrademarks  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\nForeword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\nPreface. . . . . . . . . ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 0,
            "total_chunks": 245
          }
        },
        {
          "id": "02767a1a-d1a9-49c2-a636-772954a6aca0",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .x\nAuthors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .x\nNow you can become a published author, too!  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiiiComments welcome. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . x iii\nStay connected to IBM Redbooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiv\nChapter 1.  Challenges and opportunities in AI governance for responsible AI  . . . . . . 1\n1.1  What is AI governance?  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2  Governance as a key enabler for realizing A",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 1,
            "total_chunks": 245
          }
        },
        {
          "id": "8dc1ae47-aafa-48a0-bf6a-9900b4ebad7c",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": ". . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2  Governance as a key enabler for realizing AI value . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.2.1  Concern 1: Governance is a brake on AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.2.2  Concern 2: Governance does not scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.2.3  Concern 3: Governance does not contribute to value generation. . . . . . . . . . . . . . 6\n1.3  Challenges with governance of enterprise AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.3.1  Generative AI has changed the governance game. . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.3.2  Bring together diverse stakeholder perspectives  . . . . . . . . . . . . . . . . . . . . . . . . . . 71.3.3  Technical complexity is increasing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1.3.4  Regulatory and risk complexity is increasing  . . . . . .",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 2,
            "total_chunks": 245
          }
        },
        {
          "id": "5d8cf4aa-4d7a-4b8b-b01f-516ce8b36c47",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": ". . . . . . . . . . . . . . . . . 9\n1.3.4  Regulatory and risk complexity is increasing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.4  An example of legislation and standards related to AI  . . . . . . . . . . . . . . . . . . . . . . . . . 11\n1.4.1  AI-specific legislation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n1.4.2  General regulations that apply to AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n1.4.3  Technical standards for AI governance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nChapter 2.  Introduction to IBM watsonx.governance  . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.1  Introduction to the IBM watsonx platform and its core components . . . . . . . . . . . . . . . 16\n2.2  Introduction to IBM watsonx.ai  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.3  Introduction to IBM watsonx.data . . . . ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 3,
            "total_chunks": 245
          }
        },
        {
          "id": "babf835f-6e17-4304-921e-c3718e219e80",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.3  Introduction to IBM watsonx.data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.4  Introduction to IBM watsonx.governance  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.4.1  Key capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.4.2  Use cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1\n2.4.3  Benefits of watsonx.governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.5  Reference architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2\n2.5.1  Data Onboarding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222.5.2  Data Preparation. . . . . . . . . . . . . . . . . . . . . . . . . .",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 4,
            "total_chunks": 245
          }
        },
        {
          "id": "58c4158d-f718-4385-95ee-741ff61944ab",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": ". . . . . . . . . . . . 222.5.2  Data Preparation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n2.5.3  AI Building and Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n2.5.4  AI Lifecycle Management and Governance  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nChapter 3.  Implementing AI governance strategy  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.1  Understanding the end-to-end AI lifecycle governance process. . . . . . . . . . . . . . . . . . 28\n3.2  Elements of model risk governance  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.2.1  Personas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.2.2  Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n3.2.3  Workflows . . . .",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 5,
            "total_chunks": 245
          }
        },
        {
          "id": "466fde94-8ae4-47e3-b915-4712502967fb",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n3.2.3  Workflows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 6\n3.3  Considerations to implement AI governance strategy. . . . . . . . . . . . . . . . . . . . . . . . . . 37\n3.3.1  Understanding organizational characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383.3.2  Configuring AI governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n\niv Ensuring Trustworthy AI with IBM watsonx.governance3.3.3  Leveraging out-of-the-box product content . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.3.4  Example use case. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\nChapter 4.  Onboarding a new foundation model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n4.1  Key considerations to onboard a foundation model",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 6,
            "total_chunks": 245
          }
        },
        {
          "id": "134e8219-f55d-44c5-b41d-a69890d9ae86",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": ". . . . . . . . . . . . . . . . . . . . . . 41\n4.1  Key considerations to onboard a foundation model  . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n4.1.1  Data transparency. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n4.1.2  Model evaluation and validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424.1.3  Model security and robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n4.1.4  Ensuring model health and performance  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n4.2  Considerations for legal team for approving a new foundation model  . . . . . . . . . . . . . 44\n4.2.1  Model licensing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n4.2.2  Legal obligations on the part of the vendor. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.2.3  A final note on lega",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 7,
            "total_chunks": 245
          }
        },
        {
          "id": "8b099258-08c3-4277-9e5f-8fdfa75f00b2",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "the vendor. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.2.3  A final note on legal considerations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n4.3  Ethical considerations for approving a new foundation model  . . . . . . . . . . . . . . . . . . . 47\n4.3.1  Fairness  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  47\n4.3.2  Transparency  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484.3.3  Privacy  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.3.4  Explainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n4.3.5  Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494.3.6  Third-party help. . . . . . . . . . . . . .",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 8,
            "total_chunks": 245
          }
        },
        {
          "id": "aeba031a-106b-4d78-85cf-228ac6dd6853",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": ". . . . . . . . . . . . . . . . . . . . . . . . 494.3.6  Third-party help. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n4.4  Considerations for financial stakeholders  for approving a new foundation model  . . . . 49\n4.4.1  Total cost of ownership . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504.4.2  Return on investment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.4.3  Build or buy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.4.4  Exit strategy  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.4.5  Other factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1\nChapter 5.  Assessing a new use case  . . . . . . . . . . . . . . . . . . . . . . . . ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 9,
            "total_chunks": 245
          }
        },
        {
          "id": "d18b24ca-edae-4541-aaf2-d851136069f7",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": ". . . . . 5 1\nChapter 5.  Assessing a new use case  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n5.1  Business process workflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n5.2  Approval workflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  55\n5.3  Risk identification assessment  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 565.4  Applicability asse ssment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\nChapter 6.  Governing the end-to-end lifecycle of an AI asset  . . . . . . . . . . . . . . . . . . . 61\n6.1  What is the AI lifecycle?  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n6.2  Metrics in watsonx.governance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 10,
            "total_chunks": 245
          }
        },
        {
          "id": "16fc65f3-5851-4ec5-a7b4-b0b2cecaa953",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "onx.governance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6.2.1  Drift detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6.2.2  Explainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6.2.3  Model health . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 656.2.4  Generative AI quality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n6.2.5  RAG quality metrics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n6.3  How to implement Lifecycle Governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n6.3.1  Getting started: Setting up your AI use cases. . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n6.4  Lifecycle implementation a",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 11,
            "total_chunks": 245
          }
        },
        {
          "id": "5db09c14-26cd-4710-aa2f-bb3ee0144d96",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " use cases. . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n6.4  Lifecycle implementation and considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n6.4.1  UI-driven implementation of lifecycle governance. . . . . . . . . . . . . . . . . . . . . . . . . 686.4.2  Considerations for lifecycle governance for traditional ML hosted on watsonx.ai. 70\n6.4.3  Considerations for prompt templates from another platform. . . . . . . . . . . . . . . . . 72\n6.4.4  Considerations for traditional ML from another platform. . . . . . . . . . . . . . . . . . . . 746.4.5  Governing AI embedded in a business application. . . . . . . . . . . . . . . . . . . . . . . . 74\nChapter 7.  Use cases  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n7.1  Overview of use case 1- Banking credit risk management . . . . . . . . . . . . . . . . . . . . . . 78\n7.1.1  Banking credit risk management use case . . . . . . . . . . . .",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 12,
            "total_chunks": 245
          }
        },
        {
          "id": "64a46c66-6ae7-48f5-b614-2ad92376bef5",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " . . . . . . . . . . . . . 78\n7.1.1  Banking credit risk management use case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n7.1.2  Business context. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n\n Contents v7.1.3  Client need . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n7.1.4  Client challenges. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n7.1.5  Business benefits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n7.1.6  Pilot solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  80\n7.2  Overview of use case 2 - Automated governance for universal bank's AI chatbot . . . . 80\n7.2.1  Business context. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 13,
            "total_chunks": 245
          }
        },
        {
          "id": "dd8bf764-567f-436d-8fe5-12b25d255094",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ss context. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n7.2.2  Client need . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n7.2.3  Client challenges. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n7.2.4  Business benefits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n7.2.5  Pilot solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  81\n7.3  Overview of use case 3 - Belgian biopharmaceutical company . . . . . . . . . . . . . . . . . . 81\n7.3.1  Business context. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n7.3.2  Client need . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n7.3.3  Client",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 14,
            "total_chunks": 245
          }
        },
        {
          "id": "6d535ec3-8a65-4ade-aefa-1c1066d5ee7b",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n7.3.3  Client challenges. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n7.3.4  Business benefits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n7.3.5  Pilot solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  82\nRelated publications  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\nIBM Redbooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\nOnline resources  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\nHelp from IBM  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 15,
            "total_chunks": 245
          }
        },
        {
          "id": "56ed8d75-52fd-4346-98e3-ece60bfed010",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . 85\n\nvi Ensuring Trustworthy AI with IBM watsonx.governance\n\n© Copyright IBM Corp. 2025. viiNotices\nThis information was developed for prod ucts and services offered in the US . This material might be available \nfrom IBM in other languages. However, you may be required  to own a copy of the product or product version in \nthat language in order to access it. \nIBM may not offer the products, services, or features di scussed in this document in other countries. Consult \nyour local IBM representative for information on the produc ts and services currently available in your area. Any \nreference to an IBM product, program, or service is not intended to state or imply that only that IBM product, \nprogram, or service may be used. Any functionally equi valent product, program, or service that does not \ninfringe any IBM intellectual property right may be used instead. However, it is t he user",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 16,
            "total_chunks": 245
          }
        },
        {
          "id": "507540dd-f89a-42f4-b270-2d5132b76832",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "does not \ninfringe any IBM intellectual property right may be used instead. However, it is t he user’s responsibility to \nevaluate and verify the operation of any non-IBM product, program, or service. \nIBM may have patents or pending patent applications covering subject matter described in this document. The \nfurnishing of this document does not grant you any license to these patents. You can send license inquiries, in \nwriting, to:\nIBM Director of Licensing, IBM Corporation, North Castle Drive, MD-NC119, Armonk, NY 10504-1785, US \nINTERNATIONAL BUSINESS MACHINES CORPORATIO N PROVIDES THIS PUBLICATION “AS IS” \nWITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED \nTO, THE IMPLIED WARRANTIES OF NON-INFR INGEMENT, MERCHANTABILITY OR FITNESS FOR A \nPARTICULAR PURPOSE. Some jurisdictions do not allow disclaimer of express or implied warranties in \ncertain transactions, therefore, this statement may not apply to you. \nThis information could include technical in",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 17,
            "total_chunks": 245
          }
        },
        {
          "id": "e8f8bdb5-b510-450d-b8b0-1df142fa3051",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ctions, therefore, this statement may not apply to you. \nThis information could include technical inaccuracies or  typographical errors. Changes are periodically made \nto the information herein; th ese changes will be incorporated  in new editions of the publication. IBM may make \nimprovements and/or changes in the product(s) and/or the program(s) described in this publication at any time \nwithout notice. \nAny references in this information to non-IBM websites are provided for convenience only and do not in any \nmanner serve as an endorsement of those websites. The materials at those websites are not part of the \nmaterials for this IBM product and use of those websites is at your own risk. \nIBM may use or distribute any of the information you provide in any way it believes appropriate without \nincurring any obligation to you. \nThe performance data and c lient examples cited are presented fo r illustrative purposes only. Actual \nperformance results may vary depending on specific configu",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 18,
            "total_chunks": 245
          }
        },
        {
          "id": "ddab2410-31ac-4afd-8471-da47ab465bb4",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " fo r illustrative purposes only. Actual \nperformance results may vary depending on specific configurations and operating conditions. \nInformation concerning non-IBM products was obtained from the suppliers of those products, their published \nannouncements or other publicly available sources. IBM has not tested those products and cannot confirm the \naccuracy of performance, co mpatibility or any other clai ms related to non-IBM pr oducts. Questions on the \ncapabilities of non-IBM products should be addr essed to the suppliers of those products. \nStatements regarding IBM’s future direction or intent are subject to change or withdrawal without notice, and \nrepresent goals and objectives only. \nThis information contains exam ples of data and reports used in daily business operations. To illustrate them \nas completely as possible, the exam ples include the names of individual s, companies, brands, and products. \nAll of these names are fictitious and any similarity to  actual people or busi",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 19,
            "total_chunks": 245
          }
        },
        {
          "id": "76a07491-4e87-4d93-8d01-84aa9d444643",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "rands, and products. \nAll of these names are fictitious and any similarity to  actual people or business enterprises is entirely \ncoincidental. \nCOPYRIGHT LICENSE:\nThis information contai ns sample application prog rams in source language, which illustrate programming \ntechniques on various operating platforms. You may co py, modify, and distribute these sample programs in \nany form without payment to IBM, for the purposes of developing, using, marketing or distributing application programs conforming to the application programming interface for the operating platform for which the sample \nprograms are written. These examples have not been th oroughly tested under all conditions. IBM, therefore, \ncannot guarantee or im ply reliability, serviceability, or function of  these programs. The sample programs are \nprovided “AS IS”, without warranty of any kind. IBM sha ll not be liable for any damages arising out of your use \nof the sample programs. \n\nviii Ensuring Trustworthy AI with IBM wat",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 20,
            "total_chunks": 245
          }
        },
        {
          "id": "e3f13ecb-6dc6-4da3-a020-ce10afb73c53",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "damages arising out of your use \nof the sample programs. \n\nviii Ensuring Trustworthy AI with IBM watsonx.governanceTrademarks\nIBM, the IBM logo, and ibm.com are trademarks or regi stered trademarks of International Business Machines \nCorporation, registered in many jurisdictions worldwide. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on the web at “Copyright \nand trademark information” at https://www.ibm.com/legal/copytrade.shtml  \nThe following terms are trademarks or registered trademarks of International Business Machines Corporation, \nand might also be trademarks or registered trademarks in other countries. \nIBM®\nIBM Cloud®\nIBM Research®IBM Watson®\nOpenPages®\nRedbooks®Redbooks (logo) ®\nThe following terms are trademarks of other companies:\nMicrosoft, and the Windows logo are trademarks of Microsoft Corporation in the United States, other \ncountries, or both.\nOpenShift, Red Hat, are trademarks or registe",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 21,
            "total_chunks": 245
          }
        },
        {
          "id": "1d221f0f-e883-420d-867f-7c2841da14cf",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ation in the United States, other \ncountries, or both.\nOpenShift, Red Hat, are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in the United \nStates and other countries.\nRStudio, and the RStudio logo are registered trademarks of RStudio, Inc.Other company, product, or service names may be trademarks or service marks of others. \n\n\n© Copyright IBM Corp. 2025. ixForeword\nThis trilogy of IBM® Redbooks® publications positions and explains IBM watsonx, the \nIBM strategic AI and Data platform. Each book focuses on one of the three main components of the watsonx platform:\n/SM590000IBM watsonx.ai: A next-generation enterprise studio for AI developers to train, validate, \ntune, and deploy both tradit ional ML and new generative AI capabilities powered by \nfoundation models.\n/SM590000IBM watsonx.data:  A fit-for-purpose data store built on an open-lakehouse architecture, \noptimized for different and governed data and AI workloads.\n/SM590000IBM watsonx.governance:  A set o",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 22,
            "total_chunks": 245
          }
        },
        {
          "id": "9abdeb1d-1349-4a67-b1e5-7965b4b859d1",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ptimized for different and governed data and AI workloads.\n/SM590000IBM watsonx.governance:  A set of AI governance capabilities enabling trusted AI \nworkflows, helping organizations implement and comply with ever-changing industry and government regulations.\nOrganizations have long recognized the value that IBM Redbooks provide in guiding them \nwith best practices, frameworks, clear explanations, and use cases as part of their solution evaluations and implementations.\nThis trilogy of books was only possible due to the close coll aboration involving many skilled \nand talented authors that were selected from our IBM global technical sales, development, Expert Labs, Client Success Mana gement, and consulting servic es organizations, using their \ndiverse skills, experiences, and technical knowledge across the watsonx platform.\nI would like to thank the authors, contributors, reviewers, and the IBM Redbooks team for their \ndedication, time, and effort in making these pu blications a valuab",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 23,
            "total_chunks": 245
          }
        },
        {
          "id": "16bc353d-167b-44ba-878f-a380c8acd92f",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "the IBM Redbooks team for their \ndedication, time, and effort in making these pu blications a valuable asset that organizations \ncan use as part of their journey to AI.\nI also want to thank Mark Simmonds and Deepak Rangarao for taking the lead in shaping this \nrequest into yet another successful IBM Redbooks project.\nIt is my sincere hope that you enjoy this wats onx trilogy as much as the team who wrote and \ncontributed to them.\nSteve Astorino, IBM General Manager - De velopment, Data, AI and Sustainability.\n\nx Ensuring Trustworthy AI with IBM watsonx.governancePreface\nIBM® watsonx™  is the IBM strategic AI and Data platform. This book focuses on \nwatsonx.governance , a key component of the platform.\nIBM watsonx.governance offers a comprehensive solution for governing data and AI \nworkloads within a secure and scalable environment. Built on an open architecture, it empowers organizations to manage data access, compliance, and security across hybrid multi-cloud deployments. IBM watsonx",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 24,
            "total_chunks": 245
          }
        },
        {
          "id": "60c634d8-d50a-453c-80a7-4516734023ec",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "s to manage data access, compliance, and security across hybrid multi-cloud deployments. IBM watsonx.governance simplifies data governance with built-in automation tools and integrates seamlessly with existing databases and tools, streamlining workflows and enhancing user experience\nThis IBM Redbooks publication provides a broad understanding of watsonx.governance \nconcepts and architecture, and the services that are available in the product. In addition, several common use cases and scenarios are included that should help you better understand the capabilities of this product.\nThis publication is for watsonx customers who se ek best practices and real-world examples of \nhow to best implement their solu tions while optimizing the value of their existing and future \ntechnology, AI, data, and skills investments.\nAuthors\nThis book was produced by a team of specia lists from around the world working with the\nIBM Redbooks, Tucson Center. \nDeepak Rangarao  is an IBM Distinguished Engineer an",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 25,
            "total_chunks": 245
          }
        },
        {
          "id": "20f64140-1879-4a2d-83d3-a5fcacce6fe3",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " working with the\nIBM Redbooks, Tucson Center. \nDeepak Rangarao  is an IBM Distinguished Engineer and CTO responsible for Technical \nSales-Cloud Paks. Currently, he leads the technical sales team to help organizations modernize their technology landscape with IBM Cloud® Paks. He has broad cross-industry experience in the data warehousing and analytic s space, building analytic applications at \nlarge organizations and technical pre-sales with start-ups and large enterprise software vendors. Deepak has co-authored several books on topics, such as OLAP analytics, change data capture, data warehousing, and object storage and is a regular speaker at technical conferences. He is a certified technical sp ecialist in Red Hat OpenShift, Apache Spark, \nMicrosoft SQL Server, and web development technologies.\nUpasana Bhattacharya  is a Senior Product Manager for watsonx.governance, based in \nMarkham, Canada. In this role she defines the product vision, guides its development, collaborating with cr",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 26,
            "total_chunks": 245
          }
        },
        {
          "id": "d57245ef-f640-45bb-b510-0af10b693cb5",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": ", Canada. In this role she defines the product vision, guides its development, collaborating with cross-functional teams. In her previous role she was a Product Manager for Data and AI. Upasana holds a Bachelor of Arts in Economics and Foreign Affairs from the University of Virginia and an MBA from the McCombs School of Business at the University of Texas.Note:  Other books in this series are:\n/SM590000Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai , SG24-8574\n/SM590000Simplify Your AI Journey: Hybrid, Open Data Lakehouse with IBM watsonx.data,  \nSG24-8570\n\n Foreword xiSavitha Chinnappreddy, PhD  is a Senior AI Engineering Manager at IBM with over 17 years \nof experience in AI and Data Analytics. She holds a PhD in AI and Data Analytics and is currently pursuing a post-doctorate focused on Human & AI Collaboration: Governance strategies for trustworthy AI & Safe AI systems. She has extensive Experience in managing and scaling large AI and Data Science teams, s",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 27,
            "total_chunks": 245
          }
        },
        {
          "id": "b6e8347f-ac34-42d8-a2e9-ceae9cc99bee",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " AI systems. She has extensive Experience in managing and scaling large AI and Data Science teams, she has worked closely with architecture and infrastructure teams to establish compliant pipe lines for AI and analytics, delivering impactful \nsolutions to global customers. With 11 public ations in esteemed journals and conferences, as \nwell as holding a patent, she is also an active guest speaker and participant in faculty development programs, committed to sharing her knowledge and inspiring the next generation of AI professionals.\nLarry Coyne is a Project Leader at the IBM International Technical Support Organization, \nTucson, Arizona, center. He has over 35 years of IBM experience, with 23 years in IBM storage software management. He holds degrees in Software Engineering from the University of Texas at El Paso and Project Management from George Washington University. His areas of expertise include client relationship management, quality assurance, development management, and support",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 28,
            "total_chunks": 245
          }
        },
        {
          "id": "1c854446-90f2-4459-8442-7a2db4757eb1",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "rtise include client relationship management, quality assurance, development management, and support management for IBM storage management software.\nDavid Cruz is a Data Scientist and AI Engineer working under IBM’s Client Engineering \nteam. In this role, David has been dedicated to the Federal Market where he works to implement a wide range of AI solutions for federal clients. In his prior role, he worked under the Data Science Elite team where he gained skills with IB M platforms fo r Governance, \nnamely IBM OpenScale, and this has transl ated into a growing skill set with watsonx \ngovernance. He is constantly working to implement the cutting edge of AI and AI Governance technology, and has written various blog posts on topics ranging from Unsupervised Learning techniques, to RAG how-to guides for beginners.\nShuvanker Ghosh  is a certified Executive Architect and Worldwide Platform Leader for Data \nand AI in Worldwide Solution Architecture in IBM Technology Expert Labs. With 18 years",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 29,
            "total_chunks": 245
          }
        },
        {
          "id": "b6d7a2ed-52b0-41c7-bf48-ab99857ca510",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "der for Data \nand AI in Worldwide Solution Architecture in IBM Technology Expert Labs. With 18 years of experience at IBM, he serves as a trusted adviso r to clients, offering thought leadership on \nIBM's Data and AI portfolio. He guides organizations in their responsible AI journey, helping them adopt best practices. His current focus is on defining solution blueprints and architectural patterns that assist clients in addressing their business challenges through responsible and trustworthy AI solutions. He po ssesses extensive expertise in the IBM Data \nand AI portfolio, including the watsonx platform and Cloud Pak for Data. Shuvanker has successfully led and delivered co mplex programs that involve multiple teams, providing \ntechnical management, architecture, technology thought leadership, and software development methodologies and processes. His experience spans various industries, including retail, finance, insurance, healthcare, telecommunications, and government\nPrem Piyush Goya",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 30,
            "total_chunks": 245
          }
        },
        {
          "id": "0b81d6a0-ea14-46d0-9297-10385e0785a2",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ncluding retail, finance, insurance, healthcare, telecommunications, and government\nPrem Piyush Goyal  is a problem solver with extensive experience in developing cutting-edge \ntechnologies at IBM. Specializing in full-stack development, cloud-based microservices, and AI solutions, he has worked on high-impact projects like IBM Watson® Data Platform and IBM Watson OpenScale. His expertise spans Python, JavaScript, React, Kubernetes, and \nAI-driven solutions like Explainable AI and Concept Drift Detection. Passionate about building transparent and scalable AI, he continually enhances user experience and optimizes performance for enterprise applications. His in novative mindset and pr oblem-solving abilities \nhelp drive trust and tran sparency in AI systems.\nVasfi Gucer leads projects for the IBM Redbooks team, leveraging his 20+ years of \nexperience in systems management, networking, and software. A prolific writer and global IBM instructor, his focus has shifted to storage and cloud co",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 31,
            "total_chunks": 245
          }
        },
        {
          "id": "33798bac-5ed6-46b8-b913-c67bbf7a71b1",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "software. A prolific writer and global IBM instructor, his focus has shifted to storage and cloud computing in the past eight years. Vasfi holds multiple certificatio ns, including IBM Certified Senior IT Specialist, PMP, ITIL V2 \nManager, and ITIL V3 Expert. \n\nxii Ensuring Trustworthy AI with IBM watsonx.governanceAmna Jamal PhD  is a seasoned Data and AI Subject Matter Expert (SME) at IBM, boasting \nover 8 years of expertise in data management and data science. With a Ph.D. in Engineering \nfrom the National University of Singapore, she brings a wealth of knowledge and experience to the field, driving innovation and excellence in the intersection of data and artificial intelligence.\nWarren Lucas is a member of IBM Expert Labs. Prior to his time at IBM, Warren has spent \nnearly a decade working in Regulatory Compliance, Operational Risk, and Model Risk Governance supporting a number of Fortune 50 companies in their efforts to redesign and implement internal governance processes. As a S",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 32,
            "total_chunks": 245
          }
        },
        {
          "id": "92c3f139-5ce8-4562-bcf7-0ede11d60d54",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ortune 50 companies in their efforts to redesign and implement internal governance processes. As a Solution Architect, Warr en has specialized in \nGovernance Console (IBM OpenPages®) for over seven years, where he has personally performed development, design, advisory, and configuration within the platform. Warren has a \ncurrent patent submission for a novel approach in governance and confidence assessments in large language models (LLMs); he holds a degree in Quantitative Economics.\nKaren Medhat  is a Customer Success Manager Architect in the UK and the youngest \nIBM Certified Thought Leader Level 3 Technica l Specialist. She is the Chair of the IBM \nTechnical Consultancy Group and an IBM Academy of technology member. She holds an MSc degree with honors in Engineering in AI and Wireless Sensor Networks from the Faculty of Engineering, Cairo University, and a BSc degree with honors in Engineering from the same faculty. She co-creates curriculum and exams fo r different IBM professional",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 33,
            "total_chunks": 245
          }
        },
        {
          "id": "f324a887-e98a-4194-9fb2-19d4118c8260",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "gineering from the same faculty. She co-creates curriculum and exams fo r different IBM professional certificates. She \nalso created and co-created courses for IBM Skills Acad emy in various areas of \nIBM technologies. She serves on the review board of international conferences and journals in AI and wireless communication. She also is an IBM Inventor and experienced in creating applications architecture and leading teams of different scales to deliver customers' projects successfully. She frequently mentors IT professionals to help them define their career goals, learn new technical skills, or acquire professional certifications. She has authored publications on Cloud, IoT, AI, wireless networ ks, microservices architecture, and Blockchain.\nBob Reno  is a Principal Technical Sale s Specialist with over 30 years of experience in Data \nWarehousing, Analytics, and AI. As a member of the IBM World Wide Data and AI Technical Sales team, Bob is a watsonx.governance leader working with custo",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 34,
            "total_chunks": 245
          }
        },
        {
          "id": "c92ec75d-c36d-473a-9e2c-b08144f0a5ca",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "M World Wide Data and AI Technical Sales team, Bob is a watsonx.governance leader working with customers to enable their organizations to embrace responsible AI. Bob has contributed to the creation of several IBM Certification Tests and writ ten several workshops in the wa tsonx, Cloud Pak for Data and \nData Warehousing space to enable customers and the IBM Technical Community. Prior to joining IBM, Bob has held role s as a Developer, Technical Ar chitect, and Director of Data \nWarehousing and Analytics.\nMohit Sharma  is an AI engineering lead on the Client Engineering watsonx team in \nBangalore, India. Prior to this, Mohit was associated with IBM consulting, and worked on client production projects involving classical ML and deep learning. Mohit has around 14 years of experience in AI, and worked at Hewlett Packard, Wipro (where he conceptualized the Holmes AI platform) and Accenture before joining IBM in 2018. An AI practitioner having experience in design and development of AI-based",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 35,
            "total_chunks": 245
          }
        },
        {
          "id": "32d35e20-2a97-49b1-a7c2-194991d39d46",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "fore joining IBM in 2018. An AI practitioner having experience in design and development of AI-based solutions using both open-source and commercial technologies, Mohit is interested in both data and the science behind it. He has 4 published patents to his credit, and has filed his first patent at IBM. \nMark Simmonds  is a Program Director in IBM Data and AI. He writes extensively on AI, data \nscience, and data fabric, and holds multiple author recognition awards. He previously worked as an IT architect leading complex infrastructure design and corporate technical architecture projects. He is a member of the British Computer Society, holds a Bachelor’s Degree in Computer Science, is a published author, and a prolific public speaker.\nJasmeet Singh  is a watsonx Client Success Manager with 17 years of experience in IT and 8 \nyears of experience in watsonx Technologies with IBM Technology Expert Labs team with 4 years focused as watsonx.governance and AI Governance SME. \n\n Foreword xiiiHe",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 36,
            "total_chunks": 245
          }
        },
        {
          "id": "abe199cd-3793-4075-9f5e-15fb365a6902",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "xpert Labs team with 4 years focused as watsonx.governance and AI Governance SME. \n\n Foreword xiiiHe has delivered high-quality implementations of AI Governance with big named IBM clients. \nJasmeet holds 5 patents in AI field and holds an MS degree in Cybersecurity from NC A&T State University.\nMartijn Wiertz  is the Technical Sales Leader for IBM watsonx.governance in the EMEA \nregion. In this role, he combines his technica l, analytical and industry knowledge to help \nclients understand and validate the unique value that the solution can bring to help them enact responsible AI. He has more than 25 ye ars of experience in the field of advanced \nanalytics, experiencing all major developments in the evolution of the industry first hand. Prior to his current role, Martijn was the global techni cal sales lead for IBM’ s solution to combat \nfinancial crimes and he was Dir ector, Enterprise Solutions at SPSS Inc.  when that company \nwas acquired by IBM. \nThanks to the following people for t",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 37,
            "total_chunks": 245
          }
        },
        {
          "id": "e43fe898-4c1f-4cb0-bae0-f2bfc1c625e5",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "olutions at SPSS Inc.  when that company \nwas acquired by IBM. \nThanks to the following people for their contributions to this project:\n/SM590000Steve Astorino , IBM General Manager - Developm ent, Data, AI and Sustainability\nNow you can become a published author, too!\nHere’s an opportunity to  spotlight your skills , grow your career, and become a published \nauthor—all at the same time! Join an IBM Redbooks residency project and help write a book in your area of expertise, while honing your experience using leading-edge technologies. Your efforts will help to increase pr oduct acceptance and customer satisfaction, as you expand \nyour network of technical contacts and relationships. Residencies run from two to six weeks in length, and you can participate either in person or as a remote resident working from your home base.\nFind out more about the residency program, browse the residency index, and apply online at:\nibm.com/redbooks/residencies.html\nComments welcome\nYour comments are impo",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 38,
            "total_chunks": 245
          }
        },
        {
          "id": "1a85bd89-a6d0-4de0-a0ac-1284280efea3",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ndex, and apply online at:\nibm.com/redbooks/residencies.html\nComments welcome\nYour comments are important to us!\nWe want our books to be as helpful as possible . Send us your comments about this book or \nother IBM Redbooks publications in one of the following ways:\n/SM590000Use the online Contact us  review Redbooks form found at:\nibm.com/redbooks\n/SM590000Send your comments in an email to:\nredbooks@us.ibm.com\n/SM590000Mail your comments to:\nIBM Corporation, IBM Redbooks\nDept. HYTD Mail Station P0992455 South RoadPoughkeepsie, NY 12601-5400\n\nxiv Ensuring Trustworthy AI with IBM watsonx.governanceStay connected to IBM Redbooks\n/SM590000Find us on LinkedIn:\nhttps://www.linkedin.com/groups/2130806\n/SM590000Explore new Redbooks publications, residencies, and workshops with the IBM Redbooks \nweekly newsletter:\nhttps://www.redbooks.ibm.com/subscribe\n/SM590000Stay current on recent Redbooks publications with RSS Feeds:\nhttps://www.redbooks.ibm.com/rss.html\n\n© Copyright IBM Corp. 2025. 1Chapte",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 39,
            "total_chunks": 245
          }
        },
        {
          "id": "1b7874a0-a86c-43c3-bc66-55230ad38099",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "lications with RSS Feeds:\nhttps://www.redbooks.ibm.com/rss.html\n\n© Copyright IBM Corp. 2025. 1Chapter 1. Challenges and opportunities in \nAI governance\nIn 2024, the topic of governance of Artificial Intelligence (AI) has grown enormously in both \nattention and in adoption. This chapter provides a definition of AI governance and describes the key challenges and opportunities it presents.\nThis chapter has th e following sections:\n/SM590000“What is AI governance?” on page 2\n/SM590000“Governance as a key enabler for realizing AI value” on page 4\n/SM590000“Challenges with governance of enterprise AI” on page 6\n/SM590000“An example of legislation and standards related to AI” on page 121\n\n2 Ensuring Trustworthy AI with IBM watsonx.governance1.1  What is AI governance?\nThere is not one formal definition of AI go vernance that is generally accepted. Many \norganizations have created a definition that focuses on specific elements of the overall AI governance picture. The one that, in the opinion ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 40,
            "total_chunks": 245
          }
        },
        {
          "id": "3a3210a2-1011-4d36-ad62-46b6ed50a69c",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "hat focuses on specific elements of the overall AI governance picture. The one that, in the opinion of the author, captures a broad spectrum succinctly best is this one fr om the Data & AI Alliance: \n___________________________________________________________________\n“A system of rules, practices, processe s and tools that he lp an organization \nuse AI in alignment with its values and strategies, address compliance \nrequirements and drive trustworthy performance.\n1” \n___________________________________________________________________\nIn other words, it is about two things. First, it is about the rules that an organization sets for \nthemselves to make sure their use of AI is profitable, compliant, secure and fair. And second, it is about the methods that an organization applies to ensure and document that those rules are followed using tools as the enabler.\nAI governance comprises a set of activities that run in parallel to the operational process of \ncreating, deploying and maintaining",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 41,
            "total_chunks": 245
          }
        },
        {
          "id": "a5392c38-ce58-421d-a31d-1659b77b5c03",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "f activities that run in parallel to the operational process of \ncreating, deploying and maintaining AI assets. Ta ble 1-1 contrasts the typical activities related \nto different aspects of an AI solution. Many of the activities in the AI governance  column will \nbe discussed in more detail in the following chapters of this Redbooks publication.\nTable 1-1   Contrast between typical Governance and Operations activities\n1  Data & Trust Alliance, IBM - T he urgency of AI governance, 2023Note:  Some clients also refer to AI govern ance as responsible governance. While AI \ngovernance focuses on the broader framework, responsible governance specifically highlights the ethical and societal implications.\nAspect of an AI \nsolutionTypical AI governance activities Typ ical AI operat ions activities\nFoundation models Review and approve new foundation models before \nthey're applied in your use cases.Acquire and host foundation models \nso they can be used when \ndeveloping AI solutions.\nUse cases Asse",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 42,
            "total_chunks": 245
          }
        },
        {
          "id": "46146bc8-c71d-41d2-ae35-14e018332ae5",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "cquire and host foundation models \nso they can be used when \ndeveloping AI solutions.\nUse cases Assess and review new use cases before starting \ndevelopment. \nList the expected risk mitigation and compliance \nmeasures that the technical and other stakeholders \nneed to apply.\nDetermine the level and ext ent of governance based on \nthe risk profile of the use case.Create an initial solution design and \narchitecture to support assessment \nand review.\nAI asset development Document the technical characteristics and \ndevelopment process of the AI assets.\nReview the developed AI assets for adequate risk \nmitigation and regulatory compliance.\nApprove for deployment.Create and evaluate the AI assets \nthat are needed to deliver a use case.\n\nChapter 1. Challenges and opportunities in AI governance 3Like any business practice, AI governance can benefit from applying the people, process and \ntechnology (PPT) framework. When applied to AI governance, this framework helps organizations ensure that th",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 43,
            "total_chunks": 245
          }
        },
        {
          "id": "b31103ca-5782-404f-a337-f7db9eecd2c1",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "gy (PPT) framework. When applied to AI governance, this framework helps organizations ensure that their AI systems are aligned with their strategic goals, are \ndeveloped and deployed responsibly, and deliver  value to stakeholders. Here is a breakdown \nof how the PPT framework might apply to AI governance\n2:\n/SM590000People: The first element of the PPT framework focuses on the people involved in the AI \ngovernance lifecycle, this includes technical experts, legal advisors, compliance officers, AI asset deployment Document the technical characteristics and \ndevelopment process of the AI deployments.\nReview the deployed assets for adequate risk mitigation \nand compliance.Deploy the AI assets to enable their \nday-to-day use.\nAI asset \npost-deploymentManage issues and incidents according to compliance \nand risk mitigation plans.\nReview and approve requests for changes to AI assets, \nincluding any changes to the required risk mitigation \nand compliance measures.\nReview and approve the deco",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 44,
            "total_chunks": 245
          }
        },
        {
          "id": "539daa85-4c87-450f-925d-3eb8a40f46ee",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ng any changes to the required risk mitigation \nand compliance measures.\nReview and approve the decommissioning of AI assets.\nManage periodic attestations about use cases and AI \nassets.Setup automated monitoring of \ndeployed AI assets.\nManage the AI assets from a \ntechnical perspective.\nCreate and evaluate new versions \nof AI assets as needed.\nAI embedded in \nenterprise applicationsReview and approve new embedded AI capabilities \nbefore they're applied in your use cases.<not typically involved since these \ncapabilities co me to the \norganization prebuilt>\nCompliance with \nlegal obligations \nacross use casesGather evidence to demonstrate compliance with \nobligations that go across individual use cases (for \nexample: “AI literacy” in the EU AI Act)<not typically involved>\nRegulator \ninteractionsManage inbound (for example: a request for \ninformation) interactions with regulators.\nManage outbound (for example: registering high-risk AI \nuse cases) interactions with regulators.<not typical",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 45,
            "total_chunks": 245
          }
        },
        {
          "id": "e8e8f69b-6701-4307-b693-e9f479441214",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "utbound (for example: registering high-risk AI \nuse cases) interactions with regulators.<not typically involved unless \nadditional technical information is \nrequired for a specific AI solution.>\nRegulatory change Proactively assess how regulatory proposals related to \nAI might impact the organization.\nReview regulatory changes and determine with use \ncases are affected.\nManage the activities to bring use cases and AI assets \ninto compliance with the changed regulation.<not typically involved unless the \nregulatory change requires \nadditional technical work.>\nRisk control \nevaluationsAssess effectiveness of AI risk controls. <not typically involved>\nImplement AI \ngovernance policiesCreate and maintain compliance libraries, plans and \nassessment templates.\nCreate and maintain risk/control libraries, plans and \nassessment templates.<not typically involved>Aspect of an AI \nsolutionTypical AI governance activities Typ ical AI operat ions activities\n2  Based on IBM watsonx.ai chat interface,",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 46,
            "total_chunks": 245
          }
        },
        {
          "id": "00bb6e99-8460-4649-acb4-fd9464bf8a20",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " governance activities Typ ical AI operat ions activities\n2  Based on IBM watsonx.ai chat interface, using the IBM Granite 13B Chat v2 model \n\n4 Ensuring Trustworthy AI with IBM watsonx.governancebusiness leaders, ethicists, and other stakeholders. In the context of AI governance, \npeople play a crucial role in ensuring that  AI systems are designed and used ethically, \ntransparently, and resp onsibly. This involves:\n– Building diverse and inclusive teams\n– Providing adequate training and support\n– Fostering a culture of ethical AI practices\n/SM590000Process: The second element of the PPT framework emphasizes the importance of \nwell-defined and repeatable processes for the governance of AI systems. In the context of AI governance, processes help organizations manage risks associated with AI, ensure compliance with regulations, and maintain the quality and reliab ility of AI solutions. This \nincludes:\n– Defining AI principles and policies\n– Establishing clear governance structures– Defi",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 47,
            "total_chunks": 245
          }
        },
        {
          "id": "99289b93-905d-44a6-8f62-0a425645562c",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "is \nincludes:\n– Defining AI principles and policies\n– Establishing clear governance structures– Defining roles and responsibilities\n– Defining decision-making frameworks\n– Implementing robust change management processes\n/SM590000Technology: The third element of the PPT framework focuses on the technology \ninfrastructure and tools used to enact the principles, structures and processes in \nday-to-day work. In the context of AI governance, AI governance platforms provide a centralized and integrated view of AI systems, enabling organizations to govern the lifecycle of AI solutions, manage AI risks and ensure compliance. These platforms typically include features such as:\n– An inventory of use cases and AI assets\n– Automated workflows– Risk and legal assessments\n– Compliance plans\n– Issue management– Reporting and dashboarding \n1.2  Governance as a key enab ler for realizing AI value\nAI governance serves as a critical enabler for organizations to unlock the full value of AI in a \nresponsib",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 48,
            "total_chunks": 245
          }
        },
        {
          "id": "b83014d1-f34e-41f6-9fd9-14d902c1d172",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "rnance serves as a critical enabler for organizations to unlock the full value of AI in a \nresponsible and trustworthy manner. As organizations adopt AI to achieve ambitious goals, governance provides the structure needed to scale these efforts responsibly and effectively. Executives are pushing for the adoption of AI in enterprise contexts, whic h differs significantly \nfrom consumer-focused applications.\nScaling AI to meet these objectives is only possible when governance frameworks are firmly \nin place. Without consistent governance, or ganizations risk operational inefficiencies, \ncompliance failures, brand reputation, and ethical concerns that can hinder AI adoption.\nThis section addresses three common concerns that people have about AI governance as an \nenabler of AI value.\n\nChapter 1. Challenges and opportunities in AI governance 51.2.1  Concern 1: Gover nance is a brake on AI\nIn some ways this is a true statement: governance does set boundaries in the application of \nAI and the",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 49,
            "total_chunks": 245
          }
        },
        {
          "id": "ddc3437f-43f2-4de8-af0b-7e8a431536dd",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "some ways this is a true statement: governance does set boundaries in the application of \nAI and then holds people to that.\nBut compare it to a car: what would happen if your car doesn't have brakes? How would you \neven make it out of your street without the precise control to stop at the first intersection? How would you drive on a highway if you could not make an emergency stop or reliably take an exit? How would you park your car? Without brakes you would actually never be able to \ndrive fast.\nWe have developed a governance system to make  sure millions of people can get in cars, \ntrucks and buses each day and get to their destination safely. This includes traffic laws, driver's licenses, speed limits, road si gns, technical safety devices and more.\nExtending that traffic metaphor to AI, AI governance helps you:\n/SM590000Educate your employees about the rules of the road so they don't need a huge level of \noversight every time they get in their car.\n/SM590000Train your employees how",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 50,
            "total_chunks": 245
          }
        },
        {
          "id": "1b9e910b-0aaa-4225-a09a-e6afefd267b7",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " need a huge level of \noversight every time they get in their car.\n/SM590000Train your employees how to drive their AI safely (for example through an Ethics by Design \nmethod)\n/SM590000Determine when it is safe to go fast (typically large numbers of AI use cases that an \norganization agrees are low- to no-risk).\n/SM590000Identify those use cases where extra care is required, and clearly lay out what the \nobligations are.\n/SM590000Implement guardrails in the extra dangerous ha irpin turns to catch AI mishaps even if the \ndriver is momentarily distracted.\n/SM590000Maintain your brakes by adjusting your policie s and procedures as AI  continues to evolve.\nLike brakes, AI governance, when applied properly, allows an organization to go fast most of \nthe time.\n1.2.2  Concern 2: Governance does not scale\nThat is absolutely true in the sense that manual and ad-hoc  governance doesn't scale.\nThrough the people, process, and technology ap proach referred to earlier, organizations can \nmake their",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 51,
            "total_chunks": 245
          }
        },
        {
          "id": "c09fa000-da89-47c0-91e5-7f7a841b2955",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ugh the people, process, and technology ap proach referred to earlier, organizations can \nmake their AI governance more systematic and use technology to automate and scale.\nIBM takes this approach in our own AI governance, and we now create 100s of new and \ngoverned AI use cases per quarter. Governan ce can absolutely scale  to the level of a \ncomplex global company with large AI ambitions.\nSoftware can help scale AI governance through:\n/SM590000A single enterprise inventory  of AI use cases and assets that all parties can use for their \nspecific purposes. Note that this does not mean that all AI development needs to be done on a single platform.\n/SM590000Discovery  of any AI that is already in use but not yet registered (shadow AI).\n/SM590000Integration with systems of record  - depending on the type of business you are in, you will \nalready have an inventory of your IT systems, a product catalog and other systems of record. Use technology to leverage the information you already captu",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 52,
            "total_chunks": 245
          }
        },
        {
          "id": "5c9c6481-4119-4513-8c73-e4250ca48c10",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ct catalog and other systems of record. Use technology to leverage the information you already capture there so you do not have to redo that for governance purposes.\n\n6 Ensuring Trustworthy AI with IBM watsonx.governance/SM590000Automated reports and dashboards  for specific roles and users.\n/SM590000Self-service methods  to business owners to register the relevant information about their \nuse cases. Such a data gathering can be maintained by and combine the needs of specialists (legal, ethics, security, etc.).\n/SM590000Automated assessment  of the risk mitigation and compliance requirements for a use case.\n/SM590000Automated workflows  to involve specialists, facilitate re views, resolve issues and manage \nescalation points.\n/SM590000A standardized method for technical teams to  capture metadata  about the AI that they are \ndeveloping.\n/SM590000Standardized monitoring  capabilities that are mapped to  the legal obligations and \ncorporate risk.\n/SM590000Integration with  AI platforms a",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 53,
            "total_chunks": 245
          }
        },
        {
          "id": "ece36b04-12fd-47e7-b8d8-0080e0c53a88",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " are mapped to  the legal obligations and \ncorporate risk.\n/SM590000Integration with  AI platforms and processes  to avoid duplicate work for the technical \nteams.\n/SM590000Integration with any adjacent tooling  where governance requirements might be fulfilled (for \nexample: security monito ring of AI systems).\n1.2.3  Concern 3: Governance does no t contribute to value generation\nGovernance is often approached, and justified, as a way to avoid losses such as fines, \ndamage to brand reputation or negative operational impact.\nWhile this  loss avoidance  can be of substantial value to an organization, authors from the \nNotre Dame - IBM Tech Ethics Lab make the case for a holistic ROI framework that also incorporates a value generation  perspective\n3 that includes amongst others:\n/SM590000Building unique organizational capabilities to take advantage of market opportunities more \nquickly.\n/SM590000Ability to attract and reta in talent through the or ganization's reputation.\n/SM590000A since",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 54,
            "total_chunks": 245
          }
        },
        {
          "id": "2fcec6fe-a7bd-4d7e-8615-b42aaa164daa",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "590000Ability to attract and reta in talent through the or ganization's reputation.\n/SM590000A sincere commitment to values-based leadership.\n1.3  Challenges with governance of enterprise AI\nWith the rise in popularity of ChatGPT4 and other consumer AI tools, organizations have been \nlooking for ways to apply thes e capabilities also to their bus iness environment. Like other \ntechnologies, transitioning from customer to business usage comes with specific challenges. This chapter describes the main challenges as they relate to the governance of an organization's AI initiatives.\n1.3.1  Generative AI has changed the governance game\nWhile business teams are smitten by the potential for business improvement, and the \ntechnical specialists are enamored by the latest technological breakthroughs, generative AI is \ndifferent (enough) from earlier forms of AI to approach them thoughtfully.\nThese differences impact the governance of the AI in the following ways:\n3  On the ROI of AI Ethics and Go",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 55,
            "total_chunks": 245
          }
        },
        {
          "id": "a7da46fe-9127-4fcf-81e6-0d6ce3cafa68",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "differences impact the governance of the AI in the following ways:\n3  On the ROI of AI Ethics and Governance Investme nts: From Loss Aversi on to Value Generation  \n4  OpenAIs consumer chatbot  \n\nChapter 1. Challenges and opportunities in AI governance 7/SM590000Generative AI enables use cases in new areas  of an organization increasing the \ncomplexity of governance - more use cases to review, more (types of) users to educate, \nmore room for accidental errors.\n/SM590000Generative AI carries amplified and new risks  such as bias in generated content, \nintellectual property concerns, hallucination, energy consumption and misinformation. \n/SM590000Generative AI has a different supply chain  - most organizations do not train their own large \nlanguage models, but leverage pre-trained models from commercial vendors and the open-source community. These external models need to be vetted before being applied in use cases. See Chapter 4, “Onboarding a new foundation model” on page 41 for more on",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 56,
            "total_chunks": 245
          }
        },
        {
          "id": "947287e1-5879-49f2-b1cb-3f863540b965",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "eing applied in use cases. See Chapter 4, “Onboarding a new foundation model” on page 41 for more on this topic.\n/SM590000Generative AI introduces different assets to govern  - organizations now routinely work with \nprompt templates, foundation models (FMs), and other AI assets. These assets come with different metadata and different ways to measure their performance and robustness. Chapter 6, “Governing the end-to-end lifecycle of an AI asset” on page 61 for more on this topic.\n/SM590000Generative AI is evolving rapidly . ChatGPT  broke through to the mainstream in 2023, in \n2024 many organizations turned to retrieval-augmented generation (RAG) solutions and AI assistants in various forms, and 2025 looks to be the year of agentic AI . Each of these \ndevelopments also impacts how AI is governed. To help keep up, look for AI governance partners with first-hand experience in delivering these types of cutting-edge AI solutions.\nHaving said all that, generative AI is not repl acing machine",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 57,
            "total_chunks": 245
          }
        },
        {
          "id": "01a9836f-11d0-472a-975b-c98efe313a5e",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "se types of cutting-edge AI solutions.\nHaving said all that, generative AI is not repl acing machine learning (ML) - traditional machine \nlearning remains a core technology in businesse s and continues to deliver value. Consistent \ngovernance is essential for both ML and generative AI to address overlapping and unique challenges effectively. \n1.3.2  Bring together dive rse stakeholder perspectives\nAI governance is not just a matter for the technical teams. It is an organizational team sport \nthat must be informed by a wide range of perspectives to address the diverse challenges associated with AI systems.\nFigure 1-1 shows different perspectives typically in volved in the AI-related “rules” referred to \nin 1.1, “What is AI governance?” on page 2. \n\n8 Ensuring Trustworthy AI with IBM watsonx.governanceFigure 1-1   Comprehensive AI governance involves different stakeholder perspectives\nEven if an organization does not have a dedicated department for each of these, the more \nthese perspect",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 58,
            "total_chunks": 245
          }
        },
        {
          "id": "12b73f4e-e961-4841-94d6-a71474a62dc3",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " if an organization does not have a dedicated department for each of these, the more \nthese perspectives are represen ted in an AI governance framew ork, the more effective it will \nbe. Table 1-2 gives examples of which of the AI governance activities from in paragraph 1.1 are commonly informed by which perspectives. Organizations w ill need to create their own \nversion of such a mapping and define the roles and responsib ilities of all involved parties.\nTable 1-2   Mapping stakeholder perspectives to AI governance activities \nGovernance activity\nData Science / AI\nAI Ethics\nBusiness unit\nLegal / compliance\nRisk management\n(Cyber) Security\nPrivacyProcurementOther\nAssess and approve foundation models X X X X X X X\nAssess and approve use cases X X X X X X X\nGovern the development of AI assets X X X X X X X\nGovern the deployment of AI assets X X X X XGovern the post-deployme nt of AI assets X X X X X\nCompliance with legal obligations across \nuse cases (such as AI literacyXX X\nAssess and ap",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 59,
            "total_chunks": 245
          }
        },
        {
          "id": "8cc962f1-2c01-4db3-ac77-3343230b22a5",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "X X X X X\nCompliance with legal obligations across \nuse cases (such as AI literacyXX X\nAssess and approve AI embedded in \nenterprise applicationsXXXXXXX\nRegulator interactions X\nRegulatory change XRisk control assessment X X X\n\n\nChapter 1. Challenges and opportunities in AI governance 9Given this multi-dimensional nature of AI governance, organizations should be aware of \ncommon gaps:\n/SM590000Communication gaps: AI engineers are not legal experts and vice versa, and the same \ngoes for the other stakeholders. To overcome this gap, consider:\n– Education programs to create a common base of understanding, without everyone \nhaving to know the full extent of the other's roles.\n– Automated workflows to integrate everyone's contributions into a unified framework.\n/SM590000Technology gaps: Indivi dual teams will be tempted to create  or deploy tooling for their \nspecific piece of the AI governance puzzle. While their efforts are of course with the best of intentions, it does create an increasi",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 60,
            "total_chunks": 245
          }
        },
        {
          "id": "a85ac4b0-6a8b-4942-bd3a-cc1c01576703",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ce puzzle. While their efforts are of course with the best of intentions, it does create an increasing in tegration problem as the company matures to \na more interconnected approach.\nTo overcome this gap, look for tooling that provides integrated support all of the activities and \nroles listed in the table above. Limit the use of bespoke integration to only those situations \nwhere generally available AI governance tooli ng does not have the integrated capabilities.\nIn summary, the challenge lies no t in providing isolated capab ilities to each group but in \nensuring that all these tools and workflows are interconnected effectively. For example:\n/SM590000Do the technical teams have a clear view of the compliance re quirements they will \nspecifically need to fulfill as they develop an AI solution?\n/SM590000Do risk teams have a clear view of the expos ure created by using the same AI models \nacross AI solutions that are created in-house and those that are purchased as part of an applicati",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 61,
            "total_chunks": 245
          }
        },
        {
          "id": "5e3aed42-f764-4791-95d6-739a01f42388",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " \nacross AI solutions that are created in-house and those that are purchased as part of an application?\n/SM590000Does the governance solution actively support technical teams in gathering technical \ndocumentation details in an easy way?\n/SM590000When the technical teams update an AI solu tion, will the legal teams see those changes \nand be able to re-assess them for any new compliance obligations?\nA holistic approach enables s eamless governance, fosterin g trust, accountability, and \nalignment across the organization.\n1.3.3  Technical comp lexity is increasing\nAs AI evolved from machine/deep learning to  generative AI, the technical complexity has \nincreased in the following ways:\nMore complex relationship bet ween use cases and AI assets\nIn machine learning and deep learning projects , typically there was a very tight one-to-one \nrelationship between the use case and the AI assets (models). Models were trained to perform a specific task, and if you had a different task, you would tra",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 62,
            "total_chunks": 245
          }
        },
        {
          "id": "7e9314f0-6e58-45f4-8d79-9b5a2452fdbc",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "els). Models were trained to perform a specific task, and if you had a different task, you would train another model. The use case comes first, and then the model(s) follow.\nWith generative AI and foundation models, this is typically no longer the case as these \nmodels are now pre-trained to handle a large variety of use cases. Not only does this mean a one-to-many relationship, but the relationship is also reversed: the model is already there before any specific use cases are considered. One result of this is that the use case has become more important as a governed item in its own right, separate from the (foundation) models applied to deliver a use case. Chapter 5 describes the governance considerations around use cases\n\n10 Ensuring Trustworthy AI with IBM watsonx.governanceMore AI assets\nIn machine learning and deep learning projects, typically the asset created was a trained \nmodel. Depending on the use case and the dat a, these can take many forms but they're \ngenerally all refer",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 63,
            "total_chunks": 245
          }
        },
        {
          "id": "7c24a475-091f-4215-8586-6da1c121c050",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " Depending on the use case and the dat a, these can take many forms but they're \ngenerally all referred to as models .\nWith generative AI and foundation models, there are other types of assets used to make a \nspecific use case come to life. The primary way to interact with a foundation models is \nthrough a prompt, which is typically text-based instructions on what you want the model to do. For many enterprise use cases, these are not one-off but repeated interactions that take the form of a parametrized prompt template. For example, for a retrieval-augmented generation use case, an LLM is prompted each time a user query comes in. The prompt always has the same structure and core instru ctions, with parameters for t he user's query and the context \ndata that should be used to answer the query. See Figure 1-2 on page 10.\nFigure 1-2   Example of a parametriz ed prompt template for a RAG use case\nSo instead of training models , a lot of projects now involve  creating prompts .\nAdditionally",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 64,
            "total_chunks": 245
          }
        },
        {
          "id": "718752ce-6c34-4d78-a42b-644a17527d49",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " case\nSo instead of training models , a lot of projects now involve  creating prompts .\nAdditionally, organizations might decide to fine-tune a pre-trained model to make it fit their \nspecific needs better. There are different methods to do this that result in what is effectively a new model. It is derived from the base model but is a new object in its own right that needs the appropriate amount of oversight.\nLastly, other AI techniques such as prompt chaining (where different prompts are executed in \na defined sequence) might result in other assets to be governed.\nChapter 6, “Governing the end-to-end lifecycle of an AI asset” on page 61 describes the \ngovernance considerations for different AI assets\nMore sources of AI\nIn machine learning and deep learning projects , organizations usually train their own models, \npotentially with assistance from external service providers, using either their data or trusted \nthird-party data.\nWith the advent of Generative AI and founda tion models, th",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 65,
            "total_chunks": 245
          }
        },
        {
          "id": "e163a22b-99e7-45b1-abc3-184edf3314b5",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "their data or trusted \nthird-party data.\nWith the advent of Generative AI and founda tion models, these models are typically sourced \nfrom third party as pre-trained models, such as IBM's Granite  family of models. However, \norganizations often lack adequate insight into the data used to train these external foundation models. Therefore, it is crucial for organizations to conduct a thorough assessment and review of the foundation model before adopting it for their own use. Chapter 4, “Onboarding a new foundation model” on page 41 describes the governance considerations when bringing in these externally developed models.\n\n\nChapter 1. Challenges and opportunities in AI governance 11These multi-purpose foundation models also spur on your technology providers to embed \nmore and more AI in any enterprise applications that an organization might use. Chapter 4, “Onboarding a new foundation model” on page 41 describes the governance considerations for AI embedded in business applicat ions. Ano",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 66,
            "total_chunks": 245
          }
        },
        {
          "id": "df731aae-e4d5-4396-b433-31875641649a",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "l” on page 41 describes the governance considerations for AI embedded in business applicat ions. Another trend to watch is  the prebuilt AI capabilities \nin devices such as la ptops and smartphones.\n1.3.4  Regulatory and risk complexity is increasing \nIn this section we review regulatory compliance management and operational risk \nmanagement.\nRegulatory compliance management\nThe AI regulatory space is very dynamic: There are many AI-specific legislations in force or in \nprogress across the world, there are general regulations that also apply to AI and there are several options for volu ntary commitment schemes. \nEspecially for organizations that operate across multiple regions, the regulatory challenge is \nbecoming more complex quickly and one cannot expect everybody in an organization to be a regulatory expert for A I. As a result, the role of complianc e will become more prominent in the \n“team sport” described in 1.3.2, “Bring togeth er diverse stakeholder perspectives” on page 7.\nL",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 67,
            "total_chunks": 245
          }
        },
        {
          "id": "7b9797bf-1cf9-4822-92ac-cb7ad01c2e58",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "he \n“team sport” described in 1.3.2, “Bring togeth er diverse stakeholder perspectives” on page 7.\nLuckily, regulatory compliance management is a well-establis hed discipline, which can now \nalso be applied to AI. This includes actions such as:\n/SM590000Defining compliance requirements and obligations\n/SM590000Implementing legal assessment tools\n/SM590000Connecting AI use cases with mandates\n/SM590000Defining compliance plans\n/SM590000Managing legal obligations across use cases (for example, AI literacy)\n/SM590000Managing regulator interactions\n/SM590000Managing regulatory change\n/SM590000Regulatory reporting\n/SM590000Documenting and reporting on compliance status\nAs AI is set to impact more and more busin ess processes, AI governance becomes more \nthan just a legal check box. Organizations will need to de fine/update their risk frameworks \nand enterprise policies; especially as generative AI and agentic AI bring amplified and new risks to an organization.\nOperational risk management\nO",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 68,
            "total_chunks": 245
          }
        },
        {
          "id": "d87e6663-9100-4dfd-b23b-b1628dac8e60",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ve AI and agentic AI bring amplified and new risks to an organization.\nOperational risk management\nOperational risk management is also a well-established discipline, which can now also be applied to AI. This includes actions such as:\n/SM590000Defining AI risks and mitigation strategies\n/SM590000Implementing libraries of risks and controls\n/SM590000Implementing AI risk identification tools\n/SM590000Connecting AI use cases with business processes\n/SM590000Defining test plans\n/SM590000Managing loss events, loss impacts and loss recoveries\n/SM590000Documenting and reporting on risk assessments and mitigation strategies\n\n12 Ensuring Trustworthy AI with IBM watsonx.governance1.4  An example of legislati on and standards related to AI\nLike AI in general, legislation and standards around AI are evolving quickly. In this chapter we \ncover some headlines and refer the reader to th e respective sources of these legislations and \nstandards for their current state and planned further enhancements.\n",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 69,
            "total_chunks": 245
          }
        },
        {
          "id": "e2370c6a-eed2-41fe-b68f-6c2de27e4ce0",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "rces of these legislations and \nstandards for their current state and planned further enhancements.\n1.4.1  AI-specific legislation \nAcross the world, AI-specific legislation is being developed or has been enacted. This \nparagraph describes the key characteristics of one example.\nRegulation (EU) 2024/ 1689 - The EU AI Act\nThe European AI Act  went into force on August 1st, 2024, and is applicable to any \norganization placing AI on the market in the European Union (EU), regardless of their home base. That means that organizations headquartered elsewhere in the world do have to comply with the Act for AI systems that impact EU citizens, as consumers, employees or other \nroles.\nThe AI Act regulates two things:\n1. “General purpose AI models” (the legal terminology for what we call foundation models in \nthis publication).\n– A special category is models with “ systemic risk ” - the really large models that can be \nused so widely that their potential impact is very extensive.\n2. “AI systems”\n–",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 70,
            "total_chunks": 245
          }
        },
        {
          "id": "8e853348-01d4-4b25-86fd-0ed6ce710f8c",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " models that can be \nused so widely that their potential impact is very extensive.\n2. “AI systems”\n– Some uses of AI are classified as prohibited , they are not allowed to be placed on the \nmarket in the EU. Examples are AI systems that use subliminal techniques, enact forms of social scoring or employ untargeted scraping of facial images from the internet or CCTV footage.\n– Some uses of AI are classified as high risk , they are allowed but come with a set of \nrequirements to ensure they don't violate the rights of EU citizens. Some of these \ndefined uses are industr y-specific (for exampl e in utilities, financial services and public \nsector) and some are horizontal (for example certain use cases in HR or education).\n– Some uses of AI are classified as having transparency  risk, they are also allowed and \ncome with obligations to disclose that a person  is interacting with an  AI system, or that \ncontent is created by an AI system.\nAn organization’s legal obligatio ns depend on the cl",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 71,
            "total_chunks": 245
          }
        },
        {
          "id": "e294beca-e000-4266-b45e-95ab659b1eff",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": ", or that \ncontent is created by an AI system.\nAn organization’s legal obligatio ns depend on the classification  of the system or the model, \nand the role that an organization plays with respect to that system or model (such as provider, deployer, others).\nThe Act defines penalties for:\n/SM590000Bringing prohibited AI systems onto the market.\n/SM590000Not meeting the obligations for high-risk use cases and/or general purpose AI models.\n/SM590000Providing incorrect, incomplete or misleading information to notified bodies or national \ncompetent authorities in reply to a request. \nFor large enterprises, the fines can be as high as  35 million euro or 7% of its total worldwide \nannual turnover for the preceding fi nancial year, whichever is higher.\nWhile the Act is in force sinc e August 2024, subsets of obligat ions will apply fr om different \ndates:\n\nChapter 1. Challenges and opportunities in AI governance 13/SM590000February 2025: prohibited AI systems, AI literacy requirements\n/SM5900",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 72,
            "total_chunks": 245
          }
        },
        {
          "id": "d5a03376-b05f-45fd-bfb4-735054aaafc7",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "s in AI governance 13/SM590000February 2025: prohibited AI systems, AI literacy requirements\n/SM590000August 2025: obligations for providers of general purpose AI models\n/SM590000August 2026: obligations for high-risk AI systems\n/SM590000August 2027: obligations for high-risk AI systems “ intended to be used as a safety \ncomponent of a product, or the AI system is  itself a product, covered by the Union \nharmonisation legislation5” (for example toys or medical devices)\nOther AI-specific legislation includes, for example:\n/SM590000Act on the Development of Artificial Intelligen ce and Establishment of Trust (AI Basic Act) \n(South Korea)  \n/SM590000Local Law 144 regarding automated employment decision tools (US, New York City)  \n/SM590000Executive Order on the Safe, Secure, and Trustworthy Development and Use of AI (US)  \n/SM590000Artificial Intelligence and Data Act (Canada)  \n/SM590000AI regulation: a pro-innovation approach (UK)  \n1.4.2  General regulations that apply to AI\nBesides th",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 73,
            "total_chunks": 245
          }
        },
        {
          "id": "3e98520a-27e0-499a-803e-e9f3f36b5dfd",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " regulation: a pro-innovation approach (UK)  \n1.4.2  General regulations that apply to AI\nBesides the evolving AI-specific regulations there are also many general regulations that \napply to AI use cases and systems in areas such as:\n/SM590000Non-discrimination\n/SM590000Privacy protection \n/SM590000Data protection\n/SM590000Product liability\n/SM590000Fair advertising\n/SM590000Industry-specific regula tions, such as rules around financial advice\n1.4.3  Technical standa rds for AI governance\nIn addition to regulations, there are various initiatives underway to define technical standards \nfor AI systems. The standards define the state-of-the-art tools and methods that can be \napplied when creating and using AI systems.\nFrom a governance perspective, these stan dards will provide a bas eline of “generally \naccepted” practices that organizations are encouraged to adopt.\nTechnical standards suppor ting the EU AI Act \nThe European Commission has requested the development of a set of technical s",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 74,
            "total_chunks": 245
          }
        },
        {
          "id": "a1711155-4ab7-48e4-92d8-c0d7a193a915",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "or ting the EU AI Act \nThe European Commission has requested the development of a set of technical standards \ncovering the following requirements:\n/SM590000Risk management system for AI systems.\n/SM590000Governance and quality of datasets used to build AI systems.\n/SM590000Record keeping thro ugh logging capabilities by AI systems.\n/SM590000Transparency and information provisions for users of AI systems.\n/SM590000Human oversight of AI systems.\n/SM590000Accuracy specifications for AI systems.\n/SM590000Robustness specificatio ns for AI systems.\n5  European Union, Article 6: Classification Ru les for High-Risk AI Systems  (Section 1, Paragraph (a))\n\n14 Ensuring Trustworthy AI with IBM watsonx.governance/SM590000Cybersecurity specifications for AI systems.\n/SM590000Quality management systems for providers of AI systems, including post-market \nmonitoring processes.\n/SM590000Conformity assessment for AI systems.\nTwo European standardization bodies (CEN/CENELEC) have accepted that request and",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 75,
            "total_chunks": 245
          }
        },
        {
          "id": "8430c5d8-3f0a-4dde-9276-277d7ed126a4",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ent for AI systems.\nTwo European standardization bodies (CEN/CENELEC) have accepted that request and \nhave formed a joint technical committee (“JTC21”) for the development and adoption of standards for AI and related data. \nThese technical stand ards will become important co mpliance tools since they “ will grant  a \nlegal presumption of conformity to AI systems develo ped in accordan ce with them.” \nIn other words: if you build your AI systems to the specifications in these technical standards, the EU will assume your system is  in conformity with the AI Act.\nThe following list of organizations provides useful information related to AI and AI governance \nstandards.\n/SM590000NIST RFM \nNational Institute of Standards and Technology - AI Risk Management Framework\n/SM590000ISO\nInternational Organizati on for Standard ization - Artificial Intelligence\n/SM590000OWASP Top 10 for Large La nguage Model Applications\nOpen Web Application Se curity Project (OWASP)\nNote:  Given the evolving nature",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 76,
            "total_chunks": 245
          }
        },
        {
          "id": "0789c580-4281-4cbb-9ba9-a773d46ac200",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "e Model Applications\nOpen Web Application Se curity Project (OWASP)\nNote:  Given the evolving nature of AI governance standards, a flexible approach is crucial. \nRegularly review and adopt new or updated standards as they emerge to ensure your AI systems align with the latest best practices.\n\n© Copyright IBM Corp. 2025. 15Chapter 2. Introduction to \nIBM watsonx.governance\nThis chapter provides an overview of the watsonx platform, its core components, and \nfeatures.\nThis chapter includes the following topics:\n/SM590000“Introduction to the IBM watsonx platform and its core components” on page 16\n/SM590000“Introduction to IBM watsonx.ai” on page 17\n/SM590000“Introduction to IBM watsonx.data” on page 19\n/SM590000“Introduction to IBM watsonx.governance” on page 202\n\n16 Ensuring Trustworthy AI with IBM watsonx.governance2.1  Introduction to the IBM watsonx platform and its core \ncomponents\nOrganizations today face a common challenge: ac cessing data that is trapped in silos across \nvarious s",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 77,
            "total_chunks": 245
          }
        },
        {
          "id": "f4fd8b71-edf7-4b82-93eb-7f6f887283e7",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "anizations today face a common challenge: ac cessing data that is trapped in silos across \nvarious systems. Business applications typically store their data in application-specific sources, which often leads to duplication across multiple systems to meet  different application \nneeds. This fragmentation results in disconnected datasets that are not easily accessible to the broader enterprise ecosyst em while still maintaining appr opriate access rights. As a \nresult, this limitation hinders opportunities for gaining insights and fostering innovation.\nIBM AI and data platforms enable organizations to access and integrate isolated datasets for \nanalytics purposes, en suring that the data is of high quality and trus tworthy. This capability \nallows companies to develop responsible and ethical solutions, fostering the creation of new products, offerings, and opportunities for business growth.\nThe IBM watsonx platform, along with IBM Cloud Pak for Data, is built on the robust \nfoundation of",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 78,
            "total_chunks": 245
          }
        },
        {
          "id": "36a26d73-4970-4e13-a14d-8c79a3efa965",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": ".\nThe IBM watsonx platform, along with IBM Cloud Pak for Data, is built on the robust \nfoundation of Red Hat OpenShift. This next-generation AI and data platform enables enterprises to develop and deploy AI applications, access data stored in legacy systems, catalog data while implementing governance pr actices, and improve their understanding of \ndata quality. It also allows organizations to establish policies and rules related to data privacy, ensuring that end-to-end AI solutions are ethical, responsible, and trustworthy.\nAccessing high-quality datasets can be challenging in organizations when the data assets are \nlocked in silos. The key for organizations to gain insights and bring innovation to their business is leveraging these data assets but requires a platform that enables access to data assets residing in silos, integrate the data assets, and build and deploy AI assets in an ethical and responsible way. The IBM watsonx platform consists of three core components\nFigure 2-1 on ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 79,
            "total_chunks": 245
          }
        },
        {
          "id": "25fdb224-6f3b-4006-9e8a-cdd0695a2c4b",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "hical and responsible way. The IBM watsonx platform consists of three core components\nFigure 2-1 on page 17 illustrates the components of the watsonx platform.\n\nChapter 2. Introduction to IBM watsonx.governance 17Figure 2-1   IBM watsonx platform and its core components -.ai, .data, .governance\nThis platform has three components:\n/SM590000IBM watsonx.ai: An enterprise-grade AI studio that helps operationalize and scale the \ndevelopment of AI applications by bringing together traditional machine learning and generative AI capabilitie s with high-quality data across the AI lifecycle. With watsonx.ai, AI \ndevelopers can build, train, adapt, and tune models with your enterprise data and operationalize the models to generate insights, support tasks, and automate business workflows.\n/SM590000IBM watsonx.data: A fit-for-purpose data lakehouse service, that makes it possible for \nenterprises to scale AI workloads using all th eir data optimized for governed data and AI \nworkloads. It serves as",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 80,
            "total_chunks": 245
          }
        },
        {
          "id": "cef754ea-6833-4221-8c43-d90d8d0a426d",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "scale AI workloads using all th eir data optimized for governed data and AI \nworkloads. It serves as a data source for AI, enabling the enterprise ecosystem to access trusted and quality data while enforcing policies and rules for data privacy and security. watsonx.data supports querying, governance, and open data formats to access and share data for different AI use cases such as Retrieval Augmented Generation (RAG). It is based on open-source technologies, including Presto, Iceberg, and Milvus.\n/SM590000IBM watsonx.governance: An end-to-end solution for AI governance to enable \nresponsible, transparent, and explainable AI  workflows in addition to monitoring and \nevaluation capabilities that allo w you to keep track of yo ur entire AI landscape. \nIBM watsonx.governance helps business analysts understand the trustworthiness of their AI solutions.\n2.2  Introduction to IBM watsonx.ai\nIBM watsonx.ai is a cutting-edge AI platform that empowers organizations to scale and accelerate their A",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 81,
            "total_chunks": 245
          }
        },
        {
          "id": "d88a3fc5-3fd9-41a6-9736-8f75334abcf1",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "watsonx.ai is a cutting-edge AI platform that empowers organizations to scale and accelerate their AI initiatives. \n\n\n18 Ensuring Trustworthy AI with IBM watsonx.governanceOffering a comprehensive suite of tools including a foundation model library, enterprise grade \nstudio, machine learning frameworks, and a runtime serving environment. watsonx.ai enables enterprises to build, train, and deploy models with ease. As a core component of the larger IBM watsonx platform, it drives AI-driven transformation across multiple industries.\nKey components of watsonx.ai\nThe following list describes the key components of watsonx.ai:\n/SM590000Foundation models: IBM watsonx.ai provides access to large-scale, pre-trained foundation \nmodels designed for various AI tasks. These models, such as generative AI and large language models (LLMs), can understand and generate human-like text, making them suitable for applications like customer service automation, content generation, data analysis and more.\n/SM5",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 82,
            "total_chunks": 245
          }
        },
        {
          "id": "2dc8a80b-8fbc-44bf-8e70-2b7fcf80131e",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " for applications like customer service automation, content generation, data analysis and more.\n/SM590000Generative AI: Generative AI in watsonx.ai enables businesses to create new content, \ngenerate insights, automate workflows and et c. With the capability to  train custom models, \nwatsonx.ai empowers organizations to build personalized AI-driven solutions.\n/SM590000Machine Learning and ModelOps: watsonx.ai supports the entire AI model development \nlifecycle, from data preparation and training to deployment and monitoring. Through ModelOps, watsonx.ai ensures that models are optimized, accountable, and compliant with industry standards.\n/SM590000Data Science and Analytics: With advanced data science tools, watsonx.ai allows \norganizations to analyze and us e data for training AI models. These ca pabilities provide a \nstrong foundation for data-driven decision-making.\nIBM watsonx.ai in enterprise workflows\nIBM watsonx.ai integrates seamlessly with existing enterprise workflows. By off",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 83,
            "total_chunks": 245
          }
        },
        {
          "id": "f28b1dae-24da-4d8d-8327-48ab025aafd2",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "enterprise workflows\nIBM watsonx.ai integrates seamlessly with existing enterprise workflows. By offering pre-built \nconnectors and APIs, the platform allows businesses to embed AI models into their operational workflows quickly, tailoring solutions to meet specific requirements.\nIBM watsonx.ai as a develope r and data scientist toolkit\nIBM watsonx.ai provides a flexible toolkit for developers, AI engineers, and data scientists, \nsupporting coding and no/low-coding environments. Developers can leverage SDKs, APIs, and workflows in their preferred programming languages, while non-coders benefit from \nintuitive natural language processing tools. wa tsonx.ai also supports IDEs like RStudio and \nPython Notebooks, accelerating AI model production.\nDomains where watsonx.ai is useful\nIBM watsonx.ai has been widely adopted across industries, supporting use cases such as natural language processing, image recognition, fraud detection, and predictive analytics. Key industry applications include:",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 84,
            "total_chunks": 245
          }
        },
        {
          "id": "5934ebb7-fb55-41cc-838a-0884e954b818",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ng, image recognition, fraud detection, and predictive analytics. Key industry applications include:\n/SM590000Banking and Financial Services: Automating processes such as credit scoring, fraud \ndetection, and risk assessment, whic h improves accuracy and efficiency.\n/SM590000Healthcare: Enhancing diagnostics, person alizing treatment plans, and optimizing \nworkflows using large AI models.\n/SM590000Retail: Improving customer experience through personalized recommendations (next best \noffer), basket analysis, inventory management, and automated support systems.\n/SM590000Manufacturing: Streamlining production processes, predictive maintenance, and quality \ncontrol to improve efficiency and reduce downtime.\n/SM590000Transport: Enhancing route optimization, fleet management, and safety systems for \nimproved logistics and customer service.\n\nChapter 2. Introduction to IBM watsonx.governance 19/SM590000Leisure and Luxury: Personalizing customer experiences, optimizing inventory, and \nenhancing",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 85,
            "total_chunks": 245
          }
        },
        {
          "id": "306e85d7-66a3-4fea-b24f-27335baa6942",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "SM590000Leisure and Luxury: Personalizing customer experiences, optimizing inventory, and \nenhancing marketing strategies to meet customer preferences.\nAdvantages of watsonx.ai\nThe advantages of watsonx.ai include the following items:\n/SM590000Dependability: All models offere d within the watsonx.ai pla tform are thoroughly tested, \nresulting in highly robust models  that deliver consistent results.\n/SM590000Transparent Accountability: All Granite models (IBM’s propri etary family of models) are \ntrained exclusively on open -source data, ensuring no hi dden liabilities or legal \ncomplexities when used  in AI solutions.\n/SM590000Scalability: Supports large AI  models and integrates seam lessly with data platforms to \nscale AI operations.\n/SM590000Flexibility: Offers customization options, allo wing AI models to be tailored to specific \nbusiness needs. Bring your own foundation model and upload to watsonx.ai to accomplish a range of industry and domain-specific generative AI use cases.\n/",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 86,
            "total_chunks": 245
          }
        },
        {
          "id": "24d46787-99d1-456b-a577-bdba30c7e9d3",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "pload to watsonx.ai to accomplish a range of industry and domain-specific generative AI use cases.\n/SM590000Enhanced Governance: Ensures responsible development and deployment of AI \nsolutions.\n/SM590000Faster ROI realization: Accelerates AI solution development and deployment, enabling \nbusinesses to quickly realize benefits.\nConclusion\nIBM watsonx.ai represents a significant advancement in AI, combining the creativity of generative AI with the precision of traditional machine learning. Its comprehensive suite of tools and strong focus on governance enable businesses to accelerate AI initiatives responsibly and at scale. As AI continues to evolve, watsonx.ai stands at the forefront, helping organizations unlock new possib ilities and drive impactful inno vations across many sectors.\n2.3  Introduction to IBM watsonx.data\nIBM watsonx.data provides a modern, open data  lakehouse architecture that integrates \nseamlessly across on-premises and multi-cloud environments. Its design enables o",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 87,
            "total_chunks": 245
          }
        },
        {
          "id": "18d4d20e-2a15-419b-8aac-fd82b6360f7d",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "re that integrates \nseamlessly across on-premises and multi-cloud environments. Its design enables organizations to manage all types of data workloads, from traditional analytics to AI training.\nCore features of watsonx.data\nThe following list highlights the core features of watsonx.data:\n/SM590000High-performance Data Querying and Analytics: Provides rapid querying capabilities for \nlarge datasets, empowering businesse s to extract insights efficiently.\n/SM590000Built-in Governance and Security: Ensures compliance with data privacy and security \nregulations across multi-cloud environments.\n/SM590000Cost Optimization: Reduces data warehousing costs by up to 50%, offering a more \neconomical solution for data storage and processing.\n/SM590000Shared Metadata Layer:  Facilitates seamless data acce ss and operations, streamlining \nworkflows and improving data consistency.Note:  For more information on watsonx.ai refer to IBM Redbooks Simplify Your AI Journey: \nUnleashing the Power of AI wit",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 88,
            "total_chunks": 245
          }
        },
        {
          "id": "1554a143-ed6f-421b-b5da-876079332e89",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "mation on watsonx.ai refer to IBM Redbooks Simplify Your AI Journey: \nUnleashing the Power of AI with IBM watsonx.ai , SG24-8574.\n\n20 Ensuring Trustworthy AI with IBM watsonx.governanceKey features\nIBM watsonx.data key features include the following items:\n/SM590000Scalable Data Lakehouse: wats onx.data blends the flexibilit y of data lakes with the high \nperformance of data warehouses. It is de signed for hybrid cloud environments and \nsupports open data formats like Parquet and ORC.\n/SM590000SQL-based Querying: Enables users to exec ute high-speed analytics with SQL, allowing \ndata engineers and analysts to hand le extensive datasets effectively.\n/SM590000Machine Learning Integration: Seamlessly integrates with machine learning workflows, \nenabling the development and deployment of AI models using real-time data streams. This integration enhances decision-making and automates processes within organizations.\n2.4  Introduction to IBM watsonx.governance\nIBM watsonx.governance is a dedic",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 89,
            "total_chunks": 245
          }
        },
        {
          "id": "bbf3c5ed-86b5-4b42-a7e2-07d72132453e",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " within organizations.\n2.4  Introduction to IBM watsonx.governance\nIBM watsonx.governance is a dedicated application for governance of AI to help \norganizations ensure that their use of AI is profitable, compliant, secure and fair.\n2.4.1  Key capabilities\nThe key capabilities of watsonx.gov ernance include th e following items:\n/SM590000Maintaining an inventory of all AI inside an organization, including self-built solutions, AI \nembedded in your enterprise application and AI embedded in your products and services.\n/SM590000Automated AI discovery - through integration with IBM Guardium AI Security, you can \nidentify AI deployments that are not yet registered and apply the appropriate level of governance.\n/SM590000Role-based dashboards.\n/SM590000Identify legal obligations for your use cases.\n/SM590000Identify AI risks for your use cases, leveraging the IBM AI Risk Atlas  created by \nIBM Research and the IBM AI Ethics Board .\n/SM590000Automate your use case review processes.\n/SM590000Aut",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 90,
            "total_chunks": 245
          }
        },
        {
          "id": "34e7a825-e4ed-4a65-a739-9ac66c60e31d",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "esearch and the IBM AI Ethics Board .\n/SM590000Automate your use case review processes.\n/SM590000Automate your third-party foundation model onboarding processes.\n/SM590000Automate your lifecycle governance processes such as model reviews and model change \nrequests.\n/SM590000Automate your regulatory compliance management processes such as managing \nregulatory chance and managing regulator interactions.\n/SM590000Manage legal obligations across use cases (for example, AI literacy).\n/SM590000Automate your operational risk management processes such as managing loss events, \nloss impacts and loss recoveries.\n/SM590000AI documentation through capture of metadata about the (versions of) AI assets you build.\n/SM590000Quantitative evaluations of your AI assets - dozens of pre-built metrics to measure model \nhealth, drift, quality, toxic language, PII, fairness, adversarial robustness of both machine learning and generative AI use cases.\n/SM590000Global and local explainability of machine learnin",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 91,
            "total_chunks": 245
          }
        },
        {
          "id": "45103a60-3719-4b8f-8cb8-82b22b36f711",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ne learning and generative AI use cases.\n/SM590000Global and local explainability of machine learning models.Note:  For more information on watsonx.data refer to IBM Redbooks Simplify Your AI \nJourney: Hybrid, Open Data Lakehouse with IBM watsonx.data,  SG24-8570.\n\nChapter 2. Introduction to IBM watsonx.governance 21/SM590000Standard and ad-hoc reporting.\n/SM590000Integration with AI platforms such as IBM watsonx.ai, Amazon SageMaker and Bedrock, \nGoogle Vertex AI, and Microsoft Azure.\n/SM590000Configure the solution to fit your specific requirements.\n2.4.2  Use cases\nIBM watsonx.governance is a comprehensive governance solution. Depending on a \ncustomer's situation, they could use the solution for one or more of the following use cases: \n/SM590000Comply with the EU AI Ac t or other legislation.\n/SM590000Mitigate reputational and operational risks from their use of AI.\n/SM590000Govern the onboarding of AI-enabled enterprise applications.\n/SM590000Govern the development of AI-enabled pr",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 92,
            "total_chunks": 245
          }
        },
        {
          "id": "c7c71649-3ead-4761-8d22-0e30c3e786bc",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "e onboarding of AI-enabled enterprise applications.\n/SM590000Govern the development of AI-enabled products and services.\n/SM590000Integrate AI risks into the op erational risk management of AI -enabled business processes.\n/SM590000Set up regular monitoring of an already deployed AI solution.\n2.4.3  Benefits of watsonx.governance\nBy centralizing and automating governance of AI, customers achieve benefits such as:\n/SM590000Bring together all technical and non-technical stakeholders into a common governance \nframework.\n/SM590000Decrease the risk of being fined or damaging your brand reputation.\n/SM590000Reduce the cost of compliance.\n/SM590000Free up time to deploy more models.\n/SM590000Capture model benefits earlier by reducing the time to deployment.\n/SM590000Recapture model benefits lost due to model drift.\nAs mentioned in 1.2.3, “Concern 3: Governance does not contribute to value generation” on \npage 6, using AI governance software also contributes to setting up an organization for th",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 93,
            "total_chunks": 245
          }
        },
        {
          "id": "1868b0c9-639e-4fac-86d0-339765b6ba53",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "tion” on \npage 6, using AI governance software also contributes to setting up an organization for the value generation benefits mentioned in that paragraph.\n2.4.4  Synergy between watsonx .data and watsonx.governance \nIBM watsonx.data and IBM watsonx.governance provide a holistic solution for organizations. \nWhile watsonx.data focuses on data analysis and AI workloads, watsonx.governance ensures that all data is managed responsibly, securely , and in compliance with  relevant regulations.\nBy combining the strengths of both components, organizations can develop a unified data \nstrategy that balances innovation with accountability.\n2.4.5  Synergy between wats onx.ai and watsonx.governance\nThe synergy between IBM watsonx.ai and IBM watsonx.governance makes it easy for AI \ndevelopers to contribute to AI governance processes. The integration between the two components automates even more of the work, for example:\n/SM590000Model metadata is automatically captured.\n/SM590000Model evaluations ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 94,
            "total_chunks": 245
          }
        },
        {
          "id": "c2c62fc1-56ca-43cb-b511-c76225ee4108",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "he work, for example:\n/SM590000Model metadata is automatically captured.\n/SM590000Model evaluations can be easily set up from their development workspaces (UI or \nprogrammatic).\n\n22 Ensuring Trustworthy AI with IBM watsonx.governance/SM590000Inferencing payload data can be auto-logged for fully automated runtime model \nmonitoring.\nThis comprehensive approach allows organizations to balance rapid innovation with the \nresponsibility of managing AI ri sks effectively. Th e combined capabilit ies help businesses \nstreamline their workflows and maintain an audit trail, enabling a seamless interplay between AI development and AI governance.\n2.5  Reference architecture\nFigure 2-2 illustrates a reference architecture for an end-to-end AI and da ta solution that \nleverages the IBM watsonx platform and IBM Cloud Pak for Data. This architecture is designed to build, test, deploy, manage, govern, and consume AI solutions across the enterprise.\nFigure 2-2   Reference architecture for integrated AI ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 95,
            "total_chunks": 245
          }
        },
        {
          "id": "21e5e53c-ce1f-471e-a6d8-20c1e9f16c35",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "d consume AI solutions across the enterprise.\nFigure 2-2   Reference architecture for integrated AI  and data platform for an enterprise using IBM watsonx\n2.5.1  Data Onboarding\nThe left side of Figure 2-2 illu strates the existing and legacy data sources that drive the \norganization's day-to-day operations. These data sources are often designed and developed for specific purposes and are accessible only by a select group of users or applications. \nTypically, these existing and legacy data so urces are built in silos, lacking a clear \nunderstanding of the relationships between the data assets they contain. To resolve this, \norganizations can use watsonx.data to store structured, semi-structured, and unstructured data and make it directly accessible  for AI and business intelligence (BI).\nTo integrate these disjointed data sources, we can onboard them onto IBM's open \narchitecture lakehouse, watsonx.data, which combines elements of a data warehouse and data lakes. It offers a unified pl",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 96,
            "total_chunks": 245
          }
        },
        {
          "id": "86c2f2ef-1c6f-4fc9-934d-a0d6ea594d10",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "se, watsonx.data, which combines elements of a data warehouse and data lakes. It offers a unified platform where users can store data or connect data sources to manage and analyze enterprise data.\nIBM watsonx.data allows two approaches for onboarding data into the platform:\n/SM590000Accessing the data in place.\n\n\nChapter 2. Introduction to IBM watsonx.governance 23/SM590000Replicating the data onto the platform in Iceberg open data format using various extract, \ntransform, and load (ETL) options.\nIBM watsonx.data allows users to access data in existing data warehouses and data lakes \nthrough predefined platform connectors, including Teradata, Snowflake, SingleStore, SQL Server, PostgreSQL, MySQL, MongoDB, IBM Db2, and IBM Netezza. This solution reduces data duplication, and the costs associated with storing data in multiple locations.\nSuppose existing data is stored in an external storage system such as IBM Cloud Object \nStore, Amazon S3, IBM Storage Ceph, MinIO, HDFS (in Hadoop/Cloude",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 97,
            "total_chunks": 245
          }
        },
        {
          "id": "301714f1-0a21-43bc-83e3-e5dbbb7a6f67",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "e system such as IBM Cloud Object \nStore, Amazon S3, IBM Storage Ceph, MinIO, HDFS (in Hadoop/Cloudera), Google Cloud Storage, or Azure Data Lake Storage. In that case, it can be accessed directly if the data is in Iceberg or Delta Format. If the data is stored in other common formats like Parquet or CSV, it can be accessed in its native format, followed by running ETL jobs to convert it to the open format like Iceberg and brought into the platform.\nCirata is an IBM partner that developed a cloud migration solution that automates the \nseamless transfer of continuous HDFS data and Hive metadata to watsonx.data.\nIBM watsonx.data supports loading data from existing on-premises data lakes using ETL jobs \ndeveloped with DataStage or Spark. Additionally, data loading can be done through a web console or command line interface.\nExisting data sources like Db2 for z/OS, IMS, and VSAM on the mainframe can be replicated \nusing IBM Data Gate in the Iceberg open data format within watsonx.data. Dat",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 98,
            "total_chunks": 245
          }
        },
        {
          "id": "af896a7e-16fd-4cc7-9e86-f54261034b30",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "rame can be replicated \nusing IBM Data Gate in the Iceberg open data format within watsonx.data. Data Gate is a replication technology that sy nchronizes data from IBM Z to various hybrid-cloud targets.\nThe architecture of watsonx.data enforces schema and da ta integrity, facilitating the \nimplementation of robust data security and governance mechanisms. Integrating watsonx.data with IBM Knowledge Catalog on Cloud Pak for Data provides knowledge \nworkers with self-ser vice access to data as sets, allowing them to utilize these assets to gain \ninsights. Once the data from these sources is onboarded into the platform in its raw format, the data assets from watsonx.data are imported into a governed catalog in IBM Knowledge Catalog, where they can be enriched with business semantics by mapping business terms to technical data assets (such as database tables and columns). Data quality can be assessed through profiling and runn ing data quality analyse s. Data protection ru les are establish",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 99,
            "total_chunks": 245
          }
        },
        {
          "id": "4021439a-bd9d-4887-9a0a-5b6aff09da17",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "assessed through profiling and runn ing data quality analyse s. Data protection ru les are established to \ncontrol access to sensitive data in governed catalogs, which can include denying access, redacting columns, obfuscating columns, substituting columns, or filtering rows.\nIn addition, semi-structured and unstructured data and documents can be processed, split, \nand stored in the watsonx.data vector database, Milvus, along with their metadata and vector embeddings in the same datastore. Milvus is designed to store, index, and manage embedding vectors used for similarity search and retrieval-augmented generation, empowering embedding similarity search and AI applications.\n2.5.2  Data Preparation\nBusinesses can derive a greater value by integrating data from various sources and domains into higher layers, such as t he silver and gold layers of a medallion data architecture. \nAdditionally, watsonx.data provides multiple query engines, including Presto and Spark, \nallowing users to sele",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 100,
            "total_chunks": 245
          }
        },
        {
          "id": "ee669ab0-09f3-4d4f-9ea2-d064b686fd94",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "y, watsonx.data provides multiple query engines, including Presto and Spark, \nallowing users to select the most suitable engine based on the characteristics of their workload. IBM watsonx.data seamlessly integrates with Db2 Warehouse and Netezza Performance Service, facilitating data sharin g across these products and enabling users to \nleverage the most appropriate engine for each specific task.\n\n24 Ensuring Trustworthy AI with IBM watsonx.governanceOnce structured and unstructured data is onboarded to watsonx.data in the watsonx platform, \nusers can access the data assets as long as they have the necessary access rights. They can build, train, tune, and deploy tr aditional ML models or utilize ge nerative AI mode ls to enhance \ntheir business use cases with AI.\n2.5.3  AI Buildin g and Deployment\nIBM watsonx.ai provides low-code and no-code tools, including Auto AI for creating traditional \nmachine learning models, Prompt Lab for prompt engineering, and Prompt Studio for adapting and ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 101,
            "total_chunks": 245
          }
        },
        {
          "id": "0ff6356e-9988-4d53-a4c6-52daf7e415a0",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "nal \nmachine learning models, Prompt Lab for prompt engineering, and Prompt Studio for adapting and fine-tuning generative AI models. Additionally, watsonx.ai offers a Studio and a Python Software Development Kit (SDK) designe d for data scientists and AI engineers to \nbuild, train, adapt, fine-tune, and deploy both traditional machine learning and generative AI assets.\n2.5.4  AI Lifecycle Management and Governance\nThe IBM watsonx platform enables end-to-end AI lifecycle management and governance. IBM recommends following an end-to-end process to build AI solutions that are ethical and responsible - secure, safe, transparent, and trustworthy, as shown in Figure 2-3. \nFigure 2-3   End-to-end flow for AI life cycle and governance powered by IBM watsonx\nIn this approach, an AI Use Case Owner or Requestor starts by creating an AI use case in \nwatsonx.governance. This involves outlining the business purpose of the use case and \ndetailing how AI will be utilized to achieve the intended outco",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 102,
            "total_chunks": 245
          }
        },
        {
          "id": "2fdb1ec0-95e1-4290-886c-bb23546e2762",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "usiness purpose of the use case and \ndetailing how AI will be utilized to achieve the intended outcomes.\nNext, the AI Use Case Owner identifies potential risks associated with the AI use case by \nanswering a set of questions and conducts an initial risk assessment within watsonx.governance. Once this risk identification and assessment are completed, input from legal, HR, the AI Ethics Council, operations, security, and finance is gathered. Based on the risk analysis and risk profiles associated with the use case, a decision is made to approve or \nreject the development of the AI use case and its related assets.\nIf the use case is approved for development, the AI Developer, which can be an AI Engineer \nor Data Scientist, collaborates with a Data Engineer to shape and prepare the dataset in watsonx.data for either training traditional ML models or tuning large language models.\n\n\nChapter 2. Introduction to IBM watsonx.governance 25Following the data preparation, the AI Developer can build",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 103,
            "total_chunks": 245
          }
        },
        {
          "id": "52811d46-86c1-411e-a503-b51423f79dd3",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " Introduction to IBM watsonx.governance 25Following the data preparation, the AI Developer can build and train traditional ML models or \nadapt or tune for generative AI models specific to the use case. The AI Developer then validates these traditional ML models and AI assets leveraging generative AI models and test data to evaluate performance metrics such as F1 score and ROUGE, as well as fairness, explainability, and model health (including latency, number of  transactions, and token count).\nSubsequently, a Model Validator-an independent Data Scientist or AI Engineer- evaluates the \ntraditional ML model or Generative AI assets in a pre-production environment using production-like data within watsonx.governance.\nAn AI Risk Reviewer then examines the evalua tion performance metrics, risk scorecard, and \nmodel health metrics. Based on this final risk assessment, the reviewer either approves or rejects the AI asset for deployment in production within watsonx.governance.\nFinally, the Mode",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 104,
            "total_chunks": 245
          }
        },
        {
          "id": "e671727e-47dd-4202-9a25-1d2007adc5f3",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "es or rejects the AI asset for deployment in production within watsonx.governance.\nFinally, the ModelOps Engineer deploys the approved model into production using CI/CD \npipelines and activates ongoing AI monitoring to track key metrics, such as runtime quality, drift, fairness, explaina bility, and other relevant indicato rs specific to the use case. The \nongoing monitoring metrics are published in the Governance Console in watsonx.governance. Each metric has associated thresholds, and alerts can be configured in the Governance Console to notify relevant stakeholders, such as the AI Use Case Owner or AI Risk Reviewer, when these thresholds are br eached. Once the appropriate parties receive \nnotifications about threshold breaches, they can initiate the investigation process or follow the procedures for issue and change management, which may involve redeveloping a new version or completely different AI asset.\nFacts, documentation, and evidence regarding AI assets are collected and stor",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 105,
            "total_chunks": 245
          }
        },
        {
          "id": "a8f23757-6004-4051-869b-21e44baa8305",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ly different AI asset.\nFacts, documentation, and evidence regarding AI assets are collected and stored in \nwatsonx.governance for future reviews and audits by regulatory agencies, internal stakeholders, and external partie s. Additionally, specif ic reports related to  the AI lifecycle and \nits stages can be generated in the Governance Console of watsonx.governance.\n\n26 Ensuring Trustworthy AI with IBM watsonx.governance\n\n© Copyright IBM Corp. 2025. 27Chapter 3. Implementing AI governance \nstrategy\nArtificial Intelligence (AI) governance is a mu ltifaceted process that  ensures AI systems are \ndeveloped and deployed responsibly and ethica lly. This chapter provides a comprehensive \noverview of the end-to-end AI lifecycle gove rnance process, exploring the various steps \ninvolved in achieving this goal.\nThis chapter contains the following sections:\n/SM590000“Understanding the end-to-end AI lifecycle governance process” on page 28\n/SM590000“Elements of model risk governance” on page 29\n/",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 106,
            "total_chunks": 245
          }
        },
        {
          "id": "eee66851-7623-49c3-9a3d-77822dda5ff9",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "I lifecycle governance process” on page 28\n/SM590000“Elements of model risk governance” on page 29\n/SM590000“Considerations to implement AI governance strategy” on page 373\n\n28 Ensuring Trustworthy AI with IBM watsonx.governance3.1  Understanding the end- to-end AI lifecycle governance \nprocess\nThis section describes the governance process which is divided into two primary levels: \nmacro and micro as shown in Figure 3-1. \nFigure 3-1   End-to-end AI lifecycle governance process\nMacro level\nThe macro level encompasses high-level strategic steps such as defining legal obligations, \narticulating AI principles, and extending enterprise risk frameworks. These steps ensure that the overarching governance structure aligns with organizational values and regulatory requirements.\nMicro level\nThe micro level involves more granular tas ks, including assigning business owners, \nperforming risk assessments, and documenting go/no-go decisions. These steps ensure that day-to-day operations are conducte",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 107,
            "total_chunks": 245
          }
        },
        {
          "id": "e40f9d3d-e5ec-4b41-a93a-7f4e903bb170",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ents, and documenting go/no-go decisions. These steps ensure that day-to-day operations are conducted in a manne r that supports the macro-level objectives.\nThe intersection of macro and micro-level specific s dictates actions related to AI models, such \nas model approval, deployment, and monitoring . This level ensures that each AI model is \nrigorously evaluated and managed throughout its lifecycle.\nThe process begins with model proposal approval, where a model entry is created in the \nModel Inventory and continuously updated with new information. The data scientist then uses a tool of their choice to develop the model, with training data and metrics from popular open-source frameworks automatically captured and saved to the model entry. Custom information can also be saved.\nNext, the pre-production model is evaluated for accuracy, drift, and bias, with performance \nmetadata captured and synced. The model is t hen reviewed and approved for production, and \ndeployed in the preferred pl",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 108,
            "total_chunks": 245
          }
        },
        {
          "id": "a6450646-4233-47d0-84a3-4831a4172a34",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "d synced. The model is t hen reviewed and approved for production, and \ndeployed in the preferred platform, with relevant metadata captured and synced. Finally, the production model is continuously monitored, with performance data captured and synced, \n\n\nChapter 3. Implementing AI governance strategy 29and a dashboard provides a comprehensive view  of the performance metrics for all models, \nallowing stakeholders to proactively identify and react to any issues.\nThis chapter will delve into the end-to-end AI lifecycle  governance proces s, highlighting the \nkey personas, and components of each level and how they interconnect to ensure that AI systems are developed and deployed in a manner consistent with organizational values and goals. We will also discuss the challenges an d opportunities associ ated with implementing \nthis process, emphasizing the benefits of improved transparency, accoun tability, and trust.\nThroughout this ch apter, we will examine the key components of each  gover",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 109,
            "total_chunks": 245
          }
        },
        {
          "id": "748d0f24-21b7-4eb3-8231-b1484015f7b2",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "oun tability, and trust.\nThroughout this ch apter, we will examine the key components of each  governance level and \ndiscuss how they work together to ensure responsible and ethical AI development and \ndeployment. By understanding these components, readers will ga in a comprehensive view of \nthe governance process and its importance.\nBy the end of this chapter, readers will have a thorough underst anding of the end-to-end AI \nlifecycle governance proc ess. They will be able to identify  the key componen ts of each level \nand understand how they work together to ensure that AI systems are developed and deployed responsibly and ethically. This kn owledge will be invaluable  for applying AI \ngovernance strategies within their own organizations.\n3.2  Elements of model risk governance \nTo convert macro-level requirements into micro-level ones, a framework is needed to map these requirements. This is achieved through the use of personas that define the high-level requirements, while watsonx.",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 110,
            "total_chunks": 245
          }
        },
        {
          "id": "4ee67c95-68df-4cd2-bc25-07dd9f5cde0b",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "This is achieved through the use of personas that define the high-level requirements, while watsonx.governance objects are essential for effective management and governance of AI models thro ughout their lifecycle. This chapter will explore the elements \nrequired to implement the AI governance strategy\n3.2.1  Personas\nFigure 3-2 on page 30 shows a typical governance flow which begins with defining an AI use case to solve a business problem, followed by requesting an AI asset, such as a model or prompt template, to address the issue. The process involves various roles, starting with the model owner, who defines the problem and identifies the need for an AI solution. The developer then builds the AI asset, which is subsequently tested by the validator to ensure it meets the required standards. Next, the risk officer reviews and approves the solution, taking into account organizational risk management policies. Once approved, the ModelOps engineer deploys the AI asset into production, and",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 111,
            "total_chunks": 245
          }
        },
        {
          "id": "b8d59cc5-4db6-4f84-ab46-e95640b9ed9a",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " management policies. Once approved, the ModelOps engineer deploys the AI asset into production, and finally, the application developer monitors its performance, identifying areas for improvement. Throughout this process, some organizations may choose to combine certai n roles or responsib ilities, tailoring the \ngovernance flow to their specific needs.\n\n30 Ensuring Trustworthy AI with IBM watsonx.governanceFigure 3-2   Typical personas involved in a governance process ( source )\nConsider the expertise required for your governance team. A typical governance plan may \ninclude the following roles, wh ich can sometimes be filled by the same person or, in other \ncases, represent a team of people.\n/SM590000Model Owner: This individual creates an AI use case to address a business need, \nrequests the model or prompt template, manages the approval process, and tracks the solution through the AI lifecycle.\n/SM590000Risk and Compliance Manager/ Legal Team: This person determines the policies and",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 112,
            "total_chunks": 245
          }
        },
        {
          "id": "097322af-00fd-42a2-a07a-4002fe385281",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "lifecycle.\n/SM590000Risk and Compliance Manager/ Legal Team: This person determines the policies and \ncompliance thresholds for the AI use case, such  as rules for testing fairness or screening \noutput for hateful and abusive speech.\n/SM590000Model Developer or Data Scientis t: This role involves working with the data in a dataset or \na large language model (LLM) to create the machine learning model or LLM prompt template.\n\n\nChapter 3. Implementing AI governance strategy 31/SM590000Model Validator: The validator tests the solution to ensure it meets the goals outlined in \nthe AI use case.\n/SM590000Model Evaluator: After deployment, the app developer evaluates the deployment to \nmonitor performance against the metric thresholds set by the risk and compliance manager. If performance falls below specified thresholds, the app developer collaborates with other stakeholders to address issues and update the model or prompt template.\n3.2.2  Objects\nFigure 3-3 illustrates the key comp onents of",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 113,
            "total_chunks": 245
          }
        },
        {
          "id": "faa9feae-74b5-4699-9cc2-b2f75dff1586",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "nd update the model or prompt template.\n3.2.2  Objects\nFigure 3-3 illustrates the key comp onents of AI governance, whic h is a critical aspect of \nensuring responsible and ethical AI development and deployment. The three main elements of AI governance are model risk governance, model inventory, and lifecycle tracking, and evaluation and monitoring. Model risk governance involves identifying and mitigating potential \nrisks associated with AI models, such as bias, accuracy, and security. Model inventory and lifecycle tracking involves maintaining a comprehensive record of all AI models in use, including their development, deployment, and maintenance. Evaluation and monitoring involves continuously assessing the performance and impact of AI models, identifying areas for improvement, and making necessary adjustments. By implementing these components, organizations can ensure that their AI systems are transparent, accountable, and aligned with their values and goals.\nFigure 3-3   A governe",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 114,
            "total_chunks": 245
          }
        },
        {
          "id": "7fa8893b-8693-4054-8b65-ea52122345c6",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ystems are transparent, accountable, and aligned with their values and goals.\nFigure 3-3   A governed, trusted AI lifecycle\nTo implement the framework, several objects play a crucial role in ensuring the effective management and governance of AI models throughout the lifecycle. These objects interact with different personas, including Model Developers, Model Owners, Model Validators, and Model Risk Reviewers. \nFigure 3-4 on page 32 illustrates a hierarchical  framework for implem enting AI governance \nstrategies, comprising three key components: Evaluation and Monitoring, Model Validation and Review, and Risk and Compliance. Such a framework enables the creation of multiple hierarchical structures, each with its own unique architecture, which can be used to organize and capture Models. One possible organizational structure could include divisions into groups such as geography, business unit, line of business etc.\n\n\n32 Ensuring Trustworthy AI with IBM watsonx.governanceFigure 3-4   Hier",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 115,
            "total_chunks": 245
          }
        },
        {
          "id": "6368a290-8a97-412d-9bf6-d14894f80a38",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "nit, line of business etc.\n\n\n32 Ensuring Trustworthy AI with IBM watsonx.governanceFigure 3-4   Hierarchical framework for implementing AI governance\nBusiness Entity\nBusiness Entities are abstract representations of your business structure. A business entity \ncan contain sub-entities (such as departments, business units, or geographic locations). This \nstructure is used within the system to organize access rights and simplify corporate reporting needs.\nFor example: A bank's retail lending department is a Business Entity.\nPersona interactions:\n/SM590000Model Developers: Create models that meet the specific needs of the Business Entity. For \nexample, a Model Developer may create a credit risk model for the retail lending department.\n/SM590000Model Owners: Ensure that models are aligned with the Business Entity's goals and \nobjectives. For example, the Model Owner for the retail lending department may ensure that the credit risk model is aligned with the department's risk appetite.\n/SM590",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 116,
            "total_chunks": 245
          }
        },
        {
          "id": "0a8b2035-444b-41dd-b575-56cce388f521",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "artment may ensure that the credit risk model is aligned with the department's risk appetite.\n/SM590000Model Validators: Validate models to ensure they meet the Business Entity's requirements. \nFor example, a Model Validator may validate the credit risk model to ensure it meets the retail lending department's requirements for accuracy and fairness.\nInventory\nA centralized repository that organizes, documents, and maintains an enterprise-wide collection of models or AI assets, including their usage, issues, and governance activities.\nPersona interaction:\n/SM590000Model Owner: Interacts with the inventory to document model changes and updates, track \nissues and resolve model-related problems\n\n\nChapter 3. Implementing AI governance strategy 33/SM590000Risk and Compliance Manager: Interacts with the inventory to schedule and track model \nreviews, assign and track model risk assessments, monitor model performance and identify potential compliance risks associated with models\nInventory is pr",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 117,
            "total_chunks": 245
          }
        },
        {
          "id": "b5e7b86d-dbd0-4574-9702-193b89c656ce",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "tor model performance and identify potential compliance risks associated with models\nInventory is primarily comprised of the following objects:\n1.Use Case  - The Use Case object is a subclass of Entity and a superclass of the Model \nobject. Its main function is to serve as a repository for models during development. The \nuse case encapsulates both the qualitative and quantitative requirements of the AI application to be deployed. It helps define and demonstrate how a specific model can solve a particular business problem or achi eve an objective. It describes practical \nscenarios and contexts in wh ich the model will be implement ed, offering comprehensive \ninsight into its intended uses and expected results. Additionally, use cases synchronize the watsonx.governance Evaluation and Monitoring Component (performance metrics and quality control) with the Model Inventory and Lifecycle Tracking Component (model reports). Changes in one are automatically reflected in the other. This ensures",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 118,
            "total_chunks": 245
          }
        },
        {
          "id": "20760ded-a25a-412e-8b46-d068a279b3c5",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ing Component (model reports). Changes in one are automatically reflected in the other. This ensures that users always have up-to-date information about AI models.\n2.Model - The Model object represents a quantitative method, system, or approach used \nwithin an organization to transform input data in to quantitative estimates. This is achieved \nthrough the application of statistical, economic, financial, or mathematical theories, techniques, and assumptions.\nThe Model object captures essential information, including:\n–Model Description : A detailed description of the model\n–Model Ownership : Information about the model's owner\n–Model Status : The current status of the model\n–Development Lifecycle Dates : Important dates related to the model's development\n–Model Type and Category:  Classification of the model\n–Model Risk Assessment data : Data related to the model's risk assessment\n3.Model Deployment  - The Model Deployment object is a child entity of the Model object, \nserving as a cruc",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 119,
            "total_chunks": 245
          }
        },
        {
          "id": "04313322-408e-4c97-9fca-b395b4feb0c6",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " Deployment  - The Model Deployment object is a child entity of the Model object, \nserving as a crucial component for tracking the deployment of one or more models. The Model Deployment object is designed to:\n– Govern individual usages of a Model in a production ecosystem\n– Record-specific Model Versions and their deployment locations– Inform risk tiering of Models by highlighting the number of areas or functions supported \nby each Model\n4.Model Attestation  - Model Attestation is a process that enables organizations to request \nregular sign-offs or attestations for their models. The MRG administrator initiates this process by creating a set of blank model attestations, which are then assigned to the respective model owners. These owners are re quired to answer a series of questions \nabout their models and submit their completed attestations. Typically, model attestations are conducted on an annual or quarterly basis, serving as a way to verify the completeness and accuracy of a model'",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 120,
            "total_chunks": 245
          }
        },
        {
          "id": "758026c6-eb8c-4310-bfc2-ba5866bac87d",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "n an annual or quarterly basis, serving as a way to verify the completeness and accuracy of a model's information, as well as the overall model inventory.\n5.Model Output  - For organizations seeking a more detailed approach to model \ndocumentation, the model output object offers a solution. This object enables the recording of a model's outputs, with a focus on capturing the description and overview of each output from a governanc e perspective. By utilizin g the model output object, \norganizations can maintain a more granular and comprehensive record of their models' outputs.\n\n34 Ensuring Trustworthy AI with IBM watsonx.governance6.Model Input  - For organizations seeking a more detailed approach to model \ndocumentation, the Model Input object provides a means to capture and record the inputs of a model. The object includes key fields such as:\n–Input Owner : The individual or team responsible for the input\n–Type : The classification of the input\n–Status : The current state of the inpu",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 121,
            "total_chunks": 245
          }
        },
        {
          "id": "1333a1d8-1ce4-4844-91c2-b71bc5f4ad62",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "nsible for the input\n–Type : The classification of the input\n–Status : The current state of the input\n–Description : A detailed explanation of the input\nAdditionally, a model input object can be linke d to a model output object, allowing for the \ncreation of model chains at a granular level. This provides an alternative to the model link approach, offering a more detailed and nuanced understanding of model relationships.\n7.Model Risk Scorecard  - Model risk assessments are a critical component of the model \ndevelopment and documentation process, as well as an ongoing requirement for models in production. To facilitate th is process, the Model Risk Sc orecard object is utilized to \nconduct thorough risk assessments. This process involves answering a series of questions about the model, which triggers a calc ulation of a risk score . This score, in turn, \ndetermines the model's tier, providing a clear indication of the model's risk level.\nModel validation and review\nDuring this phase, th",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 122,
            "total_chunks": 245
          }
        },
        {
          "id": "cf539836-2a5f-4176-bd1b-4bb25fdbc8b5",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ding a clear indication of the model's risk level.\nModel validation and review\nDuring this phase, the model use case and facts collected during the development phase are utilized to validate the mo del. The goal is to ensure the mo del is functionin g as intended and \nmeets the required standards.\nPersona interaction:\n/SM590000Model Validator reviews the model documentation and use-case. In case of any \nshortcomings, they challenge the inconsistence and clarify any unclear or missing information\nTo implement a model validation strategy, the following  objects are primarily utilized:\n1.Review  - The review object is a critical component of model governance, serving as a \nrecord of all model review activities. As a child of both the model deployment and model objects, it provides a comprehensive view of review outcomes. The review object is \ndesigned to capture the results of various types of reviews (for example, pre- and \npost-implementation ), and reviews consumed by independent teams",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 123,
            "total_chunks": 245
          }
        },
        {
          "id": "4af1d60e-ffdd-4fcd-a790-b3794faefb2e",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " of reviews (for example, pre- and \npost-implementation ), and reviews consumed by independent teams to ensure model integrity and effectiveness.\nBy utilizing the review object, organizations can maintain a ce ntralized record of all model \nreview activities, facilitating informed deci sion-making and effective model governance.\n2.Challenge  - The challenge object serves as a re pository for documenting and evidencing \nconcerns or issues related to any part of the Model Inventory. When a challenge is raised, the response is recorded, providing a clear audit trail. As a child of both the model and model deployment objects, the challenge object ensures that all relevant information is linked and easily accessible.\nThere could be many factors prompting di fferent personas to challenge existing \ndeployments, such as:\n– Regulatory non-compliance: Models that fail to meet new or updated regulatory \nrequirements\n– Data relevancy issues: Models that rely on outdated, incomplete, or inaccurate ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 124,
            "total_chunks": 245
          }
        },
        {
          "id": "bc0e9b29-5aef-4cb9-b924-cfffdfcfaf67",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "tory \nrequirements\n– Data relevancy issues: Models that rely on outdated, incomplete, or inaccurate data\n– Performance deterioration: Models that experience a decline in performance over time\n\nChapter 3. Implementing AI governance strategy 35When a challenge is identified, stakeholders will review it in accordance with established \nprocesses to address the concerns. This may involve remediation efforts to bring the model into compliance, update the data, or improve the model's performance.\nEvaluation an d monitoring\nTo ensure the model operates within acceptable parameters, performance metrics are \ncontinuously monitored to circumvent reputational and organizational risks. During the evaluation and monitoring phase, developers, model validators and ModelOps engineers interact with the platform to:\n/SM590000Monitor performance metrics and thresholds.\n/SM590000Analyze model outputs and detect potential issues.\n/SM590000Receive alerts and notifications for threshold breaches.\n/SM590000Inv",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 125,
            "total_chunks": 245
          }
        },
        {
          "id": "b357e40e-8b1f-4948-8777-4b5471fea5e6",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ect potential issues.\n/SM590000Receive alerts and notifications for threshold breaches.\n/SM590000Investigate and respond to potential issues or anomalies.\n/SM590000Collaborate to identify root causes and develop solutions.\nPersona interaction:\n/SM590000Model Validator interacts with the platform to: Monitor performance metrics and \nthresholds, analyze model outputs and detect potential issues, and identify data quality problems or biases\n/SM590000ModelOps Engineer interacts with the platform to Interacts with the platform to receive \nalerts and notifications for threshold breaches , investigate and respond to potential issues \nor anomalies and collaborate with Model Evaluators to identify root causes\nFor evaluation and monito ring, the following object s are primarily utilized:\n1.Metric  - The Metric object is used to record the definition of a performance measurement \nthat an organization wants to track. This  involves setting key parameters, including:\n–Metric Type:  The type of perf",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 126,
            "total_chunks": 245
          }
        },
        {
          "id": "7cdac22f-afde-4550-ade6-31e56f873080",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "on wants to track. This  involves setting key parameters, including:\n–Metric Type:  The type of performance metric being tracked\n–Threshold:  The threshold at which the metric is  considered cautionary, critical, etc.\n–Collection information: Additional details about the metric's collection and calculation\n2.Metric Value - The Metric Value object records the result of the metric performance \nmeasurement. It is designed to behave in a way to allow the organization to store time \nseries results of measurement.\n3.Change Request - The change request object is a critical component of model \ngovernance, providing a structured process for requesting, justifying, and approving changes to models after they have gone live. This object's workflows enable organizations \nto:\n–Request and justify changes: Clearly articulate the need for changes and provide \nsupporting rationale\n–Route changes for approval:  Direct changes to the relevant stakeholders and \napprovers, based on the type and impact of t",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 127,
            "total_chunks": 245
          }
        },
        {
          "id": "49ffba5d-929a-4dc1-a9da-1efc2eb1b9eb",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "oval:  Direct changes to the relevant stakeholders and \napprovers, based on the type and impact of the change\n–Obtain auditable approvals: Ensure that all changes are properly approved and \ndocumented, with a clear audit trail\nThe Change Request object allows for various  approval paths and levels of approval, \ndepending on the nature and scope of the change. This flexibility e nables organizations to \nbalance the need for control and oversight wi th the need for agilit y and responsiveness.\n\n36 Ensuring Trustworthy AI with IBM watsonx.governanceRisk and compliance\nThe increasing use of AI models in various industries has raised concerns about risk and \ncompliance. AI models can pose significant risks if not properly designed, trained, and deployed, including biases, errors, and uninten ded consequences. Moreover, AI models must \ncomply with various regulations and standards, such as the European Union AI Act, United States AI Executive Order, CCPA, and HIPAA, to ensure the protection ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 128,
            "total_chunks": 245
          }
        },
        {
          "id": "e0cf40e4-bacb-454c-8380-7c641615f727",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " European Union AI Act, United States AI Executive Order, CCPA, and HIPAA, to ensure the protection of sensitive data and \nprevent harm to individuals. To mitigate these risks, organizations must implement robust risk \nmanagement and compliance frameworks that include model risk assessments, data quality checks, and ongoing monitoring and evaluation. \nSince risk and compliance management involves  identifying, assessing, and mitigating \npotential risks associated with non-compliance with external mandates and internal policies. The following components are essential to a robust risk and compliance framework:\n1.Mandate  - Mandates represent external requirements that organizations must comply \nwith, such as laws, regulations, and standards. When necessary, mandates can be broken down into Sub-Mandate objects, which provide a more detailed understanding of the mandate's sub-sections. This hierarchical structure enables organizations to effectively manage and comply with complex regulator",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 129,
            "total_chunks": 245
          }
        },
        {
          "id": "fcf8a35e-4d38-4e45-9704-51cd70237bde",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "hierarchical structure enables organizations to effectively manage and comply with complex regulatory requirements.\n2.Requirement  - Requirements represent specific ob ligations that an organization must \nfulfill to comply with related mandates or sub-mandates. Th ese requirements are detailed \nin the requirement object, which provides a clear understanding of what the organization needs to do to meet regulatory obligations. By detailing these requir ements, organizations \ncan ensure they are meeting their regulatory obligations and maintaining compliance.\n3.Policy  - Policies represent internal guidelines adopted by an organization's Board of \nDirectors or senior governance body. These guidelines are designed to provide direction and oversight for the organization's operatio ns and decision-making processes. The Policy \nobject is used to store and manage policy information, including the policy text, which can be stored in standardized fields or as an attachment to the object.\nPolicie",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 130,
            "total_chunks": 245
          }
        },
        {
          "id": "c89bd029-95fe-4e08-96d4-7d12e6831dcf",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "e policy text, which can be stored in standardized fields or as an attachment to the object.\nPolicies are managed through a review and approval process, which ensures that they are \nproperly vetted and authorized before being implemented. \n4.Questionnaire Assessment - Questionnaires are a powerful tool for assessing risk and \ncompliance, as well as collect ing information for specific processes and asset risks. \nwatsonx.governance streamlines, standardizes, and centralizes the collection of \nquestionnaire-based assessment information, making it easier to gather insights from \nacross the organization.\nWith Questionnaire Assessment object, information from business users within the \norganization can be gathered. Respondents complete the questions and submit the finished questionnaire assessment, providing valuable insights into risk and compliance.\n3.2.3  Workflows\nThe implementation of AI governance requires structured workflows to manage the lifecycle, \nrisk, and compliance of AI model",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 131,
            "total_chunks": 245
          }
        },
        {
          "id": "1bd9acba-3317-4ff7-9aeb-fce7ae62ef3c",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " governance requires structured workflows to manage the lifecycle, \nrisk, and compliance of AI models and their associated use cases. This section outlines key workflows provided by watsonx.governance Model Risk Governance (MRG) , integrating them \ninto the end-to-end go vernance process discussed in this chapter. These workflows facilitate \nresponsible AI development, deployment, and monitoring, ensuring adherence to organizational processes and regulatory requirements. These workflows can be automated and involve multiple user interactions and feedback/approval capturing mechanisms effectively reducing the time and effort to manage the entire process of AI governance. All actions taken by users with workflows are auditable ensuring conf idence in the system \nresults. Predefined workflows are provided as part of watsonx.governance, but more can be built or existing workflows can be customized through the UI designer.\n\nChapter 3. Implementing AI governance strategy 37These available wo",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 132,
            "total_chunks": 245
          }
        },
        {
          "id": "714caa98-65e9-4ae4-b32b-26ccdfc4a475",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "omized through the UI designer.\n\nChapter 3. Implementing AI governance strategy 37These available workflows can be categorized into:\n/SM590000Models : Model Candidate, Model Validation, Model Deployment, Model Risk Assessment, \nModel Attestation, Model Decommission, and Model Change Request\n/SM590000Use Cases : Use Case Request, Use Case Stakeh older Review, Use Case Development \nand Validation, and Use Case Deployment Approval\n/SM590000Metrics : Metric Value and Metric Value Creation\n/SM590000Questionnaires : AI Assessment and Questionnaire Assessment\n/SM590000Other : Challenges and Model Risk Assessment\nFigure 3-5 illustrates a matrix of  various objects and personas involved in AI governance \nworkflows. Each row and column represent different components and stakeholders discussed in 3.2, “Elements of model risk governance” on page 29, showcasing their interconnected roles in executing standardized processes. Th is visualization unders cores the necessity of \ncollaboration and coordi",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 133,
            "total_chunks": 245
          }
        },
        {
          "id": "61bcea22-e67d-408d-a6d3-a6d05e281c60",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " standardized processes. Th is visualization unders cores the necessity of \ncollaboration and coordination among diverse stakeholders to ensure the effective implementation of AI governance workflows.\nFigure 3-5   Workflows bridging governan ce and personas for standardized processes\nBenefits of str uctured workflows\nBy integrating these workflows into the AI governance process, organizations can:\n/SM590000Ensure transparency, accounta bility, and ethical compliance throughout the AI lifecycle.\n/SM590000Mitigate risks effectively while adhering to regulatory requirements.\n/SM590000Foster trust in AI systems through robust and scalable governance practices.\n/SM590000Minimize efforts to govern the AI use cases in their entirety.\n/SM590000Standardize the approval and data capture process for all the use cases.\nThese workflows are critical in enabling organizations to manage their AI models and use \ncases with confidence and precision, aligned with the macro and micro-level governance stra",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 134,
            "total_chunks": 245
          }
        },
        {
          "id": "98e7c677-8c19-4a53-baba-193ec211dea9",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "and use \ncases with confidence and precision, aligned with the macro and micro-level governance strategies discussed  in this chapter.\n3.3  Considerations to implement AI governance strategy\nImplementing an effective AI governance strategy requires careful consideration of an \norganization's unique characteristics and needs. This section outlines the key factors to consider when configuring an AI governance solution to meet the specific requirements of an individual organization.\n\n\n38 Ensuring Trustworthy AI with IBM watsonx.governance3.3.1  Understanding orga nizational characteristics\nWhen implementing an AI governance strategy, it is essential to consider the following factors \nthat are specific to each organization:\n/SM590000Geographies : Different regions have distinct regulatory requirements, cultural nuances, \nand market conditions that impact AI governance.\n/SM590000Market sectors : Various industries, such as banking, telecommunications, and public \nsector, have unique challen",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 135,
            "total_chunks": 245
          }
        },
        {
          "id": "2d3fb0b5-13bc-4659-a4e4-136928b7b183",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "s : Various industries, such as banking, telecommunications, and public \nsector, have unique challenges and requirements for AI governance.\n/SM590000AI use cases : Organizations may employ different types of AI, such as machine learning \nor generative AI or general purpose AI, for in ternal or external purposes, which affects \ngovernance needs.\n/SM590000Organizational structure : Centralized or decentralized structures influence how AI \ngovernance is implemented and managed.\n/SM590000Tech stack : The use of specific platforms, AI in enterprise apps, or open-source \ntechnologies impacts AI governance requirements.\n3.3.2  Configuring AI governance\nBased on these organizational characteristics, the following steps can be taken to configure \nan AI governance solution:\n/SM590000Implementing legal obligations : Develop a library of mandates and assessment \ntemplates that reflect the organizat ion's specific legal requirements.\n/SM590000Establishing a risk framework : Create a library of risk",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 136,
            "total_chunks": 245
          }
        },
        {
          "id": "8b4c9e58-53da-4c08-ba31-5b0ebef82d2c",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ion's specific legal requirements.\n/SM590000Establishing a risk framework : Create a library of risks and controls, and assessment \ntemplates that align with the organization's risk management policies.\n/SM590000Defining organizational structure : Configure business entities, roles, and \nresponsibilities to match th e organization 's structure.\n/SM590000Manage collaboration and access control : Use roles and access control features to \nensure that team members have appropriate access to meet governance goals\n/SM590000Developing policies and procedures : Create workflows for lifecycle governance, risk \nmanagement, and compliance management that reflect the organization's policies and procedures.\n/SM590000Develop a communication plan : Establish a plan for communication and \ndecision-making, including the use of email, messaging tools, or other collaboration platforms\n/SM590000Implement a simple governance solution : Start with a basic implementation and build \nincrementally to a more  c",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 137,
            "total_chunks": 245
          }
        },
        {
          "id": "d18cdbb2-f547-4998-b831-c3a2b5414ccb",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "simple governance solution : Start with a basic implementation and build \nincrementally to a more  comprehensive solution\n/SM590000Plan for more complex solutions:  Consider extending the AI governance \nimplementation to include external models, custom properties, and tailored reports\n3.3.3  Leveraging out-of-th e-box product content\nWhile configuration is necessary, it is essential to note that out-of-the-box \nwatsonx.governance capabilities and product content can provid e a solid foundation for AI \ngovernance. This content can be used to illustrate  best practices and provide a starting point \nfor customization, rather than requiring organizations to start from scratch.\n\nChapter 3. Implementing AI governance strategy 39By considering these factors and configuring an AI governance solution accordingly, \norganizations can establish a strong foundation for effective AI governance and set the stage \nfor successful implementation of subsequent chapters' topics.\n3.3.4  Example use case\nWh",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 138,
            "total_chunks": 245
          }
        },
        {
          "id": "eff026de-8d16-4496-9976-f760e867e7ce",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " the stage \nfor successful implementation of subsequent chapters' topics.\n3.3.4  Example use case\nWhen implementing an AI governance strategy , an organization should start by analyzing \nvarious factors including: regi ons where it operates and utilizes AI models,  the industry it \nserves, the specific AI tools and applications it relies on, its organizational structure, and its existing technology stack (for example, IBM Watson Studio ). This evaluation will help the \norganization understand its operational landscape and tailor its AI governance strategy to address its specific needs. A well-designed governance strategy enables the organization to manage cross-team collaboration efforts, control access to sensitive data, and ensure compliance with regional regulations. Additionally, it helps support the implementation of a clear communication, aligning stakeholders and providing a strong foundation for continuous \nmonitoring, auditing, transparency, and acco untability over time.\n\n40 ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 139,
            "total_chunks": 245
          }
        },
        {
          "id": "57b89eb4-9ea7-4652-980e-cac692e0fac4",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "g foundation for continuous \nmonitoring, auditing, transparency, and acco untability over time.\n\n40 Ensuring Trustworthy AI with IBM watsonx.governance\n\n© Copyright IBM Corp. 2025. 41Chapter 4. Onboarding a new foundation \nmodel\nThe pace of innovation in artifi cial intelligence (AI) is high, es pecially in the development of \nnewer and better foundation models. New versions of major models are regularly released, and numerous smaller models are created for specific languages, such as Danish; data types like geospatial; or business domains, such as  IT programming. Additionally, a vibrant \nopen-source community ex ists, with over one million models available on Hugging Face, an \nonline marketplace for open-source AI models.\nThat increasing choice in models means there is an increasing need for organizations to \ngovern those choices.\n/SM590000Organizations should define the minimal standards a foundation model needs to meet \nbefore they will allow it to be  applied in their use cases. T",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 140,
            "total_chunks": 245
          }
        },
        {
          "id": "5e8d3fda-89c2-410e-a7d1-bd125e611708",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "rds a foundation model needs to meet \nbefore they will allow it to be  applied in their use cases. Th is chapter describes the key \nconsiderations from the point of view of different stakeholders.\n/SM590000These considerations might require a trade-off decision when the minimal standards are in \nconflict (for example, would you accept a new model that provides better performance on a certain business task, but it creates a copyright infringement risk if it's not clear where the \ntraining data was sourced?). Organizations should define who gets to make the decision \nand how it gets documented.\n/SM590000Lastly, organizations should define the worfklow to automate the evaluation of a new \nmodel candidate by the various stakeholders.\nThis chapter has th e following sections:\n/SM590000“Key considerations to onboard a foundation model” on page 42\n/SM590000“Considerations for legal team for approving a new foundation model” on page 44\n/SM590000“Ethical considerations for approving a new found",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 141,
            "total_chunks": 245
          }
        },
        {
          "id": "43344dd7-6514-4b59-8936-453e9358cc88",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "roving a new foundation model” on page 44\n/SM590000“Ethical considerations for approving a new foundation model” on page 47\n/SM590000“Considerations for financial stakeholders for approving a new foundation model” on \npage 494\n\n42 Ensuring Trustworthy AI with IBM watsonx.governance4.1  Key considerations to  onboard a foundation model\nWhen onboarding a foundation model, several key aspects must be considered to ensure a \nsmooth and effective process. The following subsections outline the crucial considerations for data acquisition and preparation, data processing and filtering, model evaluation and validation, model security and robustness, and model performance monitoring.\n4.1.1  Data transparency\nTransparency is highly desirable as it makes information available, shareable, legible, and verifiable. In the context of training a foundation model, which involves multiple stages, transparency efforts are often targeted at different parts of the pipeline. Documentation is particularly cru",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 142,
            "total_chunks": 245
          }
        },
        {
          "id": "d20224fe-9175-4ca6-b966-88cedcb9623c",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ncy efforts are often targeted at different parts of the pipeline. Documentation is particularly crucial for the data pile used for training the foundation model, where gathering information on data acquisition and preparation methodologies for foundation model training is essential. \nThere are frameworks, such as The Foundation Model Transparency Index , which evaluate \nthe transparency of foundation models across several composite indexes, including data, data labor, data access, and others. These indexes collectively aim to assess the transparency of various aspects of model development, such as data usage, labor practices, computational resources, methodologies, and strategies to  mitigate privacy an d copyright concerns \n(Bommasani, 2024\n1). \nData scientists or AI Center of Excellence or Enterprise AI team onboarding a foundation \nmodel should leverage such assessments to ensure compliance with internal and external regulations and policies. \n4.1.2  Model evaluation and validation",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 143,
            "total_chunks": 245
          }
        },
        {
          "id": "9ee6db60-57dc-4032-9b01-dd44cf5575b1",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "pliance with internal and external regulations and policies. \n4.1.2  Model evaluation and validation\nTo ensure the reliability and sa fety of large langu age models, a comprehensive evaluation \nand validation strategy is required, incorporating the following key elements\n/SM590000FMEval framework: Leverage the Foundation Model Evaluation Framework (FMEval) for \nsystematic, reproducible, and consistent validation and evaluation of new large language models.\n/SM590000Evaluation modes: Support both fine-tuning and prompting (in-context learning) \nevaluation, with readily available academic and business benchmarks.\n/SM590000Content filtering: Implement robust content filtering mechanisms to detect and mitigate the \ngeneration of hate speech, abuse, profanity (HAP), pirated content, malware, and other undesirable outputs.\nTo evaluate foundation models, a data scientist should compare the foundation model to be \nonboarded with current state-of-the-art models using a wide range of standard be",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 144,
            "total_chunks": 245
          }
        },
        {
          "id": "a3998873-6492-440b-ab7b-5be2f1c0801e",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "dation model to be \nonboarded with current state-of-the-art models using a wide range of standard benchmark evaluations across top-level categories, such as:\n/SM590000human exams ( MMLU , MMLU-Pro (Wang, 2024) )\n/SM590000common sense ( OBQA , SIQA ), \n/SM590000reading comprehension ( BoolQ , SQuAD 2.0 ), \n/SM590000reasoning ( ARC-C , GPQA ), \n/SM590000code ( HumanEval) , \n/SM590000math ( GSM8K ), \n1  Bommasani, R. K. (2024). The Foundation Model Transparency Index v1.1 May2024 . arXiv \npreprint.\n\nChapter 4. Onboarding a new foundation model 43/SM590000Hugging Face's Open LLM leaderboards\nAdditionally, it is important to  evaluate for different functi ons such as tool calling, RAG \npatterns, and other target domains specific to organizational use-cases, as discovered and discussed by data scientists or AI Center of Excellence or Enterprise AI team.\n4.1.3  Model security and robustness\nFoundation models can be vulnerable to  various security risks, including:\n/SM590000Data poisoning: Att",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 145,
            "total_chunks": 245
          }
        },
        {
          "id": "cab49ae8-400d-4a89-b8c6-c4909e6bb5c1",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ndation models can be vulnerable to  various security risks, including:\n/SM590000Data poisoning: Attackers can manipulate training data to compromise model \nperformance or inject malicious behavior.\n/SM590000Model stealing (or extraction): Attackers can steal the model itself or its weights, allowing \nthem to use it for malicious purposes or create competing products.\n/SM590000Adversarial attacks: Attackers cr aft specific input data (adversarial examples) designed to \nmislead the model, causing incorrect or malici ous outputs. This includes techniques like \nprompt injection, where carefully crafted prompts can manipulate the model's behavior.\nRed teaming (IBM, n.d.)  is crucial for data scientists to i dentify model vulnerabilities. This \ninvolves ethical hackers attempting to elicit unintended and potentially harmful behavior from the model, such as generating undesirable content through adversarial attacks and prompt injection. Data scientists responsible for onboarding foundation m",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 146,
            "total_chunks": 245
          }
        },
        {
          "id": "3e297ae4-72f7-4059-aeeb-480cdc1691d8",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "gh adversarial attacks and prompt injection. Data scientists responsible for onboarding foundation models should employ a comprehensive approach, using a combination of internal and external, automated and manual red teaming techniques to thoroughly i dentify and mitigate weaknesses. Traditional \nsecurity measures, such as data encryption (at rest and in transit), user access controls, and firewalls, are also essential.\n4.1.4  Ensuring model health and performance\nTo guarantee that a model meets the desired output and performance expectations of \nend-users, data scientists should evaluate model  health metrics relative to the use cases \nexpected to be delivered with this foundation model. Two key metrics to track are latency and throughput.\nLatency\nLatency measures the time it takes for a model to process a scoring request. It is calculated by tracking the time elapsed between rece iving a request and generating a response, \ntypically measured in millisecon ds (ms). This metric helps i",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 147,
            "total_chunks": 245
          }
        },
        {
          "id": "217483d0-eabe-43d4-9557-2120b3278e8e",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " a request and generating a response, \ntypically measured in millisecon ds (ms). This metric helps i dentify any delays or bottlenecks \nin the model's processing pipeline.\nThroughput\nThroughput measures the number of scoring requests and transaction records that a model can process per second. This metric indicates the model's ability to han dle a high volume of \nrequests efficiently.\nThe following are additional references:\n/SM590000Bommasani, R. K. (2024). The Foundation Model Transparency Index v1.1 May2024 . \narXiv preprint.\n/SM590000IBM. (n.d.). Responsible Use Guide . Note:  AIR-Bench  is a benchmark for evaluating robustness against adversarial attacks. It \nprovides a standardized way to measure how well a model resists these attacks.\n\n44 Ensuring Trustworthy AI with IBM watsonx.governance/SM590000Wang, Y. X. (2024). Mmlu-pro: A more robust and challenging multi-task language \nunderstanding benchmark. arXiv preprint arXiv:2406.01574 .\n4.2  Considerations for legal team for appro",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 148,
            "total_chunks": 245
          }
        },
        {
          "id": "44f8f949-7561-42f2-af25-dcbb0b6ffbef",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "erstanding benchmark. arXiv preprint arXiv:2406.01574 .\n4.2  Considerations for legal team for approving a new \nfoundation model\nThe Legal Team must have special considerations when approving a new Foundation Model. \nWhat kind of license is attached to the foundation model? Who owns the model? Does \nindemnification apply to the new Foundation Model in review? This  section will cover how \nwatsonx.governance can be used to answer these questions and more.\n4.2.1  Model licensing\nStandard software licensing applies to all foundation models and is a great place for the legal team to begin. Once foundation models are published, they are packaged with a license which provides a great depth of detail as to how the model can and should be used. These licenses can be found at multiple places. In  watsonx.governance, the model license can be \nfound attached to Model cards. You can access the model card by using the hamburger menu and clicking through Inventory → Models → Choose a specific model ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 149,
            "total_chunks": 245
          }
        },
        {
          "id": "73e95ae7-d608-4ed6-a47c-039198d1bff6",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " card by using the hamburger menu and clicking through Inventory → Models → Choose a specific model . The license will be \nlisted under Development/Training. The model license can also be found on the model card in watsonx.ai, under terms. Finally, before the acceptance of a foundation model onto the watsonx.governance platform, a legal team may obtain the model License through the official publication page. As of this wr iting, any model which is public ly available can have its license \nchecked through the huggingface.co  hub. Table 4-1 highlights several common model \nlicenses.\nTable 4-1   Some common Model Licenses\nTerms and conditions at tached to AI assets\nNot all AI assets come with a simple model license. For example, much of the popular OpenAI \nmodels are not covered under these licenses. In  cases like these, additional questions must \nbe answered. A legal team may want to sit directly with model providers and other teammates such as the data science team to gain a thorough u",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 150,
            "total_chunks": 245
          }
        },
        {
          "id": "4693095c-4f21-4a28-a9ee-49f43a04cfdb",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "directly with model providers and other teammates such as the data science team to gain a thorough understanding of the foundation model. Some of the questions to ask may be:\n/SM590000What sources of data are used to train the model?\n/SM590000What are the terms and co nditions under which this  model will be made available?\n/SM590000Are there contractual limitations on the use cases that the model can be used for (facial \nrecognition, military, any non-research use)? License Name Summarization\nApache 2.0 License A permissive free software license that allows users to use, modify, \nand distribute the licensed software, including in commercial \nproducts, without paying royalties to the original authors.\nLlama Community License Similar to Apache 2.0 but in addition, if the monthly active users of \nthe products or services made avai lable by or for the licensee is \ngreater than 700 million monthly active users in the preceding \ncalendar month, the licensee must  request a license from Meta",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 151,
            "total_chunks": 245
          }
        },
        {
          "id": "a56bcd57-a120-4267-a060-e12db2bfda2d",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "onthly active users in the preceding \ncalendar month, the licensee must  request a license from Meta, \nwhich Meta may grant to the licensee in its sole discretion. \nMIT License Similar to the Apache 2.0 license. This license is succinct and \npermissive for ensuring open ownership of software. \n\nChapter 4. Onboarding a new foundation model 45/SM590000Are there indemnification limitations (such as, no indemnification for customer-facing \nuse)?\n/SM590000What is the carbon footprint of the training for this model?\nBy answering these and other questions, a legal team may make the educated decision as to \nwhether or not to onboard the foundation model.\nIntellectual property ownershi p in generative AI deployments\nBeyond the ownership of the foundation models themselves, several critical intellectual \nproperty considerations arise when deploying generative AI. These concerns extend to the \nsoftware used to package and deploy these models, as well as the input data provided and the output gene",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 152,
            "total_chunks": 245
          }
        },
        {
          "id": "8614bdac-b5f8-4456-85d4-7a37df27d2e7",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ware used to package and deploy these models, as well as the input data provided and the output generated. Therefore, thorough discussions with legal counsel and the model/software provider are essential. Key questions to address include:\n/SM590000Training Data Ownership and Licensing: Does the model provider possess clear legal \nrights (including ownership or valid licenses) to use all data incorporated into the model's training dataset? This is crucial for avoidi ng potential copyright infringement claims.\n/SM590000Copyrighted Material Exclusion: What sp ecific measures has the model provider \nimplemented to identify and exclude copyrighted material from its training data? Understanding their methodology is vital for assessing the risk of IP issues.\n/SM590000Customer Data Usage for Model Training: Does the model provider use customer data to \nfurther train or refine its models? If so, what mechanisms are in place to allow customers to opt out of such usage? Transparency and control o",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 153,
            "total_chunks": 245
          }
        },
        {
          "id": "63195bb9-cb08-455d-bb7e-8318ea58093e",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "what mechanisms are in place to allow customers to opt out of such usage? Transparency and control over data usage are paramount.\n/SM590000Fine-Tuned Model Ownership: If the model pr ovider offers fine-tuning services, who \nretains ownership of the intellectual property rights in the resulting fine-tuned model: the provider or the customer? This needs to be clearly defined in contractual agreements.\nThese points should be carefully reviewed to ensure compliance with in tellectual property law \nand protect the interests of all parties involved. More on this will be covered in section “Legal \nindemnification” on page 46.\nLocal and national law implications\nAI legislation has advanced at different rates throughout the world. The only thing that is \ncommon across all government bodies is that they are all putting forth some effort to regulate AI usage. It is not in the scope of this book (and would be challenging) to document all of the world's efforts to regulate AI. This section will pro",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 154,
            "total_chunks": 245
          }
        },
        {
          "id": "ae7c7de3-da73-4af7-843b-10ce981058fd",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "d would be challenging) to document all of the world's efforts to regulate AI. This section will provide a brief introduction to  two different \nmethods that have been used to implement AI Regulation - State level, and National level. At the state level, we will quickly introduce how the United Stat es has been implementing AI \nregulation. At a national level, we mention the EU AI act in Western Europe. \nThis section has two following subsections - Implications in the United States, and \nImplications in Western Europe. These two examples are meant to serve as a comparison to how a government may enforce its AI policies. Legal teams should consult their respective Local and National laws for specifics.\nImplications in the United States\nBefore approving any foundation model, it is critical to review local and national regulations as it applies to the industry. In the US, many state laws are beginning to form which work to govern the usage of AI. The legal team should be aware of any rele",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 155,
            "total_chunks": 245
          }
        },
        {
          "id": "edcda99e-45be-4907-82eb-0b3a2c1505aa",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "e beginning to form which work to govern the usage of AI. The legal team should be aware of any relevant law and perform the appropriate questioning to cover these. Some  examples of passed legislation in the US \ninclude:\n\n46 Ensuring Trustworthy AI with IBM watsonx.governance/SM590000NYC Local Law 144 - Implemented in 2021, the first law in the United States requiring bias \nauditing against AI tools. New York has propo sed more than 30 additional AI laws since \nthen, with almost all still in Propo sal or Failed as of this writing.\n/SM590000Tennessee ELVIS Act - Signed into law on March 21, 2024. This law protects the voices of \nArtists from all disciplines against AI-generated deepfakes created without their permission.\n/SM590000Colorado Consumer Protections for Artificial Intelligence Ac t - Signed into  law May 17, \n2024. The act requires high-risk AI systems to use reasonable safeguards to protect consumers from \nalgorithmic discrimination .\nIn October of 2022, The United States Fe",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 156,
            "total_chunks": 245
          }
        },
        {
          "id": "1bbd927f-36e4-44e1-acb8-c94860497fed",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "rds to protect consumers from \nalgorithmic discrimination .\nIn October of 2022, The United States Federal Government also published guidance which \ncould serve for legal conversation s under the name of “Blueprint for an AI Bill of Rights.” This \ndocument outlines important topics which should be considered by the larger team when onboarding a Foundation Model. As of this writing, the blueprint serves only as guidance and does not enforce any official Federal law.\nImplications in Western Europe (The European Union)\nFor completeness of consideration, the legal team must review any candidate Foundation Model and how it relates to the EU AI Act. Th e Act defines a specific set of obligations for \nproviders of general purpose AI models (as foundation models are called in its legal terminology). In addition, the European Union will provide an as sociated Code of Practice \nwhich is planned to be  approved in or before May 2025. The ob ligations and the Code will \ngive buyers some concrete ex",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 157,
            "total_chunks": 245
          }
        },
        {
          "id": "f5a5545f-b1d2-4a9e-896b-660586c7357a",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "be  approved in or before May 2025. The ob ligations and the Code will \ngive buyers some concrete expectation towards a provider of a model that's brought onto the market after the Act has gone into force (August 2024).\nNote that foundation models already on the market before entry into force, need to be \ncompliant by August 2nd, 2027. Vendors might decide to withdraw their model from the market before that date to avoid the compliance implications. Don't wait too long to engage your existing model providers to understand th eir intent and plan for compliance, and make \nsure you have a plan yourself to switch models when needed.\nUsers of foundation models have no specific obligations, beyond what might result from the AI \nsystem(s) that use that foundation model.\n4.2.2  Legal obligations on the part of the vendor\nAny vendor of AI is tied to the same local and national laws as mentioned earlier. In addition, \nmost of the earlier section around model licensing also applies to the vendor.",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 158,
            "total_chunks": 245
          }
        },
        {
          "id": "412cebcc-0611-449a-b1d2-7453a93c0491",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "arlier. In addition, \nmost of the earlier section around model licensing also applies to the vendor. One more important topic left to cover is the topic of Legal Indemnification. In law,  indemnification (or an \nindemnity) is any undertaking by one party to protect another party of some financial burden.\n In AI, the usage of foundation models can expose parties to novel financial burdens. As such, \na major effort has been put forth around the idea of legal indemnification from AI models. This next subsection will cover this point.\nLegal indemnification\nAI has been controversial in its effects and re percussions. Countless lawsuits have risen \nagainst AI companies, because of this, a growing need of consumers of AI is to be indemnified against any legal repercussions. Every AI producer has a different position on indemnification and therefore, specifics must be explored on a per-foundation model basis. By approaching the legal review of Foundation Models in a systemic way, the legal tea",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 159,
            "total_chunks": 245
          }
        },
        {
          "id": "6ceb8412-05c9-4a52-aae0-4527a73ea419",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "n model basis. By approaching the legal review of Foundation Models in a systemic way, the legal team can ensure that an AI program will be implemented with proper co ntrol and in a safe and legal \nway.\n\nChapter 4. Onboarding a new foundation model 474.2.3  A final note on  legal considerations\nAs was covered in the earlier sect ion of the US Blueprint for AI Bill of Rights, no t all measures \nof AI safety are required by law. Legal consider ations typically follow the letter of the law, but \nthis is not the end of the story for what dangers an un-controlled AI system could pose. Even with all relevant legal details be ing covered, th e ethics behind any AI  effort must still be \nquestioned. \nIt is also important to note that many laws a nd regulations are vague in their definitions and \nguidance. Similar to other types of technology-focused requirements, those for AI require broad statements as to not grow stale shortly after going into effect given the current speed of evolution in t",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 160,
            "total_chunks": 245
          }
        },
        {
          "id": "fba26908-94ca-4fb9-a453-89099c692c7b",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ments as to not grow stale shortly after going into effect given the current speed of evolution in the industry. For these reasons it is important to approach any requirement that is new or without established precedent with a conservative and ethical mindset.\nThe next section 4.3, “Ethical considerations for approving a new foundation model” on \npage 47 will focus on this topic of AI ethics.\n4.3  Ethical considerations for approving a new foundation \nmodel \nWith AI usage gaining momentum, there is an increased focus on the ethical aspect of \nunderstanding how a foundation model is trained and any ethical risk it can expose if it is in used as part of an enterprise use case. When considering a new foundation model, an ethical stakeholder will want to consider at least the following dimensions.\n4.3.1  Fairness\nAn AI system or a model should be fair and free of any direct or indirect bias in its prediction. \nWith the AI system using a foundation model, the foundation model itself needs t",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 161,
            "total_chunks": 245
          }
        },
        {
          "id": "f026d362-1268-49a8-a7df-07b1143af026",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "in its prediction. \nWith the AI system using a foundation model, the foundation model itself needs to be evaluated for fairness.\nBias can be detected by evaluating the found ation model for differences in accuracy and \nperformance by using social de mographic-protected attributes.\nA model can exhibit fairness issues in various wa ys. If the model is looked at in isolation and \nwithout sufficient context, it might not appear to exhibit a favorable or unfavorable outcome. In other cases, the model output can decide the prioritization level and can introduce bias in the entire end-to-end process where the source of bias is tied to the model behavior. \nIt is critical that the business owners, stakeho lders, and designers look at the model in the \ncontext of the overall ecosystem where the training, testing, or the production of data is generated, how the model is developed and evaluated, and where the model is being used to decide different ways in which the model might lead  to a biased o",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 162,
            "total_chunks": 245
          }
        },
        {
          "id": "b244cb46-903f-4043-bc45-f6becddd7ed5",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " where the model is being used to decide different ways in which the model might lead  to a biased outcome. This is a key step in \nthe overall methodology relating to bias detection.\nIt is important to note that bias in some attributes of a model might be reasonable. For \nexample, a foundation model that is fine-tuned to pre-screen loan applications might be biased against people with poor credit. This is deemed as reasonable. Since most model bias \nis injected unwittingly through skewed data or constrained training methods, it cannot be effectively detected and resolved exclusively through manual testing or checklist validations. \n\n48 Ensuring Trustworthy AI with IBM watsonx.governance4.3.2  Transparency\nWith AI usage gaining momentum, there is an increased focus on the transparency aspect of \nthe foundation models for audit purposes. There are various governance, risk, compliance, or regulatory needs for information covering the nature and intended uses of the foundation model. Trans",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 163,
            "total_chunks": 245
          }
        },
        {
          "id": "815a88e8-508c-4fe5-909e-b09245dd148b",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "egulatory needs for information covering the nature and intended uses of the foundation model. Transparency about the model's overall accuracy, its ability to explain particular \ndecisions, its fairness regarding protected cl asses, and information about the provenance of \ntraining data and assurances that suitable privacy protections have been maintained, all should be properly documented and available for audit purposes.\nTransparency builds the trust in the AI system by increasing the unde rstanding of how the \nmodel was created and deployed and enabling th e ability to control how AI is created and \ndeployed. This can prevent undesirable situations, such as a model training with unapproved data sets, models having biases, or mode ls having unexpected performance variations. \nThis documentation of facts about the foundation model (for example model cards) can have \nthe following properties:\n/SM590000It can vary in content and are tailored to the particular foundation model being \ndoc",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 164,
            "total_chunks": 245
          }
        },
        {
          "id": "da464ed1-8fc2-4284-a97e-8e97335e7d76",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ties:\n/SM590000It can vary in content and are tailored to the particular foundation model being \ndocumented.\n/SM590000It is tailored to the needs of their target audience or consumer documenting the tasks the \nfoundation model can be used for.\n/SM590000It captures the details about the data the foundation model has been trained on.\n/SM590000It includes the information about training algorithms, parameters, fairness constraints or \nother applied approaches, and features.\n/SM590000It shows the benchmark accuracy documenting the performance of the model.\n/SM590000It contains information about the model owner (organization), model version, as well as the \nlicense model can be used with. \n4.3.3  Privacy\nAn important aspect to build trust in an AI system or model is to take measures to manage and safeguard foundation models and its data that is trained on Personal Information (PI). If a model is trained on PI without applying any sp ecific privacy techniques, and that model is \nmade publicly",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 165,
            "total_chunks": 245
          }
        },
        {
          "id": "6b1ebf63-bc13-4172-8201-a3f8b07dd25d",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "is trained on PI without applying any sp ecific privacy techniques, and that model is \nmade publicly available or shared with a nontrusted third party, the model might reach the wrong hands, and potentially violate the privacy of the people whose information was used in \ntraining it. PI must be properly handled and safeguarded wherever it is stored or used in the organization.\nThe same safeguards must be applied for models trained on proprietary, intellectual, \ncopyrighted and confidential information.\nModel providers must implement measures to prevent data leakage during inference. This \nincludes:  \n/SM590000Data minimization: Process only the absolute minimum amount of user data necessary for \nthe model to function.\n/SM590000Differential privacy: Apply noise to user data or model outputs to enhance privacy and \nmake it difficult to identify or isolate individual contributions.\n/SM590000Federated learning: Train models collaborati vely across multiple  decentralized data \nsources usin",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 166,
            "total_chunks": 245
          }
        },
        {
          "id": "2ad549c3-f0b3-43a9-9d62-18a5c0d1cc74",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "0Federated learning: Train models collaborati vely across multiple  decentralized data \nsources using techniques like federated learning, thereby avoiding the risks associated with centralizing sensitive user data\n\nChapter 4. Onboarding a new foundation model 49/SM590000Model monitoring: Continuously monitor model behavior for signs of unexpected data \nleakage or privacy violations.\n/SM590000Transparency and user control: Provide users with clear information about how their data \nis used by the model and offer options for controlling data access and sharing.\n4.3.4  Explainability\nAI systems are increasingly us ed to inform high stakes de cisions. Explainability and \ninterpretability of these systems and the models within the system are beco ming essential. \nThere are many ways to explain these models and systems, the appropriate choice depends \non the usage context and type of explanatio n that is needed by the consumer of the \nexplanation.\n4.3.5  Robustness\nAn AI system is considered ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 167,
            "total_chunks": 245
          }
        },
        {
          "id": "380a209f-8f48-4b71-bc28-1628cb312a28",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " n that is needed by the consumer of the \nexplanation.\n4.3.5  Robustness\nAn AI system is considered robust if it can continue to perform well and reliably even when \nfaced with difficult or unexpected situations. These situations can include anything from slight changes in the data it receives  to deliberate attempts to tr ick or manipulate the system. A \nrobust AI system is designed to handle these challenges effectively, minimizing mistakes and providing consistent and trustworthy results.\n4.3.6  Third-party help\nThere are third-party evaluations to help you und erstand various ethical characteristics of a \nfoundation model you are considering using.  Use these tools to speed up your model \nonboarding process.\nTwo examples from the Standard Center for Research on Foundation Models:\n/SM590000Foundation Model Transparency Index  - This index scores foundation models on 100 \ntransparency indicators ac ross several dimensions.\n/SM590000AIR-Bench  (short for “AI risk benchmark” - This ben",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 168,
            "total_chunks": 245
          }
        },
        {
          "id": "943c10dc-2c8c-45d3-81fd-b21f309eae90",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "indicators ac ross several dimensions.\n/SM590000AIR-Bench  (short for “AI risk benchmark” - This benchmarks scores foundation models on \nan extensive safety taxonomy that covers co ntent safety risks, societal risks, legal and \nrights-related risks, and syst em and operational risks.\nThere are many ot her options available. A quick web sear ch will give you plenty of options to \nconsider.\nThe IBM AI Ethics Board has published an extensive overview of AI risks, called the AI Risk \nAtlas . It lists several specific risks in the ethics dimensions listed in this paragraph. Each risk \ncomes with a description, categorizations, and links to examples of that risk in third-party publications whenever possible.\n4.4  Considerations for financial stakeholders for approving a \nnew foundation model \nA stakeholder from a Finance function will want to consider at least the following dimensions \nwhen considering a new foundation model.\n\n50 Ensuring Trustworthy AI with IBM watsonx.governance4.4.1  Tota",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 169,
            "total_chunks": 245
          }
        },
        {
          "id": "4d4c869d-d8d0-4f0c-a943-f167ba7f08b1",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "nsidering a new foundation model.\n\n50 Ensuring Trustworthy AI with IBM watsonx.governance4.4.1  Total cost of ownership\nFirst, what is the sum total of the costs associated with acquiring and using another \nfoundation model? Many models are available un der a “free” license, but there's more to the \ntotal cost of ownership than that.\nWhat is the init ial investment?\n/SM590000What are the initial license costs to acquire the foundation model? This might take the \nform of a license for a model specifically, but could (also) include licenses for a platform that the model is hosted on. Based on existi ng vendor relationships, discounts might be \navailable. As the market for generative AI evolves, new pricing models might appear.\n/SM590000What are the costs to onboard this new model ? Consider procurement, IT, legal, security \nand other cost components. The assessment that is the topic of this chapter is part of such an onboarding process and comes with a cost.\n/SM590000What are the costs o",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 170,
            "total_chunks": 245
          }
        },
        {
          "id": "ecdadd3f-85d2-4227-bd35-628f9c1c1e7e",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "s chapter is part of such an onboarding process and comes with a cost.\n/SM590000What are the costs of skills development to use this new m odel effectively? Consider \ntraining for administrators, data scientists, risk management and other relevant roles.\nWhat are the run costs?\n/SM590000If the model is deployed as SaaS, what are the charges for inferencing? Foundation model \ninferencing is typically charge d by the number of calls to the model and/or the number of \ntokens processed/generated by the model. Pricing models might differ, as does the amount charged for comparable numbers of calls/tokens.\n/SM590000If the model is deployed in-house, what are the total costs over the foreseeable lifetime of \nthis investment? Consider hard ware, electricity, cooling, personnel and all other relevant \ncost components. These costs will depend on the technical specifications of the model, \nsuch as the number of parameters or the model architecture.\n4.4.2  Return on investment\nSecond, how will the ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 171,
            "total_chunks": 245
          }
        },
        {
          "id": "14d16ded-e700-4921-9206-03f19fc287d3",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "he number of parameters or the model architecture.\n4.4.2  Return on investment\nSecond, how will the investm ent in another foundation model pay back for itself?\nExisting use cases\nWill this foundation model allow us to execute existing use ca ses better/cheaper/faster? How? \nHow much? How well does it align with our st rategic imperatives? Consider the following \nfactors, and the interplay between them:\n/SM590000Improve the accuracy of an existing application.\n/SM590000Improve legal indemnification for an existing application.\n/SM590000Improve the ease of governing one or more existing applications.\n/SM590000Reduce AI ethics risk for an existing application.\n/SM590000Reduce inferencing costs fo r an existing application.\n/SM590000Reduce energy consumption for an existing application.\nNew use cases\nWill this foundation model allow us to enabl e new use cases? How? How much? How well \ndoes it align with our strategic imperatives? For example:\n/SM590000The new model has been tuned to hand",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 172,
            "total_chunks": 245
          }
        },
        {
          "id": "0fa8d8c2-ef03-4701-871c-3ac16e9c3f45",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " it align with our strategic imperatives? For example:\n/SM590000The new model has been tuned to handle IT optimization use cases better than our \nexisting models, so we can now enable our IT developers to use generative AI and enhance their productivity by X%.\n\nChapter 4. Onboarding a new foundation model 51/SM590000The new model has been tuned to handle the Dutch language better than our existing \nmodels, so we can now enable a conversational assistant for our Dutch customers and improve first-time resolution by Y%.\n4.4.3  Build or buy\nInstead of creating a solution ourselves with a new foundation model, can we buy tooling that already does that? As more applications become AI-enabled, the benefits of buy rather than build could be:\n/SM590000Achieve the projected benefits faster with a turn-key application.\n/SM590000Enhanced functionality from a specialist vendor compared to what we could deliver in a \nfirst stage ourselves.\n/SM590000Focus scarce internal AI resources on  the most str",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 173,
            "total_chunks": 245
          }
        },
        {
          "id": "08d10434-14b0-4731-9c3a-50899ba5b9ef",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ld deliver in a \nfirst stage ourselves.\n/SM590000Focus scarce internal AI resources on  the most strategic AI projects.\n4.4.4  Exit strategy\nThe speed of innovation in AI is very high, how easily can we change course if something \nbetter comes along? Consider aspects such as:\n/SM590000Contractual cancellation periods.\n/SM590000Non-recoverable license, support or services costs already committed.\n/SM590000Early termination fees.\n/SM590000Charges to extract our data/IP/solutions from a vendor's platform.\n/SM590000Portability of assets and skills to a new solution.\n4.4.5  Other factors\nLastly, there might be factors not yet considered  by other stakeholders that would fall to the \nfinance team in an organization. For example:\n/SM590000Risks - many risks will already have been addressed by other stakeholder s as described in \nthe previous paragraphs in this chapter, but there might be specific ones to be addressed by the Finance function.\n/SM590000Sustainability - how does a new foundation",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 174,
            "total_chunks": 245
          }
        },
        {
          "id": "7ac29d83-6256-4430-bbad-e4255f180732",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ic ones to be addressed by the Finance function.\n/SM590000Sustainability - how does a new foundation model impa ct the organization's ESG \n(environmental, social, and governance) posture? How does it impact our direct and indirect emissions or freshwater usage?\n/SM590000Security - How does the model vendor protect the organization against adversarial \nattacks, data leaks and other security risks?\n\n52 Ensuring Trustworthy AI with IBM watsonx.governance\n\n© Copyright IBM Corp. 2025. 53Chapter 5. Assessing a new use case\nA use case in watsonx. governance is the starting point to solve a business problem using an \nArtificial Intelligence (AI) asset, such as a mode l or prompt template as  part of the solution. \nThe process of assessing a new use case for an organization involves following a business process facilitated by a workflow engine. This  process identifies risks, assesses applicable \nregulations and policies, and decides whether to approve or reject the use case from development th",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 175,
            "total_chunks": 245
          }
        },
        {
          "id": "162af76c-b0ed-45c6-aea1-a7e716113fe7",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "\nregulations and policies, and decides whether to approve or reject the use case from development through production to decommissioning.\nThis chapter covers the following topics:\n/SM590000“Business process workflow” on page 54\n/SM590000“Approval workflow” on page 55\n/SM590000“Risk identification assessment” on page 56\n/SM590000“Applicability assessment” on page 585\n\n54 Ensuring Trustworthy AI with IBM watsonx.governance5.1  Business process workflow \nThe typical business process to assess a new use case, shown in Figure 5-1, combines \nautomated workflows in watsonx.governance with manual checks and balances to ultimately \napprove or reject a use case and record the transparent process and findings along the way. This process can always be customized to fit an organization's specific needs.\nFigure 5-1   Typical use case assessment process workflow leveraging watsonx.governance\nThe typical process flows as follows:\n/SM590000Propose the use case: The process starts with a member of the bu",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 176,
            "total_chunks": 245
          }
        },
        {
          "id": "cd68a0ee-84a8-4f53-be69-886d7c1df14a",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " process flows as follows:\n/SM590000Propose the use case: The process starts with a member of the business proposing a use \ncase within the watsonx.governance console. The creator could be any member of the business who originates the problem to be solved. Typically, this would be a member of a business line, but could also be technical depending on the problem to be solved.\n/SM590000Questionnaire Assessments: Once the use case is created, watsonx.governance creates \nassessment questionnaires to be completed by the business line and necessary technical teams. The answers to these questions allo w watsonx.governance to auto-populate the \nuse case with applicable risks from the IBM AI  Risk Atlas that will ne ed to be addressed \nduring the life of the use case. \n/SM590000Applicability Assessments: It is important to ensure your us e case will rema in compliant \nwith external regulations, such as the EU AI Act in Europe. Once a use case is created, \nwatsonx.governance will create an EU AI",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 177,
            "total_chunks": 245
          }
        },
        {
          "id": "e22b0d9c-ddaf-41b7-be7e-c11b0cae96d2",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "uch as the EU AI Act in Europe. Once a use case is created, \nwatsonx.governance will create an EU AI Act applicability assessment to determine if the \nuse case is complaint or at risk of violating the mandates of this regulation. While the EU AI Act Assessment is installed with watsonx.governance, additional assessments are available from IBM partners for regulations that may be applicable in other regions.\n/SM590000SME Review: Once the use case and the appropriate assessments are completed, a \nsubject matter expert (SME) will review the auto -populated risks and further enrich the use \ncase with mandates, processes, and policies that could be affected by the identified risks. The SME can also setup controls to address issues that arise after the use case is approved for the next steps in the development lifecycle. watsonx.governance can help to automate the assignments of the mandates, processes, policies and controls through custom workflow created to accommodate the specific needs o",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 178,
            "total_chunks": 245
          }
        },
        {
          "id": "85d25cc1-03ed-44ed-80e7-0f93d6116160",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "processes, policies and controls through custom workflow created to accommodate the specific needs of the organization.\n\n\nChapter 5. Assessing a new use case 55/SM590000Stakeholder Review & Approval: Once SME review is complete, required business \nstakeholder will be automatically notified by wa tsonx.governance  to review the use case \nand approve or reject the case . They will also be asked to provide comments to explain the \nreview decision. The number of stakehol ders notified can be automated via a \nwatsonx.governance workflow or pre-defined in a use case template depending on organization needs.\n/SM590000Use Case Approved for Development: If all stakeholders approve of the use case, \nwatsonx.governance marks it as “Approved for Development”. This provides audited \nauthorization for development of the solution to begin and marks the end of the initial use case assessment.\n5.2  Approval workflow\nOnce a use case is created in the watsonx. governance console, a use case approval \nwor",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 179,
            "total_chunks": 245
          }
        },
        {
          "id": "8e1b9c7a-6a6e-40a0-b95b-7294e1b736c4",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "val workflow\nOnce a use case is created in the watsonx. governance console, a use case approval \nworkflow is triggered by the watsonx.gove rnance console, as shown in Figure 5-2.\nFigure 5-2   The default use case approval workfl ow executed in the watsonx.governance console\nThe use case approval workflow and a revi ew and approval process that collects key \ninformation about the use case and captures the approval or rejection of the use case from the identified stakeholders, guiding those users through all required actions along the way.\nWhile any workflow can be customized to the needs of any organization, the steps in the \ndefault use case approval workflow are:\n/SM590000Start: This step in the flow begins the moment a new use case is created. The creator is \nrequired to provide a name for the use case, and use case owner, a description of the use case, and a primary Business Entity. Optionally, the creator may also provide the purpose of the use case and its type. There are two defa",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 180,
            "total_chunks": 245
          }
        },
        {
          "id": "7a6f6d63-ab05-4707-ae5c-742f894ef83f",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ptionally, the creator may also provide the purpose of the use case and its type. There are two default use case types: AI or Non-AI.\n/SM590000Use Case Data Gathering: Once the use case is created, the approval workflow moves to \ngather more detailed information about the use case to be governed. At this stage, the use case owner or risk assessor can provide the initial assumed risk level, identify the stakeholders who will provide fi nal approval, and identify the technical owner of the use \ncase. It must also be determi ned if the use case will lever age foundation mo dels during \nthis stage in the workflow. Once all the required information is complete, the user can submit the case for Initial Approval.\n\n\n56 Ensuring Trustworthy AI with IBM watsonx.governance/SM590000Initial Approval: In this stage, the use case owner completes a Risk Identification \nassessment questionnaire and validates a series of risks automatically assigned based on answers provided. In addition, the use case o",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 181,
            "total_chunks": 245
          }
        },
        {
          "id": "e485e9ec-f31f-4240-8807-a3e3762ab606",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ates a series of risks automatically assigned based on answers provided. In addition, the use case owner can optionally complete and applicability assessmen t questionnaire for the EU AI Act. Completion of the Risk \nIdentification assessment is required to move to the Stakeholder Review stage.\n/SM590000Stakeholder Review: At this stage, the stakeholders identified in the Use Case Data \nGathering stage are notified to review the use case details, provide comments and approve or reject the use case for development. The use case cannot be approved unless all stakeholders have provided approval. On ce all stakeholders have approved the use \ncase, it is placed in “Approved for development” status. The owner and technical owner of the use case are automatically notified when the use case has been approved for development.\n5.3  Risk identification assessment\nAs part of the Initial Approval stage of the us e case assessment workflow, the use case owner \nor risk assessor completes a risk identi",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 182,
            "total_chunks": 245
          }
        },
        {
          "id": "0fbb7dfd-2b4a-4dce-b3a5-f1a6e1b29aa8",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "e of the us e case assessment workflow, the use case owner \nor risk assessor completes a risk identifi cation assessment created by an assessment \nworkflow in the watsonx governance console, as seen in Figure 5-3.\nFigure 5-3   Risk Identification Workflow as  seen in the watsonx.governance console\nA risk identification assessment is a dynamic q uestionnaire that identifies applicable risks \nfrom the IBM Risk Atlas based upon answers provided as seen in Figure 5-4 on page 57. \n\n\nChapter 5. Assessing a new use case 57Figure 5-4   Dynamic assignment of use case risks based on risk assessment answers\nRisks assigned are determined based on a “if this , then that” selection structure as seen in \nFigure 5-5.\nFigure 5-5   Dynamic risk applicability  assignment selection structure rules\nOnce assigned, each risk must be evaluated and appr ove or rejected for applicability for the \nuse case. These risks are the primary bases for assessing and managing risk throughout the lifecycle of your use cas",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 183,
            "total_chunks": 245
          }
        },
        {
          "id": "edc503f3-eeb5-4206-9a1d-b2d396e78b22",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "risks are the primary bases for assessing and managing risk throughout the lifecycle of your use case. Wh ile risks are assigned from th e IBM Risk Atlas by default, \nadditional risks or risk sources can be added as required by an organization.\nOnce all assigned risks have been approved or re jected, the use case can be approved for \nstakeholder review and defined in the primary use case approval workflow.\n\n\n58 Ensuring Trustworthy AI with IBM watsonx.governance5.4  Applicability assessment\nApplicability assessment, as seen  in Figure 5-6, should also be completed by the use case \nowner as part of the Initial Approval stage of the use case assessment workflow.\nFigure 5-6   Applicability Assessment Workflow  as seen in the watsonx.governance console\nThis assessment enables the use case owner to assess their AI use cases using a simple \ndynamic questionnaire that aids in determining whether a use case is in scope for government regulations (such as the EU AI Act ) and which risk category",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 184,
            "total_chunks": 245
          }
        },
        {
          "id": "201218f7-0b80-439a-a7e4-272a920523ac",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "r a use case is in scope for government regulations (such as the EU AI Act ) and which risk category the use case aligns \nto (Prohibited, High, Limited, Minimal,  Out of Scope) as seen in Figure 5-7. \nFigure 5-7   Dynamic assignment of risk category based on applicability assessment \n\n\nChapter 5. Assessing a new use case 59The questionnaire to determine the risk category follows a “if this then that” selection \nstructure as seen in Figure 5-8.\nFigure 5-8   The questionnaire to determine the risk category\n\n\n60 Ensuring Trustworthy AI with IBM watsonx.governance\n\n© Copyright IBM Corp. 2025. 61Chapter 6. Governing the end-to-end \nlifecycle of an AI asset\nLifecycle governance is a majo r pillar of the IBM AI Govern ance framework. This pillar \nrepresents the idea that any AI asset should be tracked throughout the lifetime of its usage, without gaps in lineage  or traceability. \nRegardless of where the AI asset originates from, watsonx.governance comes with tooling to \nmonitor performance a",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 185,
            "total_chunks": 245
          }
        },
        {
          "id": "a34f20f3-176a-4b62-9c0a-3eb5fc6c00e4",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " where the AI asset originates from, watsonx.governance comes with tooling to \nmonitor performance and also to  monitor the lifecycle stage that  the asset is in. Lifecycle \ngovernance applies to traditional ML/AI use cases as well as generative AI.\nEnd-to-end lifecycle govern ance results in increased efficiency through automated \nevaluations of models as well as the publishing of evaluation results and metrics. This also introduces traceab ility, auditability, and en hanced project management  capabilities through \ntransparent ML/AI asset lifecycle management functionality. \nThis chapter has th e following sections:\n/SM590000“What is the AI lifecycle?” on page 62\n/SM590000“Metrics in watsonx.governance” on page 64\n/SM590000“How to implement Lifecycle Governance” on page 66\n/SM590000“Lifecycle implementation and considerations” on page 676\n\n62 Ensuring Trustworthy AI with IBM watsonx.governance6.1  What is the AI lifecycle?\nBefore reading through how watsonx.governance defines the lif",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 186,
            "total_chunks": 245
          }
        },
        {
          "id": "a2eefc26-f90f-40c2-afa3-2987a7308064",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "vernance6.1  What is the AI lifecycle?\nBefore reading through how watsonx.governance defines the lifecycle of an AI asset, it is \nuseful to review the general lifecycle for data projects. \nFigure 6-1 represents the CRISP-DM methodolog y which encapsulates an industry-standard \nblueprint for completing data mining or data science projects.\nFigure 6-1   The CRISP-DM Methodology (Cross- industry Standard Process for Data Mining)\nBecause AI is a form of machine learning (albeit a very large and complex version), every \nstep of the CRISP-DM applies to the AI lifecyc le during initial found ation model development \n(commonly referred to as model pretraining.) Differences begin to appear when we consider an AI asset after it has been developed. It is unreasonable to expect every enterprise to train their own model from scratch. For this reason, in virtually every case, citizen engineers and scientists will engage with AI asse ts which have already been pr e-trained, and therefore, a \nmodified",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 187,
            "total_chunks": 245
          }
        },
        {
          "id": "0e72131b-2ff7-42db-b549-168018853e66",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "entists will engage with AI asse ts which have already been pr e-trained, and therefore, a \nmodified version of the CRISP-DM methodology should be used when considering the lifecycle of an AI asset.\nThe pre-trained nature of foundation models allows us to create a more focused view on the \nlifecycle of AI assets being used in use case s. This lifecycle can be generally depicted with \nthree chronological stages:  Develop stage, Validate stage, and Operate stage .\nFigure 6-2 on page 63 highlights the general lifecycle stages in a red box.\n\n\nChapter 6. Governing the end-to-end lifecycle of an AI asset 63Figure 6-2   A Sample Lifecycle Diagram\nWithin each stage of this AI lifecycle, action s should be taken by users to progress the AI \nasset through its lifecycle. The following section cove rs the high-level activities that take place \nwithin each of the three stages.\nDevelopment stage\nIn the development stage, users must work to stand up the initial AI use case solution. As \ndescribed in ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 188,
            "total_chunks": 245
          }
        },
        {
          "id": "021e2a1c-0770-4f99-8280-7bb904848b98",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "e development stage, users must work to stand up the initial AI use case solution. As \ndescribed in Chapter 4, “Onboarding a new foundation model” on page 41 and Chapter 5, “Assessing a new use case” on page 53, the AI use case will go through an approval process \nbefore being worked on by the appropriate pr actitioners. Once the approval process is \ncomplete, an engineer may begin to develop the technical assets for the AI use case. The engineer can achieve many things in this st age. Prompt engineering, parameter tuning, and \nsolution experimentation all fall within the deve lopment stage. For trad itional ML, this stage \nalso includes activities such as feature en gineering, exploratory data analysis, and other \npre-processing tasks which go into the model development process.\nValidation stage\nAfter development efforts are co mplete, the validation stage of the life cycle is started. This \nstage represents the process of putting development efforts through testing. An independent pr",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 189,
            "total_chunks": 245
          }
        },
        {
          "id": "7bd29ac1-960e-4098-9653-2c379e683dae",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "This \nstage represents the process of putting development efforts through testing. An independent practitioner such as an AI engineer or data scientist can use IBM's watsonx.governance to \nrun automated Prompt Evaluations, which will re turn evaluation metrics for the given AI use \ncase. A programmer can also use custom code to run evaluations. Note:  In traditional ML use cases, we also split use Testing stages in addition to validation \nstages. This will be covered in t he considerations section for ML.\n\n\n64 Ensuring Trustworthy AI with IBM watsonx.governanceSimilarly, for traditional ML, validation is achieved through the traditional data science \napproach of utilizing validat e/test datasets to unde rstand model performance.\nUsing watsonx.governance, the validation stage represents our first opportunity at automating \notherwise lengthy evaluations of ML and AI models. It is the first glance of how our models may perform against performance metrics such  as readability for Q&A LLM u",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 190,
            "total_chunks": 245
          }
        },
        {
          "id": "982fcefe-bc01-40ea-8e08-b48a7b08f5c3",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " glance of how our models may perform against performance metrics such  as readability for Q&A LLM use cases, or \naccuracy for classi fication use cases.\nOperation stage\nThe operation stage represents any AI or ML asset which reaches production-level efforts. In \nthis stage of the AI lifecycle, the model asset is live and consumed by users. Model \nmonitoring and asset lineage become central at this stage. The IBM watsonx.governance platform provides tools for monitoring critical asset metrics such as drift, fairness, and bias.\nBefore diving into implementation, the next section will briefly review the different kinds of \nmetrics that can be utilized during lif ecycle governance with watsonx.go vernance.\n6.2  Metrics in watsonx.governance\nThe metrics in this section are by no means exhaustive and watsonx.governance is capable \nof implementing custom monitors  and metrics based on user ne eds. The ability to add custom \nmonitors and metrics sets watsonx.governance apart from its competit",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 191,
            "total_chunks": 245
          }
        },
        {
          "id": "c2fad3f4-8d55-4412-aa48-2f8e1de79ec8",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "eds. The ability to add custom \nmonitors and metrics sets watsonx.governance apart from its competitors in the market. This section gives an overview to some of the most common and important kinds of metrics that can be applied to assets for effective lifecycle monitoring.\n6.2.1  Drift detection\nUsers can configure Drift v2 evaluations  in watsonx.governance to measure changes in their \ndata over time to ensure consistent outcomes  for the model. These evaluations can be used \nto identify changes in the model output, the accu racy of your predictions, the distribution of \nthe input features, the metadata and more.\nThe drift in the user deployments is always detected in comparison to a baseline data. This \nbaseline data needs to be a good representation of the ideal dataset that the user is expecting in their deployment. It can be the training data used to train the predictive model, the test data used to validate the model, or even the past production data. As part of the monitor confi",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 192,
            "total_chunks": 245
          }
        },
        {
          "id": "00dc365d-2893-4f97-aec6-2901663166bf",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "test data used to validate the model, or even the past production data. As part of the monitor configuration process, certain computations are learned on the baseline data to learn the data patterns. These can vary from dividing  data in frequency bins to learn the density \nfunctions of your input features ( feature drift ), to training auto-encoders to learn the context \nrepresented by the embeddings ( embedding drift ), training proxy/meta models to learn user \nmodel behavior ( model quality drift ) and to look at how the meta data like character counts and \nword counts is changing ( input and output metadata drift ). Any change in the data is reported \nas a metric on different dimensions. \n6.2.2  Explainability\nFor the predictive AI models, watsonx.governance gives users a sneak peek into the black box by giving localized explanations to understand how the different feature values are impacting the outcomes of the specific transact ions. By aggregating these local explanations \nfor ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 193,
            "total_chunks": 245
          }
        },
        {
          "id": "0cd5fd93-c82a-4bee-8a17-9f0893983ab6",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " impacting the outcomes of the specific transact ions. By aggregating these local explanations \nfor a sample of such transactions, a global explanation is presented so that the user can understand the general factors that are influencing the model decisions. \n\nChapter 6. Governing the end-to-end lifecycle of an AI asset 65To this end, wats onx.governance utilizes both  open-source algorithms ( LIME and SHAP ) and \nIBM Research® built contrastive explanations. By generating and analyzing data points, in \nthe vicinity or the local neighborhood of a given transaction, Local Interpretable Model-agnostic Explanations or LIME can tell which features of a structured record, words \nand phrases from a text paragraph, and which areas of the image are responsible for the model outcome. SHAP (SHapley Additive exPlanati ons) is rooted in game theory as it uses \nthe classic Shapley values, to determine how much of each of the features has contributed to the model prediction. IBM Research built contr",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 194,
            "total_chunks": 245
          }
        },
        {
          "id": "31e13bb9-093c-43f2-834a-ab8fa6475afb",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "e how much of each of the features has contributed to the model prediction. IBM Research built contrastive explanations that look at the neighborhood of a given data point, to determine how much of a delta change is required in the input features to flip the model outcome or to maintain the same outcome. A highly important feature in this case is a feature to wh ich the model is least se nsitive as they require \na large change in the value for model to flip the outcome.\n6.2.3  Model health\nTo understand the model health and performance of a given model deployment (for both predictive AI and generative AI AI-based models), it is imperative to know the how the said deployment is being used. To aid with that, watsonx.governance helps in calculating and visualizing the total number of scoring requests  received by the deployment in a given time \nperiod. Across these requests, common statistical attributes like minimum, maximum, mean, and median of the number of records  are also calculated",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 195,
            "total_chunks": 245
          }
        },
        {
          "id": "807bec86-176d-427e-b3cd-36f3f5d6eb84",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "cal attributes like minimum, maximum, mean, and median of the number of records  are also calculated. It is also important to know how \nmuch time is taken by the system to process a record and/or a request. Similarly, watsonx.governance can also measure throughput of the system by looking at the number of \nrecords and requests processed in a second. For generative AI-based deployments, the input and output token counts processed across the scoring requests can be visualized as well. If \nthere are multiple users registered on the system and using the deployments, watsonx.governance can also present the real-tim e view to see the total number of users and \nthe aggregated views to see the average number of users.\n6.2.4  Generative AI quality\nTo assess the quality of the content generated by a prompt, watsonx.governance has many Generative AI Quality metrics. Some of these metrics work in the presence of a reference input and hence work off the feedback data. However, there are reference f",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 196,
            "total_chunks": 245
          }
        },
        {
          "id": "2490bb08-6fd3-46fa-9399-56b08883b332",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "e presence of a reference input and hence work off the feedback data. However, there are reference free metrics as well, that do not require any reference input and can be calculated on the production data.\nIBM watsonx.governance also allows the use of widely available LLM models to evaluate the \nperformance of the user prompts through the LLM-as-a-judge feature.\n6.2.5  RAG quality metrics\nBy adding external sources of context to the prompt of an LLM, Retrieval Augmented Generation (RAG) systems enhance the quality of the content generated by the model. With this perspective in place, watsonx.governance can monitor both the phases of a RAG-based \nsystem. \nThe retrieval phase can be assessed by looking at the context pulled and seeing how relevant \nit is to the question asked by the user (context  relevance). By looking at retrieval precision, \none can tell if the retrieved information is directly addressing the user query. The system also looks at how the different retrieved contexts a",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 197,
            "total_chunks": 245
          }
        },
        {
          "id": "ea5f76b2-93a0-4a23-98ec-ae423801a582",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " directly addressing the user query. The system also looks at how the different retrieved contexts are ranked. If the most relevant context has the top rank, the metric reciprocal rank will be 1 el se it will be much lower. The ranking quality of \nthe retrieved information can also be measured  by Normalized Discoun ted Cumulative Gain \n(NDCG) as a higher score on this metric, indicates the retrieved contexts are ranked in the correct order. \n\n66 Ensuring Trustworthy AI with IBM watsonx.governanceThe content generation phase can be assessed not only by looking at the overall quality of the \nanswers generated by the system, but also by analyzing the content watsonx.governance can tell how much of the context is used in the answers generated. By measuring how well the answer aligns with the context (faithfulness), or  by measuring if the answer is relevant to the \nprompt (answer relevance) or by looking at the number of questions that were unanswered by the model (unsuccessful requests),",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 198,
            "total_chunks": 245
          }
        },
        {
          "id": "9a7a9ccd-30ce-4fd6-95cb-9794c82567ac",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " or by looking at the number of questions that were unanswered by the model (unsuccessful requests), watsonx.governance gives the quality of the answers. The prominent content analysis metrics measure the percentage of keywords in the answer that are derived from the context (coverage) and the overall sequence of words in the answer are direct extractions from the context (density). \nIn addition to the above metrics, along with th e faithfulness, watsonx.governance also gives \nout the top source attributions for the generated by answer by highlighting the relevant sentences in the context. This capability tries to o pen the black box, and is a step in the \ndirection of providing explainability for the foundation model- generated answers. \nWith a review of the common stages, we can now move onto implementation details for how \nto enable and complete lifecycle monitoring.\n6.3  How to implement Lifecycle Governance\nLifecycle Governance begins at the use case level. In watsonx.governance, ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 199,
            "total_chunks": 245
          }
        },
        {
          "id": "8a118a7c-0341-4112-a592-9603fb1d0367",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ment Lifecycle Governance\nLifecycle Governance begins at the use case level. In watsonx.governance, an AI use case is created and used to maintain a hierarchical organization of AI assets. This AI use case structure allows users to view their ML/AI assets  as they relate to the business problem of the \nuse case. \nOne use case can hold multiple assets with each asset being represented by a factsheet. \nThese factsheets hold all details of a given asset. Consider an AI factsheet to be a sort of “nutrition label” to the underlyin g AI asset. This factsheet is w hat will be used to  represent the \nAI asset as it moves through each stage of its lifecycle.\nThe following sections will show us how to se t up AI use cases and how to create AI \nfactsheets for AI assets.\n6.3.1  Getting started: Setting up your AI use cases\nAfter installation and adminis tration of watsonx.governance,  we can implement lifecycle \ngovernance beginning at the use case level. The purpose of lifecycle governance is mon",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 200,
            "total_chunks": 245
          }
        },
        {
          "id": "08db170a-26f6-4170-b1a9-4ac5855c9188",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "nt lifecycle \ngovernance beginning at the use case level. The purpose of lifecycle governance is monitoring the progress of AI assets individually. These AI assets progress through what is called an AI use case. These AI use cases are central to lifecycle monitoring because it allows us to group AI assets together which are working towards similar goals.\nBefore setting up lifecycle moni toring for the AI assets, we will set up our AI use case in \nwatsonx.governance.\nAI use case and AI  factsheet setup\nFollow these steps to set up the AI use case in watsonx.govenance:\n1. Use the Options menu from the watsonx home page  to access the AI use cases  page \n(under  AI governance → AI use cases ). See Figure 6-3 on page 67.\n\nChapter 6. Governing the end-to-end lifecycle of an AI asset 67Figure 6-3   AI use cases\n2. Create a new inventory using the options menu from the AI use cases  page. \nBe sure to activate Governance Console integration  to ensure that your AI Inventory is \nsynced with you",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 201,
            "total_chunks": 245
          }
        },
        {
          "id": "b7e0f780-e464-47c0-908b-373e232a8646",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ure to activate Governance Console integration  to ensure that your AI Inventory is \nsynced with your Governance console. This synchronization effect persists throughout the work done in this chapter.\n3. Once the new inventory is created, create an AI use case  and fill out all de tails for that AI \nuse case. Any details posted to this AI use case  will also be reflected in the Governance \nConsole. \nWith an AI use case successfully  created, we will move  on to tracking the lifecycle of an AI \nasset for that AI use case. This  will be achieved by attaching AI  factsheets to the AI assets \nrealizing and implementing the AI use case and using that AI factsheet to accomplish lifecycle governance. As mentioned 6.3, “How to implement Lifecycle Governance” on page 66, an AI factsheet is the organizational representation of  an AI asset which gives us all asset details. \nThe factsheet also gives us an effective vehicle for traversing between lifecycle stages.\nCreating an AI factsheet for a gi",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 202,
            "total_chunks": 245
          }
        },
        {
          "id": "2ea592d5-eb18-4f29-b373-43f67f42e263",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "s us an effective vehicle for traversing between lifecycle stages.\nCreating an AI factsheet for a given as set will be covered in de tail in subsequent \nsub-sections. Once the AI factsheet is crea ted, we can move on to lifecycle governance.\nAs described in 6.1, “What is the AI lifecycle?” on page 62, there are general activities in each \nlifecycle stage which should be completed before moving onto the next lifecycle stage. Using \nwatsonx.governance, we can achieve full lifecycle monitoring for the AI asset. The following subsections will cover the basic steps for accomplishing lifecycl e governance using \nwatsonx.governance.\nThe following subsections assume that the read er is experienced with LLM experimentation \nand that they are ready to begin governing and monitoring their AI solutions. Special considerations exist based on implementation det ails such as what platform the model is \nhosted on, and what is the use case. Thes e considerations will be  found under each \nappropriate s",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 203,
            "total_chunks": 245
          }
        },
        {
          "id": "6d765778-88ea-44f6-9b97-e5681c34935a",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "\nhosted on, and what is the use case. Thes e considerations will be  found under each \nappropriate subsection.\n6.4  Lifecycle implementation and considerations\nThis section of the chapter will assume that there is a valid AI use case configured in \nwatsonx.governance. It will also  make the assumption that a solution exists (either on \nwatsonx.ai or another third party)  which realizes that AI use case. One final major assumption \nis the existence of ground-truth data. \n\n\n68 Ensuring Trustworthy AI with IBM watsonx.governanceAs mentioned in the explanation of the validation stage, evaluations provide first-hand \nperformance reviews to assets in a qualitative and quantitative way. To achieve this, ground-truth data is critical and must be acquired in order to effectively implement lifecycle governance.\nThe goal of this section is to explain the connection between the assets (prompt template \nassets or models) that realize the use case, which lifecycle stage those assets belong in, and h",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 204,
            "total_chunks": 245
          }
        },
        {
          "id": "f664166f-b609-47f0-8940-c21a879c6173",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "te \nassets or models) that realize the use case, which lifecycle stage those assets belong in, and how we automate the lifecycle of those assets as they  progress from devel opment through to \noperation.\nBefore diving into specific considerations, here are the higher-level activities which must be \ncompleted to effectively govern the lifecycle of an AI asset. A Prompt Template Asset or a Model must exist that contributes to the implementation of an established use case. A Prompt Template Asset (PTA) can be either a Prompt Te mplate from IBM's watsonx.ai, or a detached \nprompt template representing a prompt on a 3rd party platform. Similarly, a Model is simply any ML model which solves a traditional data  science / ML use case and can be either \ndeployed in IBM watsonx.ai runtime or a 3rd party serving platform. With the asset ready, the steps are as follows:\n/SM590000Track the solution in an AI use case to place in the development stage.\n/SM590000Perform an evaluation to move the asset",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 205,
            "total_chunks": 245
          }
        },
        {
          "id": "bb7d31aa-32ce-42bc-ba14-e0fac86c3445",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "n an AI use case to place in the development stage.\n/SM590000Perform an evaluation to move the asset to the validation stage.\n/SM590000Promote the asset to a production space to move the asset into the operation stage.\nThe above three steps are repeated in some variation based on platform and use case \nspecifics. As a reminder, everyt hing accomplished in this chapter will be reflected across the \nGovernance Console for administrative and business users to be automatically updated of \nprogress.\n6.4.1  UI-driven implementa tion of lifecycle governance\nAfter installation and adm inistration, watsonx.ai s eamlessly integrates with \nwatsonx.governance. Once an AI use case is established, users can run through a typical \nwatsonx.ai workflow to begin experimenting with prompts in the prompt lab, or in a jupyter notebook environment. For instructions on how to perform this experimentation, refer to the watsonx.ai documentation  or the Redbooks publication: Simplify Your AI Journey: Unleashing",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 206,
            "total_chunks": 245
          }
        },
        {
          "id": "2f15cc75-5903-4e13-be47-42ced0d6aebd",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "r to the watsonx.ai documentation  or the Redbooks publication: Simplify Your AI Journey: Unleashing \nthe Power of AI with IBM watsonx.ai , SG24-8574.\nDevelopment\nAfter saving your prompt lab experiment, from  the project home screen of watsonx.ai, view \nyour assets tab. Find your saved prompt lab template, and using the hamburger menu of that prompt lab template, choose Track in AI use case . After following the gu ided click-through \nsetup, your Prompt Template Asset (PTA) will be automatically placed under the development stage, tracked in the AI use case of your choice.\nValidation\nWith a PTA being tracked against an AI use ca se in the development stage, users can utilize \nthe power of watsonx.governance to perform fast and effective evaluations of their assets. To perform this step, validation data is required. Note:  This subsection covers a UI-driven implementation of lifecycle governance using \nwatsonx.gov and watsonx.ai. For code-driven approaches, review 6.2.3, “Model health”",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 207,
            "total_chunks": 245
          }
        },
        {
          "id": "eaffff65-85fa-4232-b1eb-a81cbe0e75e2",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "vernance using \nwatsonx.gov and watsonx.ai. For code-driven approaches, review 6.2.3, “Model health” on page 57 and 6.2.4, “G enerative AI quality” on page 57 . The watsonx.ai documentation will \nalso be helpful.\n\nChapter 6. Governing the end-to-end lifecycle of an AI asset 69Work with use case SMEs and other teammates in order to build up a quality dataset of at \nleast 10 records in CSV format which you can use to validate the performance of your PTA. Based on your use case, the fo rmat of your data will vary. Expl ore the evaluation page for \nyour specific use case in order to identify the correct format for your validation data. For example, a summarization use case will have the following format for th e test data:\n-Original_Text1, Ground_Truth_Summary1-\n-Original_Text1, Ground_Truth_Summary2-\nOnce data is acquired, use the hamburger menu to ac cess the options of the given PTA. Click \non Evaluate to open the evaluation page. Using the Actions drop-down, click on “Evaluate Now” and",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 208,
            "total_chunks": 245
          }
        },
        {
          "id": "12f17e6d-242d-4b20-9397-8903980ff1df",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "k \non Evaluate to open the evaluation page. Using the Actions drop-down, click on “Evaluate Now” and follow through the options on-screen to set up the evaluation experiment. To review the available evaluation metrics, check the latest documentation for your version of watsonx.governance.\nAfter completing the evaluation, go back to your AI use case and scroll down. You should now \nsee this asset as being tracked in the validation stage as a result of running the evaluation experiment. Before moving onto the operation stage, it is most common for assets to sit in the validation stage while iterations happen to improve performance. Evaluations can be constantly run against assets in the validation stage and the records of the latest evaluation can be viewed from the AI factsheet for that asset.\nOperation\nOnce the validation stage is completed and the asset is ready to move into operation stage, head to your watsonx platform home page. Using the hamburger menu from the left, click on Depl",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 209,
            "total_chunks": 245
          }
        },
        {
          "id": "ba517958-0c10-4cbc-8ed8-672612647696",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "tage, head to your watsonx platform home page. Using the hamburger menu from the left, click on Deployments  to access the watsonx.ai runtime platform (traditionally known as Watson \nMachine Learning or WML), and create a new deployment space with “Production” type selected. Go back to the watsonx.ai project which holds the PTA, and using the options menu attached to the PTA, choose the Promote to Space  option. Follow the on-screen guidance to \npromote the PTA to the newly created production space.\nGo to the home page of the deployment space on watsonx.ai runtimes. From the assets tab, \nthe recently promoted PTA should be listed. Use the options menu attached to the PTA from this screen and choose the Deploy  option. Follow the guided scree n to fill out the relevant \ndetails, and click OK.\nAfter a few moments, your new PTA will be deployed (and  consumable) on the production \ndeployment space. You can now check again from your AI use case page to observe that the asset has moved from",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 210,
            "total_chunks": 245
          }
        },
        {
          "id": "c80ead04-2447-4b2f-9ad3-7f44dd5a1b12",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "t space. You can now check again from your AI use case page to observe that the asset has moved from development, to validation, and finally into the operation stage. Congratulations, you have successfully performed a lif ecycle on this AI asset!Note:  There are multiple ways to access PT A evaluations. You could access this page \nthrough your AI Factsheet, on the second tab. You could also run evaluations directly from the Prompt lab. Feel free to explore the a ccessibility of this wats onx.governance feature.\nNote:  Configuring monitors is an additional ta sk beyond the concept of simply moving \nthrough a lifecycle stage. This is specifically related to the operation stage of any AI asset; \nonce the asset is in production (operation stage), the monitors for that deployment can be configured to track specific metrics. To achieve this through the UI for a watsonx.ai prompt, use the Actions  drop down menu on the top right section of the production deployment \nand choose Activate Monito",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 211,
            "total_chunks": 245
          }
        },
        {
          "id": "50deee18-b848-484c-91d3-2047371a5f32",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ns  drop down menu on the top right section of the production deployment \nand choose Activate Monitors . Finally, follow the guided instru ctions to complete monitor \nconfiguration.\n\n70 Ensuring Trustworthy AI with IBM watsonx.governanceFrom this point on, considerat ions for each of th e following sub-sections will avoid repeating \ninformation and more ad vanced topics will be introduced throughout each se ction. At the \nhighest level, all integrations of watsonx.governance with ML/AI providers work with the same \nconcept of having an AI use ca se set up at watsonx.gover nance. The following sections will \nbecome more technical with various links to resources in the form of SDKs, notebooks, and code commands to help programmers accelerate implementation.\n6.4.2  Considerations for lifecycle go vernance for traditional ML hosted on \nwatsonx.ai\nThe same seamless integration applies for traditional ML when considering the watsonx.ai \nand watsonx.governance platforms. Assuming the initial ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 212,
            "total_chunks": 245
          }
        },
        {
          "id": "15e94c38-1508-4779-8a2c-53f4f6746f39",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ditional ML when considering the watsonx.ai \nand watsonx.governance platforms. Assuming the initial ML model has been saved as an asset to the watsonx.ai project space, it can be associated to a use case through the UI in the same manner as explained in section 6.4.1. In addition to the UI-based approach, lifecycle governance can be implemented via code. \nThis section will focus on useful APIs that can be used to achieve full lifecycle governance this \nas well as code examples. It will make an assumption of intermediate Python programming skills on behalf of the implementer.\nAPI and SDK fo r watsonx.governance\nThis section assumes an API or Python-driven SDK approach to programming is being \nundertaken. Two main API and SDK can be used to work with watsonx.governance. Other APIs also exist which can help, and they w ill be mentioned as ne cessary throughout the \nchapter.\n/SM590000AI Factsheets API and SDK - The watsonx.governance AI Factsheet API  and SDK is used \nto control the factsh",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 213,
            "total_chunks": 245
          }
        },
        {
          "id": "fbc1b3ae-ee64-4361-9ed8-90ff380132f2",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "sheets API and SDK - The watsonx.governance AI Factsheet API  and SDK is used \nto control the factsheets co mponent of watsonx. governance. It will be used to set up \nfactsheet objects and associate them with given assets.\n/SM590000OpenScale API and SDK - The watsonx.governance OpenScale API  and SDK controls \nthe computational layer of watsonx.governance. The brunt of activity comes from this API. \nUsers can accomplish a variet y of activities including:\n– Creating subscriptions to monitor deployments.\n– Performing model evaluations.– Associating existing factsheets to AI use cases.\nAdditionally, Governance Console APIs  are also available for interacting with Governance \nConsole within watsonx.governance.\nMetrics\nThe software comes out-of-box with a plethora of traditional metrics to monitor against. For more information, see Quality evaluations . Additionally, users can implement their own \ncustom evaluation metrics.\nFor custom metrics implementation, users can use this sample noteb",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 214,
            "total_chunks": 245
          }
        },
        {
          "id": "46388e8c-c8ee-4fc6-aea0-50742897b2be",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "r own \ncustom evaluation metrics.\nFor custom metrics implementation, users can use this sample notebook  from the \nIBM GitHub.Note:  This subsection will utilize code-drive n approaches. For more information, see \nTracking a machine learning model .\n\nChapter 6. Governing the end-to-end lifecycle of an AI asset 71Inventory and AI use case setup\nAI Inventories are used to hold AI use cases. These Inventories are agnostic to asset type - a \ngenerative AI use case can be held in the same inventory as a traditional ML/AI use case.\nInventory setup\nThis notebook from IBM's github showcases how to create an AI Inventory. This sample \nnotebook  demonstrates a variety of functionality including:\n/SM590000Open a new AI inventory\n/SM590000Add collaborators\n/SM590000Delete inventory\n/SM590000Modify existing inventories\nBecause the AI Inventory concept is native to watsonx.governance, it is always implemented \nregardless of where the AI assets live which are being tracked.\nAI use case setup\nThis not",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 215,
            "total_chunks": 245
          }
        },
        {
          "id": "b0174276-f69a-4426-8c37-8dcab4466238",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "lemented \nregardless of where the AI assets live which are being tracked.\nAI use case setup\nThis notebook from the IBM GitHubshowcases how to create an AI use case. The sample \nnotebook  demonstrates a variety of functionality including:\n/SM590000Storing a model as an asset into watsonx.ai (this creates the AI factsheet)\n/SM590000Create a new AI use case\n/SM590000Associate the AI use case with a watsonx.ai project\n/SM590000Track/Untrack a model under an AI use case\nDevelopment\nIn “Inventory and AI use case setup” on page 71, the linked notebook holds all relevant code \nfor placing a model into the development stage of its lifecycle. Here it is again in condensed form.\n# create an AI use case\nai_usecase = \nfacts_client.assets.create_ai_usecase(catalog_id=ai_usecase_inventory_id,name=ai_usecase_name,description=ai_usecase_desc)\n# track the model in the use casewatsonx_ai_model.track(usecase=ai_usecase,approach=decisiontree_approach,\nversion_number=\"major\",version_comment=\"major update to",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 216,
            "total_chunks": 245
          }
        },
        {
          "id": "bb477fbf-54e6-48d4-a4b5-53a98e3abed6",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "e=ai_usecase,approach=decisiontree_approach,\nversion_number=\"major\",version_comment=\"major update to previous version\")Note:  The notebook linked above also covers how to code the configuration of quality \nmonitors, fairness, drift, and explanations. It is highly recommended to consult sample \nnotebooks for thorough and up-to-date instructions. For users who are looking to create data configurations to specific data science problems, see the IBM directory  for different \noptions on creating data configurations.\nNote: The following pseudo-code requires additional code to function properly. Users \nshould consult the notebook and the API documentation linked in the previous sections for a full walkthrough.\n\n72 Ensuring Trustworthy AI with IBM watsonx.governanceValidation\nOnce the model is successfully tracked against an  AI use case, it is placed into development \nstage. If the model is already deployed into a production space,  it will instead show in the \noperation stage. This section w",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 217,
            "total_chunks": 245
          }
        },
        {
          "id": "c225c4b4-13b4-4ede-99c6-331822d4ea9c",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "eady deployed into a production space,  it will instead show in the \noperation stage. This section will assume we have a model in the development stage of the \nlifecycle. \nThere are multiple ways to move a model from the development stage into the validation \nstage. One way is to use the direct API call:\nmodel.set_environment_type(from_container=\"develop\", to_container=\"validate\"). \nFor more information, see Managing the Lifecycle Phases of a Model .\nAdditionally, you can deploy the model to a development deployment space or a validation \ndeployment space and this will trigger the lifecycle move . For more information, see Tracking \nprompt templates .\nThis functionality applies to all kinds of implementation, not just traditional ML.Users can set the environment directly if the as set is in a preceding stage, or users can \npromote models to a validation space in watsonx.ai runtimes.\nOperation\nThe operation stage is a uniquely important stage of the AI/ML lifecycle. Section 6.4.1, “UI-d",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 218,
            "total_chunks": 245
          }
        },
        {
          "id": "6a2296ba-275f-4b14-b854-8aad48a09fc4",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ation\nThe operation stage is a uniquely important stage of the AI/ML lifecycle. Section 6.4.1, “UI-driven implementation of lifecycle governance” on page 68 covered basic steps to move an asset to the operation, but many more ac tivities can be accomplished for a thorough \nimplementation of a production monitor. As mentioned in section 6.2, “Metrics in watsonx.governance” on page 64 and again in 6.4.2, “Considerations for lifecycle governance for traditional ML hosted on watsonx.ai” on page 70, a variety of metrics are available for configuration through watsonx.governance. When considering a model monitor in the operation stage, these metrics can be configured using the watsonx.governance UI or using notebooks. Review and follow the notebooks and directions provided in section 6.4.2, “Considerations for lifecycle governance for tradi tional ML hosted on watsonx.ai” on page 70 \nto establish baseline data configurations and respective monitors.\nAdditionally, code commands also exist to ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 219,
            "total_chunks": 245
          }
        },
        {
          "id": "cd1964a7-e9af-4364-b28d-7fc0dc4d90df",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ish baseline data configurations and respective monitors.\nAdditionally, code commands also exist to directly move assets into the production (operation) \nstage. Here is a snippet on how to move a model into a production deployment space to see the lifecycle stage progre ss into operation stage:\nmodel.set_environment_type(from_container=\"validate\", to_container=\"production\")\nwatsonx.governance comes with an expansive toolkit for handling the lifecycle of traditional \nML/AI assets. Considerations for third parties do exist, but the tools that we have covered up to this point will be adapted to cove r those additional considerations.\n6.4.3  Considerations for prompt  templates from another platform\nIBM watsonx.governance empowers organization s to evaluate and monitor prompt templates \nfor a variety of externally-hosted LLMs without the need to conduct inference on those models. This flexibility allows AI and Data Sc ience practitioners to work with models hosted \non platforms such as Goo",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 220,
            "total_chunks": 245
          }
        },
        {
          "id": "d61843a2-2f17-4ef8-862f-88723aac7fbc",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ility allows AI and Data Sc ience practitioners to work with models hosted \non platforms such as Google Vertex AI, Azure OpenAI, or AWS Bedrock, where all inference is performed remotely.\nThe platform offers a method known as the \ndetached prompt template  for evaluating and \nmonitoring prompt templates for externally-hosted LLMs without requiring model inference. \n\nChapter 6. Governing the end-to-end lifecycle of an AI asset 73This approach involves programmatically creating a detached prompt template asset, which \nprovides a high level of control. Evaluations are then conducted on the generated prompt output, with the results logged into watsonx.governance against the detached prompt template.\nAdditionally, one can evaluate a detached prompt template within a deployment space by \ncreating a detached deployme nt. This setup offers severa l benefits and capabilities:\n/SM590000Evaluating a prompt template within a project or space enhances the experience of \nreviewing evaluation results",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 221,
            "total_chunks": 245
          }
        },
        {
          "id": "3f0d4880-fe90-420f-b590-29816eda612e",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "a prompt template within a project or space enhances the experience of \nreviewing evaluation results.\n/SM590000One can utilize access control for projects and sp aces to invite collabo rators or restrict \naccess as needed.\n/SM590000The results of the evaluations can be tracked in factsheets related to AI use cases as part \nof the governance solution.\nThe sample code to create a detached prompt template is shown in Example 6-1.\nExample 6-1   Sample code to create a detached prompt template\nfrom ibm_aigov_facts_client import DetachedPromptTemplate, PromptTemplate\ndetached_information = DetachedPromptTemplate(\nprompt_id=prompt_id,model_id=model_id,    model_provider=model_proivder,    model_name=model_name,    model_url=model_url,    prompt_url=prompt_url,    prompt_additional_info=prompt_additional_info)\nprompt_template = PromptTemplate(\n    input=input_text,    prompt_variables=prompt_variables,    input_prefix=input_prefix,    output_prefix=output_prefix,    model_parameters = model_pa",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 222,
            "total_chunks": 245
          }
        },
        {
          "id": "b4133f44-462a-4e3b-8acf-a374d59c3c39",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "iables,    input_prefix=input_prefix,    output_prefix=output_prefix,    model_parameters = model_parameter)\nexternal_prompt_template_details = facts_client.assets.create_detached_prompt(\n    name=prompt_name,    description=prompt_description,    model_id=model_id,    task_id=task_id,    prompt_details=prompt_template,    detached_information=detached_prompt_template)project_pta_id = external_prompt_template_details.to_dict()[\"asset_id\"]\nOnce the detached prompt template is created, it follows the same lifecycle as described in \n6.4.2, “Considerations for lifecycle governance for traditional ML hosted on watsonx.ai” on page 70.Note:  The following pseudo-code requires additional code to function properly. For more \ninformation, see the notebook  and the API documentation .\n\n74 Ensuring Trustworthy AI with IBM watsonx.governance6.4.4  Considerations for tradit ional ML from another platform\nJust like prompt templates from other platforms, ML assets and deployments can exist \noutside of",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 223,
            "total_chunks": 245
          }
        },
        {
          "id": "327ec1fc-e60c-4c1c-b958-f102aad213c0",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "orm\nJust like prompt templates from other platforms, ML assets and deployments can exist \noutside of watsonx.governance  and still be monitored and tr acked by watsonx.governance. \nLifecycle governance of third-party ML mo dels deployed on other platforms can be \nimplemented through python code. IBM watsonx.governance provides all of the tools and methods necessary to achieve lifecycle governance regardless of where the asset exists. Section 6.4.2, “Considerations for lifecycle governance for traditional ML hosted on watsonx.ai” on page 70 introduces the tools which will be used to achieve these goals.\nMany of the same steps apply when implementing lifecycle governance for ML assets \ndeployed on other platforms. Review the setup steps covered in section 6.4.2, “Considerations for lifecycle governance for tradi tional ML hosted on watsonx.ai” on page 70 \nto configure the AI Inventory and AI use case and review the introductions to the packages along with the data statistics configuratio",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 224,
            "total_chunks": 245
          }
        },
        {
          "id": "868c87ca-306e-4933-9c01-4fc929b88ae0",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "AI use case and review the introductions to the packages along with the data statistics configurations note books in that section. Once those activities \nare complete, new assets can be tracked against use cases in their appropriate lifecycle stages.\nIt is worth noting that many ML processes wh ich exist on other platforms are typically \nconsidered to be in a production state. Because of this, model tracking for third-party assets is most typically an administrative and organizati onal task while that asset is being developed \nand validated in its respective environment. Once the asset arrives into the production or operation stage, monitors can be configured for thorough and effective ML governance. For full instructions on how to configure headless subscriptions for ML models which are not hosted through watsonx.governance, see this notebook .\nBatch processing can also be achieved with watsonx.governance. For step-by-step \ninstructions on achieving batch processing, see this notebook",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 225,
            "total_chunks": 245
          }
        },
        {
          "id": "6e09bf85-9fe3-4c5f-89a4-07f848a6e9c1",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " watsonx.governance. For step-by-step \ninstructions on achieving batch processing, see this notebook , the documentation .\n6.4.5  Governing AI embedde d in a business application\nIf an AI/ML asset which is consumed in a busine ss application is directly owned and operated \nby the governance team (or a department from the same organization as the governance team), any of the above techniques can be used to apply AI governance. Depending on the \nkind of AI/ML being used in the business application, a user may have to configure a detached prompt template for LLM (as described in section 6.4.3, “Considerations for prompt templates from another platform” on page 72) or they may have to configure a headless subscription for ML (as described in section 6.4.4, “Considerations for traditional ML from another platform” on page 74) If the ML/AI process is being provided by an outside vendor, various considerations should be made when look ing to apply governance to that process. \nThis section wil",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 226,
            "total_chunks": 245
          }
        },
        {
          "id": "9a4c9963-8f4c-48c2-9c50-a010f44c482b",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "s considerations should be made when look ing to apply governance to that process. \nThis section will shed light on some of the mo st common and im portant things to consider \nwhen looking to govern an existing business  application where the AI/ML process is being \nprovided through a vendor / service from outside the organization.\nConsider accessibility\nIf the governance team  is not the direct owner of an AI process, the accessi bility of that AI \nprocess must be explored and evaluated. Contact the AI vendor and learn about the accessibility to the model. Can a headless subscription or a de tached prompt template be \nconfigured to create a live monitor of that model or process? If not, what are the alternatives that the vendor can provide? Do those alternatives meet your organization's requirements for AI/ML governance?\n\nChapter 6. Governing the end-to-end lifecycle of an AI asset 75Considering requirements\nThink about what the requirements are for governing the business application ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 227,
            "total_chunks": 245
          }
        },
        {
          "id": "cc129f4c-be16-442c-b640-3583b6ddaa0a",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "nsidering requirements\nThink about what the requirements are for governing the business application or process. Is \nlive monitoring the only way to achieve those requirements? Can the requirements be achieved through some other means such as a report card of the AI process, or some other status update?\nUltimately, implementing end-to-end lifecycle monitoring of AI embedded in a business \napplication is unique. Because of the fact that the AI already exists in a business application, some assumption may be made about the stage of that AI asset (it would be considered to be in production if it is live and being consumed by users.) This would make the process shorter than tracking a model from init ial development; A user may in itiate lifecycle governance on a \nbusiness application's AI model beginning with the production or operation stage. \nGoverning an AI/ML model from a business application must be accomplished with the same \nattention to detail regardless of who is the ve ndor / pro",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 228,
            "total_chunks": 245
          }
        },
        {
          "id": "b0970874-3099-49b0-9722-63e8057ad1f8",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ation must be accomplished with the same \nattention to detail regardless of who is the ve ndor / provider. As the process continues to \nevolve for AI governance, rules and requirements should be established with regarding which AI/ML providers and vend ors are to be sbe utilized by the organization. Befo re configuring \nlifecycle monitoring for those AI /ML processes, verify ing the vendor's ability to meet any legal \nrequirements is an important starting point. Review Chapter 4, “Onboarding a new foundation model” on page 41 and Chapter 5, “Assessing a new use case” on page 53 for organizational processes and considerations to make prior to setting up lifecycle governance.\n\n76 Ensuring Trustworthy AI with IBM watsonx.governance\n\n© Copyright IBM Corp. 2025. 77Chapter 7. Use cases\nThis chapter highlights various implementations of watsonx.governance, focusing on fairness, \ndrift, regulatory compliance, and accountability. These implemen tations span multiple sectors \nsuch as healthcare,",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 229,
            "total_chunks": 245
          }
        },
        {
          "id": "7c8a3b68-256c-4863-a74e-2b3b6c3679aa",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ry compliance, and accountability. These implemen tations span multiple sectors \nsuch as healthcare, banking, and finance, levera ging robust data tracking and model auditing \nmechanisms.\nThis chapter has th e following sections:\n/SM590000“Overview of use case 1- Banking credit risk management” on page 78\n/SM590000“Overview of use case 2 - Automated governance for universal bank's AI chatbot” on \npage 80\n/SM590000“Overview of use case 3 - Belgian biopharmaceutical company” on page 817\n\n78 Ensuring Trustworthy AI with IBM watsonx.governance7.1  Overview of use case 1- Banking credit risk management\nThis section explains the concept of credit risk management, emphasizing fair, transparent \napproval processes, proactive monitoring with alerts, and automation of model metadata documentation to support credit decisions.\nIt highlights of the importance of a successf ul credit risk management system include the \nfollowing:\n/SM590000Fair and transparent approval processes.\n/SM590000Proactive r",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 230,
            "total_chunks": 245
          }
        },
        {
          "id": "fc47e693-32d2-4c5c-9fe6-5162200402ca",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ystem include the \nfollowing:\n/SM590000Fair and transparent approval processes.\n/SM590000Proactive risk detection with al erts to avoid biased decisions.\n/SM590000Automating the capture and documentation of model metadata with fact sheets to support \ncredit decisions.\nFigure 7-1 shows how watsonx.governance can improve businesses credit risk management \nsystem. Improvements are shown based on bu siness function to help illustrate the \ncross-departmental value that proper governance has in any organization.\nFigure 7-1   Credit risk management\n7.1.1  Banking credit ri sk management use case\nCredit risk management is the practice of mitigating losses by assessing borrowers' credit \nrisk - including payment behavi or and affordability. This proc ess has been a longstanding \nchallenge for financial institutions requiring co ntinuous adaptations by businesses to better \ntrack borrower behavior.\n7.1.2  Business context\nA French cooperative bank provides banking products and services, focusing",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 231,
            "total_chunks": 245
          }
        },
        {
          "id": "c880f149-c74e-42bf-9672-e07ba42cdf02",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": ".\n7.1.2  Business context\nA French cooperative bank provides banking products and services, focusing on risk management and regulatory compliance. However, regulatory monitoring was done manually, leading to inefficiencies and challenges in identifying emerging regulatory risks resulting from regulatory changes and new requirements impacting customer services. The French \n\n\nChapter 7. Use cases 79cooperative bank caters to individuals and busines ses by offering a range of banking products \nand services.\nHowever, the bank faced challenges due to:\n/SM590000A lack of centralized tools for regulatory monitoring.\n/SM590000Manual processes using Excel, resulting in inefficiencies in identifying applicable \nregulatory risks and regulatory changes.\n7.1.3  Client need\nThe client required a centralized tool to aggregate legal texts from multiple data sources, enabling lawyers to track regulatory changes and link them to specific business units and banking product offerings, such as banking card",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 232,
            "total_chunks": 245
          }
        },
        {
          "id": "b5b3ca18-c599-432d-a229-13105f0a795e",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "changes and link them to specific business units and banking product offerings, such as banking cards and insurance.\nThe client required a governance tool to:\n/SM590000Aggregate legal texts from multiple data sources.\n/SM590000Maintain a history of regulatory changes and link this data to impacted products and \nbusiness units.\n7.1.4  Client challenges\nThe client wants to address the challenges in the following areas:\n/SM590000The client faced significant hurdles in stre amlining their governance, risk, and compliance \n(GRC) processes:\n–Fragmented data systems : GRC data was scattered across multiple disparate \nsystems, creating silos and making it difficult to obtain a unified view of risks.\n–Manual dependency : Reliance on manual processes for managing GRC was \ninefficient and error-prone, especially in th e context of the monitoring and management \nof the rapid number of regulatory changes.\n/SM590000The handling of large volumes of data, which overwhelmed manual workflows and \nreduce",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 233,
            "total_chunks": 245
          }
        },
        {
          "id": "ae658688-31ac-4eeb-ba09-5f0266f52d21",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "nges.\n/SM590000The handling of large volumes of data, which overwhelmed manual workflows and \nreduced accuracy.\n7.1.5  Business benefits\nHere are several business benefits and improved risk management benefits:\n/SM590000By addressing these challenges, the solution delivered substantial business benefits:\n–Unified Governance Framework : Consolidating GRC processes into a single, \nintegrated platform provided a comprehensive view of risks, enabling better decision-making and streamlined operations.\n–AI-driven automation : Leveraging AI technologies accelerated GRC processes, \nreducing manual effort and improving efficien cy. This automation not only minimized \nhuman errors but also ensured faster compliance with evolving regulatory requirements.\n/SM590000Improved risk management : Enhanced visibility and co ntrol over GRC processes \nempowered the organization to proactively identify and address potential risks, fostering resilience and complia nce across operations.\n\n80 Ensuring Trustwor",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 234,
            "total_chunks": 245
          }
        },
        {
          "id": "7d5e7e8f-7bde-468a-9f79-2f24a587784c",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "dress potential risks, fostering resilience and complia nce across operations.\n\n80 Ensuring Trustworthy AI with IBM watsonx.governance7.1.6  Pilot solution \nThis pilot solution uses watsonx.gover nance Governance Console (OpenPages).\nIn a prior Minimum Viable Product (MVP), IBM OpenPages demonstrated value by importing \nconsumer code and GDPR laws via standard R EST API such as Légifrance REST API in \nFrance. Initially, Excel was used  for linking with the eventual goal being direct integration \nwithin IBM OpenPages.\nList of key steps:\n/SM590000Develop a Connector : Connect the Légifrance REST API and IBM OpenPages REST \nAPI to fetch and properly format regulatory data.\n/SM590000Automate Hyperlinks : Enable referencing between articles for better navigation and \nusability.\nBy streamlining legal monitoring, the solution enhances risk identification and regulatory \ncompliance, reduces prior manual steps, and improves the quality of formatting legal data.\n7.2  Overview of use case 2 - Aut",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 235,
            "total_chunks": 245
          }
        },
        {
          "id": "7724e24b-0d57-44bd-8923-232da91b00fe",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "r manual steps, and improves the quality of formatting legal data.\n7.2  Overview of use case 2 - Automated governance for \nuniversal bank's AI chatbot \nA prominent British universal bank and financia l services group, provides a broad range of \nofferings such as savings accounts, loans, insurance, and investment options. A key focus of the bank is continuous improvements on ef ficiencies around managing risk and ensuring \nregulatory compliance.\nTheir AI-powered \nchatbot  is designed to provide AI-driven solutions and must adhere to strict \nstandards of ethics, explainabilit y, and expected perfo rmance. To support this, the bank seeks \nto govern its AI systems with robust, automated, and integrated platforms as data and AI technologies evolve.\n7.2.1  Business context\nThey require a governance framework that ensures:\n/SM590000Ethical and explainable AI behavior.\n/SM590000Reliable results in alignmen t with business objectives.\n/SM590000Adequate control, testing, and audit mechanisms to ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 236,
            "total_chunks": 245
          }
        },
        {
          "id": "f50c3bc2-0863-429d-9f13-447fcb5361e5",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "in alignmen t with business objectives.\n/SM590000Adequate control, testing, and audit mechanisms to manage evolving data and AI models.\nThe solution will leverage IBM watsonx.gove rnance, an automated and integrated AI \ngovernance platform, to manage the lifecycle and compliance of the AI application.\n7.2.2  Client need\nThe client aims to automate the governance lifecycle of a chatbot  across various metrics, \nlifecycle stages, and compliance requirements.\n7.2.3  Client challenges\nThe client focused on plans to address two main challenges:\n\nChapter 7. Use cases 81/SM590000The current AI implementation did not have an adequate monitoring system to maintain a \nstable solution. \n/SM590000Need to find a way to improve mechanisms to  address the diverse aspects such as bias \ndetection, ethical compliance, handling of highly autonomous processes (HAP), and protection of personally identifiable information (PII).\n7.2.4  Business benefits\nThe following benefits support addressing the client’s ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 237,
            "total_chunks": 245
          }
        },
        {
          "id": "471ac965-309e-431f-89a4-ece627528751",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": " information (PII).\n7.2.4  Business benefits\nThe following benefits support addressing the client’s challenges:\n/SM590000Consolidated governance : Provide a unified platform for an aggregated view of risks.\n/SM590000Automation of GRC processes:  Leverage AI to streamline governance, risk, and \ncompliance (GRC) processes, signific antly accelerating time to value.\n7.2.5  Pilot solution\nThe pilot addressed these requirements with the following key features:\n/SM590000Quantified Quality Metrics : Faithfulness, answer relevance, handling of unsuccessful \nrequests, keyword inclusion, answer coverage , and spelling robustness were all measured \nto ensure high performance.\n/SM590000Governance Dashboard : Developed a comprehensive dashboard to simplify the \noversight of their AI governance lifecycle.\n/SM590000External Model Governance : Implemented governance for external models using \ndetached prompt templates.\nThis structured solution ensures bank's chatbot  AI product operates within ethical",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 238,
            "total_chunks": 245
          }
        },
        {
          "id": "19e03af2-a056-406b-8ac0-bde6bcb3f451",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "rompt templates.\nThis structured solution ensures bank's chatbot  AI product operates within ethical, regulatory, \nand performance parameters while automating the lifecycle governance for enhanced efficiency.\n7.3  Overview of use case 3 - Belgian biopharmaceutical \ncompany \nA Belgian biopharmaceutical company, is leveraging watsonx solutions to address challenges \nin website development,  including technical documentation and module reusability, for their \nDrupal-based global web ecosystem.\n7.3.1  Business context\nIBM is developing and managing over 150 global websites for the pharma company using Drupal based technology. Client  faces challenges in mainta ining high-quality technical \ndocumentation, which limits clarit y on the functionality of existing Drupal modules and creates \nbarriers for local developers seeking to reuse available modules.\n7.3.2  Client need\nClient seeks a solution th at improves the website development process by:\n/SM590000Enhancing technical documentation.\n/SM",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 239,
            "total_chunks": 245
          }
        },
        {
          "id": "4eb31931-42e6-48b2-8881-0cca09cdc9b7",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "n th at improves the website development process by:\n/SM590000Enhancing technical documentation.\n/SM590000Simplifying module discovery and reuse.\n\n82 Ensuring Trustworthy AI with IBM watsonx.governance/SM590000Supporting local developers with efficient tools and governance mechanisms.\n7.3.3  Client challenges\nThe following challenges need to be addressed:\n/SM590000Lack of clear, comprehensive technical documentation for existing Drupal modules.\n/SM590000Inefficiencies in reusing mo dules across local teams due to limited information.\n/SM590000Fragmented governance, making it difficult to maintain consistency and compliance \nacross websites.\n7.3.4  Business benefits\nSeveral key benefits are:\n– Streamlined documentation creation and maintenance with AI-driven tools.\n– Improved developer productivity throug h better access to reusable modules.\n– A unified governance framework for cons istent and efficient website development.\n7.3.5  Pilot solution\nThe following steps highlight the impleme",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 240,
            "total_chunks": 245
          }
        },
        {
          "id": "34901648-4a8e-4b2a-86be-c092a41cac30",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "t and efficient website development.\n7.3.5  Pilot solution\nThe following steps highlight the implementation of the pilot solution:\n1.Objective:  Demonstrate how watsonx solutions can enhance website development by \naddressing documentat ion challenges, impr oving module reusability, and supporting \ngovernance.\n2.Steps implemented :\nThe following steps show how the client addressed the challenges:\na.Understanding requirements :\nConducted workshops with pharma company stakeholders to identify pain points and \ngather insights into their Drupal-based ecosystem.\nb.Developing solutions :\nThe client used the following products to develop the solution:\nIBM watsonx.ai:  Used for generating and maintaining AI-driven technical \ndocumentation.\nIBM watsonx.governance : Implemented to centralize and streamline governance \nfor Drupal modules.\nc.Knowledge base creation :\nBuilt a searchable repository with detailed descriptions and usage guidelines for Drupal \nmodules.\nd.Prototype and demonstration :",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 241,
            "total_chunks": 245
          }
        },
        {
          "id": "f07516be-4592-472f-91b0-f34014476282",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "with detailed descriptions and usage guidelines for Drupal \nmodules.\nd.Prototype and demonstration :\nThe client developed the prototype to demonstrate improved productivity:\n Created a prototype showing how watsonx tools improve documentation and \nmodule discovery.\n Demonstrated how developers can leverage  these tools to enhance productivity.\ne.Feedback loop :\nThe client implemented a feedback loop to improve the solution:\n\nChapter 7. Use cases 83 Collected feedback during the pilot from client's technical teams.\n Iteratively refined the solution to a lign with client's specific workflows and \nrequirements.\n3.Outcome :\nThe outcome of the following demonstrated the solution improvement:\n– Enhanced module documentat ion quality and accessibility.\n– Increased developer efficiency by enabling effective module reuse.\n– Established a robust governance fram ework for Drupal website development.\n4.Future scope :\nThe following items were identified for future additional improvement to the ",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 242,
            "total_chunks": 245
          }
        },
        {
          "id": "3f178847-92eb-4701-b1bf-ca8229be9d1c",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "ment.\n4.Future scope :\nThe following items were identified for future additional improvement to the solution;\n– Scale the pilot soluti on across global websites.\n– Expand the use of watsonx tools to other areas of the pharmaceutical company's \ndigital ecosystem for broader impact.\n\n84 Ensuring Trustworthy AI with IBM watsonx.governance\n\n© Copyright IBM Corp. 2025. 85Related publications\nThe publications listed in this section are considered particularly suitable for a more detailed \ndiscussion of the topics covered in this book.\nIBM Redbooks\nThe following IBM Redbooks publications provide additional information about the topic in this document. Note that some publications referenced  in this list might be available in softcopy \nonly. \n/SM590000Simplify Your AI Journey: Hybrid, Open Data Lakehouse with IBM watsonx.data,  \nSG24-8570\n/SM590000Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai , SG24-8574\nYou can search for, view, download or order these documents and",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 243,
            "total_chunks": 245
          }
        },
        {
          "id": "af73ba03-5ef4-411c-a0a5-1f9a0943b718",
          "doc_id": "f02de3e3-24af-494f-874b-b0b8292e77cc",
          "content": "f AI with IBM watsonx.ai , SG24-8574\nYou can search for, view, download or order these documents and other Redbooks, \nRedpapers, Web Docs, draft and additional materials, at the following website: \nibm.com/redbooks\nOnline resources\nThese websites are also relevant as further information sources:\n/SM590000IBM watsonx documentation  \n/SM590000IBM watsonx.governance  \n/SM590000IBM watsonx product portfolio  \n/SM590000IBM AI risk atlas  \nHelp from IBM\nIBM Support and downloads\nibm.com/support\nIBM Global Services\nibm.com/services\n\n86 Ensuring Trustworthy AI with IBM watsonx.governance\n\n\n\nibm.com /redbooksPrinted in U.S.A .Back cover\nISBN 0738461970SG24-8573-00\n®\n\n",
          "metadata": {
            "source": "sg248573.pdf",
            "chunk_index": 244,
            "total_chunks": 245
          }
        }
      ]
    },
    {
      "id": "12b4be16-46b7-4277-a787-5335bdea322d",
      "url": "",
      "title": "sg248555.pdf",
      "content_chunks": [
        {
          "id": "0f471dfd-e5fc-4e82-b942-df8d29504be5",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "RedbooksFront cover\nIBM Hyper Protect Platform:\nApplying Data Protection and Confidentiality\nin a Hybrid Cloud Environment\nBill White\nRobbie AvillSandeep BattaAbhiram KulkarniTimo KußmaulStefan LiescheNicolas MädingChristoph SchlameußPeter Szmrecsányi\n\n\n\nIBM Redbooks\nApplying Data Protection and Confidentiality in a \nHybrid Cloud Environment\nFebruary 2024\nSG24-8555-00\n\n© Copyright International Bu siness Machines Corp oration 2024. All rights reserved.\nNote to U.S. Government Users Restricted Rights -- Use, duplication or disclosure re stricted by GSA ADP Schedule\nContract with IBM Corp.First Edition (February 2024)\nThis edition applies to the IBM z15, IBM z16, IBM LinuxONE III, IBM LinuxONE 4, and IBM Hyper Protect \nPlatform Second Generation.Note: Before using this information and the product it supports, read the information in “Notices” on \npage ix.\n\n  iii\n\niv Applying Data Protection and Confidentiality in a Hybrid Cloud Environment\n\n© Copyright IBM Corp. 2024. vContents\nNotices  ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 0,
            "total_chunks": 337
          }
        },
        {
          "id": "23cfa915-1b84-4a22-8f2c-30d0191b3458",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "n and Confidentiality in a Hybrid Cloud Environment\n\n© Copyright IBM Corp. 2024. vContents\nNotices  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\nTrademarks  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\nPreface  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\nAuthors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .x\nNow you can become a published author, too!  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi\nComments welcome. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xii\nStay connected to IBM Redbooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 1,
            "total_chunks": 337
          }
        },
        {
          "id": "0ab020cc-dfd4-4c20-935b-f9969d2d4751",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " . xii\nStay connected to IBM Redbooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xii\nChapter 1.  A hybrid cloud with data security in mind . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.1  Identifying the threat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . 2\n1.2  Beyond regulatory and standard frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.3  Mitigating the threat. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . 4\n1.3.1  Technical assurance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3.2  A Trusted Execution Environment for your application . . . . . . . . . . . . . . . . . . . . . . 5\n1.3.3  Reduced trust boundary and trusted computing base  . . . . . . . . . . . . . . . . . . . . . . 61.3.4  Controlling your application wi ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 2,
            "total_chunks": 337
          }
        },
        {
          "id": "7e19fe81-39b1-4a08-93ec-a2f349aa59f4",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "computing base  . . . . . . . . . . . . . . . . . . . . . . 61.3.4  Controlling your application wi th separation of duty . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.3.5  Exclusive and full control over your cryptographic key . . . . . . . . . . . . . . . . . . . . . . 7\n1.3.6  Support for your application OCI images  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71.3.7  Support for hybrid cloud . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.4  The solution explained  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  7\n1.4.1  The technology underlying the Hyper Protect Platform. . . . . . . . . . . . . . . . . . . . . . 81.4.2  Features of the Hyper Protect Platform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.4.3  Cryptography and Hyper Protect Crypto Service . . . . . . . . . . . . . . . . . . . . . . . . . 13\n1.4.4  Hyper Protect Secure",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 3,
            "total_chunks": 337
          }
        },
        {
          "id": "b4a9b307-3a10-450e-8c72-fbe8f9742a6c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "tect Crypto Service . . . . . . . . . . . . . . . . . . . . . . . . . 13\n1.4.4  Hyper Protect Secure Build. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nChapter 2.  Understanding the solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.1  IBM Hyper Protect services and a secure hybrid cloud. . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.2  IBM Cloud Virtual Private Cloud . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.2.1  IBM Cloud virtual server instance on IBM LinuxONE . . . . . . . . . . . . . . . . . . . . . . 18\n2.3  Hyper Protect Virtual Server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.3.1  Bootloader. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.3.2  Volume encryption  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 4,
            "total_chunks": 337
          }
        },
        {
          "id": "0a253354-b2e5-4035-9a81-524a5d9376d2",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " . 19\n2.3.2  Volume encryption  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192.3.3  Description of the contract  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.3.4  The attestation record . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.3.5  Logging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  21\n2.3.6  Hyper Protect layer services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.3.7  Hyper Protect Virtual Server for VPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n2.3.8  Hyper Protect Virtual Server for IBM LinuxONE and IBM Z . . . . . . . . . . . . . . . . . 232.3.9  Considerations when deploying workloads in HPVS instances  . . . . . . . . . . . . . . 23\n2.4  Hyper Protect Secure Build. . . . . . . . . . . . . ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 5,
            "total_chunks": 337
          }
        },
        {
          "id": "af4bea78-87ac-46c8-b9bb-ee06eb31897e",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " instances  . . . . . . . . . . . . . . 23\n2.4  Hyper Protect Secure Build. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n2.5  Cryptographic agility is the key to SecDevOps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2.6  Hyper Protect Crypto Services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2.6.1  Accessing cryptographic services with HPCS. . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n2.7  Crypto Express Network API for Secure Execution Enclaves. . . . . . . . . . . . . . . . . . . . 28\n2.7.1  Security considerations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n2.8  Storage and repositories in the cloud . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n2.8.1  Cloud object storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292.8.2  Blo",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 6,
            "total_chunks": 337
          }
        },
        {
          "id": "9f11a42f-269f-4e44-8d12-dfbcdbd850c3",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292.8.2  Block storage  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n\nvi Applying Data Protection and Confidentiality in a Hybrid Cloud Environment2.8.3  File storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n2.8.4  On-premises storage  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n2.9  Common usages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1\n2.9.1  Securely bring applications to hybrid cloud. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312.9.2  Digital assets infrastructure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n2.9.3  Confidential AI. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 7,
            "total_chunks": 337
          }
        },
        {
          "id": "f31b09db-2536-4f42-a8c7-6eb178144678",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ". . . . . . . 32\n2.9.3  Confidential AI. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 3\n2.9.4  Secure multi-party computation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342.9.5  Secure distributed cloud . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\nChapter 3.  Making the infrastructure secure  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n3.1  The contract  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . 38\n3.1.1  The workload section  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.1.2  The workload volumes subsection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463.1.3  The env section. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 8,
            "total_chunks": 337
          }
        },
        {
          "id": "969a5ab9-ee22-4a18-87c6-257016766f72",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.2  Contract encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  49\n3.3  Contract certificates  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n3.4  Attestation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . 54\n3.5  Logging for HPVS instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n3.6  Encrypting data volumes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\nChapter 4.  Application development in a trusted environment  . . . . . . . . . . . . . . . . . . 65\n4.1  Securing the application lifecycle  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.1.1  Develo",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 9,
            "total_chunks": 337
          }
        },
        {
          "id": "3958798c-8c42-4f25-9754-cd9fd7122593",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.1.1  Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.1.2  Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.1.3  Build  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.1.4  Release. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  68\n4.1.5  Deployment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n4.1.6  Update  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  68\n4.1.7  Application and service development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n4.1.8  Wo",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 10,
            "total_chunks": 337
          }
        },
        {
          "id": "264f37e9-b2ff-4a87-a164-6a802d0db7a8",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "service development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n4.1.8  Working with the log . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n4.1.9  Deployment automation - Terraform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n4.2  Build container image by using Hyper Protect Secure Build. . . . . . . . . . . . . . . . . . . . . 71\n4.2.1  Determine readiness. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.2.2  Install the secure build CLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n4.2.3  Create client and server certificates for secure build  . . . . . . . . . . . . . . . . . . . . . . 72\n4.2.4  Prepare user_data.yaml . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 734.2.5  Create the Hyper Protect Secure Build instance. . . . . . . . . . . . . . . . . ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 11,
            "total_chunks": 337
          }
        },
        {
          "id": "748fc259-c1d0-42d1-a76f-05e9af3135b7",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " . . . . . 734.2.5  Create the Hyper Protect Secure Build instance. . . . . . . . . . . . . . . . . . . . . . . . . . 74\n4.2.6  Configure the HPSB client with the HPVS IP address . . . . . . . . . . . . . . . . . . . . . 75\n4.3  Zero knowledge proofs: TLS server certificates and wrapped secrets . . . . . . . . . . . . . 78\n4.3.1  Passing secrets into a secure HPVS  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n4.3.2  Certificate benefits  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n4.3.3  Importing server certificate from contract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 794.3.4  Random number generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n4.3.5  Reverse proxy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n4.3.6  Basic web server (nginx) hardening . . . . . . . . . . . . . . . . .",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 12,
            "total_chunks": 337
          }
        },
        {
          "id": "c48f284a-a500-40b7-9b73-846b59bf3a4b",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ". . . . . . . . . . . 83\n4.3.6  Basic web server (nginx) hardening . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 844.3.7  Offloading NGINX TLS to HPCS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n4.4  Trust in-depth based on boot flow attestation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n4.5  Data storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . 87\n4.5.1  Encrypting block storage. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n4.5.2  Encryption state  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n4.5.3  Upgrade, backup, and disaster recovery  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 914.5.4  High Availability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1\n4.",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 13,
            "total_chunks": 337
          }
        },
        {
          "id": "e939f986-97a6-409e-b777-38f8f23a0253",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1\n4.6  Securing cloud native services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n\n Contents vii4.6.1  Confidential cluster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n4.6.2  Confidential containers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n4.6.3  Confidential service platform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n4.7  Secure supply chain with SLSA  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n4.7.1  Jenkins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n4.7.2  Source-to-image (S2I). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n4.7.3  GitHub",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 14,
            "total_chunks": 337
          }
        },
        {
          "id": "f5b3a92b-4277-4b5b-946c-5cb7e1d7d4b9",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n4.7.3  GitHub Actions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\nAppendix A.  Client contract setup sample files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\nSample YAML file with literal scalars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\nSample YAML file with double-quoted scalars. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\nSample script for certificate or key files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\nAppendix B.  Creating a Hyper Protect Virtual Server for VPC  . . . . . . . . . . . . . . . . . . 107\nUsing the IBM Cloud VPC UI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\nAppendix C.  Additional ex amples for HPSB and HPVS  . . . . . . . . . . . . . . ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 15,
            "total_chunks": 337
          }
        },
        {
          "id": "579c643e-b5b2-4dc1-9bd3-ebebbf6060d7",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " . . . . . . . 108\nAppendix C.  Additional ex amples for HPSB and HPVS  . . . . . . . . . . . . . . . . . . . . . . . 115\nHyper Protect Secure Build log  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\nHow to verify disk (volume) encryption with HPL13000I  . . . . . . . . . . . . . . . . . . . . . . . . . . 118\nAppendix D.  Encryption keys explained . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\nWhat is a master key (MK). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\nWhat are data encryption keys (DEKs)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\nWhat are key encryption keys (KEKs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\nUsing and protecting keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123How encryption keys are created ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 16,
            "total_chunks": 337
          }
        },
        {
          "id": "6c60c6cf-ffe8-4d79-87f7-abe83187fc5d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123How encryption keys are created using GREP11  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n\nviii Applying Data Protection and Confidenti ality in a Hybrid Cloud Environment\n\n© Copyright IBM Corp. 2024. ixNotices\nThis information was developed for prod ucts and services offered in the US . This material might be available \nfrom IBM in other languages. However, you may be required  to own a copy of the product or product version in \nthat language in order to access it. \nIBM may not offer the products, services, or features di scussed in this document in other countries. Consult \nyour local IBM representative for information on the produc ts and services currently available in your area. Any \nreference to an IBM product, program, or service is not intended to state or imply that only that IBM product, \nprogram, or service may be used. Any functionally equi valent product, program, or service that does",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 17,
            "total_chunks": 337
          }
        },
        {
          "id": "d660ae46-2a42-4670-82d9-071899f1beb2",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "program, or service may be used. Any functionally equi valent product, program, or service that does not \ninfringe any IBM intellectual property right may be used instead. However, it is t he user’s responsibility to \nevaluate and verify the operation of any non-IBM product, program, or service. \nIBM may have patents or pending patent applications covering subject matter described in this document. The \nfurnishing of this document does not grant you any license to these patents. You can send license inquiries, in \nwriting, to:\nIBM Director of Licensing, IBM Corporation, North Castle Drive, MD-NC119, Armonk, NY 10504-1785, US \nINTERNATIONAL BUSINESS MACHINES CORPORATIO N PROVIDES THIS PUBLICATION “AS IS” \nWITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED \nTO, THE IMPLIED WARRANTIES OF NON-INFR INGEMENT, MERCHANTABILITY OR FITNESS FOR A \nPARTICULAR PURPOSE. Some jurisdictions do not allow disclaimer of express or implied warranties in \ncertain transactio",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 18,
            "total_chunks": 337
          }
        },
        {
          "id": "3100c4b0-43ef-47a5-95f2-d52a2465d256",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ". Some jurisdictions do not allow disclaimer of express or implied warranties in \ncertain transactions, therefore, this statement may not apply to you. \nThis information could include technical inaccuracies or  typographical errors. Changes are periodically made \nto the information herein; th ese changes will be incorporated  in new editions of the publication. IBM may make \nimprovements and/or changes in the product(s) and/or the program(s) described in this publication at any time \nwithout notice. \nAny references in this information to non-IBM websites are provided for convenience only and do not in any \nmanner serve as an endorsement of those websites. The materials at those websites are not part of the \nmaterials for this IBM product and use of those websites is at your own risk. \nIBM may use or distribute any of the information you provide in any way it believes appropriate without \nincurring any obligation to you. \nThe performance data and c lient examples cited are presented fo ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 19,
            "total_chunks": 337
          }
        },
        {
          "id": "8607f3cd-f0fd-4409-adfa-eeb8272ad316",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "\nincurring any obligation to you. \nThe performance data and c lient examples cited are presented fo r illustrative purposes only. Actual \nperformance results may vary depending on specific configurations and operating conditions. \nInformation concerning non-IBM products was obtained from the suppliers of those products, their published \nannouncements or other publicly available sources. IBM has not tested those products and cannot confirm the \naccuracy of performance, co mpatibility or any other clai ms related to non-IBM pr oducts. Questions on the \ncapabilities of non-IBM products should be addr essed to the suppliers of those products. \nStatements regarding IBM’s future direction or intent are subject to change or withdrawal without notice, and \nrepresent goals and objectives only. \nThis information contains exam ples of data and reports used in daily business operations. To illustrate them \nas completely as possible, the exam ples include the names of individual s, companies, brand",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 20,
            "total_chunks": 337
          }
        },
        {
          "id": "679d4e5b-7515-4a20-8096-c51a85f7774b",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "e them \nas completely as possible, the exam ples include the names of individual s, companies, brands, and products. \nAll of these names are fictitious and any similarity to  actual people or business enterprises is entirely \ncoincidental. \nCOPYRIGHT LICENSE:\nThis information contai ns sample application prog rams in source language, which illustrate programming \ntechniques on various operating platforms. You may co py, modify, and distribute these sample programs in \nany form without payment to IBM, for the purposes of developing, using, marketing or distributing application programs conforming to the application programming interface for the operating platform for which the sample \nprograms are written. These examples have not been th oroughly tested under all conditions. IBM, therefore, \ncannot guarantee or im ply reliability, serviceability, or function of  these programs. The sample programs are \nprovided “AS IS”, without warranty of any kind. IBM sha ll not be liable for any dama",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 21,
            "total_chunks": 337
          }
        },
        {
          "id": "5d3848b9-01a2-4366-8f5e-26d5d075a6dc",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " programs are \nprovided “AS IS”, without warranty of any kind. IBM sha ll not be liable for any damages arising out of your use \nof the sample programs. \n\nx Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentTrademarks\nIBM, the IBM logo, and ibm.com are trademarks or regi stered trademarks of International Business Machines \nCorporation, registered in many jurisdictions worldwide. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on the web at “Copyright \nand trademark information” at https://www.ibm.com/legal/copytrade.shtml  \nThe following terms are trademarks or registered trademarks of International Business Machines Corporation, \nand might also be trademarks or registered trademarks in other countries. \nIBM®\nIBM Cloud®\nIBM Research®\nIBM Security®IBM Watson®IBM Z®\nIBM z16™\nRedbooks®\nRedbooks (logo) ®X-Force®z/Architecture®\nz/OS®\nz/VM®\nz16™\nThe registered trademark Linux® is used pursua",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 22,
            "total_chunks": 337
          }
        },
        {
          "id": "8ae81f50-a57f-4f7b-9887-e8f02efc94ea",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ooks (logo) ®X-Force®z/Architecture®\nz/OS®\nz/VM®\nz16™\nThe registered trademark Linux® is used pursuant to a sublicense from the Linux Foundation, the exclusive \nlicensee of Linus Torvalds, owner of  the mark on a worldwide basis.\nZowe is a trademark of the Linux Foundation.\nMicrosoft, Windows, and the Windows logo are trademarks of Microsoft Corporation in the United States, \nother countries, or both.\nRed Hat, OpenShift, are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in the United \nStates and other countries.\nOther company, product, or service names may be trademarks or service marks of others. \n\n\n© Copyright IBM Corp. 2024. xiPreface\nProtecting workloads and sensitive data throug hout their lifecycle is a great concern across \nall industries and organizations.  Increasing demands to accelerate hybrid cloud adoption and \nintegration are changing the way data is securely stored, processed, and accessed. \nIn addition, regulatory guidelines and st andards are",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 23,
            "total_chunks": 337
          }
        },
        {
          "id": "882607fb-00e4-4e59-adab-9b285feb89cf",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " is securely stored, processed, and accessed. \nIn addition, regulatory guidelines and st andards are causing many businesses and \norganizations to implement zero trust policies and privacy enhancing techniques to restrict access to workloads as \nstate of least privilege  is established. A stat e of least privilege \nensures that no user or workload has any more access to data than is necessary. Confidentiality and integrity assurance for \ndata at rest  and data in transit  is typically provided \nthrough cryptography. Nevertheless, data in use  is generally unencr ypted while it is \nprocessed by the system, which can make data in use accessible to privileged users or workloads.\nIn the past, data owners relied upon operational assurance to control access to workloads \nand data. An operational  assurance approach en sures that a service pr ovider will not access \ncustomer workloads or data through specific operational procedures and measures. However, \nwith today's constant, unpredictable,",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 24,
            "total_chunks": 337
          }
        },
        {
          "id": "851d4b60-6f8c-4ef7-9f59-b45febe85ab6",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "hrough specific operational procedures and measures. However, \nwith today's constant, unpredictable, and always changing cyberthreats, operational assurance is not enough. \nA more robust technical assurance approach that is hardware-based is needed. A Trusted \nExecution Environment (TEE) or confidential computing platform does just that. A TEE ensures that no one can access sensitive workloads and data while in use, not even the service provider. A TEE can also protect the CI/CD pipeline from bad actors, enforce supply chain protection, and provide code integrity through cryptographic proofs and encryption.\nThis IBM® Redbooks® publication outlines how to apply common concepts of data protection \nand confidentiality and make use of a privacy-enh ancing technology-based solution that can \nbe implemented in a hybrid cloud environment. It describes the TEE technologies that are offered with IBM Z® and IBM LinuxONE (such as  IBM Secure Execution for Linux), and how \nthe IBM Hyper Protect Pl",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 25,
            "total_chunks": 337
          }
        },
        {
          "id": "8784224e-3a47-47c2-8419-ade1367301f0",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "IBM Z® and IBM LinuxONE (such as  IBM Secure Execution for Linux), and how \nthe IBM Hyper Protect Platform uses them.\nThis publication discusses how the various IB M Hyper Protect services ensure zero trust \ndata-centric security and data privacy end-to-end. It al so illustrates the business value \nthrough specific use case scenarios, covering relevant aspects of workload creation and evidence collection for regulatory comp liance of software supply chains.\nThis IBM Redbooks publication is for Chief Information Security Officers (CISOs), IT \nmanagers, security architects, security admi nistrators, cloud application developers, and \nanyone who needs to plan, deploy, and manage da ta security and confidentiality in a hybrid \ncloud environment. The reader is expected to have a basic understanding of IT security and hybrid cloud concepts.\n\nxii Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentAuthors\nThis book was produced by a  working at IBM Redbooks.\nBill White  ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 26,
            "total_chunks": 337
          }
        },
        {
          "id": "1b5d45bb-b45b-4200-9b38-97ee0e4efcb4",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "a Hybrid Cloud EnvironmentAuthors\nThis book was produced by a  working at IBM Redbooks.\nBill White  is an IBM Redbooks Project Leader and Senior IT Infrastructure Specialist at IBM \nPoughkeepsie, New York.\nRobbie Avill  is a Solutions Architect with the IBM Z Client Acceleration Team for IBM Hyper \nProtect Services and IBM Cloud®  products. He has worked at IBM for 8 years since joining as \nthe first ever apprentice at the IBM Lab in the UK. Robbie has been involved in various projects over the years including IBM z/OS® Co nnect and IBM’s Kubernetes service, as well \nas the Open Mainframe Projects, Galasa and Zowe.\nSandeep Batta  started his career as a z/OS Systems Programmer. He consulted Fortune \n100 clients in the NJ/NYC area, where he had a chance to work on game changing technologies that helped restore operations after disastrous events like 9/11. Sandeep created a “self-service” platform for developers in an IBM z/VM® farm, developed Cloud Deployment strategies, modernized fron",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 27,
            "total_chunks": 337
          }
        },
        {
          "id": "83bd4c67-d4f7-4aac-9b9c-315ff50913cc",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "platform for developers in an IBM z/VM® farm, developed Cloud Deployment strategies, modernized front-end network architecture for IBM’s internal infrastructure and developed offering management tools. Sandeep is currently a Lead Solutions Architect in the IBM Hyper Protect organization, where he works with clients in financial services, insurance, and digital asse ts sectors, to address their data-protection \nrequirements.\nAbhiram Kulkarni  is the Software Architect for IBM Hyper Protect at IBM India Systems \nDevelopment Lab, Bangalore. He received his Bachelor of Engineering degree at PESIT Engineering College, Bangalore. He has over 15 years of experience in Software development and joined IBM in 2013. In his recent roles, he has worked in various development projects across IBM Z. He has wo rked on projects that are related to Secure \nService Container, Secure Execution catering toward the clients that need zero trust architecture and Confidential Computing in hybrid cloud environm",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 28,
            "total_chunks": 337
          }
        },
        {
          "id": "67ba04c2-62c3-4a2b-b562-ae8198332b1d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "rd the clients that need zero trust architecture and Confidential Computing in hybrid cloud environment. \nTimo Kußmaul  is a Solution Architect and Master Inventor at the IBM Research® and \nDevelopment Lab in Böblingen, Germany. He holds a Dipl.-Inform. diploma in Computer Science from the University of  Stuttgart, Germany. Timo work s in the Hyper Protect Client \nAcceleration Team and has more than 25 years of experience in Software Development, application of AI and Search, Hybrid Cloud, Security and Confidential Computing. Timo is the author or coauthor of over 100 patents and multiple technical papers and books.\nStefan Liesche  is an IBM Distinguished Engineer wo rking with IBM Hybrid Cloud and Hyper \nProtect Services for IBM Z and IBM LinuxONE. His main focus is on security, transparency, and protection of data and services in flexible cloud environments. Stefan has worked in various areas as a technical leader within IBM, most recently as Chief Architect for IBM Cloud Hyper Prote",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 29,
            "total_chunks": 337
          }
        },
        {
          "id": "4bc517d6-33a8-494f-8e67-5203b36c15ea",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "s areas as a technical leader within IBM, most recently as Chief Architect for IBM Cloud Hyper Protect Services and IBM Watson® Talent Portfolio. He designed and built AI driven \nsolutions that transform recruiting and career decisions within global organizations, that not only enhance quality of decisions, but also allow HR functions to enhance fairness and tackle biases. Stefan also an innovator within the E xceptional Web Experience products for several \nyears with a focus on open solutions and integration. He has 25 years of experience as a technical leader, collaborating with business partners and clients through joint projects, as well as within IBM's product development organization. \n\n Preface xiiiNicolas Mäding  is Principal Product Manager at the IBM Lab in Böblingen, Germany. He \nreceived his Dipl. Ing. Degree in Electrical and Information Technology at the Technical University of Chemnitz, German y. He joined IBM in 2001 and worked in various development \nand management pos",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 30,
            "total_chunks": 337
          }
        },
        {
          "id": "29a184e7-c501-42e5-92c6-3312759894b3",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "y of Chemnitz, German y. He joined IBM in 2001 and worked in various development \nand management positions in IBM Systems Hardware Developmen t. He joined the \nZ-as-a-Service organization as Release Manage r of the Hyper Protect Hosting Appliance in \n2018 and became product manager for the Secure Execution based offerings. In 2023, he was appointed as Principal Product Manager for the Hyper Protect Platform and Confidential \nComputing with Linux on Z. He is author or co-author of 12 patents and several technical papers.\nChristoph Schlameuß  is a Software Architect at the IBM Research and Development Lab in \nBöblingen, Germany. He holds a Dipl.-Inf. diploma in Computer Software Engineering from the University of Stuttgart, Germany. Christoph works in the Hyper Protect Services Innovation Team and has over 12 years of experience in professional software development. Including seven years of experience in devel oping Confidential Computing products like \nSecure Service Container and IBM H",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 31,
            "total_chunks": 337
          }
        },
        {
          "id": "27a70256-3367-4d94-b345-fe68d486a202",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "f experience in devel oping Confidential Computing products like \nSecure Service Container and IBM Hyper Protect. He is the author or co-author of two patents.\nPeter Szmrecsányi  is a Solution Architect at the IBM Lab in Markham, Canada. He received \nan Electrical and Electronic Master of Engine ering with Honours degree from The University \nof Birmingham in the United Kingdom. Peter has over 20 years of experience in the field of Information Technology. His areas of expertise include Hyper Protect Services (Confidential Computing on the IBM LinuxONE platform). Peter is the author or co-author of two patents.\nThanks to the following people for their contributions to this project:Divya K Konoor (IBM Senior Technical Staff Member, IBM Hyper Protect Services IaaS)\nIBM India\nRene Meyer (Principal IBM Cloud Technical Specialist)\nIBM Germany\nLouisa Muschal (Product Manager IBM Hyper Protect Services)\nIBM Germany\nBarry Silliman (Consulting IT Specialist)\nIBM USA\nNow you can become a published",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 32,
            "total_chunks": 337
          }
        },
        {
          "id": "70274f01-d12b-454e-a392-1c4821d84901",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "rvices)\nIBM Germany\nBarry Silliman (Consulting IT Specialist)\nIBM USA\nNow you can become a published author, too!\nHere’s an opportunity to  spotlight your skills , grow your career, and become a published \nauthor—all at the same time! Join an IBM Redbooks residency project and help write a book in your area of expertise, while honing your experience using leading-edge technologies. Your efforts will help to increase pr oduct acceptance and customer satisfaction, as you expand \nyour network of technical contacts and relationships. Residencies run from two to six weeks in length, and you can participate either in person or as a remote resident working from your home base.\nFind out more about the residency program, browse the residency index, and apply online at:\nibm.com/redbooks/residencies.html\n\nxiv Applying Data Protection and Confidenti ality in a Hybrid Cloud EnvironmentComments welcome\nYour comments are important to us!\nWe want our books to be as helpful as possible . Send us your c",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 33,
            "total_chunks": 337
          }
        },
        {
          "id": "30c93874-e7b2-4524-9e04-62cab065e677",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "e\nYour comments are important to us!\nWe want our books to be as helpful as possible . Send us your comments about this book or \nother IBM Redbooks publications in one of the following ways:\n/SM590000Use the online Contact us  review Redbooks form found at:\nibm.com/redbooks\n/SM590000Send your comments in an email to:\nredbooks@us.ibm.com\n/SM590000Mail your comments to:\nIBM Corporation, IBM Redbooks\nDept. HYTD Mail Station P0992455 South RoadPoughkeepsie, NY 12601-5400\nStay connected to IBM Redbooks\n/SM590000Find us on LinkedIn:\nhttps://www.linkedin.com/groups/2130806\n/SM590000Explore new Redbooks publications, residencies, and workshops with the IBM Redbooks \nweekly newsletter:\nhttps://www.redbooks.ibm.com/subscribe\n/SM590000Stay current on recent Redbooks publications with RSS Feeds:\nhttps://www.redbooks.ibm.com/rss.html\n\n© Copyright IBM Corp. 2024. 1Chapter 1. A hybrid cloud with data security \nin mind\nMoving workloads to a cloud infrastructure rais es critical questions about data sec",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 34,
            "total_chunks": 337
          }
        },
        {
          "id": "908b563b-95a5-42b4-9486-b44d9751746b",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "curity \nin mind\nMoving workloads to a cloud infrastructure rais es critical questions about data security and \ndata privacy. A workload running in a cloud infrastructure owned by  a service provider, \nmaintained by the provider’s administrators, a nd shared with other tenants requires a novel \nand complete approach to protecting data and developing applications in a trusted environment. The data security concerns might even be prohibitive when you consider applications for a hybrid cloud infrastructure, more so if you cannot validate the trust of all parties. There might also be regulatory compliance to adhere to or business-specific needs for higher levels of data security controls that mandate protection at every stage of a digital interaction or data lifecycle. \nThis chapter discusses the threats, concerns, requirements, and the solution for a hybrid \ncloud infrastructure with data security in mind. The following topics are discussed:\n/SM5900001.1, “Identifying the threat” on page 2",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 35,
            "total_chunks": 337
          }
        },
        {
          "id": "00afaabe-83dc-467e-b0b0-bdc2615caedc",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "curity in mind. The following topics are discussed:\n/SM5900001.1, “Identifying the threat” on page 2\n/SM5900001.2, “Beyond regulatory and standard frameworks” on page 3\n/SM5900001.3, “Mitigating the threat” on page 4\n/SM5900001.4, “The solution explained” on page 71\n\n2 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment1.1  Identifying the threat\nAs cyberattacks continue to increase, the cost and reputation impacts of data breaches \nremain a top concern across all businesses and organizations1. To help understand how a \ncyberattack might threaten a system, application, or data, the well-known STRIDE threat \nmodel  can be used. This model conceptualizes the potential threats into six categories:\n1. Spoofing. An entity falsely identifies as another identity\n2. Tampering with data. The malicious modification of data3. Repudiation. An entity disputing its respon sibility, ownership or authorship of data, \nresources or operations\n4. Information disclosure. The exposur",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 36,
            "total_chunks": 337
          }
        },
        {
          "id": "f8d263b6-af80-475a-8662-9eea678e673a",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ty, ownership or authorship of data, \nresources or operations\n4. Information disclosure. The exposure of information to unauthorized entities, this means to \nentities who are not supposed to have access\n5. Denial of service. An attack that tries to make a system or resource unavailable6. Elevation of privilege. An attack to get elevated privileges or privileged access to a system \nor resource \nAnother way to look at threats is the path that an attacker might use to gain access to a \nsystem, application, or data, referred to as an attack vector. There are many well-known attack vectors, some of which have been used to successfully attack applications and gain \naccess to sensitive data (reference IBM X-Force® Threat Intelligence Index 2023 ). Typically, \nan attack vector corresponds to one or more of the STRIDE threats.\nThere are also different threats, mechanisms, and implications for managing and ensuring \nconfidentiality, integrit y, and availability depen ding on the state of th e da",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 37,
            "total_chunks": 337
          }
        },
        {
          "id": "e3dbf525-fb1d-42b0-9813-fc8bfbbbc7cf",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "aging and ensuring \nconfidentiality, integrit y, and availability depen ding on the state of th e data. As shown in \nFigure 1-1, data is categorized in three distinct states:\n1. Data in persistent storage is \nat rest .\n2. Data traversing the network between a source and a destination is in transit , alternatively \ncalled data in motion or data in flight .\n3. Data being processed by a system is  in use . During processing, the data is typically stored \nin a non-persistent state in CPU cache or system memory.\nFigure 1-1   Data states\nToday, cryptography is commonly used for data at rest and data in transit to provide both data confidentiality for stopping unauthorized viewing and data integrity for preventing or detecting unauthorized changes.\nA well-established best practice is to protect data in transit by using cryptographic protocols \nlike Transport Layer Security (TLS)\n2. Such cryptographic protocols protect the confidentiality \nand integrity of the data in transit between endpoints",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 38,
            "total_chunks": 337
          }
        },
        {
          "id": "df688fe0-ce9c-4997-86d4-56017f6d3e96",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "raphic protocols protect the confidentiality \nand integrity of the data in transit between endpoints in a public or private network.\n1  Reference: Cost of a Data Breach Report 2023\n2  Reference: HTTPS encryption on the web,  Google Transparency Report\n\n\nChapter 1. A hybrid cloud with data security in mind 3Data at rest should be stored in encrypted form only. IBM Cloud Object Storage  or volume \nencryption, such as Linux Unified Key Setup (LUKS), encrypts the data before writing to persistent storage and decrypts it for reading. Thus, an attacker cannot access the data at rest even with access to the physical storage device. \nHowever, data in use is generally unencrypted and easily accessible, as it is active data being \nprocessed by the system. For further discussion about protecting data in use, see 1.3, “Mitigating the threat” on page 4.\n1.2  Beyond regulatory and standard frameworks\nIn addition to the concerns related to protecting data in use, data sovereignty3 states that data \ni",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 39,
            "total_chunks": 337
          }
        },
        {
          "id": "88fe2984-15b6-4a02-866c-6ba54855a2cb",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "In addition to the concerns related to protecting data in use, data sovereignty3 states that data \nis bound to the laws and regulations of the country in which it is collected, stored, processed, and distributed and can be subject to data protection policies. Worldwide, there are many standards and regulations that are related to protecting data at rest and in transit. Although some of the standards specify the technologies required for compliance, encryption is applied to achieve compliance for most of them. \nA broad framework like a zero trust architecture  can also provide effective protection of an \norganization's data. It works by assuming every connection, endpoint, and domain are considered a threat. See Figure 1-2. The goal of a zero trust strategy is to eliminate implicit trust and to continuously validate every stage of a digital interaction.\nFigure 1-2   Zero trust framework threats\nCommon techniques for protection of data at rest and data in transit with a zero trust archit",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 40,
            "total_chunks": 337
          }
        },
        {
          "id": "3dfaae67-7dde-4711-b3b5-58b4885ad829",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "hreats\nCommon techniques for protection of data at rest and data in transit with a zero trust architecture are defined by the National Institute of Standards and Technology (NIST).\n4 \nHowever, other than the regulatory and standard frameworks, other areas of concern are \nprovided in the following list:\n/SM590000Protecting sensitive data that is in use by app lications in a hybrid cloud infrastructure, even \nfrom privileged users\n3  Refers to the notion that a country or jurisdiction has the authority and right to govern and control the data \ngenerated within its borders. \n4  Reference:  NIST Special Publication 800- 207, Zero Trust Architecture\n\n\n4 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment/SM590000Avoiding the need to trust potentially malicious  privileged users or potentially vulnerable \nand compromised infrastructure and intermediary components in a hybrid cloud environment\n/SM590000Achieving an air-gapped solution for applications and data from any ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 41,
            "total_chunks": 337
          }
        },
        {
          "id": "f290dae5-c240-44da-9dc0-84442fdb819a",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "brid cloud environment\n/SM590000Achieving an air-gapped solution for applications and data from any potential malicious \nactor\nIn other words, identify the technologies and techniques that can help with the following \nrequirements:\n/SM590000Reduce hybrid cloud migration risk\n/SM590000Secure sensitive data in use\n/SM590000Control applications a ccording to requirements\n/SM590000Ensure compliance with re gulatory requirements\n/SM590000Cause zero additional effort for consumers \n/SM590000Have zero impact on the functionality and av ailability of applications\n1.3  Mitigating the threat\nCyberthreats are constant, unpredictable, and always changing. Therefore, many businesses \nand organizations use the US Department of Commerce National Institute of Standards and \nTechnology (NIST) standards, guidelines, and recommendations as a baseline, then apply \nstronger policies and controls as needed or required. IBM also aligns with NIST guidelines and is committed to embedding security and privacy i",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 42,
            "total_chunks": 337
          }
        },
        {
          "id": "cde97830-ab3b-465a-8426-8f525074d04c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " required. IBM also aligns with NIST guidelines and is committed to embedding security and privacy into the design of all products and services.\n5 \nA zero trust architecture can help prevent unauthorized access to data and services. \nHowever, in a hybrid cloud infrastructure, ac cess control policies or operations to ensure \nisolation of applications and data in use might not satisfy all business-specific requirements. \nIdeally, data security and isolation are impl emented in all layers of the hybrid cloud \ninfrastructure, creating an air-gapped environment.6\nThe properties and features that are required to prevent or mitigate cyberthreats include the following characteristics:\n/SM590000Technical assurance\n/SM590000A Trusted Execution Environment for your application\n/SM590000Reduced trust boundary and trusted computing base\n/SM590000Controlling your applicatio n with separation of duty\n/SM590000Exclusive and full control over your cryptographic key\n/SM590000Support for your applicatio",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 43,
            "total_chunks": 337
          }
        },
        {
          "id": "fabf6fda-f899-4af9-b6e7-9e054f9d8d47",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "/SM590000Exclusive and full control over your cryptographic key\n/SM590000Support for your application OCI images\n/SM590000Support for hybrid cloud \n1.3.1  Technical assurance\n“Computer security assurance is the degree of confidence one has that the security \nmeasures, both technical and operational, work as intended to protect the system and the information it processes.” \n7\nTechnical assurance, or security assurance by  technical measures, can be distinguished from \noperational assurance, or security assurance by operational measures. Technical assurance \n5  Reference: IBM Security® and Privacy by Design (SPbD@IBM)\n6  An air-gapped environment has no direct connection to the inte rnet or to any other computer that is connected to \nthe internet.\n7  https://csrc.nist.rip/publications/nistpubs/800-12/800-12-html/chapter9.html\n\nChapter 1. A hybrid cloud with data security in mind 5is provided by the hardware, the firmware, and the software stack of a system and is included \nin the system ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 44,
            "total_chunks": 337
          }
        },
        {
          "id": "96afb332-4229-4a2d-be82-ac5a63161a11",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ed by the hardware, the firmware, and the software stack of a system and is included \nin the system technically. In contrast, operational assuranc e addresses whether required \nprocedures and regulations are followed and are compliant to operational requirements.\nThe difference between technica l assurance and opera tional assurance is illustrated in \nFigure 1-3. Operational assurance ensures that service providers do not access client workloads. Technical assurance ensures that service providers cannot access client workloads.\nFigure 1-3   Technical assurance versus operational assurance\n1.3.2  A Trusted Execution Environment for your application\nA hardware-based Trusted Execution Environment (TEE) is an execution environment that provides hardware-based technical assurance of the following properties:\n/SM590000Data confidentiality. Unauthorized actors cannot  view data while it is in use within the TEE.\n/SM590000Data integrity. Unauthorized actors cannot ad d, remove, or alter data w",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 45,
            "total_chunks": 337
          }
        },
        {
          "id": "35ae8650-38ae-4164-bbb7-57b87e0ee8c7",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "se within the TEE.\n/SM590000Data integrity. Unauthorized actors cannot ad d, remove, or alter data while it is in use \nwithin the TEE.\n/SM590000Code integrity. Unauthorized actors cannot add, remove, or alter code that is running in \nthe TEE. Examples of unauthorized actors: \n– other applications on the host system– the host operating system and hypervisor– other tenants of the host system– the cloud provider– privileged actors like system administrators– parties with physical access to the hardware\n/SM590000Code confidentiality. The TEE protects the code while in use from being viewed or \naccessed by unauthorized actors.\n/SM590000Programmability. The TEE can be programmed with arbitrary code.\n/SM590000Attestation. The process with which the TEE can provide evidence or measurements of its \norigin and current state. This evidence can be verified by another persona, for example the auditor persona, who can decide whether to trust the application running in the TEE or not. Typically, this",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 46,
            "total_chunks": 337
          }
        },
        {
          "id": "2785fa75-6ecb-4646-a585-ee049559fb9f",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " persona, who can decide whether to trust the application running in the TEE or not. Typically, this evidence is represented as  an attestation record. The contents of this \nrecord can be verified against workload and environment expectations. The attestation record is signed by a trusted key that is an chored in hardware that can be vouched for by \na trusted manufacturer. This signature provides assurance that the attestation record was \ncreated by the correct component and was not altered by an unauthorized entity.\nThe TEE provides code and data confidentiality and integrity. The TEE thus isolates your \napplication code and data from access and modification by unauthorized actors. The TEE isolates your application code and data from access and modification by other privileged or non-privileged actors, which includes potentially  malicious administrators, and other tenants.\n\n\n6 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentThis process must be technically a",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 47,
            "total_chunks": 337
          }
        },
        {
          "id": "2f14ceb2-522d-4820-abd2-faaaec77722f",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " Data Protection and Confidentiality in a Hybrid Cloud EnvironmentThis process must be technically assured by an implementation that is anchored in hardware. \nNo unauthorized entity is allowed to access or modify the code or the data of the application.\n1.3.3  Reduced trust boundary and trusted computing base \nYou can run your application in a TEE with technically assured isolation that you can remotely \nverify through attestation. Instead of trusting all the components and actors in your hybrid cloud infrastructure, you reduce the trust assumptions to the hardware-based implementation of the underlying TEE and the attestation protocol. In this scenario, components and actors can include those of your public cloud provider.\n1.3.4  Controlling your application with separation of duty\nThe infrastructure needs to ensure exclusive cont rol over your application and its properties is \nprovided to authorized personas only. This means that there is a process in place that ensures only you and",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 48,
            "total_chunks": 337
          }
        },
        {
          "id": "e399c24f-318f-4e57-ac41-860da79f5674",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "d to authorized personas only. This means that there is a process in place that ensures only you and authorized personas can control the application code, the properties of the runtime environment, an d how data at rest is protected,  such as controlling the seeds for \nvolume encryption. In addition, only you and authorized personas can control cryptographic keys and can pass secrets, such as cryptographic seeds for key derivation, to the application.\nSeparation of duty\nThe infrastructure needs to ensure separation of duty that allows multiple personas by defining different aspects and properties of your application. Also, a process needs to be defined to allow the different personas to cooperate securely. A persona must be able to define the properties and secrets of your application or its environment. Other personas must be prevented from accessing, modifying, or overwriting these properties and secrets. The infrastructure provides technical assurance of separation of duties and ena",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 49,
            "total_chunks": 337
          }
        },
        {
          "id": "6f26c50d-dd91-4a7e-ae39-457c16df3cf4",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "perties and secrets. The infrastructure provides technical assurance of separation of duties and enables secure cooperation of the various personas.\nMultiple personas can securely pass secr ets to your application\nA persona must be able to pass secrets to the application in a secure way, which means the \nsecret must be protected from other personas, the hybrid cloud infrastructure, and other \nactors. \nFor example, secrets can comprise TLS certificate and keys, cryptographic keys, \ncryptographic seeds used for key derivation, seeds used for volume encryption, and so on.\nSeparation of duty allows a first persona, a Workload Provider, to define the Open Container \nInitiative (OCI)  image for the application that should be run in a confidential computing \nenvironment. A second persona, a Workload Deployer, can define the TLS certificate and key that should be used by the application. This way, both personas cooperate to setup the application in confidential computing. The separation of dut",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 50,
            "total_chunks": 337
          }
        },
        {
          "id": "703d39a8-db8a-449e-a3f0-ffa53e163ff9",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "y, both personas cooperate to setup the application in confidential computing. The separation of duty process ensures that the Workload Provider cannot access or tamper the TLS certificate and key, and the Workload Deployer cannot access or tamper the OCI image.\nIn addition, both personas can cooperate to define a cryptographic seed for LUKS based \nvolume encryption or another type of cryptographic encryption or signature. Here, the first Workload Provider persona defines a first part of the seed, and the Workload Deployer defines a second part of the seed. The separation of duty process ensures that the seed parts are provided to the secure enclave with out exposing them to the other persona by \nkeeping them confidential from each other. In the reference architecture both parts of the seed are combined in the TEE to create the LUKS encryption key. This means that the resulting combined seed never leaves the TEE. Each persona knows only one part of the \n\nChapter 1. A hybrid cloud with ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 51,
            "total_chunks": 337
          }
        },
        {
          "id": "5a65fe65-ee0c-4bbb-9033-16b627bcacd1",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "seed never leaves the TEE. Each persona knows only one part of the \n\nChapter 1. A hybrid cloud with data security in mind 7seed. Thus this process prevents individual pe rsonas from being able to re-create the LUKS \nencryption key and a ccess data at rest. \n1.3.5  Exclusive and full contro l over your cryptographic key\nA cloud hardware security module (HSM) must provide full and exclusive control over your \ncryptographic keys. Without such mechanisms, privileged actors in the hybrid cloud environment can use known attack vectors to gain access to your keys and cryptographic material. A malicious privileged actor might then tamper or misuse your keys and secrets, for example, by maliciously decrypting or signing sensitive data.\nNote that control over your crypto graphic keys must be technically \nassured  by using \nhardware-based mechanisms.\n1.3.6  Support for your application OCI images\nThe TEE must support common programming and deployment models, such as OCI images. The TEE does not r",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 52,
            "total_chunks": 337
          }
        },
        {
          "id": "7017512c-889a-42e8-8837-4f84a21f20f2",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "he TEE must support common programming and deployment models, such as OCI images. The TEE does not require a specific programming model for your code. Also, it does not impose special limitations or requirements on your code. This means you can run your own \nOCI images in the TEE without code changes. The application developer does not need to learn a specific programming model. There is code comp atibility between the TEE and \nnon-confidential computing environments.\n1.3.7  Support for hybrid cloud\nThe infrastructure must support a hybrid cloud environment, comprising support for on-premises environments and provide confiden tial computing as a service in the cloud. \nIn addition, the infrastr ucture must not impose limits on sca lability of your application. This \nmeans, the infrastructure must scale with your performance and memory requirements.\n1.4  The solution explained\nConfidential computing8 can help enhance the zero-trust architecture by providing a trusted \nexecution environme",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 53,
            "total_chunks": 337
          }
        },
        {
          "id": "1426c0ae-7a44-4ba2-b0c6-f0d6997fdb29",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " computing8 can help enhance the zero-trust architecture by providing a trusted \nexecution environment for applications, even in an untrusted environment. The hardware-based security mechanism that is offered by confidential computing enables the processing and storage of sensitive data in a safe enclave that is isolated from the host \nsystem and other potentially vulnerable components. \nThis isolation is technically assured. Hence, the hardware-based and firmware-based \nprotection mechanisms provide technical means  for ensuring that unauthorized actors are \nprevented from accessing your application and your data. The access is prevented even if an attacker overcomes the access control systems of the cloud provider or gains access to \nanother vulnerable component in the cloud infrastructure. Confidential computing adds a layer of data security on top of a zero trust architecture by providing technically assured isolation, sometimes called a \nvirtual air-gap , of applications in T EEs.",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 54,
            "total_chunks": 337
          }
        },
        {
          "id": "503b1da5-e014-4e50-bcb8-0c18e2a72683",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "iding technically assured isolation, sometimes called a \nvirtual air-gap , of applications in T EEs. This virtual air-gap \nprevents malicious inside actors like authorized administrators from dumping the memory of \nyour application or directly accessing data of your application. The virtual air-gap prevents access to the file system or memory or your application.\n8  Confidential computing is the protection of data in us e by performing computation in a hardware-based, attested \nTEE. Reference: A Technical Analysis of Confidential Computing, a publication of the Confidential Computing \nConsortium, November, 2022.\n\n8 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentThe Hyper Protect Platform as the confidential computing solution can help solve the data \nprotection concerns in a hybrid cloud environment, an on-premises environment, and in an IBM Cloud-based SaaS model. \nThe solution consists of the following components:\n/SM590000A Trusted Execution Environment fo",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 55,
            "total_chunks": 337
          }
        },
        {
          "id": "e98ba2de-8e9c-419a-b400-e5c5fd4e0293",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "del. \nThe solution consists of the following components:\n/SM590000A Trusted Execution Environment for containerized applications in hybrid cloud \nenvironments.\nIt is available as IBM Hyper Protect Virtual Server on IBM LinuxONE and IBM Z platforms \nfor on-premises environments or as IBM Hype r Protect Virtual Server for Virtual Private \nCloud (VPC) in IBM Cloud. \n/SM590000A hardware security module (HSM) such as IB M Hyper Protect Crypto Services in IBM \nCloud and IBM LinuxONE or IBM Z platform with Crypto Express features for on-premises use.\n/SM590000A component for secure CI/CD: IBM Hyper Protect Secure Build \nThe solution provides a set of secure processes and patterns that are explained in the \nsubsequent sections. Also, the components that make up the solution are discussed in more detail in Chapter 2, “Understanding the solution” on page 15.\n1.4.1  The technology underlying the Hyper Protect Platform\nModern data-serving systems must immediately scale up and scale out in size, pe",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 56,
            "total_chunks": 337
          }
        },
        {
          "id": "c2aaf98f-71db-4a5e-aed5-c93119b473f8",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "per Protect Platform\nModern data-serving systems must immediately scale up and scale out in size, performance, and features. Virtualization is a key technology  that enables a hardware system to achieve \nthis level of scaling performance and maintain dense packaging and optimal use of resources.\nVirtualization is one of the core strengths of the IBM LinuxONE and IBM Z platforms, with the \ngoal of maximizing usage of computing resources and lowing the overall cost and resource requirements for running critic al workloads for enterprises.\nThe embedded architecture and hardware with the IBM LinuxONE and IBM Z platforms is \ndesigned around the ability to pa rtition resources to  be used independently in distinct \nvirtualized environments. The resources include compute, memory, and I/O connectivity of \nboth storage and network.\nHypervisors are a core part of the virtualizatio n technology stack with the IBM LinuxONE and \nIBM Z platforms. They are designed to enable si multaneous execution o",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 57,
            "total_chunks": 337
          }
        },
        {
          "id": "f0a79439-c323-42b0-a54e-0d0615de59ce",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ck with the IBM LinuxONE and \nIBM Z platforms. They are designed to enable si multaneous execution of multiple operating \nsystems and allocate the correct amount of virtualized resources. Hypervisors are necessary to securely run, manage, and isolate virtual servers or logical partitions on the IBM LinuxONE and IBM Z platforms. \nIBM Secure Execution for Linux\nThe IBM Hyper Protect Platform uses Secure Ex ecution for Linux. This is a hardware-based \nsecurity technology, which was introduced with the IBM Z and IBM LinuxONE platforms specifically for Kernel-based  Virtual Machine (KVM) guests\n9. It is designed to provide \nscalable isolation for individual workloads to help protect them from not only external attacks, but also insider threats. Secure Execution for Linux can help protect and isolate Linux workloads on-premises, or on IBM Z and IBM LinuxONE in hybrid cloud environments.\nSecure Execution for Linux isolates and protec ts KVM guests from hypervisor access. The \nhypervisor admin",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 58,
            "total_chunks": 337
          }
        },
        {
          "id": "12777e87-021f-4237-b6d7-5dc3a03496e3",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " Execution for Linux isolates and protec ts KVM guests from hypervisor access. The \nhypervisor administrator can manage guests and deploy workloads, but cannot view data. \n9  Virtual machines (VMs) are also referred to as guests or images.\n\nChapter 1. A hybrid cloud with data security in mind 9Multiple applications that are running in a logical partition (LPAR) under KVM have fully \nisolated environments.\nTo achieve this, the IBM LinuxONE and IBM Z firmware provides an ultravisor . The ultravisor \nis a trusted firmware component. It uses me mory-protection hardware, and the owner of a \ngiven KVM guest can securely pass secret information to the ultravisor by using the public host key.\nFigure 1-4 illustrates the Secure Execution for Linux environments  running in KVM guests in \nan IBM LinuxONE or IBM Z platform.\nFigure 1-4   Secure Execution for Linux environment\nFor more information, refer to IBM Secure Execution components .\nKernel-based virtual machines\nKernel-based virtual machine (",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 59,
            "total_chunks": 337
          }
        },
        {
          "id": "3816edd8-f8c1-4462-9e5c-55cf4e498214",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "er to IBM Secure Execution components .\nKernel-based virtual machines\nKernel-based virtual machine (KVM) is a key software technology for IBM LinuxONE and \nIBM Z platforms. It is a Type 2 hypervisor10 that provides simple, cost-effective virtualization \ntechnology for Linux workloads. It allows sharing and managing of allocated resources and can coexist with other types of virtualization technologies that are simultaneously running in IBM LinuxONE or IBM Z.\nOne of the advantages of KVM vi rtualization is the familiar standa rd Linux user interfaces for \nopen source developers, which can help make adoption and integration easier with hybrid environments.\nTogether with Secure Execution for Linux, data and workloads that run in KVM guests are \nisolated and protected from being inspected or  modified from the moment the guest is built, \nthrough the boot process, and during workload execution.\n10  A Type 2 hypervisor runs as a software layer. It does not run directly on the un derlying hard",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 60,
            "total_chunks": 337
          }
        },
        {
          "id": "28bbecae-5175-4420-83a8-4c51ef896eb6",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ".\n10  A Type 2 hypervisor runs as a software layer. It does not run directly on the un derlying hardware, but rather like an \napplication in an OS, sharing and managing its al located resources with virtual machines.\n\n\n10 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentKVM on IBM LinuxONE and IBM Z is supported through the following Linux distribution \npartners:\n/SM590000Red Hat Enterprise Linux (RHEL)\n/SM590000SuSE Linux Enterprise Server\n/SM590000Canonical Ubuntu Server\nIsolation through IBM Proces sor Resource/Systems Manager\nThe IBM LinuxONE and IBM Z platforms have a unique implementation of its hypervisor at \nthe hardware and firmware level. It is part of the base system that fully virtualizes all system resources and runs without any extra software. This type 1 hypervisor\n11 runs directly on bare \nmetal. With it, multiple  isolated partitioned environments can be created on the same physical \nserver. These isolated environments are known as logical part",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 61,
            "total_chunks": 337
          }
        },
        {
          "id": "fdaff63b-b5fd-47c8-8973-b2bd282a0d6f",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "s can be created on the same physical \nserver. These isolated environments are known as logical partitions (LPARs). \nEAL 5+ isolation and cryptographic key protection \nIBM LinuxONE and IBM Z platforms feature EAL 5+ isolation. EAL5+ is a regulatory certification for logical partitions that verifies the separation of each partition to improve security.\n12 Therefore, you can run many virtual servers concurrently. The platforms can \nisolate and protect each LPAR, whether z/VM, z/OS, or KVM, as though they were running on physically separated servers.\nThe LPARs are isolated from each other, but VMs or containers within the same LPAR are not \nas isolated. Secure Execution for Linux is an IBM LinuxONE or IBM Z hardware capability. By \nusing Secure Execution for Linux, a KVM hypervisor can isolate virtual machines and containers from each other within an LPAR.\nA Processor Resource/Systems Manager-based LPAR is the only technology that is \ncommercially available to provid e this certified leve",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 62,
            "total_chunks": 337
          }
        },
        {
          "id": "b891eae6-af4a-48cc-9ec5-0acaba273a2d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "er-based LPAR is the only technology that is \ncommercially available to provid e this certified level of isolat ion between logical partitions.\nIn addition, cryptographic key protection that is used by Secure Execution for Linux, is \nachieved by dedicated cryptographic coprocessors . The CP Assist for Cryptographic Function \n(CPACF) delivers cryptographic and hashing ca pabilities in support of key protection \noperations. The Crypto Express adapter is used to create the fortified data perimeter by using the IBM LinuxONE or IBM Z protected key in which the keys that are used in the encryption process are not visible to the applications and operating system.\nEach LPAR on an IBM LinuxONE or IBM Z platform has its own uniquely generated and \nassigned cryptographic keys that are held in a secure hardware area. This configuration provides a level of cryptographic isolation between secure environments that required by \nmany regulatory compliance frameworks.\n1.4.2  Features of the Hyper Protec",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 63,
            "total_chunks": 337
          }
        },
        {
          "id": "14b1859b-6ee7-41a1-a33c-11b50b4bd2bf",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "onments that required by \nmany regulatory compliance frameworks.\n1.4.2  Features of the Hyper Protect Platform\nThe features that are offered by the Hyper Protect Platform create the foundation for an end-to-end secure environment. This platform provides protection of code and data and supports a consistent developer experience.\nBring your container runtime\nFor OCI images, Hyper Protect Virtual Server (HPVS) provides  a trusted container runtime \nthat provides the benefits and  properties of a TEE. HPVS s upports any OCI images that are \nbuilt for IBM LinuxONE and IBM Z, which means images do not need to be adapted \n11  A type 1 hypervisor runs directly on the underlying computer's physical hardware, interacting directly with its CPU, \nmemory, and physical storage and network I/O.\n12  Reference to EAL 5+ isolation: \nhttps://www.bsi.bund.de/SharedDocs/Zertifikate_CC/CC/Serveranwendungen_Virtualisierung/1133.html\n\nChapter 1. A hybrid cloud with data security in mind 11specifically for HPV",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 64,
            "total_chunks": 337
          }
        },
        {
          "id": "63d50658-7dbe-4e7a-b800-a50b0480f2a5",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "rtualisierung/1133.html\n\nChapter 1. A hybrid cloud with data security in mind 11specifically for HPVS. The a pplication code does not need to be chan ged and doe s not need \nto adhere to a specific programming model. Yo u can run your existing application code and \nimages and on the Hyper Protect Platform.\nVirtual air-gap: memory protection and isolation\nThe IBM Hyper Protect Platform uses IBM Secure Execution for Linux to ensure data and \ncode confidentiality and integrity of the deployed application, including against privileged users and infrastructure components. \nSeparation of duty\nHPVS supports separation of duty  with predefined personas, as  described in Table 1-1. The \npredefined personas are based on least priv ilege and zero trust principles. There is no \nassumed trust for what is expected to be deployed.\nTable 1-1   Personas for the separation of duties\nPersona Description\nContainer Image Provider An application can consist of one or more container images. The Container Ima",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 65,
            "total_chunks": 337
          }
        },
        {
          "id": "f7eb9155-cf81-4477-ad23-8da1eb6fefe8",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ntainer Image Provider An application can consist of one or more container images. The Container Image \nProvider is responsible for building the images in a secure manner and according to best \npractices. This ensures that the images are valid and free fr om vulnerabilities. Logs and \nevidence of the build are retained for potential future use for auditing.\nWorkload Provider This can be the same persona as the Container Image Provider. Alternatively, the \nWorkload Provider is a separate persona, which can combine container images from \ndifferent sources or different Container Image Providers. The Workload Provider \npersona defines one or more containers to be deployed and defines certain properties \nof the application environment, such as  seeds used for data volume encryption. \nNo one else can change the container images or  redefine the application properties that \nare specified by the Workload Provider.For this purpose, the Workload Provider us es the workload section of the contrac",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 66,
            "total_chunks": 337
          }
        },
        {
          "id": "7ce02b12-19bf-47ef-91b7-0f56062d14e0",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " Workload Provider.For this purpose, the Workload Provider us es the workload section of the contract to \ndefine the container images and application properties in a secure manner. They can \nthen pass the encrypted contract section to  other personas without exposing the content \nof the workload contract section. \nWorkload section is described under “Contract mechanism” on page 12.\nWorkload Deployer This persona is responsible for dep loying an HPVS instance for running the application \nin the hybrid cloud environment.\nThe Workload Provider provides the encrypt ed workload contract section to the \nWorkload Deployer. This allows the Workl oad Deployer to supplement the contract \nsection that is provided by the Workload  Provider with an additional environment \ncontract section that defines further proper ties of the application and its environment. \nSome of the properties include information such as defining where the logs should be \nsent to and seeds for data volume encryption.\nThe Wor",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 67,
            "total_chunks": 337
          }
        },
        {
          "id": "801872f2-46f2-4c4d-9299-a11ad717dddb",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ion such as defining where the logs should be \nsent to and seeds for data volume encryption.\nThe Workload Deployer is responsible for the application deployment and application \navailability.\nThe following list describes some of th e aspects of the Workload Deployer :\n/SM590000It can control the networki ng, compute, and storage resources made available to \nthe application.\n/SM590000It can influence network traffic in and out of the application.\n/SM590000It cannot change the container images to be deployed\n/SM590000It cannot change the application properties that are defined by the Workload \nProvider.\nIf the Workload Provider and the Workload Deployer specify seeds for data volume \nencryption, both seeds are combined. This me ans that individual personas do not have \nenough information for decrypting encrypted data volumes.\n\n12 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentContract mechanism\nHPVS uses a contract mechanis m to enable the Wo rkload Provider an",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 68,
            "total_chunks": 337
          }
        },
        {
          "id": "349259f4-4390-49dd-af33-c2fe0ca91f27",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ud EnvironmentContract mechanism\nHPVS uses a contract mechanis m to enable the Wo rkload Provider and the Workload \nDeployer personas to define the container images and the properties of the application and its environment in a secure way.  The contract is a document comprising multiple sections, \nwhich can be independently encrypted. The co ntract can be signed a nd is provided to HPVS \nduring deployment. During the initialization pr ocess of the HPVS instanc e, the Hyper Protect \nContainer Runtime (HPCR) decrypts the contract, verifies the contract signature, and creates the passphrase that is used for volume encryption based on the seeds that are contained in the contract. The container images are set up as defined in the contract according to the properties in the environment variables.\nThe contract has several sections, two of these are mandatory: \n/SM590000Workload (mandatory). This section contains th e definition of the application workload in \nthe form of a docker compose file",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 69,
            "total_chunks": 337
          }
        },
        {
          "id": "a51d35fb-682e-41d4-b205-ccd52e5a9780",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "s section contains th e definition of the application workload in \nthe form of a docker compose file or a pod descriptor. It defines one or more container images, the container image registry where it resides, and the information and credentials that are required to download and validate the image.\nThe section can also comprise information about data volumes, the seed for deriving the \nvolume encryption passphrase , and environment variables.\n/SM590000env (mandatory). This section describes the environment for the application. It comprises \nseveral subsections to define information about logging, such as where the logs should be sent to; data volumes; another seed for deriving the volume encryption passphrase; environment variables; and optionally, the public part of the contract signing key.\n/SM590000attestationPublicKey (optional). This section pr ovides a public RSA key, which is used to \nencrypt the attestation document.\n/SM590000envWorkloadSignature (optional). This section contai",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 70,
            "total_chunks": 337
          }
        },
        {
          "id": "003c6089-4960-4aac-b1dc-b91805b346e5",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " to \nencrypt the attestation document.\n/SM590000envWorkloadSignature (optional). This section contains the signature of the other sections \nof the contract, pinning to a specific workload part to the env part. An Auditor, a Workload Provider, or a Workload Deployer persona can choose to sign a contract before it is passed as input.\nOnly the Hyper Protect Platform can decrypt an  encrypted contract. Therefore, by using the \ncontract mechanism, the Workload Provider persona can define and encrypt the workload \nsection of the contract, then pass it to the Workload Deployer persona. This way, the \nWorkload Provider can hide the content of the workload section of the contract, such as the actual container images of the application, from the Workload Deployer, and allow the Workload Deployer to provision the HPVS instance for the application.Auditor The Auditor is the pers ona with respons ibility for veri fying the integrity of the HPVS \ninstance and the deployed application. Fo r example, ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 71,
            "total_chunks": 337
          }
        },
        {
          "id": "415d05d2-4611-4e85-9ca7-574e0d1ba6fc",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "lity for veri fying the integrity of the HPVS \ninstance and the deployed application. Fo r example, the auditor ensures that the \nexpected container image and application proper ties are used by the application. It does \nthis by obtaining and verifying a trusted attestation record. T he contents of this \nattestation record can be verified against the expected contract sections, comprising the \ncontainer image definition and the applicat ion properties. The Auditor, or possibly \nanother persona, like the Workload Provider or  Workload Deployer, can choose to sign \na contract before it is passed as input. A contract signature is an optional feature that \ncan be used with the contract. Contracts t hat are in plain text or encrypted can be \nsigned. \nInfrastructure/System admin This role includes the system or cloud administrator and support personas of the \ninfrastructure like a Site Reliability Engineer (SRE). This role has responsibility for the \nunderlying hardware and infrastructure, s",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 72,
            "total_chunks": 337
          }
        },
        {
          "id": "91a4c99a-1095-4797-bf21-acbce956064c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "lity Engineer (SRE). This role has responsibility for the \nunderlying hardware and infrastructure, such as networks, but must not be able to \naccess confidential data or tamper with the application. Persona Description\n\nChapter 1. A hybrid cloud with data security in mind 13The Workload Deployer can define and encrypt the env section of the contract. The Workload \nDeployer then combines the workload and the env section, optionally adds the envWorkloadSignature and the attestationPublicKey sections, and then deploys the HPVS instance using the contract. As both sections of the contract are encrypted, no intermediate infrastructure component and no other party including privileged actors can view the contents of the contract. By adding the envWorkloadSignature, the contract can be protected against modification or tampering.\nAttestation\nThe HPVS instance provides an A ttestation Record that is secu rely generated and signed by \nthe HPCR during instance initialization. This Attestation Re",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 73,
            "total_chunks": 337
          }
        },
        {
          "id": "d2b2da5e-9186-44ba-8176-caedd56af21f",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "t is secu rely generated and signed by \nthe HPCR during instance initialization. This Attestation Record is made available to the \ncontainers in this HPVS instance. The signing key is published and can be validated to a 3rd-party certificate authority. Optionally, the Attestation Record can be encrypted by a public key that is defined by the Workload Deployer an d provided in the Contract. It is a best practice \nthat the Auditor provides the public key to the Workload Deployer and is part of the signature of the envWorkloadSignature of the contract. The encryption provides proof to the Auditor that no one can replay the attestation record. \nThe Attestation Record contains measurements of the original base image, the compressed \nroot filesystem, and the cloud initialization options, which include the contract of this HPVS instance. \nAttestation enables the Auditor to validate th e integrity of the HPVS in stance and th e integrity \nof the contract. The Auditor can compare the attestatio",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 74,
            "total_chunks": 337
          }
        },
        {
          "id": "d3e27cce-172f-4c15-9b99-4ce16dc3d3cf",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ty of the HPVS in stance and th e integrity \nof the contract. The Auditor can compare the attestation record against the reference values specific for the HPCR. The Auditor can also compare the checksum of the user data element of the attestation record against the checksum of the expected contract. If this validation succeeds because the checksums are identical, then this proves that the HPVS instance uses the expected  contract. This means that the HPVS instance will run the expected \nenvironment and container images that are defined in the workload section of the contract.\nData volume encryption\nThe data volume that can be a ttached to an HPVS instance is protected by Li nux Unified Key \nSetup (LUKS) volume encryption. The LUKS passphrase is automatically derived from the seeds that are provided by the Workload Provider persona and the Workload Deployer persona. This means that the Workload Provider or the Workload Deployer individually cannot re-create the LUKS passphrase because t",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 75,
            "total_chunks": 337
          }
        },
        {
          "id": "253656e5-fff6-4dce-8099-1dc635e08fb3",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "rkload Provider or the Workload Deployer individually cannot re-create the LUKS passphrase because they know only their respective seed. Also, no other persona or party can re-create the LU KS passphrase because they do not know any \nof the seeds that are used for passphrase derivation.\n1.4.3  Cryptography and Hy per Protect Crypto Service\nA hardware security module (HSM) is a device or service that safeguards and manages \nsecrets, such as cryptographic keys, and pe rforms cryptographic functions such as key \ncreation, key derivation, encryption, decryption, and a signature. An HSM contains one or more cryptographic processors. A cloud HSM is  a cloud service that provides the same \nfunctions as a physical HSM.\nHyper Protect Crypto Services (HPCS) is a hybrid cloud key management service, which is \nbased on FIPS 140-2 Level 4 certified hardware on the IBM LinuxONE or IBM Z platform.\nHPCS supports various programming models like PKCS11 or GREP11. GREP11 is a \nstateless programming model",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 76,
            "total_chunks": 337
          }
        },
        {
          "id": "e4dd015c-e8ec-4ba2-82f0-9d0b6aa7e300",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " supports various programming models like PKCS11 or GREP11. GREP11 is a \nstateless programming model with which cryptographic functions are run in the HSM. Cryptographic material, such as keys, are created in the HSM but are stored outside of the HSM by the client application.\n\n14 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentAn application deployed to HPVS can use a stateless cryptographic API like GREP11 to \ncreate and use cryptographic keys. It can store the keys in the boundary of the HPVS instance on attached devices and use volume encryption and protection of memory and data in use.\nYou can use separation of duty that is enabled  by the contract concept and cloud HSMs to \ndesign and implement advanced cryptographic mechanisms for deriving keys from combined seeds. Combined seeds are a combination of different seeds owned by Workload Provider and Workload Deployer. \n1.4.4  Hyper Protect Secure Build\nThe application container images must be built in a st",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 77,
            "total_chunks": 337
          }
        },
        {
          "id": "66f6f361-9f6c-441f-b09b-cc93a9a0aaf0",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " Deployer. \n1.4.4  Hyper Protect Secure Build\nThe application container images must be built in a standardized, repeatable manner, and built by using secure components. The Container  Image Provider persona is responsible for \nfollowing state-of-the-art CI/CD processes and best practices.\nIn addition, the Container Im age Provider can use Hyper Pr otect Secure Build (HPSB) to \nbuild a trusted container ima ge within a secure enclave that  is provided by HPVS. The \nenclave is isolated such that developers can ac cess the container only by using a specific API \nand the cloud administrator cannot access the contents of the container. Therefore, the image that is built can be highly trusted. Specifically, the build server cryptographically signs the image, and a manifest, which is a collection of materials that are used during the build and which can be used for audits. Because the encl ave protects the signing keys, the signatures \ncan be used to verify whether the image and manifest are ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 78,
            "total_chunks": 337
          }
        },
        {
          "id": "1e3c0eda-5088-49cf-8a97-32513e864e18",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "protects the signing keys, the signatures \ncan be used to verify whether the image and manifest are from the build server, and not elsewhere.\nIn summary, HPSB uses HPVS to protect the bu ild process, the build  results, the build \nevidence, and the signing keys for signing the images and the manifest against malicious privileged actors. This prevents potential ma licious insiders from inserting malware or \notherwise tampering with the application container images and from stealing the signing keys for container image signature.\nMore detailed descriptions of the components that make up the Hyper Protect Platform, as \nwell as some common use cases can be found in Chapter 2, “Understanding the solution” on page 15.\n\n© Copyright IBM Corp. 2024. 15Chapter 2. Understanding the solution\nWith the integration of IBM Hyper Protect services and confidential computing, you can \nachieve end-to-end data and workload protection in a hybrid cloud environment. The IBM Hyper Protect Platform is a featur",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 79,
            "total_chunks": 337
          }
        },
        {
          "id": "b25ee11e-476b-4b5a-b1e7-442d0b89aeb6",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ta and workload protection in a hybrid cloud environment. The IBM Hyper Protect Platform is a feature of IBM Li nuxONE and IBM Z that offers hardware-level \nsecurity and isolation for virtual servers. It not only protects data and workloads in production, but also allows you to securely build, deploy, and manage mission-critical applications in a \nhybrid cloud environment.\nThe hybrid cloud solution that is discussed in this chapter consists of trusted virtual servers \nand services that ensure your workloads and dat a are always secure, private, and protected \nfrom internal and external threats.\nThis chapter contains  the following topics:\n/SM5900002.1, “IBM Hyper Protect services and a secure hybrid cloud” on page 16\n/SM5900002.2, “IBM Cloud Virtual Private Cloud” on page 18\n/SM5900002.3, “Hyper Protect Virtual Server” on page 18\n/SM5900002.4, “Hyper Protect Secure Build” on page 24\n/SM5900002.5, “Cryptographic agility is th e key to SecDevOps” on page 25\n/SM5900002.6, “Hyper Protect C",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 80,
            "total_chunks": 337
          }
        },
        {
          "id": "6c79edfa-cb84-4c45-890a-58ac4c635ff1",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "900002.5, “Cryptographic agility is th e key to SecDevOps” on page 25\n/SM5900002.6, “Hyper Protect Crypto Services” on page 25\n/SM5900002.7, “Crypto Express Network API for Secure Execution Enclaves” on page 28\n/SM5900002.8, “Storage and repositories in the cloud” on page 29\n/SM5900002.9, “Common usages” on page 312\n\n16 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment2.1  IBM Hyper Protect service s and a secure hybrid cloud\nIBM Hyper Protect services that are offered wit h IBM Cloud and the IBM Secure Execution for \nLinux1 on IBM Z and IBM LinuxONE platforms can provide a solution that isolates workloads \nand protects sensitive data throughout its lifecycle, regardless if its state is at rest, in transit, or in use.\nFigure 2-1 illustrates the compon ents of an example secure hybrid cloud solution. The \nsolution consists of a public cloud domain, repr esented by the IBM Cloud, and a private cloud \ndomain, represented by an on-premises cloud. Each cloud domain ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 81,
            "total_chunks": 337
          }
        },
        {
          "id": "c4fe6382-d95b-4313-a071-be026f30ea5d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " the IBM Cloud, and a private cloud \ndomain, represented by an on-premises cloud. Each cloud domain has multiple components that provide specific isolat ion and protection capabilities.\nFigure 2-1   Secure hybrid cloud solution example\nThe IBM Cloud Virtual Private Cloud (VPC) domain includes the following components:\n/SM590000Hyper Protect Secure Build. A trusted container image within a secure enclave that is \nprovided by Hyper Protect Virtual Server.\n/SM590000IBM Container Registry (ICR). A highly available, scalable, and encrypted private image \nregistry. A storage and distribution service with public or private images that are used to create containers. The ICR is hosted and managed by IBM.\n/SM590000Hyper Protect Crypto Services . A single-tenant key management service with which you \ncan create, import, rotate, and manage keys with standardized APIs.\n/SM590000IBM Cloud Object Storage (COS). A flexible storage for unstructured data, with additional \nservices like secure encryption",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 82,
            "total_chunks": 337
          }
        },
        {
          "id": "c78ec700-eac5-4977-a782-793ef416aba5",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ge (COS). A flexible storage for unstructured data, with additional \nservices like secure encryption, backup and recovery, data archiving, and fast data transfer.\n1  IBM Secure Execution for Linux allows  you to create a confidential com puting environment that has encrypted \nLinux images running on a public, privat e, or hybrid cloud with data in-use protection. It requires feature codes \n0115 and 3863.\n\n\nChapter 2. Understanding the solution 17The IBM LinuxONE or IBM Z on-premises cloud domain includes the following components:\n/SM590000Hyper Protect Virtual Server. Deploy isol ated workloads, protected by IBM Secure \nExecution for Linux, which is also known as confidential computing or secure enclave. \n/SM590000Hyper Protect Container Runtime. Consists of  different components or services that use \ncertain sections in the  contract to ensure data protection. The HPCR has container \nruntime support, and the image is not SSH enabled. It is a locked-down image. \n/SM590000Block storage.",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 83,
            "total_chunks": 337
          }
        },
        {
          "id": "519a6b9e-d7d4-4f3b-98bd-1ffeaa41f328",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ntime support, and the image is not SSH enabled. It is a locked-down image. \n/SM590000Block storage. Allows for the creation of storage volumes to which Linux on IBM Z images \ncan connect. The storage volumes can be encrypted to protect data at rest. \n/SM590000Crypto Express adapters. Tamper sensing and responding FIPS 140-2 level 4 hardware \nsecurity modules (HSMs) that can perform advanced symmetric and asymmetric cryptographic operations and can securely store encryption keys.\n/SM590000Crypto Express Network API for Secure Execution Enclaves. Runs in an IBM Secure \nService Container LPAR and provides a REST API for application access to the Crypto Express adapters and domains.\nThe following Hyper Protect components can be deployed either on-premises or in IBM \nCloud:\n/SM590000Hyper Protect Virtual Server (HPVS)\n2\n/SM590000Hyper Protect Secure Build (HPSB)\n/SM590000IBM Container Registry (ICR)\nThere are advantages to using these components  on IBM Cloud. Because the infrastructure \na",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 84,
            "total_chunks": 337
          }
        },
        {
          "id": "e797c48a-fced-4edf-b1f0-ef46fcfaf1e9",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ry (ICR)\nThere are advantages to using these components  on IBM Cloud. Because the infrastructure \nand management of IBM Cloud are the responsibility of IBM, no on -premises staff need to be \ntrained to operate these resources. On IBM Cloud, the resources are ready to be used. No resource provisioning or acquisition is necessa ry, and no time needs to be spent on design, \ninstall, and setup of the infrastructure.\nThere are also disadvantages to running these components on IBM Cloud. Thought must be \ngiven to the availability of these components, so  redundant internet links or direct links might \nbe needed. \nConsideration should also be given to where the developer staff resides or works. If the staff \nworks remotely, then IBM Cloud might be more suitable for H PSB and ICR as it can be more \neasily accessible to the development staff. \nIf the development staff is on-site, then an on-premises deployment might make more sense \nas it is more easily secured from outside tampering or access",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 85,
            "total_chunks": 337
          }
        },
        {
          "id": "471f7763-5a7f-4510-af5d-bd1e3ae1dab9",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ises deployment might make more sense \nas it is more easily secured from outside tampering or access violation and can be more readily available as there are no dependencies on external network links. \nThe services and components that make up the secure hybrid cloud solution are described in \nsubsequent sections.\n2  On-premises offerings include HPVS for IBM LinuxON E or IBM Z, and the IBM Cloud offering includes \nHPVS for VPC.Note:  HPSB has documentation for both IBM Cloud deployment  and on-premises \ndeployment .\n\n18 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment2.2  IBM Cloud Virtual Private Cloud\nIBM Cloud Virtual Private Cloud (VPC) is a pu blic cloud offering with which you create a \nprivate cloud computing environment on a shared public cloud infrastructure. Bu using an IBM \nCloud VPC  you can define and control a highly res ilient virtual network that is logically \nisolated from all other public cloud tenants. Hence, creating a private, secure place",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 86,
            "total_chunks": 337
          }
        },
        {
          "id": "82ff94c8-5500-4e75-9109-63db6dc667ef",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " is logically \nisolated from all other public cloud tenants. Hence, creating a private, secure place on the public cloud.\nLike with any other public cloud offering, you can choose the required compute, storage, and \nnetworking resources to be prov isioned with maximum av ailability and scalability, plus various \ncost-effective options. \nIBM Cloud VPC is purpose-built with inherent security to meet regulatory compliance \nstandards, and multiple hardware and software solutions for IaaS, PaaS, and hybrid cloud needs. IBM Cloud VPC has a global network of multizone region s and availability zones for \nquick access, low-cost migration, low latency, and certified security. It supports hybrid or multicloud platforms.\nA VPC can be created in IBM Cloud by following the instructions in Creating and configuring a \nVPC. \n2.2.1  IBM Cloud virtual serv er instance on IBM LinuxONE\nA virtual server instance on IBM LinuxONE is an IBM Cloud VPC option that can be used to \nhelp migrate or recompile compo",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 87,
            "total_chunks": 337
          }
        },
        {
          "id": "01ad4ace-dd18-4cba-830f-a3040b396ef7",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ance on IBM LinuxONE is an IBM Cloud VPC option that can be used to \nhelp migrate or recompile components to the s390x instruction set architecture (ISA). This option can also be used to test container-based workloads and solutions before their deployment within an HPVS as de scribed in 2.3, “H yper Protect Virtual  Server” on page 18. \nAll this with security and compliance in mind.\nThe IBM Cloud Virtual Private Cloud (VPC) user interface (UI) can be used to create virtual \nserver instances with simple steps. For more information, see Virtual server for VPC .\nThe available s390x instance profiles  and s390x virtual server images  can be selected based \non your requirements. \n2.3  Hyper Protect Virtual Server\nAn HPVS takes advantage of IB M Secure Execution for Linux to create a se cure boundary \naround each workload, which ensures unauthorized users do not have access to the workload or data. Workloads are locked down by  individual, instance-level, secure enclaves. \nFigure 2-2 on page",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 88,
            "total_chunks": 337
          }
        },
        {
          "id": "51530e55-d8d5-40b9-bf6f-0836f1b0d3c5",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "data. Workloads are locked down by  individual, instance-level, secure enclaves. \nFigure 2-2 on page 19 illustrates the components that make up the Hyper Protect Platform. \nFor example, the Hyper Protect Container Runtime (HPCR) includes the Bootloader and Hyper Protect Services to validate the authenti city and trust of the workload. This section \nprovides an outline of the architectural components such as contracts with separation of duty, volume encryption, attestation, and Hyper Protect services.\n\nChapter 2. Understanding the solution 19Figure 2-2   Technical assurance - Hyper Protect Platform\n2.3.1  Bootloader\nThe bootloader is the first part of the Linux start-up process that brings up a KVM virtual \nmachine.\nThe bootloader is responsible for the following:\n/SM590000Setting up the LUKS encrypted boot partition.\n/SM590000Writing the Operating System into the boot partition.\n/SM590000Decrypting the user provided contract and placing a decrypted copy in a known file system \nlocation",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 89,
            "total_chunks": 337
          }
        },
        {
          "id": "abe2bf86-bf5a-4daf-a24c-1bc9978eb81c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "0Decrypting the user provided contract and placing a decrypted copy in a known file system \nlocation to be processed by the other components after the boot completes. The bootloader destroys the private key after decryption. \n/SM590000Writing a cryptographically signed attest ation document of various checksums of \nimportant components.\nAll steps are done at every boot. That means that the root volume file system is not persistent \nacross reboots. All remote access such as SSH or login through a serial console are disabled by default.\n2.3.2  Volume encryption\nBoth the root vo lume and user data volumes in th e HPVS instance are automatically \nencrypted with Linux Unified Key Setup (LUKS) encryption. The root volume is re-created and encrypted during every boot, with the original content provided with the HPCR image. A passphrase for the root volume is not stored.\nUser data volume encryption is configured by using the\n seed that is provid ed in the workload \nand env sections of the cont",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 90,
            "total_chunks": 337
          }
        },
        {
          "id": "ab53a3c6-8b2c-4998-aa83-2266b646e587",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ion is configured by using the\n seed that is provid ed in the workload \nand env sections of the contract. During the HPVS instance initiation, the volumes are attached and encrypted by using the seed to create a LUKS passphrase. If the seed information or the user data volume is not configured, the HPVS instance fails to initiate.\n\n\n20 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentEach hour the volume encryption status check daemon examines the crypto headers of the \nroot volume and user data vo lumes that are attached to th e HPVS instance, and then writes \nthe information messages about volume encryption status into the log.\n2.3.3  Description of the contract\nThe implementation of the Hyper Protect Platform includes, in part, an encrypted secure execution image, the HPCR image. The contract is passed into an HPCR image as user data through the \ncloud-init  process3. To protect the contract from any type of intrusion, a public \nand private key pair4 is crea",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 91,
            "total_chunks": 337
          }
        },
        {
          "id": "d5aec66a-e2a0-4d6b-b515-c380594eabb6",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "rocess3. To protect the contract from any type of intrusion, a public \nand private key pair4 is created by an IBM internal bu ild pipeline for the HPCR image. This \nkey pair allows for the encryption of the contract sections. The public X509 certificate of the contract encryption public key is published by IBM and can be validated by any persona with the 3rd party certificate authority root key.\nEach persona, like the Workload Provider or the Workload Deployer can independently \nencrypt their contract section by using this contract en cryption public key. The contract \nencryption private key exists only inside the secure execution-encrypted HPCR image. This image can be decrypted by using only secure execution and keys that are rooted in hardware of an IBM LinuxONE or IBM Z platform. During  deployment of an HPVS instance, this key is \nused by the Bootloader component to decrypt the contract.\nContract enforcement withi n the HPCR image ensures the following conditions:\n/SM590000The con",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 92,
            "total_chunks": 337
          }
        },
        {
          "id": "475070d5-a9aa-489a-bc50-8b62135f0180",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ract.\nContract enforcement withi n the HPCR image ensures the following conditions:\n/SM590000The contract cannot be deployed outside of the HPCR image.\n/SM590000Secrets contained inside the contract cannot be retrieved outside the HPCR image.\n/SM590000Only containers that are spec ified by the Workload Provider  can run in the HPVS instance.\n/SM590000The Workload Provider and Workload Deployer can independently provide their section of \nthe contract, sharing only the contra ct section after it is encrypted.\n/SM590000User data volumes are encrypted based on the secret seeds that are contained in the \ncontract.\n/SM590000Data volumes can be reattached to a new H PVS instance if the same  secret seeds are \ndefined within the contracts.\nThis concept of contract enforcement allows the Workload Provider, the Workload Deployer, \nand the Auditor personas to cooperate and ensure confidentiality and integrity of the information: \n/SM590000The Workload Provider creates and encrypts the workload se",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 93,
            "total_chunks": 337
          }
        },
        {
          "id": "d18b1b48-7ad9-4f26-be5b-93d21ffeb5d2",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "d integrity of the information: \n/SM590000The Workload Provider creates and encrypts the workload section and passes it securely \nto the Workload Deployer. \n/SM590000The Auditor creates the attestationPublicKey and passes it securely to the Workload \nDeployer.\n/SM590000The Workload Deployer creates the env section for the contract.\n/SM590000The Workload Deployer combines the \nenv workload  and attestationPublicKey  sections, \nand calculates and adds the signature across the env and workload sections.\n/SM590000The Workload Deployer provides the contract in its user data  section at provision time.\n3  This software package (cloud-init) autom ates the initialization of virtual server instances during system boot.\n4  A public key is part of the owner's digital certificate and is  available for anyone to use. However, a private key is \nprotected by and available only to the owner of the key.\n\nChapter 2. Understanding the solution 212.3.4  The attestation record\nThe attestation record is pro",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 94,
            "total_chunks": 337
          }
        },
        {
          "id": "26567a9b-7d4e-4dd8-9f6b-bcd9012f8833",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "\nChapter 2. Understanding the solution 212.3.4  The attestation record\nThe attestation record is provided to the workload in the file system in the directory \n/var/hyperprotect. T he workload can then make this record available outside the HPVS \ninstance and ultimately to the Auditor persona. The Auditor can verify the record and verify that deployment has occurred into an HPVS instance before any data is accessible to the workload.\nRefer to 3.4, “Attestation” on page 54 for more information. \n2.3.5  Logging\nAll components log to syslog as the central place for logs. Log output is forwarded to the \ningestion backend defined in the contract. The components do not log any sensitive pieces of information, such as PI data or credentials. The detail level of logging can differ from component to component, but the guiding principle is to provide enough information to identify problems without flooding the logs.\nErrors are logged once, by using the error log level. Each error features a uniqu",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 95,
            "total_chunks": 337
          }
        },
        {
          "id": "bb7235cf-bfbf-4d14-9791-8ff47d0c43b7",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "flooding the logs.\nErrors are logged once, by using the error log level. Each error features a unique identifier for \nthe issuing component and the error situation. These identifiers are part of the API of HPCR, so they can be used in automation components to react on error situations. Identifiers are kept stable across semantic releases.\n2.3.6  Hyper Protect layer services\nThe Hyper Protect Platform uses layer services within the HPCR image that validate a user contract signature, validate the authentication of registries, validate the integrity of OCI images being brought up within the HPVS inst ance, encrypt th e volumes, and ensure the \nconfidentiality by not allowing access to the HPVS instance.\nThe Hyper Protect layer consis ts of the following services: \n/SM590000Logging service\n/SM590000Contract validation service\n/SM590000Registry authentication service\n/SM590000Image service\n/SM590000Signature service\n/SM590000Storage service\n/SM590000LUKS passphrase service\n/SM590000Containe",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 96,
            "total_chunks": 337
          }
        },
        {
          "id": "c697558a-27ce-4852-8bb5-28d7be64b4da",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "M590000Signature service\n/SM590000Storage service\n/SM590000LUKS passphrase service\n/SM590000Container service\n/SM590000Catch service\nLogging service\nThe logging component is responsible for the setup of the logging configuration. This \nconfiguration is available in the contract and allows a Workload Deployer to configure a  \nlogging backend, such as Mezmo , or a custom backend compatible with the rsyslog protocol. \nThe logging component validates the configuration and transforms it into a configuration for the rsyslog component that is used as a log forwarder.\nIf the logging configuration is in valid, this is indicated on the serial console, and the start of \nthe virtual server  instance fails.\n\n22 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentContract validation service\nThe contract validation component validates the contract syntactically and semantically \nagainst a JSON schema. If validation fails, so does the start of the virtual server instance.\nRegistr",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 97,
            "total_chunks": 337
          }
        },
        {
          "id": "5c35da7b-e995-4b07-a1a7-e3e121e6e14a",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "gainst a JSON schema. If validation fails, so does the start of the virtual server instance.\nRegistry authen tication service\nThe registry authentication component assembles all credentials in the contract that are \nrequired to authenticate against a remote container registry and transforms them into the configuration format required by the OCI runtime.\nImage service\nThe image service assembles all information in the contract related to the validation of OCI \nimages, for example, Red Hat signatures, and converts them into the format that is required by the OCI runtime.\nSignature service\nThe signature service verifies the optional contract signature.\nStorage service\nThe storage service initializes attached storage volumes according to their configuration in the contract. It creates necessary partitions, encrypts them through LUKS2 encryption or opens an existing LUKS2 encrypted layer. After the successful execution of the storage service, storage is mounted to the file system ready to b",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 98,
            "total_chunks": 337
          }
        },
        {
          "id": "ad6044ae-34fb-4dea-b015-6b02ca58e5af",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "er the successful execution of the storage service, storage is mounted to the file system ready to be consumed by OCI images.\nLUKS passphrase service\nThe LUKS passphrase service computes the pass phrase that is used for LUKS2 encryption \nof the storage volumes.\nContainer service\nThe container service is responsible for starti ng the OCI images by using the OCI runtime of \nchoice. After the successful execution of this service, the configured containers are running.\nDepending on the selection within the workload section of the contract, a different container \nservice is used to run the container. When specifying the compose subsection within the \ncontract, docker compose will be used. When specifying the play subsection within the \ncontract, podman kube will be used. Refer to 3. 1.1, “The workload sect ion” on page 39 for \nconfiguration details. \nCatch service\nThe catch service monitors the successful start of  the other services. If there is failure, it will \nautomatically shut down th",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 99,
            "total_chunks": 337
          }
        },
        {
          "id": "30d7e073-80e6-4b9c-b551-35fbc589a5bb",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "he successful start of  the other services. If there is failure, it will \nautomatically shut down the VSI after a grace period.\n2.3.7  Hyper Protect Vi rtual Server for VPC\nThe HPVS for VPC is built in the IBM Cloud Vi rtual Private Cloud (VPC ) infrastructure for \nextra network security. You can choose from various profile sizes and grow as needed to protect containerized applications, and you ca n pay-as-you-go on an hourly basis. You can \nalso use existing or common network security groups  and logging  infrastructure.\nWhen using IBM Cloud services, there is no need to understand the intricacies of their \nphysical deployment of the infrastructure. De ployment of the services happen automatically \nas part of the managed-service offering. The provisioning process itself depends on a \ncontract as input in the user-data field in the IBM Cloud UI for HPVS for VPC.\n\nChapter 2. Understanding the solution 23HPVS for VPC with Secure Exec ution can be provisioned from the IBM Cloud portal Vir",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 100,
            "total_chunks": 337
          }
        },
        {
          "id": "e1d3ee1b-b161-4b1a-a9b1-137772298630",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " the solution 23HPVS for VPC with Secure Exec ution can be provisioned from the IBM Cloud portal Virtual \nPrivate Infrastructure by a registered IBM cloud subscriber. The IBM Cloud command line interface, ibmcloud, can be used to create an instance of HPVS for VPC by selecting the ibm-hyper-protect-container-runtime (HPCR) stock image and the appropriate Secure \nExecution Profile. The HPCR stock image that is associated with the Hyper Protect operating system is periodically updated to provide security fixes and new functionality. To be able to use an updated stock image, depl oy a new instance of HPVS for VPC using your contract and \nthe current version of the HPCR image.\n2.3.8  Hyper Protect Virtual Serv er for IBM LinuxONE and IBM Z\nThe HPVS for IBM LinuxONE and IBM Z is a private, on-premises, cloud deployment solution \nwhere containers can be initiated as KVM guests that run an HPCR image, as shown in Figure 2-2 on page 19. \nThe Secure Execution on Linux feature ( 0115) and CP Ass",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 101,
            "total_chunks": 337
          }
        },
        {
          "id": "e232d94c-87e0-4d82-859b-6c64de33fc8c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " image, as shown in Figure 2-2 on page 19. \nThe Secure Execution on Linux feature ( 0115) and CP Assist for Cryptographic Function \n(CPACF) feature ( 3863 ) are required to enable confidentiality and integrity by protecting and \nisolating the data at the virtual machine (KVM guest) level.\nTo learn how to setup a KVM host LPAR and start KVM guest images, see  Introducing IBM \nSecure Execution for Linux .\n2.3.9  Considerations when depl oying workloads in HPVS instances\nIt is generally accepted that the adoption of  cloud-native principles in a hybrid cloud \nenvironment can enable workloads to become more cost-effective and convenient to run as separate components by using a microservices approach. Microservices typically have their \nown technology stack, including the database and data management, and communicate with other microservices over REST APIs.\nMultiple HPVS instances can be used for each microservice with the infrastructure providing \nthe necessary capabilities to secure netwo",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 102,
            "total_chunks": 337
          }
        },
        {
          "id": "12dd64a4-8254-4f0a-b316-df7e0e707c0d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " for each microservice with the infrastructure providing \nthe necessary capabilities to secure networking communicati on between them. Even in the \nabsence of cloud native orchestration such as  Kubernetes, independent scaling of services \nand independent upgrades or updates of services can be achieved.\nFor example, you can use auto mation tools and cloud servic es to deploy multiple HPVS \ninstances and implement failover, load balancing,  and scaling and elimin ate single points of \nfailure. \nTypically, with state-of-the-art cloud services  and automation tools no human intervention is \nrequired during failure events as the requests can be automatically routed around failures. Therefore, do not start multiple independent workloads or microservices within the same HPVS instance.\nEach instance should be a single unified worklo ad or microservice, which can be composed \nof multiple containers. The containers that are specified in a contract are assumed to be mutually dependent, so if dep",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 103,
            "total_chunks": 337
          }
        },
        {
          "id": "05a3b87a-3bc4-4bc8-a87a-3fc75a2c3807",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ers. The containers that are specified in a contract are assumed to be mutually dependent, so if deployment of one container fails then the workload stops.\nAs an expected practice, HPVS instances are intended to be deleted and re-created during \nthe lifetime of a workload. Changes to the root volume do not persist after reboot, so be aware of the following conditions and behaviors: \n/SM590000A contract is set when an HPVS instance is deployed and cannot be changed. \n/SM590000HPVS instances cannot be upgr aded. Theref ore, snapshots of the attached boot volumes \nare not recommended.\n\n24 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment/SM590000Updated OCI container images require a new contract, and a new HPVS instance must be \ndeployed.\n/SM590000Updated API keys, passwords, and so on, require a new contract and a new HPVS \ninstance deployment.\n/SM590000An updated HPCR image requires a new HPVS in stance. The previous contract can be \nreused if the encryption ke",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 104,
            "total_chunks": 337
          }
        },
        {
          "id": "aed3ad4c-d731-4f59-8622-1adae6073698",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " HPCR image requires a new HPVS in stance. The previous contract can be \nreused if the encryption keys for the updated HPCR image are unchanged from the prior image.\n/SM590000Reboot re-pulls container images although ro ot volume changes are not persistent. This \nmeans that contract de tails relating to pulling images mu st remain valid through all HPVS \ninstance reboots after creation.\n/SM590000No workload data should be held on root or boot volumes. The workload data is deleted by \na reboot or update.\n/SM590000Externally attached data volumes have an independent lifecycle and can be reattached to \nthe HPVS instance assuming that  contract sections relating to storage remain consistent.\nTools such as Te rraform can help make generating new contra cts and replacing HPVS \ninstances straightforward. For more information, see 4.1.9, “Deployment automation - Terraform” on page 70.\n2.4  Hyper Protect Secure Build\nAccording to OWASP-SAMM , the Secure Build practice must emphasize the importa",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 105,
            "total_chunks": 337
          }
        },
        {
          "id": "0bad3821-153d-4600-b1e7-f17fd741a939",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " Protect Secure Build\nAccording to OWASP-SAMM , the Secure Build practice must emphasize the importance of \nbuilding software in a standardized, repeatable manner, and doing so by using secure components. \nThe goal is to reach a Software Assurance Maturi ty Model (SAMM) maturity-level of three in \nthe build process to help  with the following tasks: \n/SM590000Prevent known defects from entering the production environment\n/SM590000Define mandatory security c hecks and ensure that building non-compliant artifacts fails\n/SM590000Analyze the dependencies used for security issues \nSecurity should be effective and at the same time it should not be cumbersome to implement \nbest practices for the following personnel that participate in the secure build process: \n/SM590000Cloud administrators. Create the Secure Build Server and register the repository for the \napplication developer\n/SM590000Application developers. Use the Secure Build Server to build and deploy applications from \na GitHub repos",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 106,
            "total_chunks": 337
          }
        },
        {
          "id": "ee1ed819-dbd5-444d-b676-1469b3334e94",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "cation developers. Use the Secure Build Server to build and deploy applications from \na GitHub repository\nBy using HPSB, a trusted container image can be built within a secure enclave that is \nprovided by IBM HPVS. The enclave is highly  isolated, where deve lopers can access the \ncontainer only by using a specific API and the cloud administrator cannot access the contents of the container. Therefore, the image that is bu ilt can be highly trusted. Specifically, the build \nserver cryptographically signs the image, and a manifest for audit purposes. The manifest is a collection of materials that are used during the build. Because the enclave protects the signing keys within the enclave, the signatures can be used to verify that the image and \nmanifest files are from the HPSB.\nThe HPSB can be used to securely build source code from a GitHub repository, publish the \nbuilt image to a repository like Docker hub or IBM Container Registry  and deploy the built \nimage as an HPVS instance.\n\nCha",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 107,
            "total_chunks": 337
          }
        },
        {
          "id": "d51ff5bb-d1d0-4ced-9214-2a0ec643b3f4",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ory like Docker hub or IBM Container Registry  and deploy the built \nimage as an HPVS instance.\n\nChapter 2. Understanding the solution 25For more details and sample configuration files, see I BM Hyper Protect Secure Build .\n2.5  Cryptographic agility is the key to SecDevOps\nCryptographic algorithms can break or become weak or obsolete over time. \nRecommendations and regulations on which cryptography to use often change. Therefore, businesses and organizations must replace the underlying cryptography that they use today, \nand it is not the only  time such a change will be requ ired. For more in formation, see NIST \nCryptographic Algorithm Validation Program .\nWith change comes also an opportunity to rethink how applications use complex \ncryptography such that future enhancements, updates, and patches are simpler to apply. For that reason, the ability to rapi dly change cryptograph ic algorithms and key strengths used to \nencrypt and sign data becomes essential. \nWhen thinking about cryp",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 108,
            "total_chunks": 337
          }
        },
        {
          "id": "047558a6-08b6-478f-a2c6-da820fe7f166",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "rithms and key strengths used to \nencrypt and sign data becomes essential. \nWhen thinking about cryptographic agilit y, consider the following requirements:\n/SM590000Build a cryptographic inventory and create a roadmap: \n– Build a cryptographic inventory as a reusable security asset where crypto is used \n– Perform a risk assessment and gap analysis – Evaluate vendor products – Develop plans for use of stronger cryptography – Understand the open source effect – Use a bottom-up approach \n/SM590000Design and run with crypto graphic agility in mind: \n– Manage internal and external dependencies\n– Make it simple to change underlying algorithms, methods, or protocols – Verify changes by automating as much as possible– Prepare for future changes \nDevelop new applications to be as flexible as possible to react to new situations. \n2.6  Hyper Protect Crypto Services\nHyper Protect Crypto Services  (HPCS) is a dedicated key management service (KMS) and \nhardware security module (HSM) that is built ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 109,
            "total_chunks": 337
          }
        },
        {
          "id": "e3d2d4b4-0940-4fef-bcc8-50ce2b80255f",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "HPCS) is a dedicated key management service (KMS) and \nhardware security module (HSM) that is built on the technical assurance of the Hyper Protect \nPlatform (see Figure 2-3 on page 26). \n\n26 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentFigure 2-3   HPCS - high-level view\nHPCS is built on a FIPS 140-2 Level 4-certified HSM. With HP CS, you have complete control \nof your encryption keys for cryptographic operationservice that is known as keep your own key (KYOK). This is in contrast to bring your own key (BYOK) based services. For a detailed comparison on why KYOK is better than BYOK, refer to How is Keep Your Own Key different \nfrom Bring Your Own Key?\nWith the Unified Key Orchestrator (UKO) functi on you can perform encryption key lifecycle \nmanagement from a single pane of glass on IBM Cloud into other cloud environments like AWS, Azure, and Google Cloud. UKO also doubles up as a repository for all encryption keys that can be synced and pushed to multiple",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 110,
            "total_chunks": 337
          }
        },
        {
          "id": "711af193-8795-4eba-bc43-6452f22dedc5",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "KO also doubles up as a repository for all encryption keys that can be synced and pushed to multiple keystores with the push of a button. \nHPCS allocates physical crypto-units to every instance provisioned on IBM Cloud in \naccordance with MZR Load Balancing and High Av ailability standards. In  some geographies, \nHPCS also provides cross-regional availab ility to protect the cust omers from regional \ndisasters. \nFor more information, refer to Hyper Protect Crypto Services  and Hyper Protect Crypto \nServices General FAQs .\n2.6.1  Accessing cryptogr aphic services with HPCS\nHPCS supports multiple APIs and programming models that are integrated with other \nservices in the IBM Cloud. HPCS services are accessible from the IBM Cloud UI and programmatically over public and private endpoints. A whole suite of middleware, such as object storage, open data foundation (ODF) cl usters, Kubernetes clusters, databases, load \nbalancers, and so on, can in tegrate seamlessly with HPCS.\nAuthentication a",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 111,
            "total_chunks": 337
          }
        },
        {
          "id": "83ed3040-8a28-4e80-a8db-5e25815236fc",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "usters, databases, load \nbalancers, and so on, can in tegrate seamlessly with HPCS.\nAuthentication and authorization of access to the resources in IBM Cloud is done by IBM \nCloud Identity and Access Management . Applications accessing HPCS through the \nendpoints must do so with an API-Key, which is part of a Service-ID, which in-turn, is a part of an Access Group.\n\n\nChapter 2. Understanding the solution 27HPCS offers the following applicatio n programming interfaces (APIs):\n/SM590000Key Protect API\n/SM590000GREP11 API\n/SM590000PKCS#11 API\nKey Protect API \nHPCS supports Key Lifecycle Management activi ties like creating, maintaining, protecting, \nand deleting cryptographic keys along with other Key Actions like:\n/SM590000Wrap a Key. Use a root key to wrap or encrypt a data encryption key (DEK)\n/SM590000Unwrap a Key. Use a root key to unwrap or decrypt a data encryption key\n/SM590000Rewrap a Key. Use a root key to rewrap or re-encrypt a data encryption key\n/SM590000Rotate a Key. Create a",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 112,
            "total_chunks": 337
          }
        },
        {
          "id": "387be5f9-4dc6-4d3e-b0b7-30121c502e22",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " a Key. Use a root key to rewrap or re-encrypt a data encryption key\n/SM590000Rotate a Key. Create a new version of a root key\n/SM590000Set a key for deletion\n/SM590000Unset a key for deletion\n/SM590000Enable a key\n/SM590000Disable a key\nFor more details on how to perform the previously listed actions, see IBM Cloud Key Protect \nMethods .\nGREP11 API \nThe FIPS 140-2 Level 4 HSM that HPCS uses is built on the IBM Crypto Express features \noperating in EP11 mode. EP11 is a stateless API5, which provides functionality similar to the \nindustry-standard Public-Key Cryptography Standards (PKCS) #11 API. PKCS #11 API \ndefines a platform-independent API to cryptographic tokens, such as hardware security modules (HSM) and smart cards. Existing applications can use PKCS #11 to benefit from enhanced security with secure key cryptography and from the stateless programming model, which makes the cryptographic operations much more efficient and scalable. \nFor more inform ation about EP11 capabilit ies",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 113,
            "total_chunks": 337
          }
        },
        {
          "id": "f12ea3ae-1ae8-42ef-92af-cdd4629ebe12",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "graphic operations much more efficient and scalable. \nFor more inform ation about EP11 capabilit ies and extensions, see Enterprise PKCS #11 \nintroduction .\nHPCS provides a set of EP11 over gRPC (GREP11) API calls, with which the cryptographic \nfunctions are run in the cloud HSM of HPCS. The GREP11 API is a stateless interface for \ncryptographic operations on the cloud that is based on EP11 and gRPC. \nAn application deployed to HPVS can use a stateless cryptographic API like GREP11 to \ncreate and use cryptographic ke ys. With GREP11, cryptographic material like keys is created \nin the HSM, but is stored outside of the HSM by the client application. \nThe application can store the keys in the b oundary of the HPVS inst ance on atta ched devices \nand use volume encryption and protection of memory and data in use. \nYou can use the features that are provided by HPVS, such as separation of duty, with features \nof HPCS to design and implement advanced cr yptographic mechanisms for deriving ke",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 114,
            "total_chunks": 337
          }
        },
        {
          "id": "7990972b-8c1a-41e3-9f02-040fb19e936b",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "y, with features \nof HPCS to design and implement advanced cr yptographic mechanisms for deriving keys \nfrom combined seeds. The combined seeds are a combination of different seeds that are owned by the Workload Provider and Workload  Deployer, and defined in the respective \nsections of the contract.\n5  Stateful services keep track of sessions or information about previous or pending interactions or transactions and \nthus react differently to the same inputs based on that information. Stateless services do not maintain such \ninformation on the side of the service, but rely on clients to maintain “state” information about sessions or previous \ninteractions.\n\n28 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentRefer to “How encryption keys are created using GREP11” on page 124 for more information. \nThe topic describes using the stateless capabilities of the EP1 1 interface by keeping state \ninformation like encryption keys, seed-parts and  backup-key parts insi ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 115,
            "total_chunks": 337
          }
        },
        {
          "id": "f045c049-5ab8-4004-9086-114791f4ba71",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "interface by keeping state \ninformation like encryption keys, seed-parts and  backup-key parts insi de secure enclaves \ncreated for an HPVS.\nPKCS#11 API \nHPCS also provides a PKCS#11 library that allows  user applications to interact directly with \nthe FIPS 140-2 Level 4 HSM through the PKCS#11 API. PKCS11 is a standard that specifies an API, called Cryptoki, to perform cryptographic operations. The Cryptoki API follows a simple object-based approach by presenting to applications a common, logical view of the \nHSM, called a cryptographic token.\nCryptoki isolates an application from the details of the cryptographic device, which makes the \napplication portable and usable with any cryp tographic device that supports the PKCS#11 \nstandard. For more information on cryptographic functions supported by HPCS, refer to: Introducing PKCS#11 . \n2.7  Crypto Express Network AP I for Secure Execution Enclaves\nWith Crypto Express Network API for Secure Execution Enclaves, REST API is used to \nconfig",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 116,
            "total_chunks": 337
          }
        },
        {
          "id": "60445aa4-4fc9-4ca8-8956-61090e18dbf1",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " Enclaves\nWith Crypto Express Network API for Secure Execution Enclaves, REST API is used to \nconfigure the c16 server, which provides gRPC API to access Crypto Express domains that are assigned to the IBM Z or IBM LinuxONE LP AR. After the configuration, your applications \ncan access the c16 gRPC API through the IBM GREP11 interface, provided by the GREP11 server, to securely connect from a Secure Execution for Linux LPAR to the Crypto Express domains. It is possible to enable the c16 server to send logs to a configured Rsyslog server to view logs.\nFigure 2-4 illustrates an architec ture overview of Crypto Ex press Network API for Secure \nExecution for Linux.\nFigure 2-4   Architecture overview of Crypto Express Network API\n\n\nChapter 2. Understanding the solution 292.7.1  Security considerations\nThe Crypto Express Network API for Secure Ex ecution Enclaves' REST API is critical for \nconfiguring and managing the c16 server, so it must never be accessed by any untrusted person or system.",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 117,
            "total_chunks": 337
          }
        },
        {
          "id": "8e68f9b3-dea6-4bb0-a381-29bc70e96087",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "iguring and managing the c16 server, so it must never be accessed by any untrusted person or system. Users are resp onsible for controlling access to  the API to keep it secure.\nThe c16 API invokes crypto operations on Crypto Express domains that are made accessible \nby the Crypto Express Network API for Secure  Execution Enclaves. Therefore, the c16 API \nmust be kept secure, and it is not recommended to expose the c16 API over a public network. The c16 API is protected through mTLS with the certificate authority (CA) configured through the Crypto Express Network API. Anyone with a valid certificate that the CA issues can \naccess the API. Users are responsible for controlling access to the CA and issuing client certificates only to trusted clients.\nFor more information, see Crypto Express Network API fo r Secure Execution Enclaves .\n2.8  Storage and repositories in the cloud\nData storage requirements can vary by use case. Applications at the front end of interaction \nwith clients need ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 118,
            "total_chunks": 337
          }
        },
        {
          "id": "44963a3a-8650-4bff-8a74-7c9d2507d485",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " requirements can vary by use case. Applications at the front end of interaction \nwith clients need to be responsive. Any data that is handled by these applications must be accessible to meet the response time requirements of the application. Because of the distributed nature of applications running in the cloud, the application design can be simpler with a design that is independent of storage. For example, the application developer might decide to outsource the data ava ilability requirements to a data service. In such cases, the \ninfrastructure provider must provide several options for data storage, based on latency and access pattern. The provider must also allow data to be moved seamlessly from one storage platform to another. Also, data protection concerns, which are inherent to a cloud environment for data at rest, might require that all storage services be connected to an available key management system. \nData in IBM Cloud storage services, such as cloud object storage, block s",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 119,
            "total_chunks": 337
          }
        },
        {
          "id": "821c36c1-8370-4b37-8df1-13ae4dd8ef22",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "le key management system. \nData in IBM Cloud storage services, such as cloud object storage, block storage, and file \nstorage, is encrypted by default using randomly generated keys. For added security, you can also use HPCS to create and keep your own root keys\n6 to protect your data through envelope \nencryption.7\nThe following subsections describe the different storage options in a cloud environment.\n2.8.1  Cloud object storage\nIBM Cloud Object Storage (COS) is an industry-leading cloud service that is ideal for storing large volumes of data. It  provides best-in-class security an d data durability at near-infinite \nscalability, complemented  with immutable data retention, audit controls and continuous \ncompliance, which is ideal for meeting the demands of business and regulatory requirements.Note:  The GREP11 client and the GREP11 server are shown in the same KVM host LPAR. \nHowever, the GREP11 client can run anywhere that has connectivity to the GREP11 server if the environment is t",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 120,
            "total_chunks": 337
          }
        },
        {
          "id": "93245a92-b471-47c5-a8d7-8e7d0fd4bc00",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "he GREP11 client can run anywhere that has connectivity to the GREP11 server if the environment is trusted.\n6  The root key is generated by using AES 256\n7  Envelope encryption is the pr actice of encrypting data with a data encryption key (DEK) and then wrapping the \nDEK with a root key that is kept inside HPCS, which you can fully manage.\n\n30 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentCOS has built-in integration with other services in IBM Cloud and security services. Access to \nCOS is available for applicatio ns and services from inside IBM Cloud, other cloud services \nand on-premises through RESTful APIs that are exposed on public and private network endpoints. Data in COS can be protected with encryption keys that are obtained from key management systems like HPCS. \nOther cloud service providers offer object storage like S3 from AWS and Azure Blob Storage \nfrom Azure. For more information, see IBM Cloud Object Storage .\n2.8.2  Block storage\nBlock sto",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 121,
            "total_chunks": 337
          }
        },
        {
          "id": "1d4ea9eb-66cf-43f6-a93a-53af5c8c53b0",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "age \nfrom Azure. For more information, see IBM Cloud Object Storage .\n2.8.2  Block storage\nBlock storage, sometimes referred to as block-level storage, is a technology that is used to \nstore data files on storage area networks (SANs) or cloud-based storage environments. Block storage divides data into blocks and then stores those blocks as separate pieces, each with a unique identifier. The SAN places those blocks of  data wherever it is most efficient. That \nmeans it can store those blocks across different systems and each block can be configured or partitioned to work with different operating systems.\nBlock storage allows for the creation of raw storage volumes to which server-based operating \nsystems can connect. Block storage also decouples data from user environments, allowing that data to be spread across multiple environments. This creates multiple paths to the data and allows the user to retrieve it quickly. When a user or application requests data from a block storage system, ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 122,
            "total_chunks": 337
          }
        },
        {
          "id": "130b1aec-41a8-4991-b651-bef1de91d834",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " user to retrieve it quickly. When a user or application requests data from a block storage system, the underlying storage system reassembles the data blocks and presents the data to the user or application.\nBlock Storage services in IBM Cloud are av ailable in the VPC environments. For more \ninformation, see What is block storage? .\n2.8.3  File storage\nFile storage, which is also referr ed to as file-level or file-based storage, is normally associated \nwith Network Attached Storage (NAS) technology. NAS presents storage to users and applications by using the same ideology as a tr aditional network file system. In other words, \nthe user or application receives data through directory trees, folders, and individual files. This functions similarly to a local hard disk. However, NAS or the Network Operating System (NOS) handle access rights, file sharing, file locking, and other controls.\nFile storage can be easy to configure, but access to data is constrained by a single path to the \ndata",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 123,
            "total_chunks": 337
          }
        },
        {
          "id": "07e8b2a1-f169-4678-a6d7-a396183b47a4",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "le storage can be easy to configure, but access to data is constrained by a single path to the \ndata, which can impact performance compared to block or object storage. File storage also operates only with common file-level protocols, such as a New Technology File System (NTFS) for Windows or a Network File System  (NFS) for Linux. This might limit usability \nacross dissimilar systems. Fo r more information, see: What is file storage? .\n2.8.4  On-premises storage\nOn-premises storage is categorized according to principles of storage tiering that assigns \ndata to various categories of storage media based on requirements such as cost, availability, \nperformance, and recovery objectives. Recovery objectives are usually stated as Recovery Time Objective (RTO) and Recovery Point Ob jective (RPO) policies,  which are crucial \ncomponents of an organization’s data life cycle strategy.\nStorage tiering is a component of Information Lifecycle Management (ILM) that helps \norganizations minimize stor",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 124,
            "total_chunks": 337
          }
        },
        {
          "id": "9c011004-9854-45a7-b767-b1d4ba7d671a",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ing is a component of Information Lifecycle Management (ILM) that helps \norganizations minimize storage costs and ensures performance and compliance. Storage tiering is applied to the following data classes:\n\nChapter 2. Understanding the solution 31/SM590000Mission critical. For data that should not ex perience any downtime and is usually assigned \nto the fastest storage devices like Fl ash and electronic storage devices.\n/SM590000Hot. For data that needs to be accessed frequently to support business-critical \napplications such as Customer Relationship Management (CRM) and Enterprise Resource Planning (ERP). This category also requires the fastest storage.\n/SM590000Warm. For data that is accessed less frequently. This kind of data can be stored on \nmedium-speed storage devices like disk storage\n/SM590000Cold. For data that might never be accessed again but must be retained for the \norganization to meet regulatory requirements. Such data is usually saved on tape and other archival stora",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 125,
            "total_chunks": 337
          }
        },
        {
          "id": "97143f06-75df-472b-8abe-7d052da56154",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ization to meet regulatory requirements. Such data is usually saved on tape and other archival storage devices.\nLUKS encryption with envelope encryption of the root keys in HPCS can be used to protect \ndata for on-premises storage. For a tutorial, see Protect LUKS encryption keys with IBM cloud \nHyper Protect Crypto Services and IBM Key Protect .\nFor more information on various st orage devices and solutions, see IBM Storage .\n2.9  Common usages\nIn general, any application that either uses sensitive data or secrets, or comprises sensitive \ncode, or needs to conform to compliance and regulatory requirements can take advantage of confidential computing with the Hyper Protect Platform.\nThis section discusses multiple exemplary use cases and explains how the Hyper Protect \nPlatform can be used to implement it. In particular, these use cases are discussed:\n/SM590000Securely bring applications to hybrid cloud\n/SM590000Digital assets infrastructure\n/SM590000Confidential AI\n/SM590000Secure mul",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 126,
            "total_chunks": 337
          }
        },
        {
          "id": "85bccdd6-2937-497a-9364-8764072693a2",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " to hybrid cloud\n/SM590000Digital assets infrastructure\n/SM590000Confidential AI\n/SM590000Secure multi-party computation \n/SM590000Secure distributed cloud\n2.9.1  Securely bring app lications to hybrid cloud\nEnterprises are moving their applications to cloud to reduce cost, simplify and consolidate \ntheir IT environment, and take advantage the flexibility of hy brid cloud. However, these \napplications contain many types of sensitive, sometimes regulated data that needs to be protected. Examples for this are applicatio ns in the financial services, healthcare, \ngovernment, and nonprofit domains.\nThe biggest barrier for bringing sensitive applications to the cloud is that the cloud \ninfrastructure is owned and operated by t he cloud provider. There are well-established \nmethods to protect data-at-rest and data-in-motion. However, there was always a concern about data-in-use, until Confidential Computing with the Hyper Protect Platform became available in the cloud. The Hyper Protect Plat",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 127,
            "total_chunks": 337
          }
        },
        {
          "id": "aae6c622-e774-4295-8cd4-21f315412aa4",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "tial Computing with the Hyper Protect Platform became available in the cloud. The Hyper Protect Platform  protects the code of the application and the \ndata-in-use from a malicious system ad ministrator or other privileged actors.\nIt thus enables taking advantage of hybrid cloud for application modernization and design of \ncloud native applications and ensures that compliance with regulations, data sovereignty, and data protection requirements are met. The Hyper Protect Platform also ensures protection of \ntrade secrets and intellectual property that can be part of the application, such as proprietary business logic, private AI training data, and AI models.\n\n32 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentThe following list includes features of HPVS that are relevant in the context of application \nmodernization:\n/SM590000Secrets can be used to establish instant trust to a newly  deployed HPVS instance as Zero \nKnowledge Proofs are possible. The instance can",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 128,
            "total_chunks": 337
          }
        },
        {
          "id": "3ecfb972-658f-4154-be74-bd78543057fb",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "nt trust to a newly  deployed HPVS instance as Zero \nKnowledge Proofs are possible. The instance can be integrated and authenticated in an existing flow. Even if the new instance is in a remote data center or cloud.\n/SM590000HPVS provides LUKS volume encryption fo r attached storage devices. The LUKS \npassphrase can be derived from two seeds, which are defined by and known to separate personas. This ensures that no individual persona can reconstruct the passphrase.\n/SM590000HPVS provides Attestation with which an Auditor persona ca n validate the HPVS instance.\n/SM590000Secure Build can be used to build the application OC I images in HPVS, thus protecting the \nbuild process, the built OCI images, the build manifest and signing keys for signing the images.\n2.9.2  Digital assets infrastructure\nIn a digital assets infrastructure, maintain ing control over private keys is extremely \nchallenging and poses a major risk, especially when managing thousands if not millions of \nwallets. Loss of ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 129,
            "total_chunks": 337
          }
        },
        {
          "id": "73f8ac1e-268a-4dc5-8dac-310e531044a1",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ing and poses a major risk, especially when managing thousands if not millions of \nwallets. Loss of control of an account’s private key through cyberattack or mishandling can result in irrevers ible asset loss.\n8\nThe Hyper Protect Platform provides a truste d platform for reliable and scalable digital \ncustody applications, for managing, transferring, and storing high value digital assets in highly secure wallets.\nTo provide a secure Digital Assets Infrastructu re, you can use features of the Hyper Protect \nPlatform: \n/SM590000Trusted Container Runtime\n/SM590000Memory protection\n/SM590000Data in use protection\n/SM590000Volume encryption\n/SM590000Attestation\n/SM590000Contract mechanism\n/SM590000Separation of duty\nIn addition, key creation, key derivation, and further cryptographic operations such as \nencryption and signing are done by using HPCS on FIPS 140-2 Level 4 certified hardware that is provided by IBM LinuxONE and IBM Z. Cryptographic keys and access tokens can be kept inside th",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 130,
            "total_chunks": 337
          }
        },
        {
          "id": "2821b7dc-8554-4459-beaa-ff01f56c54bd",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "at is provided by IBM LinuxONE and IBM Z. Cryptographic keys and access tokens can be kept inside the HPVS instance and cannot  be accessed even by  privileged actors.\nTo protect high-value transactions and comply wi th industry regulation, many applications are \nisolated, and communications with other applications are restricted. A key regulation that has emerged is the requirement of cold storage or alike for security purposes. However, today's cold storage solutions have several limitations that include protection of keys and assets, manual operations,  and the inability to scale due to the manual process.\nThe IBM Hyper Protect Offline Signing Orchestrator is an alternative approach to cold \nstorage. It is designed to broker communicati ons between two different applications that are \ndesigned not to communicate directly. Hyper Protect Offline Signing Orchestrator is deployed in HPVS instances on IBM LinuxONE and IBM Z.  Hyper Protect Offline Signing Orch estrator \nhelps protect hig",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 131,
            "total_chunks": 337
          }
        },
        {
          "id": "ff1b4d37-b117-49a1-9a08-c7b306154744",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "instances on IBM LinuxONE and IBM Z.  Hyper Protect Offline Signing Orch estrator \nhelps protect high-value transactions by offeri ng additional security layers that include \ndisconnected network operations, time-based securi ty, and electronic transaction approval by \nmultiple stakeholders. Offline Signing Orchestrator also changes the digital asset transaction \n8  Reference: Microsoft mitigates Chi na-based threat actor\n\nChapter 2. Understanding the solution 33signing process from a manual operation to  an automated and policy-driven one, by \neliminating the operational involvement without eliminating the human control.\n2.9.3  Confidential AI\nData that is used for training and testing AI and ML models can be valuable, sensitive, and regulated. Also, AI and ML models can constitute intellectual property or trade secrets, be sensitive, and require confidentiality.\nThe Hyper Protect Platform can protect AI proc esses, like training, test, and inference by \nusing features like memory pro",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 132,
            "total_chunks": 337
          }
        },
        {
          "id": "ea1fe6e9-0803-45ef-a61b-0c297cfe45a4",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "orm can protect AI proc esses, like training, test, and inference by \nusing features like memory protection, code and data confidentiality, code and data integrity, volume encryption, attestation, and the contract mechanism with separation of duty. \nFor example, the Hyper Protect Platform can be us ed to build confidential AI as a service in a \nhybrid cloud environment. Separation of duty is possible to allow a Workload Deployer persona to provision confidential AI in an HPVS instance and ensure this persona cannot access the AI model or related data. \nFigure 2-5 shows an example of a complete end-to-end flow for securely training a machine \nlearning model and using this model inference. This all occurs within a confidential computing environment with boundaries of protection around all the components.\nFigure 2-5   Confidential AI example\nYou can also add additional AI acceleration, such as by using the IBM Telum processor  that \ncontains on-chip acceleration for artificial intelligenc",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 133,
            "total_chunks": 337
          }
        },
        {
          "id": "c9b463eb-9734-401d-9bf5-60671fa413c0",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " as by using the IBM Telum processor  that \ncontains on-chip acceleration for artificial intelligence, with the IBM LinuxONE and IBM Z platforms.\nAs another example, confidential, federated learning can be applied to cases in which \nmultiple parties have private data to combine and analyze without exposing the underlying data or machine learning models to any of the other parties. This technology can be applied to preventing fraud in financial services, dete cting or developing cures for diseases in the \n\n\n34 Applying Data Protection and Confidentiality in a Hybrid Cloud Environmenthealthcare industry, or gaining business insight. As an illu stration, multiple hospitals might \ncombine data to train a machine learning mo del to clinically a nalyze medical images. \n2.9.4  Secure multi-party computation\nSecure multi-party computation (SMPC) enables multiple parties, each holding their own \nprivate data, to collaborate in a computation without revealing any of the private data. The two imp",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 134,
            "total_chunks": 337
          }
        },
        {
          "id": "5724cc3f-4a35-4a60-b8dd-c70aadebe2b0",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "private data, to collaborate in a computation without revealing any of the private data. The two important requirements on SMPC are privacy and correctness. The privacy requirement states that parties learn their computation output and nothing else. The correctness requirement states that each party receive its correct output. Therefore, no adversary must be able to cause the result of the computation to deviate from the function that the parties set out to compute.\nA primary concern is how the confidentiality and integrity of the data can be preserved when \ncalculations happen outside the party’s direct control.\nFor example, banks can collaborate to find patterns of anti-money laundering. See \nFigure 2-6. Each bank brings encrypted financial-transactions data to a confidential computing enclave, such as an HPVS, where data can be safely decrypted. The multi-party collaboration (MPC) application runs fraud-detection algorithms in the confidential computing enclave. Malicious administra",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 135,
            "total_chunks": 337
          }
        },
        {
          "id": "306b39f6-7ea3-415f-943e-576626744ae3",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "lication runs fraud-detection algorithms in the confidential computing enclave. Malicious administrators or operators cannot see financial transaction data from any \nbanks collaborating on the MPC platform.\nFigure 2-6   Banks collaborate to find patterns of anti-money laundering\nBy using the Hyper Protect Pl atform and confidential computing, organizations can now \nensure that data is protected against tampering and compromise, and data sovereignty and privacy regulations can be  fulfilled. This includes threats within the partnering organizations \nand validating the integrity of the code processing that data. \nThe data can be combined and analyzed and the results can be sent in an encrypted format \nback to each party. Data remains protected throughout the entire process: while in transit, in use, and at rest.\n\n\nChapter 2. Understanding the solution 352.9.5  Secure distributed cloud \nA secure distributed computing application can comprise a front end running in a TEE on \nmobile, person",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 136,
            "total_chunks": 337
          }
        },
        {
          "id": "61235f23-0452-498a-88b1-6a9e920f7903",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ecure distributed computing application can comprise a front end running in a TEE on \nmobile, personal computing, or point of sale (POS) devices and a back end in a hybrid cloud infrastructure. Such a type of application can be  required for security cr itical applications like \npayment systems and in financial services, healthcare, or in other regulated industries.\nThe back-end infrastructure can use the Hyper Pr otect Platform to provide protection of the \ncode and data in use by the back end. Data in transit between the back end and the front end is typically protected by mTLS.\n\n36 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment\n\n© Copyright IBM Corp. 2024. 37Chapter 3. Making the infrastructure secure\nThe previous chapters described the deficiencies of traditional infrastructures in the context of \nincreased cyberthreats that are posed by malicio us actors, whether internal or external. The \nchapters included strategies to mitigate risks by using infrastr",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 137,
            "total_chunks": 337
          }
        },
        {
          "id": "3f895014-1295-4a1e-8f4c-4dd78c239e0c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " whether internal or external. The \nchapters included strategies to mitigate risks by using infrastructures built on the Hyper Protect Platform and described the relevant qualities of the components and services that comprise the solution. \nThere are several distinct steps that are involved in configuring Hyper Protect Virtual Server \n(HPVS) instances, which can quickly become overwhelming wit hout some guidance. This \nchapter provides step-by-step instructions with sample configuration file s, where required, to \ndeploy a secure infrastructure for confidential computing. \nBefore you start, you need an IBM Cloud account  for a Virtual Private Cloud (VPC) or KVM to \nhost LPARs on IBM LinuxONE or IBM Z to deploy the examples in this chapter. For more information, see System requirements.\nRefer to Appendix B, “Creating a Hyper Protect Virtual Server for VPC” on page 107 for \ndetailed steps on deploying an HPVS instance by  using the IBM Cloud Virtual Private Cloud \n(VPC) user interface (U",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 138,
            "total_chunks": 337
          }
        },
        {
          "id": "d3e24c86-ff46-4651-9cc8-bb3794eccf65",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "on deploying an HPVS instance by  using the IBM Cloud Virtual Private Cloud \n(VPC) user interface (UI).\nYou must also complete Setting up and configuring IBM Hyper Protect Virtual Servers  before \nyou can deploy HPVS instance s on IBM LinuxONE or IBM Z.\nThe deployment of HPVS consists of the following aspects:\n/SM5900003.1, “The contract” on page 38\n/SM5900003.2, “Contract encryption” on page 49\n/SM5900003.3, “Contract certificates” on page 53\n/SM5900003.4, “Attestation” on page 54\n/SM5900003.5, “Logging for HPVS instances” on page 56\n/SM5900003.6, “Encrypting data volumes” on page 633\n\n38 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment3.1  The contract\nThe contract is a definition file in the YAML form at that is specific to the Hyper Protect Virtual \nServers (HPVS) instance, wh ich is also called an im age. This file must be  created by  the user \nas a prerequisite for creating an instance. After the file is created, it must be passed as input \nthrough the ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 139,
            "total_chunks": 337
          }
        },
        {
          "id": "7938dc69-9664-4918-ab63-6f2a3ac6cadf",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "uisite for creating an instance. After the file is created, it must be passed as input \nthrough the User Data field when an instance is created.\nThe HPCR image decrypts the contract, if it is encrypted, validates the contract schema, \nchecks for contract signature, creates the passp hrase to encrypt the disk device, and starts \nthe container that is specified in the contract.\nThe contract has severa l sections, each one documenting how the H PVS instance should be \nbuilt: \n/SM590000workload (mandatory).\n/SM590000env (mandatory).\n/SM590000attestationPublicKey (optional). This section provides a public RSA key as part of the \ncontract, which is used to encrypt the attestation document and the attribute must be named as attestationPublicKey.\n/SM590000envWorkloadSignature (optional section). This section contains the signature of the other \nsections of the contract.\nThe workload and env sections are mandatory because the information that is provided in \nthese sections defines how the HPVS ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 140,
            "total_chunks": 337
          }
        },
        {
          "id": "c91b28b5-2f87-4397-b88b-e2433c16b9de",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ions are mandatory because the information that is provided in \nthese sections defines how the HPVS instance is built. The contents of these sections come from two different personas, namely the Workload Provider and the Workload Deployer. See “Separation of duty” on page 11 for a description of the predefined personas.\nThe Workload Provider persona provides information about the container, or workload that \nneeds to be starte d on the HPVS instance. It incl udes the following information:\n/SM590000Name of the container \n/SM590000Container registry and where it resides \n/SM590000Credentials of the container registry \n/SM590000Image digest \n/SM590000Notary server information, which is required for image validation \n/SM590000Volumes to be present and attached\n/SM590000Environment variables that need to be passed to the container\n/SM590000Docker compose file or Pod descriptors with the container information\nThe Workload Deployer persona works closely with the infrastructure staff. This pe",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 141,
            "total_chunks": 337
          }
        },
        {
          "id": "dd0a2f63-7535-4b89-b760-77e4d9899f4a",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ainer information\nThe Workload Deployer persona works closely with the infrastructure staff. This persona \nreceives the workload information, preferably an encrypted workload section, from the Workload Provider persona. The Workload Deployer then creates the env section of the contract. The env section has information that is specific to the environment. Usually, it is information that the Workload Provider persona does not have and does not need to know. An \nexample is information about the “Logging” instance, which the Workload Deployer persona creates, before it adds information to the env section of the contract.Note:  If a docker compose file is used, only one container is supported. However, pod \ndescriptors, a podman construct, support one or multiple containers\n\nChapter 3. Making the infrastructure secure 393.1.1  The workload section\nThis is one of the most important sections of the contract. The workload section can have \nmultiple subsections and the purpose of the s ubsectio",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 142,
            "total_chunks": 337
          }
        },
        {
          "id": "7a8000a1-3cc0-4ea8-bc90-57a74e2240a4",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " the contract. The workload section can have \nmultiple subsections and the purpose of the s ubsections is to provide information that is \nrequired for bringing up the workload.  The workload section is defined with type: workload . \nThis section is mandatory and it encompasses the following subsections:\n/SM590000auths. This subsection is optional and required only when authentication is needed to \ndownload or get the image file from the registry.\n/SM590000compose. For a single container. compose subsection must exist if play subsection is not \ndefined. The compose subsection and the play  subsection are mutually exclusive. \n/SM590000play. For multiple containers. pl ay subsection must exist if the compose subsection is not \ndefined. The compose subsection and the play  subsection are mutually exclusive. \n/SM590000images. This subsection is optional and used for validating the integrity of signed images.\n/SM590000volumes. This subsection is optional and used only when a data volume is m",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 143,
            "total_chunks": 337
          }
        },
        {
          "id": "2c86ae57-f2ff-40af-8021-403e41d9fc90",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "f signed images.\n/SM590000volumes. This subsection is optional and used only when a data volume is mounted on the \nHPVS guest.\nExample 3-1 shows a high-level sample of the workload section of the contract. The minimum \nthat a workload section needs is the compose section. The other sections can be added based on the requirement.\nExample 3-1   High-level sample of the workload section\ntype: workloadauths:  <registry url>:    password: <password>    username: <user name>  <registry url>:    password: <password>    username: <user name>\ncompose:\n  archive: <base64 encoded of tgz of docker-compose.yaml>\nimages:\n  dct:    <docker image name (without the tag, an example is docker.io/redbookuser/s390x:)>:      notary: \"<notary URL>\"      publicKey: <docker content trust signed public key>    <docker image name>:      notary: \"<notary URL>\"      publicKey: <docker content trust signed public key>\nvolumes:\n  <volume name>:    mount: \"<data volume mount path>\"    seed: \"<Passphrase of the luks e",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 144,
            "total_chunks": 337
          }
        },
        {
          "id": "d5c4622e-2197-4d8d-9271-e2aee78683ff",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ">\nvolumes:\n  <volume name>:    mount: \"<data volume mount path>\"    seed: \"<Passphrase of the luks encryption>\"    filesystem: \"ext4\"\n\n40 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentThe auths subsection\nThe auths subsection consists of information about the container's registry. If a public image \nis used in the contract, you do not need the auths subsection because no credentials are required. The auths subsection is required only if the container images are private. This subsection does not have any image informatio n, as shown in the following sample. This \nsubsection needs to contain the name of the image registry and the credentials, such as username and password, for the same. The key must be the hostname of the container registry or the following string for the default docker registry:\nhttps://index.docker.io/v1/\nExample 3-2 shows an IBM Cloud Registry snippet. For more information about using the API \nkey, see Using client software to authenticate ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 145,
            "total_chunks": 337
          }
        },
        {
          "id": "9daf8f46-6f72-4b24-ae2c-c2e08f120c35",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "y snippet. For more information about using the API \nkey, see Using client software to authenticate in automation .\nExample 3-2   IBM Cloud registry\nauths:\n  us.icr.io:    password: <apikey>    username: iamapikey\nThe compose subsection\nThe compose subsection consists of an archive subsection. The archive subsection contains the Base64 encoded GNU compressed tar (TGZ) file archive of the docker-compose.yaml  file \nwith any other file such as certificates and configuration files. Because the HPCR image uses Docker Engine and Docker Compose to start the container, the information about the container must first be created by using a standard docker-compose file. This file is then archived and Base64 encoded, and the output of this is provided as the value to the archive subsection within the compose section. For more information, see Docker Compose \noverview .\nThe mount points specified under the volumes information of the docker-compose file might \nbe aligned with the volume mount point ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 146,
            "total_chunks": 337
          }
        },
        {
          "id": "19be6a2c-5483-4405-afd7-409147dd9ff3",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "er the volumes information of the docker-compose file might \nbe aligned with the volume mount point that is specified in the workload section of the \ncontract.\nBoth “yaml” and “yml” formats support docker-compose files. See Example 3-3.\nExample 3-3   Docker-compose file\nversion: '3'\nservices:  nginx:    image: nginx@sha256:e73ba8654ba7fd1834e78a3d4e9d72ffaaa3372d42996af5c34ba3e1abc293e8\n user: 0:0\n    restart: always    ports:    - 80:80Note:  Running a build as part of a docker compose file is not supported. Make sure that \nyour docker compose file does not have a build section.\n\nChapter 3. Making the infrastructure secure 41Use the tar command to get the Base64 encoded archive file. See Example 3-4. The Base64 \noutput is available in the compose.b64 file.\nExample 3-4   Command to get the Base64 encoded archive file\ntar czvf - -C <COMPOSE_FOLDER> . | base64 -w0 > compose.b64\nCopy the content of compose.b64  file as a value of the compose -> archive subsection. See \nExample 3-5.\nExampl",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 147,
            "total_chunks": 337
          }
        },
        {
          "id": "af47cd86-666f-4391-b3da-975e6811181d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ntent of compose.b64  file as a value of the compose -> archive subsection. See \nExample 3-5.\nExample 3-5   Copy of compose.b64 file content\ncompose:  archive: <paste the content of compose.b64 >\nFor this example, the response is similar to the output in Example 3-6.\nExample 3-6   Output of the compose -> archive\ncompose:\n  archive: H4sIAKOFmGIAA+2RTW6DMBBGs84pRuyB8Q8k+DIRwZOGtmBkkyrcvhgnLVVV1EWkqhJv4ZHt8ednWZvqhWxcmaYzjpKhed08HETMpQRfd3k2VeRhPpEJCUxymTPkIuOALBOIG8DHq3zn4vrSjiqdLY/nsv+xb2w7nRZywlPgo/4THNm3uiKntgCWdO1aowmZnwLUTflECpwo8Jpu9NyZ2zvQgdADFEudoXyQzSu+fPPzseSvedo6qj\nV7mDa2anZbdH8totL6somtUlvX8K4SJshDsFKU2NmFvAZuMc9U37wceeys+Y6BI8Fi6+6vxK5RS+YFDh6R\nNu//tuVlZWVJd4BcjKckQAIAAA=\nThe play subsection\nIn the play subsection, you can define the workload through Pod descriptors . Each pod can \ncontain one or more container definitions. Descriptors can be provided in one of the following ways:\n/SM590000In plain YAML format in the resources subsection of play. This section is an array of",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 148,
            "total_chunks": 337
          }
        },
        {
          "id": "0f0424b4-833b-4e2b-9cd4-cee1cd72bf30",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ways:\n/SM590000In plain YAML format in the resources subsection of play. This section is an array of \ndescriptors and supports two types of descriptors: Pods  and ConfigMaps .\nExample 3-7 illustrates how to define t he resources in the play subsection.\nExample 3-7   Resources in the play subsection\nworkload: |\n  type: workload  play:    resources:      - apiVersion: v1        kind: PodTip: There are use cases when the workload section is pre-encrypted in which the registry \nis not known. For example, when the Workload Provider wants to allow the Workload Deployer to use a registry mirror or a private container registry. In such a case, it is possible to dynamically override the registry and pull credentials. This is a coordinated effort between the Workload Provider and the Workload Deployer. For more information, see Using a dynamic registry reference .\nNote:  Make sure that the compose tgz file contains only directories and regular files. Links \nor pipes are not supported and will re",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 149,
            "total_chunks": 337
          }
        },
        {
          "id": "461cca27-2d05-4759-be58-2e46f39fec47",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " tgz file contains only directories and regular files. Links \nor pipes are not supported and will resu lt in an error during deployment.\n\n42 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment        metadata:\n          name: busybox        spec:          containers:          - name: main            image: ...            command:            - printenv            envFrom:            - configMapRef:                name: contract.config.map                optional: false          restartPolicy: Never\n/SM590000In the archive subsection of play. The archiv e is a Base64 encoded, compressed, tar file. \nThe Pods or ConfigMaps are represented as YAML files, top level in th is tar file. The file \nmay also contain extra files and all files are ex tracted to the host file system before starting \nthe Pods. The current working directory is the directory in which the files have been extracted, so it's possible to use a volume mount with a relative path to mount files or direct",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 150,
            "total_chunks": 337
          }
        },
        {
          "id": "6e05bfe1-e3cd-4f13-91eb-e8f333765a72",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "been extracted, so it's possible to use a volume mount with a relative path to mount files or directories from the contract.\n/SM590000In a template format in the templates subsection of play. This section is an array of \ndescriptors in the YAML format. Pods or Co nfigMaps may have points of variability (POV) \nthat are not known at the time of writing the descriptors. These POVs may be represented as templates and the values are provided at deployment time from information in the contract. Go templates  are used as the templating syntax, which is the same as used for \nHelm charts , so templates can easily be exchanged with Kubernetes. The following \nBuilt-In objects are supported:\n– Environment: this object contains the environment variables as merged between the \nworkload and the environment section. See Example 3-8. The object is available as {{ .Env }}.\nExample 3-8   Environment variables\nworkload: |  type: workload  play:    templates:      - apiVersion: v1        kind: Pod        m",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 151,
            "total_chunks": 337
          }
        },
        {
          "id": "86f4a962-e5e3-4626-be69-a7fedc5c7e37",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "les\nworkload: |  type: workload  play:    templates:      - apiVersion: v1        kind: Pod        metadata:          name: busybox        spec:          containers:          - name: main            image: \"{{ .Env.REGISTRY }}/hpse-docker-busybox-s390x@sha256:732efa374f1e6c964caeacab0bcb370385ee386041a14d4a32176462e3f75c7b\"            command:            - printenv            envFrom:            - configMapRef:                name: contract.config.map                optional: false          restartPolicy: Neverenv:\n\nChapter 3. Making the infrastructure secure 43  env:\n    REGISTRY: docker-eu-private.artifactory.swg-devops.com/sys-zaas-team-hpse-dev-docker-local/zaas\nEnvironment variables\nIn the contract, environment variables can be defined in the workload and env sections. Both sets of variables are merged together with  workload taking prec edence. Pods use the \nconcept of a ConfigMap to define configuration, so HPCR represents the merged environment sections as a special ConfigMap n",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 152,
            "total_chunks": 337
          }
        },
        {
          "id": "b55f4ca1-b25a-4997-a493-62590deb1c43",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "to define configuration, so HPCR represents the merged environment sections as a special ConfigMap named contract.config.map. Example 3-9 mounts all environment variables from the contract into the container.\nExample 3-9   Environment variables in the contract\napiVersion: v1kind: Podmetadata:  name: busyboxspec:  containers:  - name: main    image: ...    command:    - printenv    envFrom:    - configMapRef:        name: contract.config.map        optional: false  restartPolicy: Never\nPod communication\nPod communication can be container-to-container, pod-to-host, or pod-to-pod\n/SM590000Container-to-container\nContainers inside one Pod communicate to each other through the localhost. Each \ncontainer needs to listen on a different port because, per design, they share the same IP address.\n/SM590000Pod-to-host\nUsually, a Pod needs to expose at least one of  its containers to the host so that the \nexposed container is accessible through the IP address on the host through a mapped port. For t",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 153,
            "total_chunks": 337
          }
        },
        {
          "id": "43ffc4c6-4a71-4b4c-9774-c3adef3394d3",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "the \nexposed container is accessible through the IP address on the host through a mapped port. For this use case, use the hostPort feature on a container. This is not best practice when you use Kubernetes, for which a service would be used instead.\nSee Example 3-10 for a snippet.\nExample 3-10   Pod communication\napiVersion: v1\nkind: Podmetadata:    name: nginx-with-busyboxImportant:  Specify both hostPort and containerPort explicitly. If you specify only \ncontainerPort, ports are not bound.\n\n44 Applying Data Protection and Confidentiality in a Hybrid Cloud Environmentspec:\n    containers:        - image: ...          name: frontend          ports:            - containerPort: 80              hostPort: 80          volumeMounts:            - mountPath: /etc/nginx              name: local-frontend              readOnly: true        - command:            - httpd            - -vv            - -f            - -p            - \"8080\"            - -h            - /www          image: ...        ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 154,
            "total_chunks": 337
          }
        },
        {
          "id": "62c788ff-8f86-40b3-a49b-56be66d6fff8",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "-f            - -p            - \"8080\"            - -h            - /www          image: ...          name: backend          volumeMounts:            - mountPath: /www              name: local-backend              readOnly: true    volumes:        - hostPath:            path: ./www            type: Directory          name: local-backend        - hostPath:            path: ./nginx            type: Directory          name: local-frontend\n/SM590000Pod-to-Pod\nTo reach from one Pod to another, expose a hostPort on the target Pod. The source Pod can \nthen make a request to the host on the exposed port to get to the target Pod.\nThe source Pod can find the IP address of the host through the following command:\n    ip route | awk '/default/ { print $3 }'\nVolumes\nFor HPCR, volumes are managed by the volumes section in the contract. Based on this \ninformation, HPCR will encrypt and mount external blo ck devices on the host. To mount these \nvolumes into the pod, use the hostPath  mount option on th",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 155,
            "total_chunks": 337
          }
        },
        {
          "id": "7f655148-93a0-4c78-b319-7b1e02f72b74",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "o ck devices on the host. To mount these \nvolumes into the pod, use the hostPath  mount option on the volume. See Example 3-11.\nExample 3-11   Mount volume into pod\napiVersion: v1\nkind: Podmetadata:  name: busyboxspec:\n\nChapter 3. Making the infrastructure secure 45  containers:\n  - name: main    image: ...    volumeMounts:    - name: test-volume      readOnly: true      mountPath: /fromHost  volumes:  - name: test-volume    hostPath:      path: /var/hyperprotect      type: Directory  restartPolicy: Never\nThe images subsection\nThe images subsection is meant only for an image that is signed. See Example 3-12.\nThe following list includes aspects of images that are described by docker compose:\n/SM590000The container image that is listed in the docker -compose file can be signed or not signed \nby using Docker Content Trust (DCT).\n/SM590000The following example shows an image URL:\n<container registry >/<username or namespace >/<image nam e>\nAn example with defined variables: \nus.icr.io/myna",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 156,
            "total_chunks": 337
          }
        },
        {
          "id": "8a8f38a6-5809-4cfe-a2ad-cdea11f3f2f9",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "registry >/<username or namespace >/<image nam e>\nAn example with defined variables: \nus.icr.io/mynamespace/my-haproxy:\n/SM590000The following contents show an example of a notary URL:\nnotary: \"https://notary.us.icr.io\"\n/SM590000The publicKey is the public key corresponding to the private key by which the images are \nsigned using DCT. Use the following command to get the public key:\ncat ~/.docker/trust/tuf/us.icr.io/<username>/<imagename>/metadata/root.json\nExample 3-12   Image that is signed using DCT\nimages:\n  dct:    us.icr.io/mynamespace/my-haproxy:      notary: \"https://notary.us.icr.io\"      publicKey: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpRENDQVM2Z0F3SUJBZ0lSQUxCMXBPYlpEQlRRc09GSFlxazMzaWd3Q2dZSUtvWkl6ajBFQXdJd0tqRW8KTUNZR0ExVUVBeE1mZFhNdWFXTnlMbWx2TDNCeVlXSm9ZWFF4TWpNdmJYa3RhR0Z3Y205NGVUQWVGdzB5TWpBMApNVE14TURFd01ETmFGdzB6TWpBME1UQXhNREV3TUROYU1Db3hLREFtQmdOVkJBTVRIM1Z6TG1samNpNXBieTl3CmNtRmlhR0YwTVRJekwyMTVMV2hoY0hKdmVIa3dXVEFUQmdjcWhrak9QUUlCQmdncWhrak9QUU1CQndOQ0FBU1AK",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 157,
            "total_chunks": 337
          }
        },
        {
          "id": "d39b733b-6cf3-408d-affa-b80694a77d11",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "amNpNXBieTl3CmNtRmlhR0YwTVRJekwyMTVMV2hoY0hKdmVIa3dXVEFUQmdjcWhrak9QUUlCQmdncWhrak9QUU1CQndOQ0FBU1AKWGsrelE2MlFZNjI3MWQ1cTBMZHY3SGc3QzZkMGZOUlRsQmJXekhOWWFDZzlpU0piYnVNdjVBY0JmMjlqQi83eApqYzhzVitxMksyemtkTHV4QWxGWm96VXdNekFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJCkt3WUJCUVVIQXdNd0RBWURWUjBUQVFIL0JBSXdBREFLQmdncWhrak9QUVFEQWdOSUFEQkZBaUIzd0JTa0IxaXAKZHZZYlBMbFBmS3RZT0hsYnZzUllKa0FZM2hnY0xuNWhwQUloQUt6cmhsU3p4K1I5bmdtMTBlZVkyaFNCRmgrawpMWHp6SFkwaktTVzhyM1FhCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0KNote:  The volumes field here defines the data on the host to be mounted into the pod. It's \ndifferent from volumes in the HPCR contract.\n\n46 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentFor an image that is not signed, no entry is re quired in the images subsection. However, for \nunsigned images, a digest is required. Complete the following steps to get the digest:\n1. Log in to the container registry dashboard.\n2. Open the image.3. Click Tag, and then click Dige",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 158,
            "total_chunks": 337
          }
        },
        {
          "id": "36eea1b3-a2ed-4481-8a1b-e906125415f4",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ":\n1. Log in to the container registry dashboard.\n2. Open the image.3. Click Tag, and then click Digest.\nAfter you get the digest, add this digest in the docker-compose.yaml  file. The following entry is \nan example:\nservices:\n  <imagename >:\n    image: \ns390x/redis@sha256:db467ab5c53bdeef65762a7534e26fecb94a0f218bd38afd2eaba1a670c472b1\nThe following images are described by Pod descriptors:\n/SM590000Container images that are described by Pod descriptors can be validated by Red Hat \nSimple Signing.\n/SM590000If the image is referenced by a digest, the service allows its usage without additional \nchecks.\n/SM590000Images without a digest need a GPG key to be validated. The key is transferred in Base64 \nencoded binary format that can be crea ted as shown in the following example:\ngpg --export ${KEY_ID} | base64 -w0\nThis key is conveyed through the rhs subsection of the images section. This section is a map \nwith the image identifier as the key and the GPG key in the publicKey field as shown ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 159,
            "total_chunks": 337
          }
        },
        {
          "id": "04551f6f-01b2-45ce-aaca-6ac5e9494957",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "tion is a map \nwith the image identifier as the key and the GPG key in the publicKey field as shown in the following example:\nimages:  rhs:\n      OCI-image-identifier:\n        publicKey: abcdef\n3.1.2  The workload volumes subsection\nThe volumes subsection needs to be provided in the contract only if a data volume is attached \nto the instance at the time of creation. The information that is provided in this subsection is used to mount the attached data volume, which is  provided by the user, and is later encrypted \nusing the seeds that are provided in the workload and env sections. You can provide any path of your choice for the mount field. The path that is provided by the user is used internally to mount the data volume. The mount path that is provided in the contract must match the path provided under the volumes subsection of the docker-compose.yaml  file, so that all the data \nassociated with the container workload is stored in this data volume.\nThe volumes subsection has support f",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 160,
            "total_chunks": 337
          }
        },
        {
          "id": "9286ee16-3681-4297-8ac1-5fc78933f0fa",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ated with the container workload is stored in this data volume.\nThe volumes subsection has support for auto encryption of the data volume with \nuser-provided seeds. If a data volume is attached to the HPVS instance, it is encrypted automatically with the seeds that are provided through the seed field in the volumes subsections of the contract. Thus, two seeds must be provided, one through the workload section, by the Workload Provider persona and the other through the env section by the Workload Deployer persona. These two seeds ar e internally converted to UTF8 sequences \nand then concatenated. Later, the hash (SHA256) of the concatenated sequence is computed as a hexdigest, which is used as the LUKS passphrase to encrypt the data volume. \n\nChapter 3. Making the infrastructure secure 47You can use the following command to validate the hexdigest:\necho -n \"seed1seed2\" | sha256sum\nThis is how the seed can be provided in the workload section of the contract. For more \ninformation about ho",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 161,
            "total_chunks": 337
          }
        },
        {
          "id": "90ae4dd6-afa4-4c13-ba1b-619f0b762920",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "how the seed can be provided in the workload section of the contract. For more \ninformation about how the seed input can be provided through the env section, see 3.1.3, “The env section” on page 47. It is mandatory to provide both the seeds for encryption. If only one of the seeds is provided then encryption fails and the instance shuts down.\nNote that for deployments on IBM Cloud it is possible to add a higher level of encryption \nprotection and control to the data-at-rest by integrating with Hyper Protect Crypto Services \n(HPCS). Starting with ibm-hyper-protect- container-runtim e-1-0-s390x- 11 for HPVS for VPC \ninstance or ibm-hy per-protect-conta iner-runtime-23.6.2- encrypt.crt for HPVS instance on IBM \nLinuxONE or IBM Z version 23.6.2, HPCS can be used to generate a random value as the third seed and wrap it with the root key. The LUKS passphrase is generated by using three seeds: the seed in the metadata partition and the two seeds from the contract. For more information, see Se",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 162,
            "total_chunks": 337
          }
        },
        {
          "id": "72f2a213-b450-40a3-a395-eeaa0c088a2c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "the seed in the metadata partition and the two seeds from the contract. For more information, see Securing your data .\nThe following snippet is an example of the volumes subsection:\nvolumes:\n  test:    filesystem: ext4\n    mount: /mnt/data\n    seed: workload phraseFor more information on the volume subsection, see HPVS for VPC: The workload - volumes \nsubsection and HPVS on IBM LinuxONE and IBM Z: Th e workload - volumes subsection .\n3.1.3  The env section\nThe env section is one of the most important sections in a contract and is mandatory. The env \nsection of a contract deals with information that is specific to the cloud environment and is not \nknown to the Workload Provider persona. This section is created by the Workload Deployer persona.\nThe env section is defined with type: env . The env section includes the following \nsubsections:\n/SM590000logging. This subsection is mandatory and te lls the HPVS service w here to send logging \ndata.\n/SM590000volumes. This subsection is optional",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 163,
            "total_chunks": 337
          }
        },
        {
          "id": "9485fa99-3a59-45dd-8e2e-593d5c031a24",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " te lls the HPVS service w here to send logging \ndata.\n/SM590000volumes. This subsection is optional and must be used only when a data volume is \nattached.\n/SM590000signingKey. This subsection is optional and mu st be used only when a contract signature \nis used.\n/SM590000env. This subsection is optional and used to specify values for env variables when defined \nby the Workload Provider, specifically.\n\n48 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentThe logging subsection\nThe minimum subsection that is required for this  section is the logging subsection. For more \ninformation, see Logging for Hyper Protect Virtual Server for VPC  and Logging for IBM Hyper \nProtect Virtual Servers .\nThe following snippet is an example of the logging subsection:\nlogging:\n  logDNA:    hostname: <host name of the Log Analysis instance>\n    ingestionKey: <ingestion Key of the Log Analysis instance>\n    port: <port default-6514>\nThe env - volumes subsection\nBefore continuing wit",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 164,
            "total_chunks": 337
          }
        },
        {
          "id": "6f7a047c-c556-4bde-bd9c-794c4f45ecc9",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " Analysis instance>\n    port: <port default-6514>\nThe env - volumes subsection\nBefore continuing with this subsection, read 3.1.2, “The workload volumes subsection” on \npage 46. For auto volume encryption of the attached data volume two customer seeds must be provided, one in the workload - volumes subsection, and the other in the env - volumes subsection. Currently only one volume is supported. The seeds can be any random text. However, note that if the same volume is used with a different contract, then the seed must match the contract that was used when the volume was firs t used. Otherwise an error will \noccur during deployment.\nThis is an example of the env - volumes subsection:volumes:\n  test:\n    seed: env phrase\nAs mentioned, you can integrate with HPCS to generate a third seed and wrap it with your \nroot key. See the following example. For more information, see Securing your data .\nvolumes: test:   \n   kms:   \n     - apiKey: \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"      ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 165,
            "total_chunks": 337
          }
        },
        {
          "id": "f82ec4ee-a4f2-439c-bf35-3417ad6ff411",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "a .\nvolumes: test:   \n   kms:   \n     - apiKey: \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"          crn: \n\"crn:v1:bluemix:public:hs-crypto:us-south:a/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx:key:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"  \n       type: \"public\"\n     - apiKey: \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"   Note:  The name of the volume must be the same as that in the workload - volumes \nsubsection.\n\nChapter 3. Making the infrastructure secure 49       crn: \n\"crn:v1:bluemix:public:hs-crypto:us-south:a/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx:key:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"  \n       type: \"private\"\n   seed:\"seed1\"   \n   kmsTimeout: 10  \nsigningKey subsection\nFor information about how to use the signingKey, see “Contract signature” on page 52.\nenv subsection\nIf Pod descriptors are used in the workload section, see the example for the template format \nin “The play subsection” on page 41.\nIf a docker compos",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 166,
            "total_chunks": 337
          }
        },
        {
          "id": "045f6284-48e3-4e86-8023-f56b11822c48",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "on, see the example for the template format \nin “The play subsection” on page 41.\nIf a docker compose file is used in the workload section:\nIf the docker compose file has an environment section, use the following snippet as an \nexample:\nenvironment:  KEY1: \"${Value1}\"\n  KEY2: \"${Value2}\"\nWhen the docker compose file has an environment section, as shown in the previous \nexample, it is possible to pass the values in the env section of the Workload Deployer. The following example shows how to specif y the values for the env variables:\nenv: value1: \"abc\"\n value2: \"xyz\"\nFor details about each section and subsection of the contract, see IBM Hyper Protect Virtual \nServers: About the Contract\n3.2  Contract encryption\nWhen the HPVS instance boots, the bootloader decrypts the cont ract. It takes the contents of \neach of the sections in the contract and decrypts the sections that are encrypted. \nAlthough the contract can be passed through the User Data field without encryption, the \nalways encryp",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 167,
            "total_chunks": 337
          }
        },
        {
          "id": "3a236633-6cca-487f-802b-45fb86ac5d6d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "though the contract can be passed through the User Data field without encryption, the \nalways encrypt the contract with an encryption ce rtificate to protect all sensitive information. \nIt is possible to encrypt all the sections of the contract or just one. At a minimum, it is best \npractice to encrypt sections that have login credentials for container registries or LogDNA ingestion keys, for example. \nEach encryption and attestation certificate is signed by the IBM intermediate certificate,\n1 \nwhich in turn is signed by DigiCert Trusted Root G4. For more information about the certificates, see DigiCert Trusted Root Authority Certificates .\n1  An intermediate certificate acts as a layer between t he certificate authority and the end user's certificate.\n\n50 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentWhere to find the encryption certificate \nFor HPVS for VPC, and instance in  IBM Cloud, do the following steps:\n1. For instructions to download the encryption ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 168,
            "total_chunks": 337
          }
        },
        {
          "id": "da0e3c35-e144-4f93-a36d-296d4c7e4617",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " and instance in  IBM Cloud, do the following steps:\n1. For instructions to download the encryption certificate, see Downloading the encryption \ncertificate and extracting the public key .\n2. For instructions to validate the encryption certificate, see Validating the contract \nencryption certificate .\nFor the HPVS instan ce on IBM LinuxONE or IBM Z do the following steps: \n1. Follow the instructions from: \nhttps://www.ibm.com/docs/en/hpvs/2.1.x?topic=servers-downloading-hyper-protect-container-runtime-image  \nLook for the file with name ibm-hyper-protect-container-runtime-XX.YY.Z-encrypt.crt  \nWhere XX.YY.Z is the version being used. \nThe certificates are available in the  /config/certs  directory of the downloaded TAR.GZ \npackage.\n2. Validate the encryption certificate by using the instructions found in the download \npackage. \nEncryption of the workload  section of the contract\nTo encrypt the workload section used in a contract, on an Ubuntu image, perform the \nfollowing steps: \n/SM59",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 169,
            "total_chunks": 337
          }
        },
        {
          "id": "4109b1e2-e6dd-4395-ae5a-93851e2c7b9b",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "pt the workload section used in a contract, on an Ubuntu image, perform the \nfollowing steps: \n/SM590000Create the docker-compose.yaml  file based on the workload requirements. For example:\nservices:\n  redisnode01:\n    image: \ns390x/redis@sha256:db467ab5c53bdeef65762a7534e26fecb94a0f218bd38afd2eaba1a670c472b1\n    ports:\n      - \"6379:6379\"For more information, see Docker Compose overview .\n/SM590000Create the workload section of the contract and add the contents in the workload.yaml  file. \nDo not include the top-level element \"workload\" of the workload section if it is encrypted. Example 3-1 on page 39 shows an example of a workload section in a format that can be encrypted. \nFor more information see the following documentation:\n– HPVS for VPC: IBM cloud: The workload section\n– HPVS on IBM LinuxONE or IBM Z: IBM Hyper Protect Virtual Servers: The workload \nsection\n/SM590000Export the complete path of the workload.yaml  file and the public certificate (this can be \nibm-hyper-protect-co",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 170,
            "total_chunks": 337
          }
        },
        {
          "id": "16a3d03d-cfaa-457e-be7e-129d5864db25",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "mplete path of the workload.yaml  file and the public certificate (this can be \nibm-hyper-protect-container-runtime-1-0-s390x-11-encrypt.crt  for HPVS for VPC or \nthe file in the /config/certs  directory if you are using HPVS for IBM LinuxONE or IBM Z \non-premises:Note:  For illustration purposes on ly, the sample code in t he examples throughout this \nchapter is unencrypted code.\n\nChapter 3. Making the infrastructure secure 51WORKLOAD=\"< PATH to workload.yaml >\"\nCONTRACT_KEY=\"< PATH to public certificate …encrypt.crt >\"\n/SM590000Create a random password. The contract is encrypted through symmetric AES with a \nrandom PASSWORD):\nPASSWORD_W=\"$(openssl rand 32 | base64 -w0)\"\n/SM590000Encrypt password with \nibm-hyper-protect-container-runtime-1-0-s390x-11-encrypt.crt :\nENCRYPTED_PASSWORD_W=\"$(echo -n \"$PASSWORD_W\" | base64 -d | openssl rsautl \n-encrypt -inkey $CONTRACT_KEY -certin | base64 -w0 )\"\n/SM590000Encrypt the workload.yaml  file with a random password:\nENCRYPTED_WORKLOAD=\"$(echo -n",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 171,
            "total_chunks": 337
          }
        },
        {
          "id": "18c36e18-f013-4dbb-9f2a-07a8ca2ce731",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "w0 )\"\n/SM590000Encrypt the workload.yaml  file with a random password:\nENCRYPTED_WORKLOAD=\"$(echo -n \"$PASSWORD_W\" | base64 -d | openssl enc \n-aes-256-cbc -pbkdf2 -pass stdin -in \"$WORKLOAD\" | base64 -w0)\"\n/SM590000Get the encrypted section of the contract using the echo command:\necho \"hyper-protect-basic.${ENCRYPTED_PASSWORD_W}.${ENCRYPTED_WORKLOAD}\"\n/SM590000Paste the encrypted section of the contract to the user-data.yaml  file with the prefix \n\"workload:\" or use the echo command to start a new user-data.yaml  file with an encrypted \nworkload section:\necho \"workload: \nhyper-protect-basic.${ENCRYPTED_PASSWORD_W}.${ENCRYPTED_WORKLOAD}\" > user-data.yaml\nIt should be noted that the prefix hyper-protect-basic is mandatory.\nEncryption of environment (e nv) section of the contract\nWe are using an Ubuntu image for our examples. Similar to the workload section, these are \nthe steps to encrypt the env section used in a contract:\n/SM590000Create the env section  of the contract and add the con",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 172,
            "total_chunks": 337
          }
        },
        {
          "id": "953bc0a6-ba64-47b1-9cd9-dd0269cc4e28",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "the env section used in a contract:\n/SM590000Create the env section  of the contract and add the contents in the env.yaml  file. Do not \ninclude the top level element \"env\" in the envi ronment subsection that is to be encrypted.\nFor more information see:\n– HPVS for VPC: \nhttps://cloud.ibm.com/docs/vpc?topic=vpc-about-contract_se#hpcr_contract_env\n– HPVS on IBM LinuxONE or IBM Z: \nhttps://www.ibm.com/docs/en/hpvs/2.1.x?topic=servers-about-contract#hpcr_contract_env\n/SM590000Export the complete path of the env.yaml  file and the public certificate. For HPVS for VPC, \nthe public certificate can be ibm-hyper-protect-container-runtime-1-0-s390x-11-encrypt.crt. For HPVS for IBM \nLinuxONE or IBM Z on-premises, use the file in the /config/certs  directory:\nENV=\"<PATH to env.yaml >\"\nCONTRACT_KEY=\"< PATH to public certificate …encrypt.crt >\"\n/SM590000Create a random password. The contract is encrypted through symmetric AES with a \nrandom PASSWORD:\nPASSWORD-E=\"$(openssl rand 32 | base64 -w0)\"\n/SM",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 173,
            "total_chunks": 337
          }
        },
        {
          "id": "39563c76-1d1f-4767-a63b-57a09fc9e26e",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "pted through symmetric AES with a \nrandom PASSWORD:\nPASSWORD-E=\"$(openssl rand 32 | base64 -w0)\"\n/SM590000Encrypt password wit h public certificate:\nENCRYPTED_PASSWORD_E=\"$(echo -n \"$PASSWORD_E\" | base64 -d | openssl rsautl \n-encrypt -inkey $CONTRACT_KEY -certin | base64 -w0 )\"\n\n52 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment/SM590000Encrypt the  env.yaml  file with a random password:\nENCRYPTED_ENV=\"$(echo -n \"$PASSWORD_E\" | base64 -d | openssl enc -aes-256-cbc \n-pbkdf2 -pass stdin -in \"$ENV\" | base64 -w0)\"\n/SM590000Get the encrypted section of the contract by using the echo command:\necho \"hyper-protect-basic.${ENCRYPTED_PASSWORD_E}.${ENCRYPTED_ENV}\"\n/SM590000Paste the encrypted section of the contract to the user-data.yaml  file with the prefix \n\"evn:\", or use the echo command to append the encrypted section of the contract to the \nuser-data.yaml  file started in the previous workload section: \necho \"env: hyper-protect-basic.${ENCRYPTED_PASSWORD_E}.${ENCR",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 174,
            "total_chunks": 337
          }
        },
        {
          "id": "301c1d6e-583c-44d7-9822-cc558816d57f",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ted in the previous workload section: \necho \"env: hyper-protect-basic.${ENCRYPTED_PASSWORD_E}.${ENCRYPTED_ENV}\" >>  \nuser-data.yaml\nContract signature\nContract signature is an optional feature that can be used to sign a contract before it is passed as input. Any contract can be signed regardless of it being encrypted (fully or partially) or in plain text. Va lidation of the contra ct signature is per formed by the HPVS \ninstance. The purpose of this signature feat ure is to ensure that the workload and env \nsections are always used together and are not tampered with by a third party. The signature of the workload and the env sections are added as the value to the envWorkloadSignature section of the contract. The following are two sections in a contract that are relevant while creating and adding a contract signature:\n/SM590000envWorkloadSignature. This is a section where the signature of the other sections of the \ncontract is added. This section is not required for a contract that is n",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 175,
            "total_chunks": 337
          }
        },
        {
          "id": "3d256366-32c3-41b1-bd58-f17fb15d918f",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " the other sections of the \ncontract is added. This section is not required for a contract that is not signed.\n/SM590000signingKey. This is a subsecti on that must be added to the env section of the contract. \nThis holds the value to the user-generated pu blic key, whose corresponding private key \nwas used to create the contract signature.\nComplete the following steps to create the contract signature. We are using an Ubuntu image \nfor these examples:\n/SM590000Use the following commands to generate a key pair to sign the contract. Note that \n\"CustomPassphrase\" is the passphrase to generate keys; you can use your own:\nopenssl genrsa -aes128 -passout pass:CustomPassphrase -out private.pem 4096openssl rsa -in private.pem -passin pass:CustomPassphrase -pubout -out \npublic.pem\n/SM590000Use the following command to save the signing key from the public.pem certificate in yaml \ncompatible format:\nkey=$(awk -vRS=\"\\n\" -vORS=\"\\\\\\n\" '1' public.pem)echo ${key%\\\\n}\nThe key must be added to the env se",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 176,
            "total_chunks": 337
          }
        },
        {
          "id": "945b91bc-9634-49ac-aad0-edce7ee65d7c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "\nkey=$(awk -vRS=\"\\n\" -vORS=\"\\\\\\n\" '1' public.pem)echo ${key%\\\\n}\nThe key must be added to the env section of the contract with the prefix \"signingKey: \" \nbefore it is encrypted. \n/SM590000To add the key, run the following command below in the same directory where the \nenv.yaml  file is located to append the signing key to the env.yaml  file: \necho ${key%\\\\n} >> env.yaml\nAfter appending the signing key, see “Encryption of environment (env) section of the \ncontract” on page 51 to encrypt the  env.yaml  file with the signing key.\n/SM590000Create the contract.txt file. Add the value of workload first then add the value of env from \nthe user-data.yaml  file. Ensure that there is no space or new line after workload and \nbefore env. Also, ensure that there is no new line or space at the end of the file. It is \n\nChapter 3. Making the infrastructure secure 53recommended to cross-check the binary content of the contract.txt  file with tools such \nas hexdump. In the binary file dump, make sure th",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 177,
            "total_chunks": 337
          }
        },
        {
          "id": "4aa10b65-1f7b-428a-b0ee-9bb480425e95",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "content of the contract.txt  file with tools such \nas hexdump. In the binary file dump, make sure that there is no 0a ASCII value as the last \nentry. The contract.txt  file should look sim ilar to the following text:\nhyper-protect-basic.js7TGt77EQ5bgkjhC0pViFTRHqWtn..............hyper-protect-ba\nsic.VWg/5/SWE+9jLkjhr8q4i.........\nAlternatively, if you defined the workload and env sections as discussed in 3.1.1, “The \nworkload section” on page 39 and 3.1.3, “The env section” on page 47, then you can create the  contract.txt  file with the echo command:\necho \n\"hyper-protect-basic.${ENCRYPTED_PASSWORD_W}.${ENCRYPTED_WORKLOAD}hyper-protect-basic.${ENCRYPTED_PASSWORD_E}.${ENCRYPTED_ENV}\" > contract.txt\n/SM590000A signature can then be generated with the echo command:\necho $( cat contract.txt | openssl dgst -sha256 -sign private.pem | openssl enc \n-base64) | tr -d ' '\n/SM590000This can then be added to the user-data.yaml  contract with the prefix \n\"envWorkloadSignature:\" or use the echo comm",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 178,
            "total_chunks": 337
          }
        },
        {
          "id": "122f3cd7-cf18-4328-b02c-42125d6a8af6",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " added to the user-data.yaml  contract with the prefix \n\"envWorkloadSignature:\" or use the echo command to append the signature directly in the \nuser-data.yaml  file that was started in the workload section and add to the env section:\necho \"envWorkloadSignature: `echo $( cat contract.txt | openssl dgst -sha256 \n-sign private.pem -passin pass:CustomPassphrase | openssl enc -base64) | tr -d ' '`\"  >> user-data.yaml\n3.3  Contract certificates\nTo protect the contract a public and private key pair is created as part of the Hyper Protect Secure Build (HPSB) pipeline that  is used to generate the HPCR image. This key pair is used \nto provide confidentiality for contract contents. The public X509 certificate of the Contract Encryption public key is published by IBM and can be validated with the 3rd party certificate authority root key by any persona out-of-band. \nEach persona independently encrypts their contra ct section using this Contract Encryption \npublic key. The contract encryption priv",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 179,
            "total_chunks": 337
          }
        },
        {
          "id": "7e3b7148-cf8c-414e-9feb-25340b7f9cb4",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "pts their contra ct section using this Contract Encryption \npublic key. The contract encryption private ke y is randomized during image build and exists \ninside the Secure Execution encrypted HPCR image only. Such image can only be decrypted by using Secure Execution and keys rooted in hardware of the IBM Z or IBM LinuxONE \nplatform. During deployment this key is used by the Bootloader to decrypt the Contract and the Bootloader ensures protection of this key from User space and the Workload. \nCertificate Revocation List\nA Certificate Revocation List (CRL) is a list of digital certificates that have been revoked by \nthe Certificate Authority (CA) before their scheduled expiration date and should no longer be trusted. According to RFC 5280, a revoked certificate can be in one of two states:\n1. Revoked. A certificate is  irreversibly revoked if: \n– it is discovered that the CA improperly issued a certificate– the private-key for the certificate is compromised for some reason\n2. Hold. A ce",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 180,
            "total_chunks": 337
          }
        },
        {
          "id": "b2393962-5ad0-4d0d-9a03-c3a247b09d0c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "sued a certificate– the private-key for the certificate is compromised for some reason\n2. Hold. A certificate can be put on hold if there is a chance that a private key is \ncompromised. The hold can be removed if it is determined that the private key was not compromised.\n\n54 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentThe HPCR images used to build  HPVS instances are accompanie d by a CRL, so the user \ncan verify that the certificates being used to validate contract encryption certificate and validate attestation certificate are in fact valid.\nFor more details, consult IBM Hyper Protect Virtual Server fo r VPC: Validating the Certificates  \nand IBM Hyper Protect Virtual Serv er: Validating the certificates  \n3.4  Attestation\nAttestation is the process with which a TEE or Hyper Protect Platform can provide evidence \nor measurements of its origin and current state. This evidence can be verified by another party, for example a party assuming the Auditor perso",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 181,
            "total_chunks": 337
          }
        },
        {
          "id": "12eedbce-a1c2-4fc5-bb4b-b7aeee63605e",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "tate. This evidence can be verified by another party, for example a party assuming the Auditor persona, who can then decide whether to trust the application running in the TEE or not. Typically, the attestation record is signed by a trusted key that is anchored in hardware that can be vouched for by a manufacturer. This is \nnecessary to assure the Auditor that is representing the party validating the evidence was not created or altered by an unauthorized component or actor. The attestation record enables validation and proof by the Auditor persona with root of trust based in 3rd-party authority. See Figure 3-1.\nFigure 3-1   Verifying trust in an attestation record\nThe HPVS instance provides an attestation record  that is securely generated and signed by \nthe Bootloader during the instance deployment. The signing key is published as a X509 certificate and can be validated out-of-band to a 3rd-party certificate authority. The attestation record is available to the workload only if the at",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 182,
            "total_chunks": 337
          }
        },
        {
          "id": "472c63b7-e478-4d62-a3fe-297723e05ed0",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " 3rd-party certificate authority. The attestation record is available to the workload only if the attestationPublicKey  section is provided in the \ncontract.\nThe Workload Provider persona can implement me ans for providing the attestation record to \nthe Auditor persona. The Auditor can then verify, out-of-band, the environment in which the workload was started. \nFigure 3-2 on page 55 highlights the creation and management of the certificate hierarchy \nthat is involved in signing th e Attestation Record. The Attestation Record is signed by the \nHyper Protect Attestation Signing Key (HPASK). The HPASK can be confirmed by the \npublished intermediate certificate. The intermediate certificate is signed by a 3rd party \n\n\nChapter 3. Making the infrastructure secure 55certificate authority, which is proven by the root certificate of that given certificate authority, \nthus completing the chain of trust. \nFigure 3-2   Validating content  of an attestation record\nAn optional public key for encryp",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 183,
            "total_chunks": 337
          }
        },
        {
          "id": "4f592996-112e-431b-9adb-5d72617c8c83",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " trust. \nFigure 3-2   Validating content  of an attestation record\nAn optional public key for encrypting the Attestation Record at the point of creation is provided \nby the Auditor persona. The public key is defined in the contract section: attestationPublicKey. The private part of this key is kept secret by the Auditor. The hash of this public key is added to the Attestation Record. By encrypting the Attestation Record and including the hash of the public key that is used for encryption within the Attestation Record, only the Auditor can decrypt the attestation record. Only the auditor ca n validate that the workload that is deployed \nin the enclave is the expected and untampered version of the workload that is expected to be deployed. \nThis attestation record contains measurements of what has been started and can be used to \nvalidate that the environment is the one deployed based on the following measurements: \n/SM590000The original base image. The compressed root file system to be s",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 184,
            "total_chunks": 337
          }
        },
        {
          "id": "070fabaf-098a-4266-9985-480376e4ad7a",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "e following measurements: \n/SM590000The original base image. The compressed root file system to be stored in the LUKS \nencrypted partition. \n/SM590000The cloud initialization options, including t he contract. The Hyper Protect Attestation \nSigning Key confirms with signature that the image got created in the HPSB pipeline. \nThe reference values for the measurements spec ific to the HPCR Image used are originated \nin the HPSB pipeline. Ot her measurements like the Cloud in itialization options  are dependent \non the contract or can be used by the workload to provide insight about identifying individual \ninstance, which enables the Auditor persona to  validate that the deployment is as expected.\nFor more details, consult the following web pages:\n/SM590000IBM Cloud Hyper Protect for VPC: Attestation  \n/SM590000The Second Generation of Hyper Protect Platform\n/SM590000Confidential Computing with SUSE Linux Ente rprise Base Container Images Using the \nIBM Hyper Protect Platform\n\n\n56 Applying",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 185,
            "total_chunks": 337
          }
        },
        {
          "id": "ed09ff61-aa76-4de2-8206-76f82881094d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ith SUSE Linux Ente rprise Base Container Images Using the \nIBM Hyper Protect Platform\n\n\n56 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment3.5  Logging for HPVS instances\nTo start an HPVS instance, set up logging first by  adding the logging co nfiguration in the env \nsection of the contract. The instance reads the configuration and configures logging. All other services start only after logging is configured. If the logging configuration is incorrect, the instance will not start and an error message is displaye d in the serial console.\nThe logs include startup logs, service logs issued by the HPVS instance, and container logs.\nIBM Log Analysis in IBM Cloud\nLogging to Log Analysis depends on the state and health of the Log Analysis service. Service \noutages might lead to a loss of log data. If yo u are logging data for audit purposes, consider \nusing a logging service by performing the following steps:\n1. Log in to your IBM Cloud account.\n2. Provision a Log A",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 186,
            "total_chunks": 337
          }
        },
        {
          "id": "b8bd24f4-1bdd-4418-b63f-296324b19506",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "service by performing the following steps:\n1. Log in to your IBM Cloud account.\n2. Provision a Log Analysis instance. Choose a plan according to your requirements.\n3. After creating the Log Analysis instance, click it to open it and click Open dashboard .\n4. Click the question mark icon at the lower left  of the page to view Install Inst ructions. \n5. On the Add Log Source page, under Via Syslog, click rsyslog .\n6. Make a note of the ingestion key value at the upper right of the page, and the endpoint \nvalue. The endpoint value is contained in that starts with *.*. In Example 3-13, the \nendpoint value is syslog-u.au-syd.logging.cloud.ibm.com .\nExample 3-13   Start log analysis\n### START Log Analysis rsyslog logging directives ###\n## TCP TLS only ##\n$DefaultNetstreamDriverCAFile /etc/ld-root-ca.crt # trust these CAs$ActionSendStreamDriver gtls # use gtls netstream driver$ActionSendStreamDriverMode 1 # require TLS$ActionSendStreamDriverAuthMode x509/name # authenticate by hostname$Action",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 187,
            "total_chunks": 337
          }
        },
        {
          "id": "d700e6ca-2aa6-4a48-bc77-dcb3aff3ae06",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "riverMode 1 # require TLS$ActionSendStreamDriverAuthMode x509/name # authenticate by hostname$ActionSendStreamDriverPermittedPeer *.au-syd.logging.cloud.ibm.com## End TCP TLS only ##\n$template LogDNAFormat,\"<%PRI%>%protocol-version% %timestamp:::date-rfc3339% \n%HOSTNAME% %app-name% %procid% %msgid% [logdna@48950 key=\\\"bc8a5ba9aa5c0c12b26c6c45089228a4\\\"] %msg%\"\n# Send messages to Log Analysis over TCP using the template.\n*.* @@syslog-u.au-syd.logging.cloud.ibm.com:6514;LogDNAFormat\n### END Log Analysis rsyslog logging directives ##7. Add these values in the env logging subsection of the contract: \n    env:\n      logging:        logDNA:Tip: For workloads that produce sensitive info rmation, it is poss ible to encrypt log \nmessages.\n\nChapter 3. Making the infrastructure secure 57          hostname: ${RSYSLOG_LOGDNA_HOSTNAME}\n          ingestionKey: ${LOGDNA_INGESTION_KEY}\nFor more information, see “The logging subsection” on page 48.\nWhen the HPVS instance boots, it extracts the Log Analy",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 188,
            "total_chunks": 337
          }
        },
        {
          "id": "7d6d3fd4-b2ee-4cea-845f-ef70f74e012b",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "on, see “The logging subsection” on page 48.\nWhen the HPVS instance boots, it extracts the Log Analysis info rmation from the contract and \nconfigures accordingly, so that all the logs are routed to the endpoint specified. The information can be seen on the console window of HPVS during boot up. After that happens, open the Log Analysis dashboard and view the logs from the virtual server instance.\nsyslog\nLogging can be configured with a generic syslog backend such as an rsyslog server or a Logstash server. The HPVS instan ce uses TLS with mutual auth entication to connect to the \nlogging backend. Find the following pieces of information to configure logging:\n/SM590000Syslog hostname and port.\n/SM590000Certificate Authority (CA). The certificate used to verify the certificate chain both for client \nand server authentication. The same CA must be used for both the client and server certificates.\n/SM590000Client certificate. Used to  prove the client to the server, signed by the CA.\n/SM590",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 189,
            "total_chunks": 337
          }
        },
        {
          "id": "0e2f2b2e-eed9-4603-ba48-b9c142414988",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ates.\n/SM590000Client certificate. Used to  prove the client to the server, signed by the CA.\n/SM590000Client key. A private key used by the vi rtual server instance to establish trust.\nComplete the following parts of the contract with the information. The certificates and the key \nmust be in PEM format\n2:\nenv: |\ntype: env\n  logging:\n    syslog:\n      hostname: ${HOSTNAME}      port: 6514      server: ${CA}\n      cert: ${CLIENT_CERTIFICATE}\n      key: ${CLIENT_PRIVATE_KEY}\n2  A container format that includes public  certificate or the entire certificate chain (private key, public key, root \ncertificates).Note:  Make sure to use a strong digest algorithm for the certificates. Otherwise, the syslog \nserver might reject the certificates.\nAlso, the port value can be changed to any valid TCP port number, however, 6514 is the \ndefault port that is used for secure logging. If you use the Crypto Express Network API, also called the Crypto Appliance, the default port must be used. A different p",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 190,
            "total_chunks": 337
          }
        },
        {
          "id": "9b120ed6-9f49-4abb-8985-a1c7e148e60a",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " Express Network API, also called the Crypto Appliance, the default port must be used. A different port configuration is not supported by the Crypto Appliance.\n\n58 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentPreparation steps\nYou can use the following procedure to create the required certificates and keys. The example \nuses openssl  and shows bash  syntax.\n1. Create a CA private key and a certificate signing request (CSR).\nPrepare the ca.cnf  configuration file:\n[ req ]default_bits = 2048\ndefault_md = sha256\nprompt = noencrypt_key = no\ndistinguished_name = dn\n[ dn ]\nC = US\nO = Logstash Test CACN = ca.example.org\n2. Create the key and certificate:\n# create private key\nopenssl genrsa -out ca-key.pem 4096# create CSR\nopenssl req -config ca.cnf -key ca-key.pem -new -out ca-req.csr\n# create self-signed CA\nopenssl x509 -signkey ca-key.pem -in ca-req.csr -req -days 365 -out ca.crt\n3. Create files used on the rsyslog server:\nPrepare the server.cnf  configuration f",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 191,
            "total_chunks": 337
          }
        },
        {
          "id": "de68d693-6eb9-44c8-9411-b368b7cb34f6",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " 365 -out ca.crt\n3. Create files used on the rsyslog server:\nPrepare the server.cnf  configuration file. It is important to set the default_md value to at \nleast sha256 . Make sure to complete the correct information for the dn field. It is preferred \nto use a domain name for CN, but an IP is also acceptable. For more information, see the OpenSSL documentation on Subject Alternative Name.\n– Example that uses a hostname:\n[ req ]default_bits = 2048\ndefault_md = sha256\nprompt = noencrypt_key = no\ndistinguished_name = dn\n[ server ]Note:  Make sure to update dn with your values. The actual values can be selected, and \nthey do not play a role for the subsequent processing.\n\nChapter 3. Making the infrastructure secure 59subjectAltName = DNS:${HOSTNAME}\nextendedKeyUsage = serverAuth\n[ dn ]\nC = US\nO = Rsyslog Test Server\nCN = ${HOSTNAME}\n– Example that uses an IP address:\n[ req ]\ndefault_bits = 2048default_md = sha256\nprompt = no\nencrypt_key = nodistinguished_name = dn\n[ server ]\nsubjectAltName",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 192,
            "total_chunks": 337
          }
        },
        {
          "id": "2604f072-cdba-4efb-843d-835478d4bf19",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "048default_md = sha256\nprompt = no\nencrypt_key = nodistinguished_name = dn\n[ server ]\nsubjectAltName = IP:${IP}\nextendedKeyUsage = serverAuth\n[ dn ]\nC = US\nO = Rsyslog Test ServerCN = ${IP_OR_HOSTNAME}\n4. Create the key and certificate. Ensure the server certificate server.crt  contains a SAN for \nthe IP or the hostname, depending on whether the server is accessed through IP or hostname.\n    # create private key\n    openssl genrsa -out server-key.pem 4096\n    # create CSR for the server certificate    openssl req -config server.cnf -key server-key.pem -new -out server-req.csr\n    # have the CA created in (1) sign the certificate\n    openssl x509 -req -in server-req.csr -days 365 -CA ca.crt -CAkey ca-key.pem \n-CAcreateserial -extfile server.cnf -extensions server -out server.crt\n5. Create files used on the client side for the HPVS instance.\nPrepare the client.cnf configuration file:\n    [ req ]\n    default_bits = 2048\n    default_md = sha256\n    prompt = no    encrypt_key = no\n\n60 Apply",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 193,
            "total_chunks": 337
          }
        },
        {
          "id": "4b9b1d0c-8d07-4a68-9a1e-20dfbf00d261",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " req ]\n    default_bits = 2048\n    default_md = sha256\n    prompt = no    encrypt_key = no\n\n60 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment    distinguished_name = dn\n    [ dn ]\n    C = US    O = Logstash Test Client\n    CN = client.example.org\n6. Create the key and certificate:\n    # create private key\n    openssl genrsa -out client-key.pem 4096\n    # create CSR for client auh\n    openssl req -config client.cnf -key client-key.pem -new -out client-req.csr    # have the CA created in (2) sign the certificate\n    openssl x509 -req -in client-req.csr -days 365 -CA ca.crt -CAkey ca-key.pem \n-CAcreateserial -out client.crt\n    # export key to PKCS#8 format    openssl pkcs8 -topk8 -inform PEM -outform PEM -nocrypt -in client-key.pem \n-out client-key-pkcs8.pem\nClient setup steps\n1. Configure the contract with the template in Example 3-14.\nExample 3-14   Contract template\nenv: |\ntype: env\n    logging:        syslog:            hostname: ${HOSTNAME}            por",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 194,
            "total_chunks": 337
          }
        },
        {
          "id": "5cd69a72-edc6-4198-8287-3500dcc91241",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "emplate\nenv: |\ntype: env\n    logging:        syslog:            hostname: ${HOSTNAME}            port: 6514            server: ${CA}            cert: ${CLIENT_CERTIFICATE}            key: ${CLIENT_PRIVATE_KEY}\n2. Use the content of the follo wing files in preparation to  fill in the placeholders:\n– The value for ${CA} can be found in the ca.crt\n file. See Step 1 on page 58 . \n– The value for ${CLIENT_CERTIFICATE} can be found in the client.crt  file. See Step \n5 on page 59.\n– The value for ${CLIENT_PRIVAT E_KEY} can be found in the client-key-pkcs8.pem \nfile. See Step 5 on page 59Note:  Make sure to update dn with your values. Whether the actual values play a role \ndepends on the StreamDriver.Authmode setting. In this example, we use the setting StreamDriver.Authmode=\"x509/certvalid\". In this case, the value of dn does not play a \nrole because all valid client certificates ar e accepted. Adjust this according to your \nneeds. For more information, see StreamDriver.Authmode .\n\nChapter 3.",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 195,
            "total_chunks": 337
          }
        },
        {
          "id": "b0d306cd-b41e-420d-96e7-700a2386b440",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " Adjust this according to your \nneeds. For more information, see StreamDriver.Authmode .\n\nChapter 3. Making the infrastructure secure 61– The values for ${HOSTNAME}, ${ CA}, ${CLIENT_CERTIFICATE}, and \n${CLIENT_PRIVATE_KEY} are strings without extra encoding or escaping. Regardless \nof their, ensure you use valid YAML For more information, see Scalars . \nIn Example 3-15, “|” (the pipe symbol) can be us ed to provide literal values, so the value of \nthe certificates or keys can simply be pasted into the YAML file. The correct indentation must be observed. Note that the certificate values are truncated with “…”. For the complete example, see Append ix A, “Client contract setu p sample files” on page 99.\nExample 3-15   Client cert ificate - literal values \nenv: |\ntype: env\n   logging:      syslog:         hostname: ${HOSTIP} # eg 10.0.0.8 or ${HOSTNAME}         port: 6514         server: |            -----BEGIN CERTIFICATE-----            MIIEuDCCAyCgAwIBAgIUBR9g6L5hivov7eNT00HSXW39oD0wDQ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 196,
            "total_chunks": 337
          }
        },
        {
          "id": "47e22e17-e82e-41f6-aa6e-ea1efb8ca040",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "           -----BEGIN CERTIFICATE-----            MIIEuDCCAyCgAwIBAgIUBR9g6L5hivov7eNT00HSXW39oD0wDQYJKoZIhvcNAQEL            BQAwXzELMAkGA1UEBhMCVVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFDASBgNVBAcM            C0xvcyBBbmdlbGVzMQwwCgYDVQQKDANJQk0xFzAVBgNVBAMMDmNhLmV4YW1wbGUu            …            -----END CERTIFICATE-----         cert: |            -----BEGIN CERTIFICATE-----            MIID0zCCAjsCFFS5goaaDyhsJsUHv5WooqDg9gqGMA0GCSqGSIb3DQEBCwUAMF8x            CzAJBgNVBAYTAlVTMRMwEQYDVQQIDApDYWxpZm9ybmlhMRQwEgYDVQQHDAtMb3Mg            QW5nZWxlczEMMAoGA1UECgwDSUJNMRcwFQYDVQQDDA5jYS5leGFtcGxlLm9yZzAe            …            -----END CERTIFICATE-----         key: |            -----BEGIN RSA PRIVATE KEY-----            MIIEogIBAAKCAQEAvQoaZ9z2ZU0sKCoJ+lTyzI7vN3Mhc2QOsSYBWxrQIFUt4WW1            pinXXOqlo4iPRnQsQPzhkN8blZrgI2SFk1N8IdK8JHFcO9yVWEKmnxNeIOgiOvjr            k3nSTkDH7GZyZe0p0d+Dbk671P4cKoxi32JgSK2iFelZnYrgELiZFWbIZfKuy4Yz\n            …\n            -----END RSA PRIVATE KEY-----\nIn Examp",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 197,
            "total_chunks": 337
          }
        },
        {
          "id": "17be2e5a-f821-4886-8a93-33fdf34afb67",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "oxi32JgSK2iFelZnYrgELiZFWbIZfKuy4Yz\n            …\n            -----END RSA PRIVATE KEY-----\nIn Example 3-16 the new lines are replaced with \\n and carriage returns are deleted to \nmake sure the content fits in one line between the inverted commas. For more information, see scalars in double-quoted style . The certificate values are truncated with “…”. For the \ncomplete example, see Append ix A, “Client contract setu p sample files” on page 99.\nExample 3-16   Client certificate - double-quoted\nenv: |\ntype:env\n   logging:      syslog:         hostname: ${HOSTIP} # eg 10.0.0.8 or ${HOSTNAME}         port: 6514         server: \"-----BEGIN CERTIFICATE-----\\nMIIFCTCCAvECFEp7wJLz4jNStIsVH2dUeHDN26ZyMA0GCSqGSIb3DQEBCwUAMEEx\\nCzAJBgNVBAYTAlVTMRkwFwYDVQQKDBBMb2dzdGFzaCBUZXN0IENBMRcwFQYDVQQD\\nDA5jYS5leGFtcGxlLm9yZzAeFw0yMzAxMDUxNjU0MTNaFw0yNDAxMDUxNjU0MTNa…\\n-----END CERTIFICATE-----\\n\"\n\n62 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment         cert: \"-----BEGIN \nCERTI",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 198,
            "total_chunks": 337
          }
        },
        {
          "id": "0adcb4ad-f72c-4726-b6d7-eadcd9e6ae11",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "g Data Protection and Confidentiality in a Hybrid Cloud Environment         cert: \"-----BEGIN \nCERTIFICATE-----\\nMIIFETCCAvkCFBhx5DuYtRzCxRx8Bo+WIS2LFI2uMA0GCSqGSIb3DQEBCwUAMEEx\\nCzAJBgNVBAYTAlVTMRkwFwYDVQQKDBBMb2dzdGFzaCBUZXN0IENBMRcwFQYDVQQD…\\n-----END CERTIFICATE-----\\n\"         key: \"-----BEGIN PRIVATE KEY-----\\nMIIJQQIBADANBgkqhkiG9w0BAQEFAASCCSswggknAgEAAoICAQCtj437cgRishCl\\n0w9PrEyqSxJLjeDb7jgR1iI82ic/YqMRR0b+DnsIGcg5pR9nK+DcVz1E4EyGphro…\\n-----END PRIVATE \nYou can also use other valid YAML variations. Out of the two variations described, the \nliteral variation is considered more user friend ly as it allows for eas ier visualization of the \ncomplete YAML file. However, unless a YAML compatible editor is used, spaces must be added to the beginning of the pasted lines to match the correct indentation. In this case, it might be easier to use double-quoted variants a nd a simple script to output the certificate \nand key files in the correct fo rm. See Appendix A, “Client cont ract s",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 199,
            "total_chunks": 337
          }
        },
        {
          "id": "72991ba7-7927-4752-ab75-3eab29e64a26",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "t to output the certificate \nand key files in the correct fo rm. See Appendix A, “Client cont ract setup sample files” on \npage 99 for sample script yaml_doublequoted_input.sh .\nServer setup steps\nThere are many ways to set up a compatible server endpoint. The following steps provide a \nsimple setup of an rsyslog server: \n1. Wwe are using an Ubuntu image for our examples. Install the required server packages:\napt-get install rsysl og rsyslog-gnutls\n2. Get certificates and keys from the preparation steps: \n– ca.crt - from Preparation Step 1 on page 58 , copy to /certs/ca.crt\n– server.crt - from Preparation Step 3 on page 58 , copy to /certs/server.crt\n– server-key.pem - from Preparation Step 3 on page 58 , copy to /certs/server-key.pem\n3. Configure the rsyslog server in the /etc/r syslog.d/server.conf file. See Example 3-17.\nExample 3-17   rsyslog server config file\n# make gtls driver the default and set certificate files\n$DefaultNetstreamDriver \"gtls\"$DefaultNetstreamDriverCAFile /cert",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 200,
            "total_chunks": 337
          }
        },
        {
          "id": "5fc985ae-777d-418a-8f73-247969d9f02a",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " default and set certificate files\n$DefaultNetstreamDriver \"gtls\"$DefaultNetstreamDriverCAFile /certs/ca.crt$DefaultNetstreamDriverCertFile /certs/server.crt$DefaultNetstreamDriverKeyFile /certs/server-key.pem# provides TCP syslog receptionmodule(load=\"imtcp\"        StreamDriver.Name=\"gtls\"        StreamDriver.Mode= \"1\"        StreamDriver.Authmode=\"x509/certvalid\" # Currently, CA does not support#       StreamDriver.Authmode=\"anon\" # Use this for CA server)input(type=\"imtcp\" port=\"6514\")# Template for incoming logs$template incoming-remote-logs,\"/var/log/remotelogs/%FROMHOST-IP%/%PROGRAMNAME%.log\"*.* ?incoming-remote-logs    \nFor more information, see rsyslog . The example config will log the received logs to the \ndirectory and file / var/log/remotelogs/%FROMHOST-IP%/%PROGRAMNAME%.log.  So host-ip  is \nthe IP of the remote host that is sending logs and the program name is the running application that is sending logs. In a producti on configuration, you might want to forward \nthe logs ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 201,
            "total_chunks": 337
          }
        },
        {
          "id": "69426a19-b21f-4e00-bd07-0d99a0a58057",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "plication that is sending logs. In a producti on configuration, you might want to forward \nthe logs to a database, but this is outside of the scope of this documentation.\n\nChapter 3. Making the infrastructure secure 63If you are setting up a logging server that will also be used by the Crypto Express Network \nAPI, then uncomment the line StreamDriver.Authmode=\"anon\" and comment the previous line, StreamDriver.Authmode=\"x509/certvalid\", because the Crypto Appliance is not compatible with StreamDriver.Authmode=\"x509/certvalid\".\n4. Restart the syslog service:\nservice syslog restart\n3.6  Encrypting data volumes\nThe data volume that can be at tached to an HPVS instance is  protected by a Linux Unified \nKey Setup (LUKS) encryption passphrase that is derived from seeds provided during deployment. \nFor workloads on IBM Cloud, a higher level of encry ption protection and control is possible by \ncombining the seeds with an additional secret that is generated by, and held within, the HPCS. HPCS i",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 202,
            "total_chunks": 337
          }
        },
        {
          "id": "aa9b836a-3e2e-45e6-9230-ed36f57073d1",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ombining the seeds with an additional secret that is generated by, and held within, the HPCS. HPCS is backed by FIPS 140-2 Level 4-certified hardware, which is the highest offered by any cloud provider in the industry.\nBefore proceeding with this of fering, thought should be given to the availability aspects of \nHPCS from the on-premises environment, such as additional physical internet connections. \nHow your data volume is encrypted\nWithout your own key, the data volume that is attached to the instance is encrypted \nautomatically with two seeds that are provided in the  workload - volume s and env - volumes \nsections of the contract. The seeds are internally converted to UTF8 sequences and then concatenated. The hash (SHA256) of the concatenated sequence is computed as a hexdigest, which is used as the LUKS passphrase to encrypt the data volume. For more information, see 3.1, “The contract” on page 38.\nProtecting your sensitive data with your own key\nKey management service (KMS) suppo",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 203,
            "total_chunks": 337
          }
        },
        {
          "id": "d0e96769-2c06-4c6e-a4ed-c3cc033083d0",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "act” on page 38.\nProtecting your sensitive data with your own key\nKey management service (KMS) support is integrated in HPCS for HPVS for VPC with \nibm-hyper-pro tect-container-r untime-1-0-s390x-11 and for HPVS for IBM LinuxONE or IBM Z \nwith product version 2.1.5. Note:  The gnutls package imposes a requirement for the signatures for the client \ncertificate. For more information, see Digital signatures. \nAlso, in this configuration, logs are accepted from any client certificate that is signed by \nthe certificate authority through the x509/certvalid mode. This might change depending on the StreamDriver.Authmode setting. For more information, see StreamDriver.Authmode .\n\n64 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentHPCS generates a random value as the third seed and wraps it with the customer root key \n(CRK). The wrapped seed is stored in the metadata partition of your data volume. The LUKS passphrase is generated by using three seeds: the seed in the me",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 204,
            "total_chunks": 337
          }
        },
        {
          "id": "2a2a36c7-0130-4577-b169-dffe0466c451",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ition of your data volume. The LUKS passphrase is generated by using three seeds: the seed in the metadata partition, which is unwrapped first, and the two seeds from the contract. See Figure 3-3.\nFigure 3-3   Integration with key management service\nNote:  For new HPVS instances, the data volume is partitioned into two parts. The first \npartition of 100 MiB is reserved for internal metadata only. It is not to be accessed by a workload. The second partition remains as the data volume for workload. Only new volumes are partitioned.\n\n\n© Copyright IBM Corp. 2024. 65Chapter 4. Application development in a \ntrusted environment\nThis chapter describes how IBM Hyper Protect Services can be used to establish a trusted \napplication development and deployment process. It shows how secure practices can be used to improve each part of the development process and different parts of deployed applications.\nThe chapter topics include improvements of  the application lifecycle and application \ndevelopmen",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 205,
            "total_chunks": 337
          }
        },
        {
          "id": "097a1580-ed1e-4138-b4b9-1b1b133606da",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "s.\nThe chapter topics include improvements of  the application lifecycle and application \ndevelopment, test, build, and release. It includes descriptions of initial and update deployment, and aid for secure implementation of the different steps is provided. Where feasible, multiple options are provided as alternatives. Less secure implementation or configuration options might be omitted.\nThe concepts are demonstrated with sample code that uses a combination of shell, makefiles, \nand Terraform commands to show how the resources can be deployed. Sample code of deployed applicatio ns is written in golang  and JavaScript .\nFor more information about the source code from this chapter, see IBM hyperprotect GitHub \nrepository .\nRequired configuration values are supplied in the form of environment variables and \nTerraform variable files. For convenience, the environment variables can be set from .env* \nfiles located next to the Makefile files within the different directories. The required valu",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 206,
            "total_chunks": 337
          }
        },
        {
          "id": "23bb387e-2d8e-4a7b-a9b8-811674fa1068",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " .env* \nfiles located next to the Makefile files within the different directories. The required values are outlined and describe d in sample files.\nIn this chapter, the following topics are discussed:\n/SM5900004.1, “Securing the application lifecycle” on page 66\n/SM5900004.2, “Build container image by using Hyper Protect Secure Build” on page 71\n/SM5900004.3, “Zero knowledge proofs: TLS server certificates and wrapped secrets” on page 78\n/SM5900004.4, “Trust in-depth based on boot flow attestation” on page 85\n/SM5900004.5, “Data storage” on page 87\n/SM5900004.6, “Securing cloud native services” on page 92\n/SM5900004.7, “Secure supply chain with SLSA” on page 954\n\n66 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment4.1  Securing the application lifecycle\nThe use cases that are described in this chapter are based on the application lifecycle of a \nsecured application that presents an interface for making monetary payments. The application is called \nSamplePayment",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 207,
            "total_chunks": 337
          }
        },
        {
          "id": "6b3857a9-8764-4076-8a14-c37a57686f8b",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "on that presents an interface for making monetary payments. The application is called \nSamplePaymentSystem and shows how sensitive payment-related \ninformation, such as credit card data, is used in a secure way. A key requirement is to run this \napplication in a confidential computing environment to ensure PII data that is in use is protected from malicious actors.\nFigure 4-1 shows a high-level overview of the Hyper Protect components that are involved in \nthe development lif ecycle of the SamplePaym entSystem application. \nFigure 4-1   SamplePa ymentSystem applicat ion lifecycle using Hyper Protect services\nFor an explanation of the different Hyper Protect components, refer to Chapter 2, \n“Understanding the solution” on page 15. For a description of the personas see, “Separation of duty” on page 11.\nThe subsequent section includes discussi on of the development lifecycle phases:\n/SM590000Development\n/SM590000Test\n/SM590000Build\n/SM590000Release\n/SM590000Deployment\n/SM590000Update\n\n\nCh",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 208,
            "total_chunks": 337
          }
        },
        {
          "id": "457b4b25-2e4e-4d0c-afb0-40aed4c58be7",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "00Development\n/SM590000Test\n/SM590000Build\n/SM590000Release\n/SM590000Deployment\n/SM590000Update\n\n\nChapter 4. Application developme nt in a trusted environment 674.1.1  Development\nCode is stored and developed against a source-code management system, which is \npredominantly Git. The most prominent hosted Git service currently is GitHub. For more information, see github.com .\nCode scanning and tests are run by a pipeline such as Jenkins, Tekton on Red Hat OpenShift \nPipelines, GitHub Actions. Also, see 4.7, “S ecure supply chain with SLSA” on page 95. You \ncan use the pipeline model to run various checks on source code that is committed to a repository. These checks are typi cally run against the source co de directly, but can also be \nrun during the build steps, test cases, or ephemeral automatic test deployments. Not all checks that are run need to be blocking or critical . It is also possible to  run checks that are not \npreventing integration of the code but are rais ing only informa",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 209,
            "total_chunks": 337
          }
        },
        {
          "id": "4833275d-f27c-46b2-be67-9ce4b1097142",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ssible to  run checks that are not \npreventing integration of the code but are rais ing only informational findings. To distinguish \nthis, the repository is configured with the required status checks. For more information, see About protected branches .\nThese repository settings can also be used to force commit signatures to allow \ncryptographically secured tracking of the code authorship. Other source-code-hosting facilities typically provid e similar functionality.\n4.1.2  Test\nThe functionality is typically secured by tests on multiple test layers. While in development, \nthe code is locally tested by using unit tests. These unit tests are designed to be portable and quick to run, and are often implemented against mocked services and static test data.\nAfter a push into the source-code management, more complicated tests can be run. These \ntests can include integration tests against ot her services that are deployed within a test \ndeployment.\nDepending on the stage in the application li",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 210,
            "total_chunks": 337
          }
        },
        {
          "id": "bbcb4422-1b3b-4fd7-9a34-044dab871039",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "r services that are deployed within a test \ndeployment.\nDepending on the stage in the application lifecycle, the application is then either promoted for \nintegration, acceptance testing, or for production deployment.\nThe steps that follow are the same for all cases and differ by only the environment in which \nthe application is deployed.\n4.1.3  Build\nDepending on the specific implementation of the development pipeline and application, the developer can choose to push the developed and tested code forward into the integration test. To be able to run the application within a secured environment, it needs to be built into either a custom image or into a container to be started within the HPCR image.\nWith Hyper Protect Secure Build (HPSB), you can build a trusted container image within a \nsecure enclave that is provided by an HPVS instance. The enclave is highly isolated, where \ndevelopers can access the container only by using a specific hardened API and the cloud administrator cannot acc",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 211,
            "total_chunks": 337
          }
        },
        {
          "id": "2b11fcab-9176-40ee-94e9-94372c403015",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "an access the container only by using a specific hardened API and the cloud administrator cannot access the contents of the container. Therefore, the image that is built can be highly trusted. Specifically, the build server cryptographically signs the image and a \nmanifest. The manifest is a collection of materials that are used during the build and can be used for audits. Because the enclave protects the signing keys within the enclave, the signatures can be used to verify whether the image and manifest are from the build server and not from elsewhere.\n\n68 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentFor more details on configur ing and using HPSB in IBM Cloud  VPC and IBM LinuxONE or \nIBM Z, see Configuring and using Hyper Protect Secure Build in Hyper Protect Virtual \nServers for VPC and Building your applications wit h Hyper Protect Secure Build\n4.1.4  Release\nReleases can be targeted to internal or exte rnal deployment for different purposes ranging \nf",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 212,
            "total_chunks": 337
          }
        },
        {
          "id": "3fdc4643-d2c6-4dd7-ac0c-158730d17f4b",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "lease\nReleases can be targeted to internal or exte rnal deployment for different purposes ranging \nfrom integration testing to production use. \nAlong with the published container images the Workload Provider persona publishes the \nencrypted workload section of the contract.\nFor detailed information on the contract creation see 3.1, “The contract” on page 38 and IBM \nCloud VPC: The workload section and IBM Hyper Protect Virtual Servers: About the contract .\nThe workload section of the contract defines wh ether there is a single container through the \ncompose subsection or multiple containe rs though the play subsection. For the \nSamplePaymentSystem application, we configure three containers in the play subsection. For \nmore information, see hyperprotect/redbook-samples/sf248555 .\nThe workload section of the contract can op tionally contain the references and login \ncredentials to the container registry and required volumes. Because it is encrypted by the contract encryption key of the w",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 213,
            "total_chunks": 337
          }
        },
        {
          "id": "0fc966cc-10cf-41b4-bd06-bbd1e4721965",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ainer registry and required volumes. Because it is encrypted by the contract encryption key of the workload image, the workload section is not readable by the Workload Deployer persona or anyone else, such as a system administrator. \n4.1.5  Deployment\nTo deploy the application, the Workload Deployer persona complements the workload section of the contract with the env section as needed for the current scenario.\nFor detailed information on the contract creation see 3.1.3, “The env section” on page 47  or \nIBM Cloud: The env section and IBM Hyper Protect Virtual Servers: The env section .\n4.1.6  Update\nUpdating a service that is  deployed on a Hyper Prot ect Virtual Server (HPVS)1 can affect \nmultiple components. When a new Hyper Protect Container Runtime (HPCR) image is released, it is recommended to update to the new version. New revisions of the images are typically released every four to six weeks and contain fixes and new features.\nFurthermore, the containers and services running wi",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 214,
            "total_chunks": 337
          }
        },
        {
          "id": "5ff4817b-aefb-4564-8aeb-1fb02066e898",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "to six weeks and contain fixes and new features.\nFurthermore, the containers and services running within the runtime might need to be \nupdated for the same reasons of applying fixes and new features.\nBecause the HPCR follows a container lifecycle, an update always requires a restart of the \nHPVS instance and is thus disr uptive for the node. In cases where high availability (HA) \ncapabilities are required for the overall service, it is ne cessary to stagge r the rollout and Note:  The contract should always be encrypted with an encryption certificate to protect all \nsensitive information. See 3.2, “Contract encryption” on page 49 for details.\nFor illustration purposes only, we show unencrypted sample co de throughout this chapter.\n1  An HPVS instance is also referred to as a Virtual Server Instance (VSI) as part of a Vi rtual Private Cloud within IBM \nCloud. \n\nChapter 4. Application developme nt in a trusted environment 69update of each HPVS so that the overall service re mains availab",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 215,
            "total_chunks": 337
          }
        },
        {
          "id": "e0737c1f-bea9-4a3d-a081-fb9d6211e7bc",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "lopme nt in a trusted environment 69update of each HPVS so that the overall service re mains available duri ng the update. All \nupdates are triggered by updates to the contract.\nUpdating the HPCR image\nWhen a new image is available within the cloud or on-premises, it can be selected when starting the HPVS. When a differ ent image is selected, it might be necessary to re-encrypt the \ncontract with HPCR-Contract-E ncryption-Public-Key. When working with a separate Workload \nProvider, they must provide the encrypted workload section first. \nTo build a complete contract for HPVS on IBM LinuxONE or IBM Z, see Downloading the IBM \nHyper Protect Container Runtime image\nTo build a complete contract for HPVS for VPC, the crea tion can be done by using the \nIBM Cloud virtual server for VPC User Interface. See Virtual server for VPC . Also, refer to \nAppendix B, “Creating a Hyper Protect Virtual Server for VPC” on page 107 for the corresponding steps.\nUpdating service  containers\nWhen new contain",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 216,
            "total_chunks": 337
          }
        },
        {
          "id": "e545401b-cd74-410d-b1a7-d8b0e3be66db",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "rver for VPC” on page 107 for the corresponding steps.\nUpdating service  containers\nWhen new container versions for the service are available, the H PVS instance must be \nrestarted to pull the image and run the new service version.\nWhen a floating tag with container signing is used to define the container versions, the \nchange might not be visible on the contract, and is in effect only at runtime. Otherwise, the digest of the images needs to be updated.\nDepending on the workload, the composition of containers might change.\n4.1.7  Application an d service development\nWithin the context of the zero-trust architecture, each service must be implemented with \nsecurity in mind. This is true for services co mmunicating over the public internet and within \nthe seemingly protected intranet.\nTo implement services following this posture, it  is useful to consider  each microservice or \nservice to be always under attack. Because of this, no connection should be trusted. Each \nconnection must be pr",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 217,
            "total_chunks": 337
          }
        },
        {
          "id": "7a347d4b-2e75-44c6-98f1-e10076619a97",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "e always under attack. Because of this, no connection should be trusted. Each \nconnection must be protected and verified.\nFurthermore, the development team must be prepared to fix and redeploy any service rapidly.\nAs previously outlined, there exist multiple attack vectors on any service that make it \nnecessary to update the service code itself or its dependencies. See 1.1, “Identifying the threat” on page 2. To be able to develop, test, and deploy service updates, various best practices can be followed.\nA good starting point is the 12-factor app. For more information, see The Twelve-Factor App . \nDeveloping services this way co ntributes to testability, portab ility, scalability, and security.\n4.1.8  Working with the log\nThe log of the HPVS image does contain information th at originated from  multiple sources \nwithin the HPVS.\nUpon connection to the log server, the boot log is replayed. The first item that is replayed is \nthe kernel startup. After the in itial kernel messages,  the r",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 218,
            "total_chunks": 337
          }
        },
        {
          "id": "02a52831-a43a-443f-9eb5-3a32d950cdc3",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ". The first item that is replayed is \nthe kernel startup. After the in itial kernel messages,  the rest of the HPVS boot process is \n\n70 Applying Data Protection and Confidentiality in a Hybrid Cloud Environmentvisible. The boot and HPVS configuration is internally orchestrated using systemd. After the \ncontract validation and mounting of eventual data volumes the log shows the start of the workload containers.\nDifferent boot stages record their states into the log for audit purposes. Notable log tokens \nhave the format HPL[0-9]{5}[IE ]. Other tokens are logged during the boot and deployment \nprocess of an HPVS instance.\nFor example, a successful startup of the instance logs the special token HPL10001I. Se e \nExample 4-1.\nExample 4-1   Successful startup example\nhpcr-dnslookup: HPL14000I: Network connectivity check completed successfully.hpcr-logging: HPL01010I: Logging has been setup successfully.hpcr-disk-mount: HPL07003I: Mounting volumes donehpcr-container-play: HPL15004I: The pod ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 219,
            "total_chunks": 337
          }
        },
        {
          "id": "ecf08aa8-bedb-4894-9b69-4396249e7c66",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ccessfully.hpcr-disk-mount: HPL07003I: Mounting volumes donehpcr-container-play: HPL15004I: The pod started successfully.verify-disk-encryption: HPL13000I: Verify LUKS Encryptionverify-disk-encryption: HPL13003I: Checked for mount point /, LUKS encryption with 1 key slot foundverify-disk-encryption: HPL13001I: Boot volume and all the mounted data volumes are encryptedhpcr-catch-success: HPL10001I: Services succeeded -> systemd triggered hpl-catch-success service\nA failure to start the containers that are defined within the contract logs the special token \nHPL10000E  and schedule a shutdown of the HPVS. See Example 4-2.\nExample 4-2   Failed startup example\nhpcr-logging: HPL01010I: Logging has been setup successfully.\nhpcr-disk-mount: HPL07003I: Mounting volumes done hpcr-catch-failure: VSI has failed to start!hpcr-catch-failure: HPL10000E: One or more service failed -> systemd triggered hpl-catch-failed servicehpcr-catch-failure: Shutdown scheduled, use 'shutdown -c' to cancel.systemd: ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 220,
            "total_chunks": 337
          }
        },
        {
          "id": "3edf4350-0085-4b79-b30c-73f098635ad5",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "pl-catch-failed servicehpcr-catch-failure: Shutdown scheduled, use 'shutdown -c' to cancel.systemd: Finished Trigger Catch failed service and shutdown.\nThe workload containers are expected to log to stdout and stderr. These outputs are gathered \ninto the journal of the HPVS. Fr om the journal,  the messages are forwar ded to the configured \nlog server. \nLog configuration\nThe forwarding of the log to a remote server is mandatory and is configured within the env section of the contract. For details on the log configuration see 3.5, “Logging for HPVS instances” on page 56. \nA single logDNA instan ce can be used to capt ure the logs of mult iple HPVSs and other \nservers.\n4.1.9  Deployment automation - Terraform\nTo easily deploy and ma nage applications bas ed on HPVS, a certain deg ree of automation is \nencouraged. The primary mode to drive the deployment and configuration of cloud infrastructure and workload is Terraform. For more information, see Terraform .\n\nChapter 4. Application devel",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 221,
            "total_chunks": 337
          }
        },
        {
          "id": "ec6c2a64-38e6-4bb5-9836-2a5743811f83",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "cture and workload is Terraform. For more information, see Terraform .\n\nChapter 4. Application developme nt in a trusted environment 71The IBM-Cloud/ibm Terraform plug-in is required to manage all deployments on IBM Cloud. \nIBM provides a separate ibm-hyper-protect/hpcr Terraform plugin specific ally to simplify the \nhandling of the HPVS contract.\nFor more information, see the following websites:\n/SM590000Sample Terraform te mplates for IBM Cloud\n/SM590000Terraform samples for Hyper Protect Virtual Servers for VPC\n/SM590000ibm-hyper-protect/linuxone-vsi-automation-samples  \n4.2  Build container image by using Hyper Protect Secure Build\nYou can build a trusted container image within a secure enclave that is provided by an IBM \nHPVS. See Figure 4-2. The enclave is highly is olated. Developers ca n access the secure \nbuild server by using a specific API, and the administrator cannot access the contents of the secure build server. Therefore, the image that is built can be highly trusted. S",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 222,
            "total_chunks": 337
          }
        },
        {
          "id": "1b83b015-0e4a-4a07-88a6-c6674df5e6c5",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "the contents of the secure build server. Therefore, the image that is built can be highly trusted. Specifically, the build server cryptographically signs the image, and a manifest. The manifest, a collection of materials that are used during the build, is fo r audit purposes. Because the enclave protects \nthe signing keys within the enclave, the signatures can be used to verify that the image and manifest are from the secure build server.\nFigure 4-2   Trusted container image\nTo securely build container images, follow the process in the subsequent sections:\n/SM590000“Determine readiness” on page 71\n/SM590000“Install the secure build CLI” on page 72\n/SM590000“Create client and server certificates for secure build” on page 72\n/SM590000“Prepare user_data.yaml” on page 73\n/SM590000“Create the Hyper Protect Secure Build instance” on page 74\n/SM590000“Configure the HPSB client with t he HPVS IP address” on page 75\n4.2.1  Determine readiness\nEnsure that you meet the following hardware or softw",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 223,
            "total_chunks": 337
          }
        },
        {
          "id": "8fd58b3e-35be-4b7c-9048-904af4361641",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " address” on page 75\n4.2.1  Determine readiness\nEnsure that you meet the following hardware or software prerequisites: \n\n\n72 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment/SM590000Linux management server from where you can run the build CLI tool \n/SM590000Recommended 2 CPUs/4 GB memory or more\n/SM590000KVM hosts in IBM Secure Execution mode are supported by these distributions:\n– Red Hat Enterprise Linux 9.0 with service– Red Hat Enterprise Linux 8.4 with service– SUSE Linux Enterprise Serv er 15 SP3 with service\n– Ubuntu Server 20.04 LTS with service\n/SM590000KVM guests in IBM Secure Execution mode  are supported by these distributions:\n– Red Hat Enterprise Linux 9.0 with service– Red Hat Enterprise Linux 8.4 with service– Red Hat Enterprise Linux 7.9 with service– SUSE Linux Enterprise Serv er 15 SP3 with service\n– SUSE Linux Enterprise Serv er 12 SP5 with service\n– Ubuntu Server 20.04 LTS with service\n/SM590000Attestation is available on IBM z16™ and IBM ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 224,
            "total_chunks": 337
          }
        },
        {
          "id": "5fee0b98-d7c7-453a-b092-8f82ca8f68c6",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ervice\n– Ubuntu Server 20.04 LTS with service\n/SM590000Attestation is available on IBM z16™ and IBM LinuxONE 4 by these distributions:\n– Red Hat Enterprise Linux 9.1 with service– Red Hat Enterprise Linux 8.7 with service\n/SM590000Python 3.8 (Python 2.x is not supported) \n/SM590000Access to GitHub, for hosting the source code\n/SM590000Dockerfile (everything that you need to build your container image)\n/SM590000Access to IBM Cloud Registry\n/SM590000(Optional) Access to IBM Cloud Object Storage (COS) Service\n/SM590000Access to IBM HPVS for VPC\n/SM590000Get the encrypted workload section of the contract file of Secure Build from Configuring \nand using Hyper Protect Secure Build in Hyper Protect virtual server for VPC\n4.2.2  Install the secure build CLI\nUse the following steps to install the se cure build command line interface (CLI):\n1. On the client machine where Linux is installed, verify the OS version. See Example 4-3.\nExample 4-3   Verify the OS version\n$ cat /etc/os-release | grep 2",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 225,
            "total_chunks": 337
          }
        },
        {
          "id": "2fd416dd-31a9-4d47-b602-f72a6fb7f338",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " the OS version. See Example 4-3.\nExample 4-3   Verify the OS version\n$ cat /etc/os-release | grep 20.04 \nVERSION=\"20.04.6 LTS (Focal Fossa)\" PRETTY_NAME=\"Ubuntu 20.04.6 LTS\" VERSION_ID=\"20.04\" \n2. Download the secure build CLI code to the client machine where linux is installed. See \nExample 4-4.\nExample 4-4   Download secure build CLI\n$ git clone git@github.com:ibm-hyper-protect/secure-build-cli.git Cloning into 'secure-build-cli'... \n4.2.3  Create client and serve r certificates for secure build \nUse the following steps to create certificates  for the secure build CLI and secure build \ncommunication:\n1. Create a client cert ificate for the secure build CLI a nd secure build communication. See \nExample 4-5 on page 73.\n\nChapter 4. Application developme nt in a trusted environment 73Example 4-5   Create client certificate \n$ ./build.py create-client-cert --env sbs-config.json \nINFO:__main__:parameter file sbs-config.json renamed to sbs-config.json.2023-07-21_14-35-53.197898 INFO:root:c",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 226,
            "total_chunks": 337
          }
        },
        {
          "id": "5863f953-ab95-4238-ae39-2d71acdb251d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "n__:parameter file sbs-config.json renamed to sbs-config.json.2023-07-21_14-35-53.197898 INFO:root:client_certificate: generating client CA and certificate \n2. Create a server certificate for the secure build CLI and secure bu ild communication. See \nExample 4-6.\nExample 4-6   Create server certificate \n$ ./build.py create-server-cert --env sbs-config.json INFO:root:server_certificate: using supplied pem files cert_directory=.HPSBContainer-2e6ce21c-4c05-4101-abea-564f3adbd8e4 capath=./.HPSBContainer-2e6ce21c-4c05-4101-abea-564f3adbd8e4.d/client-ca.pem cakeypath=./.HPSBContainer-2e6ce21c-4c05-4101-abea-564f3adbd8e4.d/client-ca-key.pem INFO:root:server_certificate: Generating server certificate INFO:root:server_certificate: Successfully generated server CSR INFO:root:server_certificate: Successfully generated server certificate \n3. Get the environment key value pair to be used in instance-create command. See \nExample 4-7.\nExample 4-7   Get environment key value pair \n$ ./build.py instanc",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 227,
            "total_chunks": 337
          }
        },
        {
          "id": "353e9842-a283-4471-8bdf-8619b61e63e4",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "create command. See \nExample 4-7.\nExample 4-7   Get environment key value pair \n$ ./build.py instance-env --env sbs-config.json INFO:root:client_certificate: using supplied pem files client_crt_key=.HPSBContainer-2e6ce21c-4c05-4101-abea-564f3adbd8e4 capath=./.HPSBContainer-2e6ce21c-4c05-4101-abea-564f3adbd8e4.d/client-ca.pem cakeypath=./.HPSBContainer-2e6ce21c-4c05-4101-abea-564f3adbd8e4.d/client-ca-key.pem WARNING:gnupg:gpg returned a non-zero error code: 2 INFO:__main__: \n****** Copy below environment variables and use in env contract as environment \nvariables. ******  \nCLIENT_CRT: \"LS0tLS1CRUdJTiBD……RCBDRVJUSUZJQ0FURS0tLS0tCg==\" \nCLIENT_CA: \"LS0tLS1CRUdJTiB……5EIENFUlRJRklDQVRFLS0tLS0K\" SERVER_CRT: \"LS0tLS1CRUdJTiBDRVJUSUZJ……ZCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\" SERVER_KEY: \"LS0tLS1CRUdJTiBQR1Ag……U1NBR0UtLS0tLQo=\" \n4.2.4  Prepare user_data.yaml\nUse the following steps to prepare the encrypted env section of the contract:\n1. Compose a plain text contract. See Example 4-8.\nExample 4-8",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 228,
            "total_chunks": 337
          }
        },
        {
          "id": "12b48867-11af-4267-a0cb-052d402da82d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ncrypted env section of the contract:\n1. Compose a plain text contract. See Example 4-8.\nExample 4-8   Plain text contract \nenv: | \n  type: env   logging:     logDNA:       hostname: syslog-a.us-***.ibm.com \n\n74 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment      ingestionKey: ********** \n      port: 6514   volumes:     hpsb:       seed: \"******\"   env:     CLIENT_CRT: LS0tLS1CRUdJTiBD……RCBDRVJUSUZJQ0FURS0tLS0tCg==     CLIENT_CA: LS0tLS1CRUdJTiB……5EIENFUlRJRklDQVRFLS0tLS0K     SERVER_CRT: LS0tLS1CRUdJTiBDRVJUSUZJ……ZCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K     SERVER_KEY: LS0tLS1CRUdJTiBQR1Ag……U1NBR0UtLS0tLQo= \n2. Encrypt the env section of the contract (see Example 4-9).\nExample 4-9   Encrypted env section \nenv: hyper-protect-basic.ItMcZ+CaxWp4YbUMs2eVF6o7hiaRDMhgwWPwaTWChg2a/. \n/7cwKUEthQ1ww=\n3. Get the encrypted secure build workload of the contract. See Example 4-10.\nExample 4-10   Encrypted workload section\nworkload: \nhyper-protect-basic.JNNGRfeic/H4j5XcOLMM",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 229,
            "total_chunks": 337
          }
        },
        {
          "id": "fcc591f0-778c-4f3a-9c07-5bd1a0382059",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " 4-10.\nExample 4-10   Encrypted workload section\nworkload: \nhyper-protect-basic.JNNGRfeic/H4j5XcOLMMCA2KrAUpUu5+gcO5……ax4E2mCzHZMUuHVk3U \n4. Add the encrypted content from the env and workload sections of the contract to prepare \nthe user_data.yaml. See Example 4-11.\nExample 4-11   Combined encrypted env and workload sections\nenv: hyper-protect-basic.ItMcZ+CaxWp4YbUMs2eVF6o7hiaRDMhgwWPwaTWChg2a/. \n/7cwKUEthQ1ww= workload: hyper-protect-basic.JNNGRfeic/H4j5XcOLMMCA2KrAUpUu5+gcO5……ax4E2mCzHZMUuHVk3U \n4.2.5  Create the Hyper Prot ect Secure Build instance \nThe HPSB instance can run in an HPVS for VPC instance and in HPVS on IBM LinuxONE or \nIBM Z running in a KVM host LPAR: \n/SM590000To create an HPVS for VPC instance for the HPSB using IBM Cloud VPC UI, see \nAppendix B, “Creating a Hyper Protect Virtual Server for VPC” on page 107 for a setup example.\nAlso see Creating a Hyper Protect Virtual Server for VPC instance . \nA virtual private cloud (VPC) can be created in  IBM Cloud by followi",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 230,
            "total_chunks": 337
          }
        },
        {
          "id": "b27f14fc-dd60-42bb-97e9-581e15b1d576",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ual Server for VPC instance . \nA virtual private cloud (VPC) can be created in  IBM Cloud by following the instructions in \nCreating and configuring a VPC . \n/SM590000To create the HPVS in a KVM host L PAR, follow the in structions in Example of bringing up \nIBM Hyper Protect Virtual Servers on a KVM host by using the virsh utility . You must \nprovide the combined env and workload section obtained from the previous step, and use them as the content of the user-data file for deployment.\nTo configure your  HPSB instance see Bringing up the Hyper Protect Secure Build on the \nKVM LPAR .\n\nChapter 4. Application developme nt in a trusted environment 754.2.6  Configure the HPSB client with the HPVS IP address\nUse the following steps to configure the HPSB client with the HPVS IP address in  \n/etc/hosts :\n1. Ensure the floating IP addr ess of the HPSB server is ma pped to the hostname in the \n/etc/hosts  file, which is given during the certificate creation. See Example 4-12.\nExample 4-12   Veri",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 231,
            "total_chunks": 337
          }
        },
        {
          "id": "2393afc7-94d4-409a-ad77-e1d159ee31ad",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "c/hosts  file, which is given during the certificate creation. See Example 4-12.\nExample 4-12   Verify the floating IP address\n$ cat sbs-config.json | grep HOSTNAME \n\"HOSTNAME\": \"test.xyz.com\", \n$ cat /etc/hosts | grep test.xyz.com  \n150.239.221.33 test.xyz.com \n2. Check that the HPSB client and HPSB server  are able to communicate. See \nExample 4-13. \nExample 4-13   Verify communications\n$ ./build.py status --env sbs-config.json INFO:__main__:status: response={     \"status\": \"\" }\n3. Update the  sbs-config.json  with the GitHub repo where the source code and dockerfile \nis present. Include the registry details for where the built container image needs to be pushed. See Example 4-14. \nExample 4-14   HPSB configuration details\n$ cat sbs-config.json {    \"HOSTNAME\": \"test.xyz.com\",     \"CICD_PORT\": \"443\",     \"IMAGE_TAG\": \"\",     \"CONTAINER_NAME\": \"HPSBContainer\",     \"GITHUB_KEY_FILE\": \"~/.ssh/id_rsa\",     \"GITHUB_URL\": \"https://github.com/ibm-hyper-protect/paynow-website\",     \"GITHUB_B",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 232,
            "total_chunks": 337
          }
        },
        {
          "id": "cef5fedf-f3d1-4f5e-9ab6-760c7ba71d9d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ".ssh/id_rsa\",     \"GITHUB_URL\": \"https://github.com/ibm-hyper-protect/paynow-website\",     \"GITHUB_BRANCH\": \"main\",     \"DOCKER_REPO\": \"devuser/samplepaymentsystem\",     \"DOCKER_USER\": \"devuser\",     \"DOCKER_PASSWORD\": \"*******\",     \"IMAGE_TAG_PREFIX\": \"v3\",     \"DOCKER_CONTENT_TRUST_BASE\": \"False\",     \"DOCKER_CONTENT_TRUST_BASE_SERVER\": \"\",     \"DOCKER_RO_USER\": \"devuser\",    \"DOCKER_RO_PASSWORD\": \"******\",     \"RUNTIME_TYPE\": \"vpc\"} \n4. Initialize the HPSB server with configuration. See Example 4-15.\nExample 4-15   Initialize the HPSB server\n$ ./build.py init --env sbs-config.json \nINFO:__main__:init: response={    \"status\": \"OK\" \n\n76 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment}\n5. Initiate the build  for the HPSB server. See Example 4-16.\nExample 4-16   Build for the HPSB server\n$ ./build.py build --env sbs-config.json \nINFO:__main__:build: response={     \"status\": \"OK: async build started\" } \n6. Check the progress stat us of the HPSB server build. S",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 233,
            "total_chunks": 337
          }
        },
        {
          "id": "77dcabe8-142a-49c3-8f64-df326494c62e",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "    \"status\": \"OK: async build started\" } \n6. Check the progress stat us of the HPSB server build. See Example 4-17.\nExample 4-17   Check the status of the HPSB server build\n$ ./build.py status --env sbs-config.json \nINFO:__main__:status: response={    \"build_image_tag\": \"1.3.0.11\",    \"build_name\": \"\",    \"image_tag\": \"\",    \"manifest_key_gen\": \"\",    \"manifest_public_key\": \"\",    \"status\": \"github cloned\"} \n7. To check the final status of the build, run the command shown in Example 4-18.\nExample 4-18   Check the status after the HPSB build\n$ ./build.py status --env sbs-config.json\nINFO:__main__:status: response={    \"build_image_tag\": \"1.3.0.11\",    \"build_name\": \"docker.io.devuser.samplepaymentsystem.v3-f29b1ab.2023-07-25_09-01-40.401144\",     \"image_tag\": \"v3-f29b1ab\",    \"manifest_key_gen\": \"soft_crypto\",    \"manifest_public_key\": \"manifest.docker.io.devuser.samplepaymentsystem.v3-f29b1ab.2023-07-25_09-01-40.401144-public.pem\",    \"status\": \"success\"}\n8. Run the command in Example",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 234,
            "total_chunks": 337
          }
        },
        {
          "id": "9b460a8e-d43f-47e2-8752-78d7d99bd96f",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "29b1ab.2023-07-25_09-01-40.401144-public.pem\",    \"status\": \"success\"}\n8. Run the command in Example 4-19 to see the build logs. \nExample 4-19   HPSB logs\n$ ./build.py log --log build --env sbs-config.json\nINFO:__main__:2023-07-25 08:59:36,446  build_task               INFO    starting a build...\nThe full build log can be found in the Example C-1 on page 116.\n9. Get digest for the built image. See Example 4-20.\nExample 4-20   Digest for the build image\n$ ./build.py get-digest --env sbs-config.json \n\nChapter 4. Application developme nt in a trusted environment 77Digest value of the built image: \ndocker.io/devuser/samplepaymentsystem@sha256:d10e26e72a2f83a3fdf8a6a79da5b88f1b6747ce0af9309749afc55295973bd8 \n10.To get the signed public key, use the command in Example 4-21.\nExample 4-21   Signed public key\n$ ./build.py get-signed-image-publickey --env sbs-config.json\nINFO:__main__:Downloaded signed image public key to file docker.io-devuser-samplepaymentsystem-public.key \n$ cat docker.io-dev",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 235,
            "total_chunks": 337
          }
        },
        {
          "id": "42976658-b996-43d6-b859-47424ec52f9f",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "igned image public key to file docker.io-devuser-samplepaymentsystem-public.key \n$ cat docker.io-devuser-samplepaymentsystem-public.key\nLS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJsVENDQVR1Z0F3SUJBZ0lRV2RmQStQcm9tVjRwd2RVS01KYnFqREFLQmdncWhrak9QUVFEQWpBeE1TOHcKTFFZRFZRUURFeVprYjJOclpYSXVhVzh2WVdKb2FYSmhiV3N2YzJGdGNHeGxjR0Y1YldWdWRITjVjM1JsYlRBZQpGdzB5TXpBM01qVXdPVEF4TXpoYUZ3MHpNekEzTWpJd09UQXhNemhhTURFeEx6QXRCZ05WQkFNVEptUnZZMnRsCmNpNXBieTloWW1ocGNtRnRheTl6WVcxd2JHVndZWGx0Wlc1MGMzbHpkR1Z0TUZrd0V3WUhLb1pJemowQ0FRWUkKS29aSXpqMERBUWNEUWdBRWplMmFPYTVPYUE4UHJFUThHNTgybTZxWmlJaEFvYWo2bDZaaThsaDFwM01VbTBLNAp4TVBJcytZNHA2TzVzeE9tRFpFaW9SbmJOeU1NRDJrN05zNXVLYU0xTURNd0RnWURWUjBQQVFIL0JBUURBZ1dnCk1CTUdBMVVkSlFRTU1Bb0dDQ3NHQVFVRkJ3TURNQXdHQTFVZEV3RUIvd1FDTUFBd0NnWUlLb1pJemowRUF3SUQKU0FBd1JRSWdZUnZObW1nRkg1dTBSNnlENUhxcDFTcW9zM2k5cVczbWxRWVlJN2oyZXJVQ0lRRFZGNjhXbXo0RQplUXExeVJvaHIwZXpZck52OEh4eXQvUS9CMDBlQUY1NzlRPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=>  \n11.Get the manifest files, which consist of ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 236,
            "total_chunks": 337
          }
        },
        {
          "id": "09ef97ac-24cc-4a96-85b4-566a324763ff",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "MDBlQUY1NzlRPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=>  \n11.Get the manifest files, which consist of files needed for auditor to check on the build. See \nExample 4-22.\nExample 4-22   Manifest files\n$ ./build.py get-manifest --env sbs-config.jsonINFO:__main__:get-manifest manifest_name: manifest.docker.io.devuser.samplepaymentsystem.v3-f29b1ab.2023-07-25_09-01-40.401144 \n12.Use the command in Example 4-23 to verify the manifest files.\nExample 4-23   Verifying the manifest files\n$ ./build.py get-manifest --env sbs-config.json  --verify-manifest\nINFO:__main__:get-manifest manifest_name: manifest.docker.io.devuser.samplepaymentsystem.v3-f29b1ab.2023-07-25_09-01-40.401144INFO:__main__:verify_manifest: manifest_name=manifest.docker.io.devuser.samplepaymentsystem.v3-f29b1ab.2023-07-25_09-01-40.401144 test=0INFO:__main__:verify=OK\n13.Get the state image, which c an be used later to bring up the HPSB instance with the same \nset of GitHub and registry configuration. It is important to safely stor",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 237,
            "total_chunks": 337
          }
        },
        {
          "id": "246e27e5-eec7-45f8-8332-c9e721810886",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "PSB instance with the same \nset of GitHub and registry configuration. It is important to safely store this state image. This image consists of keys that are needed to push the image into the registry. \nExample 4-24   Get state image\n$ ./build.py get-state-image --env sbs-config.jsonINFO:__main__:state:name: docker.io.devuser.samplepaymentsystem.v3-f29b1ab.2023-07-25_09-01-40.401144\n\n78 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment4.3  Zero knowledge proofs: TLS server certificates and \nwrapped secrets\nWithin the context of the zero knowledge architecture, the communication between different \ncomponents requires authentication of the components against each other. These authentications depend on secrets that are generated by the components and depend on certificates that are used to exchange PINs and secret keys.\nThis section explains how secrets can be generated safely and securely, how secret keys and \ncertificates can be injected into components, and how ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 238,
            "total_chunks": 337
          }
        },
        {
          "id": "5b3b5ee6-8767-4b91-9f88-6fed166e5c40",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ted safely and securely, how secret keys and \ncertificates can be injected into components, and how some common communication components, such as reverse proxies, can be configured and hardened against attacks.\n4.3.1  Passing secrets  into a secure HPVS\nTo establish verified and trusted connections to other parts of the IT landscape, it is required \nto pass information to HPVS instances. The mechanism used to pass the information must be checked for integrity and confidentiality. \nWith such a unidirectional communication ch annel, it is possible to distribute shared \ninformation and even shared secrets. The following list includes examples of shared secrets:\n/SM590000CA and server certificates to establish c onnections to only trusted servers or peers\n/SM590000Secret keys used for identification of an instance\n/SM590000Credentials used to log in to other services\nIn the HPVS instance this unidirectional comm unication channel is provided by the contract. \nFor more information, see IBM ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 239,
            "total_chunks": 337
          }
        },
        {
          "id": "3f636c7e-332b-4c54-a964-a42c899cfbf9",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "s unidirectional comm unication channel is provided by the contract. \nFor more information, see IBM Cloud: About the contract .\n4.3.2  Certificate benefits\nCertificates are generally used to support the di stribution of public keys. Embedding a public \nkey within a certificate does provide three main benefits:\n1. The certificate is signed by a certificate au thority called issuer, relaying trust to the \ncontained public key. This allo ws building a verifiable key hier archy of which only the public \nroot key must be distributed. \n2. The validity of the certificate can be defined. This is done by definition of a validity time \nframe and the option to embed revocation information that can be checked additionally. \n3. Each certificate can restrict the intended usage of the public key.\nFor a complete definition, see Internet X.509 Public Key Infrastructure Certificate and \nCertificate Revocation  List (CRL) Profile .\nWithin the context of se curing a container running in HPVS, certificates",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 240,
            "total_chunks": 337
          }
        },
        {
          "id": "d5741998-f65e-4cc1-ab38-56253c3bf775",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "tion  List (CRL) Profile .\nWithin the context of se curing a container running in HPVS, certificates are useful in many \nways. Certificates are used to simplify the verification of keys that are used to encrypt and sign the contract and to allow attestation of the started virtual server instances.\nCertificates can be used by the Workload Prov ider and the Workload Deployer to introduce \ntrusted keys within the contract. These can also be used to extend the trust into keys introduced within the workload. This is either being done by introducing an owned certificate authority (CA) or preparing a new CA for this pu rpose. It is also possible to generate and use \na key within HPCS for the CA.\n\nChapter 4. Application developme nt in a trusted environment 79For example guidance on manual generation of self-signed certificates, see Generate root CA \nkey and certificate .\n4.3.3  Importing server certificate from contract\nA common function of  an HPVS is to provide a communica tion end point in",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 241,
            "total_chunks": 337
          }
        },
        {
          "id": "f985dc8e-2331-4091-93fd-60c3cdb81f01",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " certificate from contract\nA common function of  an HPVS is to provide a communica tion end point in the form of a \nsecure web server. Apart from the recommended hardening of the web server configuration, configure the server to use TLS / HTTPS. For more information, see SSL/TLS Best Practices \nfor 2023 .\nThe simplest way to achieve this as the Workload Provider is to add the required files to the \nworkload directly. This has the downside that a ll instances of the workload would be using the \nsame secret. A better option is to let the Workload Deployer supply the secrets and dependent information for each instance. In the current example, the Workload Deployer pre-generates a key and a certificate for the server to use. Certificate and key are then set as environment variables within the compose or play file.\nIt is a best practice to encode keys and certificates in Base64 when handing them through the \ncontract. How this is introduced within the container differs from the composition ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 242,
            "total_chunks": 337
          }
        },
        {
          "id": "240ee861-a8e5-4add-a2f5-bf9dac68858a",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "hem through the \ncontract. How this is introduced within the container differs from the composition method that is used to orchestrate the containers.\nSecret environment definition in compose\nWhen you use the compose option to define th e workload, the environment attribute can be \ndefined to declare variables. For more information, see docker docs: Use the environment \nattribute . \nThe value of the variables can then be deferred to the env section. See Example 4-25.\nExample 4-25   Defer vari ables to the env section\n# set by the workload provider in compose\nenvironment:  HTTPS_CERT: \"${HTTPS_CERT_VALUE}\"  HTTPS_KEY: \"${HTTPS_KEY_VALUE}\"\nSecret environment definiti on in play with templates\nWhen using the play option to define the workload, the env attribute can be defined to declare \nvariables. For more information, see kubernetes: Define an environment variable for a \ncontainer .\nThe value of the variables can then be deferred to the env section using templates. See \nExample 4-26.\nEx",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 243,
            "total_chunks": 337
          }
        },
        {
          "id": "8f6d1826-78f9-4cb7-b6b1-7856f506abd0",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "alue of the variables can then be deferred to the env section using templates. See \nExample 4-26.\nExample 4-26   Defer variables to the env section using templates\nworkload: |  type: workload   play:     templates:     - apiVersion: v1 ...       env:       - name: HTTPS_CERT         value: {{ .Env.HTTPS_CERT_VALUE }}       - name: HTTPS_KEY \n\n80 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment        value: {{ .Env.HTTPS_KEY_VALUE }}\nSecret environment defin ition using config map\nIn simpler workload setups, it might be feasible to not use a template. Instead, you can set all \nvariables from the env section as environment variables of a container by using the special content map file, contract.config.map. In the contract.config.map file , the Workload \nDeployer adds secrets in the env section of the contract. See Example 4-27.\nExample 4-27   Variables settings in the env section\n# set by deployer in env sectionenv: |  type: env    HTTPS_CERT_VALUE: LS0tLS1CRUd",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 244,
            "total_chunks": 337
          }
        },
        {
          "id": "87f6195f-76d1-4df9-ac2f-c539d25eb239",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "n the env section\n# set by deployer in env sectionenv: |  type: env    HTTPS_CERT_VALUE: LS0tLS1CRUdJTiBDRVJUS...0tLS0tCg==    HTTPS_KEY_VALUE: HVkp4BMwFunWgmay...OBwM0w==\n4.3.4  Random number generation\nAn important part of each application is the generation of random numbers. Random numbers are needed in cryptographic calculations, simulation, and other usages. IBM z/Architecture®, also known as s390x, of the IBM LinuxONE and IBM Z platforms provides special support for these functions.\nGenerating a new random key\nThe HPCS offering provides multiple improvements to generate strong keys over other environments. This extension is provided by the IBM LinuxONE and IBM Z cryptographic \ncoprocessors (Crypto Express adapter in CCA coprocessor mode or in EP11 mode).\nLinux users might already be fa miliar with the default kernel in terface to the random number \ngenerator. /dev/urandom is used for non-blocking random numbers, and /dev/random  is used \nfor entropy based random numbers.\nWhen you",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 245,
            "total_chunks": 337
          }
        },
        {
          "id": "90c56a70-980a-40d4-8150-0fec8fad55fb",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "or non-blocking random numbers, and /dev/random  is used \nfor entropy based random numbers.\nWhen you run Linux on Z, the additional pseudo random number generator, (PRNG) device, \n/dev/prandom , and true random number generator (TRNG) device, /dev/trng , are provided \nfor enhanced random number generation. See Table 4-1.\nTable 4-1   Pseudo random and random number generator devices\nFor more information, see Device Drivers, Features, and Commands .\nGenerating a new r andom key using HPCS\nIn addition to the random sources built into every IBM LinuxONE and IBM Z, the \ncryptographic accelerator hardware or hardware security module (HSM) can be used to Tip: When using this method, use only an encrypted contract to protect the secret key. \nThis can be encoded with base64 -w 0 server.crt . The application can then read and \napply these values on startup.\nLinux Linux on Z\nPseudo Random /dev/urandom /dev/prandom\nRandom /dev/random /dev/trng\n\nChapter 4. Application developme nt in a trusted envi",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 246,
            "total_chunks": 337
          }
        },
        {
          "id": "27826d51-11f7-41a3-9e5d-61e51b700e30",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "dom /dev/prandom\nRandom /dev/random /dev/trng\n\nChapter 4. Application developme nt in a trusted environment 81generate keys within the tamper resistant HSM alone without depending on or being \ninfluenced by the VM.\nIBM provides a golang grep11  library to interface with the HSM provided by the HPCS. For \nmore information, see IBM-Cloud/hpcs-grep11-go .\nThe library does include coding examples to  perform simple key operations, such as \ngenerate, encrypt, decrypt, sign, verify, and more.\nCA backed by HPCS\nTo further improve the protection for keys used by a workload, it is possible to store and use \nthe key within an HSM. This section explains the usage of the HSM provided by the HPCS service to protect a CA root key and to use it for the signing of certificates.\nTo be able to use the HPCS service, the access  information must be passed into the virtual \nserver instance. This can be done through the workload section of the contract by the Workload Provider or through the env section of ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 247,
            "total_chunks": 337
          }
        },
        {
          "id": "fb9dad86-0ecd-41f5-86dd-d2ab2ebc207c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "through the workload section of the contract by the Workload Provider or through the env section of the contract by the Workload Deployer.\nBy using the play section within the contract, the HPCS access information is made available \nto the instance. Se Example 4-28.\nExample 4-28   HPCS access information\nworkload: |  type: workload  play:    templates:    - apiVersion: v1      kind: Pod      metadata:        name: samplepaymentsystem      spec:        containers:        - name: backend          image: icr.io/ibm/samplepaymentsystem@sha256:aa921f4009b33b926aeae931fef2b0536514e7a62ae013cee6c345b1ac7f11ba…      env:      - name: HPCS_ADDRESS        value: {{ .Env.HPCS_ADDRESS }}      - name: HPCS_KEY        value: {{ .Env.HPCS_KEY }}…env:…  env:    HPCS_ADDRESS: ep11.us-south.hs-crypto.cloud.ibm.com:8082    HPCS_KEY: SFBDU19BUElfS0VZCg==\nThese environment variables HPCS_ADDRESS and HPCS_KEY can now be used within the \nworkload. See Example 4-29.\nExample 4-29   Environment variables\n// Rea",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 248,
            "total_chunks": 337
          }
        },
        {
          "id": "b4c0ef42-f9ba-4d77-8c99-a595f140a80e",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " can now be used within the \nworkload. See Example 4-29.\nExample 4-29   Environment variables\n// Read the HPCS configuration from the environmentvar (\n\n82 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentAddress     = os.Getenv(\"HPCS_ADDRESS\")\nAPIKey      = os.Getenv(\"HPCS_KEY\")IAMEndpoint = \"https://iam.cloud.ibm.com\")\nWith the newly defined HPCS_ADDRESS and HPCS_KEY variables, you can generate a \nnew RSA keypair to be used within the certificate. See Example 4-30.\nExample 4-30   RSA keypair\nvar callOpts = []grpc.DialOption{grpc.WithTransportCredentials(credentials.NewTLS(&tls.Config{MinVersion: tls.VersionTLS12})),grpc.WithPerRPCCredentials(&util.IAMPerRPCCredentials{APIKey:   APIKey,Endpoint: IAMEndpoint,}),} func main() {context := context.Background()conn, err := grpc.Dial(Address, callOpts...)if err != nil {panic(fmt.Errorf(\"could not connect to server: %s\", err))}defer conn.Close() cryptoClient := pb.NewCryptoClient(conn)_, _, _ = GenerateKeyPair(context",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 249,
            "total_chunks": 337
          }
        },
        {
          "id": "7d2c2ace-104f-45aa-8b68-8f9acd82fa06",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ", err))}defer conn.Close() cryptoClient := pb.NewCryptoClient(conn)_, _, _ = GenerateKeyPair(context, cryptoClient)} // generate a new 4096 bit RSA key pairfunc GenerateKeyPair(context context.Context, cryptoClient pb.CryptoClient) ([]byte, []byte, error) {generateKeyPairResponse, err := cryptoClient.GenerateKeyPair(context, \ngenerateRSA4096KeyPairRequest())\nif err != nil {panic(fmt.Errorf(\"generate RSA key pair error: %w\", err))} return generateKeyPairResponse.PubKeyBytes, generateKeyPairResponse.PrivKeyBytes, nil} func generateRSA4096KeyPairRequest() *pb.GenerateKeyPairRequest {return &pb.GenerateKeyPairRequest{Mech: &pb.Mechanism{Mechanism: ep11.CKM_RSA_PKCS_KEY_PAIR_GEN,},PubKeyTemplate: util.AttributeMap(ep11.EP11Attributes{ep11.CKA_ENCRYPT:         true,ep11.CKA_VERIFY:          true,ep11.CKA_MODULUS_BITS:    4096,ep11.CKA_PUBLIC_EXPONENT: 65537,}),PrivKeyTemplate: util.AttributeMap(ep11.EP11Attributes{\n\nChapter 4. Application developme nt in a trusted environment 83ep11.CKA_PRIV",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 250,
            "total_chunks": 337
          }
        },
        {
          "id": "809b1752-3eb6-48b7-9ec9-ef11e7183f4c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "p(ep11.EP11Attributes{\n\nChapter 4. Application developme nt in a trusted environment 83ep11.CKA_PRIVATE:     true,\nep11.CKA_SENSITIVE:   true,ep11.CKA_VERIFY:      false,ep11.CKA_ENCRYPT:     false,ep11.CKA_DECRYPT:     false,ep11.CKA_SIGN:        true,ep11.CKA_EXTRACTABLE: false,}),}}\nThe resulting key pair can be used to create a new certificate. Although it is possible to create \na CA this way if that is required for the use case, it is recommended to not use a CA with a key in the clear like this. This method should rather be used to generate keys that are required to be in the clear.\nAfter this generation, the key is open in the clear and available for use as a TLS certificate \nwithin a reverse proxy or web server.\nIn our SamplePaymentSystem  example, we use the generated RSA key as a key for a \ncertificate.\n4.3.5  Reverse proxy\nTo adhere to the zero trust architecture, all communication between different services must be authenticated and encrypted.\nThe communication between the ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 251,
            "total_chunks": 337
          }
        },
        {
          "id": "30ecd841-1637-465d-bce7-8b195f402c35",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ation between different services must be authenticated and encrypted.\nThe communication between the reverse proxy and internal services might or might not be \nencrypted, based on security needs. Internal communication does not need to be encrypted.\nIn case any inter-service communication leaves a single pod,\n2 the communication should be \nauthenticated and encrypted because pods might be moved into other scopes.\nA reverse proxy within the pod can be used for various purposes, such as SSL/TLS \ntermination, caching, content serving, load balancing, and authentication.\nExisting proxies SSL certificates can be set up between the reverse proxy and backend or \nother services to check a JSON Web Token (JWT) authentication. See Example 4-31.\nExample 4-31   SSL certificate checks\ncontainers:  - name: frontend    image: docker.io/library/nginx@sha256:67f9a4f10d147a6e04629340e6493c9703300ca23a2f7f3aa56fe615d75d31ca    ports:      - containerPort: 80        hostPort: 80      - containerPort: 443  ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 252,
            "total_chunks": 337
          }
        },
        {
          "id": "314c7001-31b6-47d0-969a-cb3e544bf50b",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "aa56fe615d75d31ca    ports:      - containerPort: 80        hostPort: 80      - containerPort: 443        hostPort: 443Note:  Note: For more examples on how to use the grep11 go library, see \nIBM-Cloud/hpcs-grep11-go/blob/master/examples/server_test.go .\n2  A pod encapsulates one or more applications or containers. \n\n84 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment    volumeMounts:\n      - mountPath: /etc/nginx        name: contract-nginx        readOnly: true      - mountPath: /www        name: contract-www        readOnly: true\nThe nginx server can be configured. See Example 4-32.\nExample 4-32   nginx server configuration\nerror_log /dev/stderr info;\n events {}http {    access_log /dev/stdout;    server {        # reverse proxy with tls termination        listen 443 ssl;...        location ~ .ico {            # serve static content            root /www;        }        location / {            # serve dynamic content from backend            index index.html",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 253,
            "total_chunks": 337
          }
        },
        {
          "id": "abe0a7f5-0014-4b5b-baf6-8f13ed85307d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "   }        location / {            # serve dynamic content from backend            index index.html;            proxy_pass https://localhost:8443/;            proxy_ssl_trusted_certificate /etc/nginx/backend.crt;            proxy_ssl_session_reuse on;            proxy_ssl_verify on;        }    }}\nNo logs are written into the containers file system. Instead, all logs are written to stdout to be \nsurfaced on the container host. On the host, the logs are further integrated into the journal and forwarded to the configured remote log server.\nWhen possible, container volumes should be read-only to prevent any modification and \npotential compromise of other containers that use the same volume mounts for configuration.\n4.3.6  Basic web server (nginx) hardening\nThe use of a reverse proxy like nginx allows the protection of secondary services within or outside of the deployed pod. To maximize this protection, the configuration should be hardened to allow only the minimum required fo r the func",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 254,
            "total_chunks": 337
          }
        },
        {
          "id": "3b2c8df3-9663-494f-8c5c-48dfd2c764a3",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "is protection, the configuration should be hardened to allow only the minimum required fo r the function of the services. This includes \nthe used protocols, encryption, timeouts, and enforcement of certain headers. See Example 4-33 on page 85.\nFor more information about nginx security configuration, see Security Controls .\nFor more information, see SSL/TLS Best Practices for 2023 .\n\nChapter 4. Application developme nt in a trusted environment 85Example 4-33   Reverse proxy \nhttp {\n...    server {        # reverse proxy with tls termination        listen 443 ssl;        ssl_certificate sample.test.crt;        ssl_certificate_key sample.test.key;        ssl_protocols TLSv1.2 TLSv1.3;        ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384;        ssl_prefer_server_ciphers on;        ssl_session_cache s",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 255,
            "total_chunks": 337
          }
        },
        {
          "id": "7974acb3-d790-432d-8d5d-05540ef86450",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "CM-SHA256:DHE-RSA-AES256-GCM-SHA384;        ssl_prefer_server_ciphers on;        ssl_session_cache shared:SSL:50m;        ssl_session_tickets off;         # set default headers        add_header X-Frame-Options SAMEORIGIN;        add_header X-Content-Type-Options nosniff;        add_header X-XSS-Protection \"1; mode=block\";        add_header Content-Security-Policy \"default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval' https://code.jquery.com/, img-src 'self'; style-src 'self' 'unsafe-inline'; font-src 'self'; object-src 'none'\";        add_header Strict-Transport-Security \"max-age=31536000; includeSubdomains; preload\";...\n4.3.7  Offloading NGINX TLS to HPCS\nWith the nginx server running as a reverse proxy within the application pod,  it is also possible \nto further offload the TLS handling completely to HPCS. For more information, see Use IBM \nCloud Hyper Protect Crypto Services to offload NGINX TLS.\n4.4  Trust in-depth based on boot flow attestation\nAfter successful depl",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 256,
            "total_chunks": 337
          }
        },
        {
          "id": "0a8fde49-ac07-4922-8844-38e7eb47bcfd",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "vices to offload NGINX TLS.\n4.4  Trust in-depth based on boot flow attestation\nAfter successful deployment of a service, the Workload Deployer should verify that the \nservice has been deployed as specified. This includes the running containers as specified within the contract as well as the underlying HPCR image.\nDuring deployment of the virtual server instance in the cloud, an attestation record is created. \nIt contains hashes of the following items:\n/SM590000The original base image\n/SM590000The root partition at the moment of the first boot\n/SM590000The root partition at build time\n/SM590000The cloud initialization options\nThe attestation record is signed by the attestation key.As an extra protection layer, you can provide a public key within the contract as \nattestationPublicKey (see Example 4-34 on page 86). When the public key is provided, the \n\n86 Applying Data Protection and Confidentiality in a Hybrid Cloud Environmentattestation record ( se-checksums.txt ) is encrypted and gen",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 257,
            "total_chunks": 337
          }
        },
        {
          "id": "23a38661-dabc-4807-8e16-60c7623c39f5",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "dentiality in a Hybrid Cloud Environmentattestation record ( se-checksums.txt ) is encrypted and generated ( se-checksums.txt.enc ). \nOnly one of the files is present depending on if a public key was provided or not.\nExample 4-34   Public key provided with the attestation record\nenv: |...attestationPublicKey: |-  -----BEGIN PUBLIC KEY-----   MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA6v4drcQa2Rm6+Gdp3+xG   InhWhTG1TVN8kzCAZmZfwiYxc6RE0TVIUAqEwcXxo0lA4Qp8rGRpR5RFsdGVry95    yachZkW4utO/0d6BHDEG306hQVYMZYQmj8GX7IFHbGv/eqdvZJIWm0m3Oc+NLJRXpU9XdAY31J4tzxRQwyM1wsDt/3u7TWGi5P7cZMfjYeoP1CyWJujvCTWGNH855OcUBoF3833NcJSKpN3LOo/v71IMp3H4Frq5OJuek/QQIkhPN9muNlyUBaOujp5WDWaclHwwqd2dNRx6Z0X/X4nLI3OY8rrScFcU+TnZKHaVGQ2eFTMdGDLJehTUG0VOeuvb  +7OQRCg+VP2AyolsU53Q20Dr+AEr49qjdxN+8Pjw94AVdtC5dvTPmG08p9bB9IHx3ycMH2RxgkixsBMtZ+iu4vXOzw9zIynQhsUQoKE9prJtLI7O11+JfpMVaJtkYaUa  zb2wnsj740CwhucP8q2wtqyKbaz8xj8wXXRGNCvkERzYaOYzcRQb9t+IPb5iDD3Oeq7cA+uM5n3CXB7cpDluq1SPIpWmUztCbvcWmI0HPcpPlPa+98t3izAHpkl/90Td  qcZ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 258,
            "total_chunks": 337
          }
        },
        {
          "id": "c63525b5-65c9-4db4-a551-2da21c501bc9",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "XRGNCvkERzYaOYzcRQb9t+IPb5iDD3Oeq7cA+uM5n3CXB7cpDluq1SPIpWmUztCbvcWmI0HPcpPlPa+98t3izAHpkl/90Td  qcZSox81iEMg4RUzxLVpVM7FR6+CN6nVSck0P/8mhf1A1dzoGeLN2UGRlTfpDmDU1cRQ95+mlxdGk7sSwZpvRA0CAwEAAQ==-----END PUBLIC KEY-----\nThe hash of this public key is added to the attestation record to ensure that the record can be \nviewed only by the compliance authority, and the expected authority can be easily identified \nthrough that hash.\nWithin the env volume section of the compose contract, add /var/hyperprotect . See \nExample 4-35.\nExample 4-35   Volume section\nvolumes:  - \"/var/hyperprotect/:/var/hyperprotect/:ro\"\nThe attestation record is signed by the attestation signing key, and the signature is a provided \nseparate file se-signature.bin .\nThe attestation signing key can be confirmed by the IBM intermediate certificate. The IBM \nintermediate certificate is si gned by DigiCert, which is proven by the root certificate of \nDigiCert, thus completing the chain of trust.\nThe encryption and attestati",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 259,
            "total_chunks": 337
          }
        },
        {
          "id": "94041356-1cd9-4f54-8399-71c9bd7e40b9",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " the root certificate of \nDigiCert, thus completing the chain of trust.\nThe encryption and attestation certificates are signed by the IBM intermediate certificate and \nthis has been signed by the IBM Digicert intermediat e cert (which in turn is signed by DigiCert \nTrusted Root G4). For more information about the certificates, see DigiCert Trusted Root Authority Certificates.\nTo validate the attestation record and hashes, obtain the attestation record se-checksums.txt  \nand the signature file se-signature.bin  from your HPVS instance. To do so, you can \nimplement your container to provide the attestation record and the signature file. The attestation record and the signature file are made available to your container in the /var/hyperprotect  directory.\nSample node.js code snippet implementation to get attestation .zip files. See Example 4-36 \non page 87.\n\nChapter 4. Application developme nt in a trusted environment 87Example 4-36   Get attestation .zip files\napp.get('/api/v1/attestatio",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 260,
            "total_chunks": 337
          }
        },
        {
          "id": "82b7759c-f7a1-4a26-b602-6c06dad97854",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " nt in a trusted environment 87Example 4-36   Get attestation .zip files\napp.get('/api/v1/attestation', function(req, res) {\n    console.log('GET ' + req.path);    const fileName = 'attestation.zip';    const fileType = 'application/zip';    var zip = new admzip();    try {        zip.addLocalFile(\"/var/hyperprotect/se-checksums.txt.enc\");    }    catch (e) {        zip.addLocalFile(\"/var/hyperprotect/se-checksums.txt\");    }    zip.addLocalFile(\"/var/hyperprotect/se-signature.bin\");    var zipFileContents = zip.toBuffer();    res.writeHead(200, {        'Content-Disposition': `attachment; filename=\"${fileName}\"`,        'Content-Type': fileType,      })    res.end(zipFileContents);}); app.get('/api/v1/attestationdocument', function(req, res) {    console.log('GET ' + req.path);    res.type('txt').sendFile('/var/hyperprotect/se-checksums.txt');Y});\nFor the sample code that we used in our environment, see \nIBM/hyperprotec/redbook-samples/sg248555 .\nYou can decrypt the encrypted attestat",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 261,
            "total_chunks": 337
          }
        },
        {
          "id": "5d2c6842-fe34-47f0-96a0-c17d775ca9e3",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " environment, see \nIBM/hyperprotec/redbook-samples/sg248555 .\nYou can decrypt the encrypted attestation by using the provided script For more information \nfor HPVS for VPC, see IBM Cloud: Decrypting the attestation document . For more information \nfor IBM LinuxONE or IBM Z, see IBM Hyper Protect Virtual Servers: Decrypting the \nattestation document .\n4.5  Data storage\nHPVS does not suppor t persistent st orage over reboot on the root  partition. Therefore, the \nroot volume is deployed in the same state on each boot of the HPVS. This means that all data must be stored on  user data volumes that are attached to the HPVS.\nTo provide effective data at rest protection, all encryption must happen within the trusted \nexecution environment (TEE) before any data blocks or objects are written back to external storage.\nThe two main storage classes are block storage and object storage.\n4.5.1  Encrypting block storage\nLinux Unified Key Setup (LUKS) is the standard for Linux volume encryption. It en",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 262,
            "total_chunks": 337
          }
        },
        {
          "id": "43e86cfa-f93f-4463-a396-fe7df6c18817",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ting block storage\nLinux Unified Key Setup (LUKS) is the standard for Linux volume encryption. It enables \nsecure management of multiple user passwords. LUKS stores all necessary setup information in the partition header, which enables users to transport or migrate data seamlessly.\n\n88 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentThe user data volumes of a H PVS are encrypted using LUKS by default. To open and mount \na LUKS protected partition, a password is required. This password is derived from the password seeds that are supplied within the workload and env sections of the contract.\n4.5.2  Encryption state\nHPVS periodically logs the encryption state of  the attached block st orage devices to the \ninternal journal. Because the journal is forwarded to a remote log server, this output can be used to audit the encryption state of the data at rest.\nIn case the default cadence is not sufficient, the workload can request additional audit output \nby logging the",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 263,
            "total_chunks": 337
          }
        },
        {
          "id": "bc8ac415-65fd-4aba-a565-ea036812d63e",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " default cadence is not sufficient, the workload can request additional audit output \nby logging the special token HPL13000I . See Example 4-37.\nExample 4-37   Request additional audit output\nworkload: |\n...        containers:        - name: verify-disk-encryption-trigger          image: docker.io/library/ubuntu:22.04          command: echo\n    args:\n          - HPL13000I...\nLogging the token to stdout of any of the containers triggers the verify-disk-encryption \nservice, which adds information about the encryption state of the attached block devices to the log. See Example 4-38.\nExample 4-38   Verify disk (volume) encryption\nconmon[991]: HPL13000I...verify-disk-encryption: HPL13003I: Checked for mount point /, LUKS encryption with 1 key slot found\nverify-disk-encryption: HPL13001I: Boot volume and all the mounted data volumes \nare encrypted\nA complete example of the log is displayed in the Example C-2 on page 118.\nFor more information on disk (volume) encryption verification, see IBM ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 264,
            "total_chunks": 337
          }
        },
        {
          "id": "c5ad3747-8dc6-4675-bf32-9eee0a5d6ab4",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "the Example C-2 on page 118.\nFor more information on disk (volume) encryption verification, see IBM Cloud: Verifying disk \nencryption status .\nHPCS user data volume encryption\nAn HPVS instance can also be  configured to use an addit ional HPCS seed for the LUKS \npassword. Because this secret is tied to the HPCS instance, it can also be remotely controlled.\nWhen this option is used, the key state on the HPCS instance is monitored to lock the \npartition when the HPCS key is disabled or removed.\nThe service can be configured within the env par t of the contract, which allows the use of a \ndifferent HPCS instance in ea ch deployed HPVS instance and volume. See Example 4-39 on \npage 89.\n\nChapter 4. Application developme nt in a trusted environment 89Example 4-39   HPVS volume encryption\nenv: |\n  type: env  volumes:    payment-data-volume:      seed: \"secret-env-seed1\"      kms:        - apiKey: \"${var.ibmcloud_api_key}\"          crn: \"${var.hpcs_crn}\"          type: \"private\"      kmsTimeou",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 265,
            "total_chunks": 337
          }
        },
        {
          "id": "fea3ce5d-07e7-4039-8fed-4c6a3ddca34d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "y: \"${var.ibmcloud_api_key}\"          crn: \"${var.hpcs_crn}\"          type: \"private\"      kmsTimeout: 10  signingKey: \"xxxxxxxxx\"...\nWhen it is possible, give precedence to the pr ivate API endpoint before the public endpoint. \nWhen configured this way, the communication does not need to leave the VPC network. If only the private API endpoints of the HPCS serv ice are used, it is po ssible to completely \ndisable the public API endpoint on the service. This further reduces the attack surface.\nThe additional protection by an HPCS secret can be added to an existing volume. To do so, \nthe workload and env seeds still mu st match to allow access to th e volume. This is reflected in \nthe HPVS log with an entry that is  similar to the following message:\nhpcr-disk-mount[699]: HPL07011I: Migration started from non-BYOK to BYOK for \nvolume [/dev/vdd]\nThe KMS connection can be changed to another KMS but cannot be removed as this is \nindistinguishable from a downgrade attack.\nIn case the data nee",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 266,
            "total_chunks": 337
          }
        },
        {
          "id": "1200f0ab-a2ad-4004-a0a6-3f34a3769b18",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "MS but cannot be removed as this is \nindistinguishable from a downgrade attack.\nIn case the data needs to be exported, this must be done on the workload level.If the KMS connection is lost or the KMS key is  locked at any time, the HPVS restarts. After \nthe restart, the HPVS starts pollin g for an enabled key for a limited  time to re-o pen the volume \nand re-start the workload. This can be retried anyt ime by restarting the HPVS.\nThe required crn can be easily obtained by using the ibmcloud  CLI command as shown in \nExample 4-40.\nExample 4-40   Retrieve service instance\n$ ibmcloud resource service-instance \"sample-hpcs\"Retrieving service instance sample-hpcs in all resource groups under account XXX's Account as xxx@ibm.com...OK\nName:                  sample-hpcs\nID:                    crn:v1:bluemix:public:hs-crypto:us-south:a/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx::GUID:                  xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxLocation:              us-sout",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 267,
            "total_chunks": 337
          }
        },
        {
          "id": "9fdc1351-fa53-4a74-a109-e67df743930b",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "xxxxxxxxx::GUID:                  xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxLocation:              us-southService Name:          hs-cryptoService Plan Name:     standardResource Group Name:   xxxState:                 activeType:                  service_instanceSub Type:              kms\n\n90 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentLocked:                false\nCreated at:            2023-07-30T14:16:13ZCreated by:            IBMid-xxxxxxxxxxUpdated at:            2023-08-17T08:30:13ZLast Operation:        ...\nWhen you use Terraform to manage HPCS, the information can also be acquired from the \nTerraform state. See Example 4-41.\nExample 4-41   Manage Terraform information\n$ terraform show# ibm_hpcs.hpcs:resource \"ibm_hpcs\" \"hpcs\" {    created_at            = \"2023-07-30T14:16:13Z\"    created_by            = \"IBMid-xxxxxxxxxx\"    crn                   = \"crn:v1:bluemix:public:hs-crypto:us-south:a/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxx",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 268,
            "total_chunks": 337
          }
        },
        {
          "id": "b593cad2-525e-4ee3-8871-fd067f93a38c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ":bluemix:public:hs-crypto:us-south:a/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx::\"...    extensions            = {        \"allowed_network\"         = \"public-and-private\"        \"endpoints.private\"       = \"https://api.private.us-south.hs-crypto.cloud.ibm.com:8080\"        \"endpoints.privateGrep11\" = \"https://ep11.private.us-south.hs-crypto.cloud.ibm.com:8080\"        \"endpoints.public\"        = \"https://api.us-south.hs-crypto.cloud.ibm.com:8080\"        \"endpoints.publicGrep11\"  = \"https://ep11.us-south.hs-crypto.cloud.ibm.com:8080\"    }...    id                    =\"crn:v1:bluemix:public:hs-crypto:us-south:a/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx::\"\n    location              = \"us-south\"\n    name                  = \"redbook-sample\"    plan                  = \"standard\"    service               = \"hs-crypto\"    service_endpoints     = \"public-and-private\"    state                 = \"active\"    status                = \"active\"    u",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 269,
            "total_chunks": 337
          }
        },
        {
          "id": "46c77ae0-ce08-4e4a-ba7b-e670c5738377",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " = \"public-and-private\"    state                 = \"active\"    status                = \"active\"    update_at             = \"2023-08-17T08:30:13Z\"...}\nThe results of the output is the crn of the service: \n\"crn:v1:bluemix:public:hs-crypto:us-south:a/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:xxxxx\nxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx:: \". \nTo secure a disk (volume) with this, the key id must be specified in the end of the crn. The \noutput is similar to the following example: \"crn:v1:bluemix:public:hs-crypto:us-south:a/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:xxxxx\nxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx:key:xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxxxx \".\n\nChapter 4. Application developme nt in a trusted environment 914.5.3  Upgrade, backup, and disaster recovery\nAlthough all volumes can be ba cked up by taking a snapshot, th is is not necessary for HPVS \nroot volumes. Because the root volume is cleanly prepared on  each boot, it will always be in \ngood shape after a reboot.\nAll valuable application data must be stored on data",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 270,
            "total_chunks": 337
          }
        },
        {
          "id": "d92b112a-707a-4030-bd0c-9c40f73521a4",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "t will always be in \ngood shape after a reboot.\nAll valuable application data must be stored on data volumes. The data volumes can be \nsnapshot. Snapshots can be restored into new data volumes that can be attached to the same or another HPVS instance. When attempting this, the following criteria must be met:\n/SM590000The workload volume seed within the contract must be the same\n/SM590000The env volume seed within the contract must be the same\n/SM590000The HPCS instance attached to the data volume must provide the same key\nIt is possible to change the workload containe rs and the provided environment data between \nsnapshot and restore. This means, if it is needed, that it is possible to move data volumes to another region and update the workload in one step.\nFor details about snapshots of data volumes, refer to the following documentation: \n/SM590000For creating snapshots, see Creating snapshots .\n/SM590000For restoring a volume, see Restoring a volume from a snapshot .\n/SM590000For man",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 271,
            "total_chunks": 337
          }
        },
        {
          "id": "375d0204-ad23-4c7a-85ec-d6955c3bdc48",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "apshots .\n/SM590000For restoring a volume, see Restoring a volume from a snapshot .\n/SM590000For managing a snapshot, see Managing snapshots.\nWhen creating snapshots while an HPVS is running, it is possible that the snapshot does \ncontain inconsistent data that cannot be read by the application. Therefore, it is recommended that the HPVS is re booted or restar ted before a snapshot  is requested. The \nreboot or restart flushes all data from memory to disks (volumes). Alternatively, the workload \ncontainers might provide modes that allow for safe  snapshots. This is highly dependent on the \nworkload and cannot be controlled by the IBM infrastructure or container runtime image.\n4.5.4  High Availability\nHPVS does not provide any servic es in support of a high ava ilability (HA) environment. The \ncontainers run as defined within the contract. The Workload Deployer persona of the application is responsible for providing enough instances and a configuration for the overall service to perform",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 272,
            "total_chunks": 337
          }
        },
        {
          "id": "f8c9d2ba-9628-4b32-9e8a-335ed6cd5f2c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "is responsible for providing enough instances and a configuration for the overall service to perform properly. Note:  Create a separate API key to provide within the contract instead of using the same \nAPI key within the contract and to deploy the contract. Using different keys does provide the benefit that the key can be restricted and follow its own lifecycle.\n\n92 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentFor more dynamic applications, including sca ling and workload migration, Kubernetes3 is the \nindustry standard. \nDeployment of pods that are protected by Secure Execution for Linux is allowed within the \nCloud Native Community: Confidential Container Project. For more information, see confidential-containers .\n4.6  Securing cloud native services\nKubernetes is an open source container orchestr ation platform that is extensively used in the \nindustry for managing, scaling, and automating cloud native services.\nKubernetes has four key underlying infrast",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 273,
            "total_chunks": 337
          }
        },
        {
          "id": "458b82bb-4ecb-4613-8b0d-c85c64537c0c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " managing, scaling, and automating cloud native services.\nKubernetes has four key underlying infrastructure layers to protect, which include the public, \nprivate, or hybrid cloud infrastructure, clus ters, containers, and code. From a security \nperspective, each layer relies on the next layer. For example, the code layer benefits from strong security at the public, private or hybrid  cloud infrastructure, cluster, and container \nlayers. You cannot safeguard against poor security standards at the other layers by addressing security at only the code layer. \nDepending on the attack surface of the service, focusing on specific aspects of security is \nimportant. For instance, if  a critical service, \nService A , is in a chain of other resources with a \nseparate service, Service B , which is exposed to a resource exhaustion or vulnerable API \nattack, then the risk of comp romising Service A is high. \nIf services are run in the same cluster with a shared set of resources and common base \nwher",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 274,
            "total_chunks": 337
          }
        },
        {
          "id": "cb70c6db-9f67-424b-8d6a-1691cea0e743",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " high. \nIf services are run in the same cluster with a shared set of resources and common base \nwhere a privilege escalation can be achieved through one service (Service B), then potentially, Service B can be used to attack or exploit Service \nThere are three areas of concern for securing Kubernetes: \n1. Cluster components like kubelet\n4 that are configurable \n2. Applications that run in the cluster 3. Appropriate level of protection for APIs \n4.6.1  Confidential cluster\nWith the ability to protect virtua l machines through conf idential computing, it is also possible \nto create confidential clusters. For more information, see Red Hat OpenShift Container \nPlatform for IBM Z and LinuxONE 4.13 . Virtual machines, by using confidential computing, \nprotect the control plane and worker or compute nodes from the underlaying infrastructure. Although such clusters addre ss several attack vectors, one ke y aspect to consider is the \nprotection boundary. Kubernetes requires multiple APIs to oper",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 275,
            "total_chunks": 337
          }
        },
        {
          "id": "6227031a-0501-4ca0-bf10-553bbe9b3bef",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ", one ke y aspect to consider is the \nprotection boundary. Kubernetes requires multiple APIs to operate a cluster, and with possible vulnerability of those APIs, the cluste r might be infiltrated.  At that point, a \ncyberattacker is within the protection boundary with administrator privileges at the control node. Therefore, the compute nodes and workloads that are running within the cluster are considered compromised.\nA secondary aspect to consider is that clusters  are managed more and more by other parties. \nThis can be either by cloud providers offeri ng managed Kubernetes clusters or by a service \npartner managing a cluster in a data center. Either way, confidential clusters need to protect a \n3  If a pod (or the node it runs on) should fail, Kubernetes can automatically cr eate a new replica of that pod to \ncontinue operations\n4   Kubelet is an agent that runs on each node in a cluster to ensure the container or containers are running in a pod.\n\nChapter 4. Application developme nt",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 276,
            "total_chunks": 337
          }
        },
        {
          "id": "419e09c8-907b-4205-a9f3-fc539256363d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ter to ensure the container or containers are running in a pod.\n\nChapter 4. Application developme nt in a trusted environment 93larger attack surface and must rely on network-level segmentation for security to protect \nagainst lateral movement5 if one part of the cluster is compromised.\nTherefore, a more enhanced security approach th at uses confidential containers is needed, \nparticularly when considering the principles of zero trust. \n4.6.2  Confidential containers\nConfidential containers is an open source community working to enable cloud-native \nconfidential computing by using TEE to protect containers and data. The community has the following goals:\n/SM590000Allow cloud-native application owners to enforce application security requirements\n/SM590000Transparent deployment of unmodified containers\n/SM590000Support for multiple TEE and hardware platforms\n/SM590000Create a trust model that separates Cloud Service Providers (CSPs) from guest \napplications\nThanks to the confidential con",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 277,
            "total_chunks": 337
          }
        },
        {
          "id": "bf7ce8d9-50aa-42f4-b0bd-bf2d74a58d86",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "hat separates Cloud Service Providers (CSPs) from guest \napplications\nThanks to the confidential containers comm unity, there is now an operator to deploy \nconfidential containers runtime and required configurations on a Kubernetes cluster. The confidential containers operator provides a means to deploy and manage confidential containers runtime on Kubernetes clusters. The primary resource describes runtime details such as installation type, source, and nodes to deploy. \nConfidential containers work by embedding a Kubernetes pod inside a virtual machine (VM) \ntogether with an engine called the enclave software stack. There is a one-to-one mapping between a Kubernetes pod and a VM-based TEE or enclave. The container images are kept inside the enclave and can be either signed or encrypted. \nThe enclave software stack is measured, which means that a trusted cryptographic algorithm \nis used to authenticate its content. It contains  the enclave agent, which is responsible for \ninitiating at",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 278,
            "total_chunks": 337
          }
        },
        {
          "id": "2a2f70e7-bcdc-47db-82a7-238c2a06244f",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "to authenticate its content. It contains  the enclave agent, which is responsible for \ninitiating attestation and for fetching the secrets from the key management service. \nThe supporting components for the solution are the container image registry and the relying \nparty, which combines the attestation service and key management service. \nAfter the VM has been started, the flow can be summarized in the following four steps: \n1. A request is sent to the attestation service through the enclave agent. In response to this, \nthe attestation service sends a cryptographic challenge for the agent to prove the workload's identity using the measurement of the enclave. If the enclave agent effectively \nsolves this challenge, the a ttestation service informs the key management service that it \ncan proceed with delivering the secrets. \n2. After the workload's authorization to r un is confirmed, the key management service \nlocates the secrets that are associated with the workload and sends them to t",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 279,
            "total_chunks": 337
          }
        },
        {
          "id": "4999ccb7-84f2-4323-b520-797aebf30737",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ey management service \nlocates the secrets that are associated with the workload and sends them to the agent. Decryption keys are among the necessary  secrets for the vo lumes to be used. \n3. The image management service inside the enclave downloads container images from the \ncontainer images registry, verifies them, and decrypts them locally to encrypted storage. At that point, the container images become usable by the enclave. \n4. The enclave software stack creates a pod and containers inside the virtual machine and \nstarts running the containers. All containers within the pod are secured. \n5  Techniques used by cyberattackers to  move deeper into a network after gaining access to obtain increased \nprivileges.\n\n94 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentConfidential containers go beyond prior efforts of isolating the container from only the \ninfrastructure administrator, by also is olating the containers from the Kubernetes \nadministrator. The tenant",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 280,
            "total_chunks": 337
          }
        },
        {
          "id": "051ad289-9b61-4e11-a142-706483e35f72",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ture administrator, by also is olating the containers from the Kubernetes \nadministrator. The tenant can fully use the abstraction of a managed Kubernetes to develop-once-deploy-anywhere. The tenant can deploy data and workloads with technical assurance into a fully private and isolated enclav e even if the latter is hosted and managed on \nthird-party infrastructure. \nMoreover, confidential containers reduce the set of components that interact with protected \nworkload and data so that most components within the Kubernetes cluster can operate unchanged to address the risk of compromising data. This approach fosters speed of innovation and prioritizes data protection. \nAnother key component to consider in a Kubernetes based environment is the kubelet. Not \nonly in cloud environments, this component can be delivered and managed by a 3rd party.\nThe advantage of confidential containers is that  if a service exploitation leads to a privilege \nescalation to attack, vulnerable kubelet or APIs ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 281,
            "total_chunks": 337
          }
        },
        {
          "id": "b6ba40e1-571d-43ba-b299-90b0fc226825",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "t  if a service exploitation leads to a privilege \nescalation to attack, vulnerable kubelet or APIs at the cluster level are addressed. See Figure 4-3. \nFigure 4-3   Confidential clusters  versus confidential containers\nData and operation protection is enforced at the pod runtime level. This zero trust model \nbolsters security by strictly validating and authorizing the communication and access requests, enhancing the overall security posture of containerized environments. \n4.6.3  Confidential service platform\nFor data security based on technical assurance through zero trust policy, enforcement is \nessential to have the security within the moment of deployment. This fosters workloads confidentiality with deployment, and 3rd-party attestation can be done for validation. Hence, data and workload can be protected from the very start, based on technical measures rather than trust in a third-party service.\nSecure Execution offers the ability to have secrets being in jected as part of the enc",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 282,
            "total_chunks": 337
          }
        },
        {
          "id": "4180abdb-0a34-4fe1-8c16-58386dcf637f",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "arty service.\nSecure Execution offers the ability to have secrets being in jected as part of the encrypted \nimage. Such secrets can be used for zero knowledge proofs andto decrypt artifacts within the TEE. \nThe notion of user-controlled policy enforcement at deployment for confidential containers \ncan be combined with Hyper Protect encrypted contracts. Therefore, the protection of data with technical assurance throughout all stages of the data lifecycle ensures data sovereignty where the complete control over the actual data lies with the cloud user and not the cloud provider.\nThis concept can be expanded to build a confiden tial service platform in which key aspects, \nlike identify and access management (IAM), key management, attestation of workloads, policy enforcement and audit/logging records are deployed. They are deployed by using \nconfidential containers and other workloads built on a container platform that inherits the premier zero trust value proposition. For this, the confid",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 283,
            "total_chunks": 337
          }
        },
        {
          "id": "edfeff90-38db-4545-853e-7220e75c5307",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "on a container platform that inherits the premier zero trust value proposition. For this, the confidential container projects need to \n\n\nChapter 4. Application developme nt in a trusted environment 95further mature. Not only regulation needs to be defined, but also corresponding certification \nneeds to be established by the industry and acknowledged by the regulatory parties.\n4.7  Secure supply chain with SLSA\nSupply chain levels for software artifacts (SLSA) is a security framework. It is a checklist of standards and controls that prevent tampering, improve integrity, and secure both packages and infrastructure. By using this framework, you can ensure that every link in the chain is as \nresilient as possible, as illustrated in Figure 4-4. \nFigure 4-4   Secure supply chain - high-level overview\nIndustry consensus established SLSA as a set of guidelines for supply chain security that can \nbe adopted incrementally. SLSA specifications are useful for consumers and producers of software. C",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 284,
            "total_chunks": 337
          }
        },
        {
          "id": "d1c70bc3-a642-4780-84a9-5dc35cf7d9e0",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "\nbe adopted incrementally. SLSA specifications are useful for consumers and producers of software. Consumers can use the SLSA framework to decide whether to trust a software package or not. Producers can adhere to the SLSA guidelines to add security to their supply chain.\nSLSA provides the following benefits:\n/SM590000A standardized terminology for discuss ing software supp ly chain security\n/SM590000A method to enhance the security of your  incoming supply chain by assessing the \nreliability of the artifacts you use\n/SM590000A practical checklist to enhance the security of your own software\n/SM590000A method to track your progress toward complying with the Executive Order  standards in \nthe Secure Software Development Framework (SSDF)\nSLSA is designed to automate tracking of code handling, spanning from source to binary, by \nsafeguarding against tampering, regardless of the complexity of the software supply chain. This approach can provide greater confidence t hat the analysis and rev",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 285,
            "total_chunks": 337
          }
        },
        {
          "id": "b73667b9-7898-4b4f-ab8b-0320f439bc3e",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "f the software supply chain. This approach can provide greater confidence t hat the analysis and review conducted on the \nsource code remains valid for the binary that is consumed after the build and distribution process. \nSLSA protects against the following possible security  vulnerabilities:\n/SM590000Modification of code by providing a “seal” to code after source control to ensure it is not \nmodified.\n/SM590000Artifacts that have been uploaded but were not built using the anticipated CI/CD platform. \nArtifacts are “stamped” with their build platform when created.\n/SM590000Build platform threats.\n\n\n96 Applying Data Protection and Confidentiality in a Hybrid Cloud EnvironmentSLSA is only one part of a complete approach to supply chain security. Because of this, there \nare several areas that sit outsid e of SLSA's current framework th at still need to be considered \nalong with SLSA. Areas outside of SLSA’s current framework include the following examples:\n/SM590000Quality of code. SLSA ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 286,
            "total_chunks": 337
          }
        },
        {
          "id": "92137a67-5db6-453b-b8cd-12733b15e1d7",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " outside of SLSA’s current framework include the following examples:\n/SM590000Quality of code. SLSA cannot determine if the source code that was written follo ws secure \ncoding practices.\n/SM590000Producer trust. SLSA's Build Track protects from tampering after or during the build. \nHowever, it cannot address organizations that purposefully create malicious software.\n/SM590000Transitive trust for dependencies. An artifact’s SLSA level does not depend on the level of \nits dependencies. At the time of writing, there is no SLSA level that can refer to both an artifact and its transitive dependencies.\n4.7.1  Jenkins\nJenkins is an open source web server that is built for automation, and allows developers to build, test and deploy their software. This ensures that developers are able to implement continuous integration and continuous delivery (CI/CD). Jenkins interact s with various servers \nand components, meaning security is a crucial consideration. Because of the way Jenkins plug-ins and ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 287,
            "total_chunks": 337
          }
        },
        {
          "id": "f8cad45c-35a0-4c2b-808f-820c70b5e8eb",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "nd components, meaning security is a crucial consideration. Because of the way Jenkins plug-ins and their dependencies function, provid ing a basic authentication mechanism is not \nsufficient to fully secure the complete pipeline.  There are multiple options for configuration \nwithin Jenkins settings to enable, customize, or disable various security features.\nThe Jenkins Security Advisory process is wh ere Jenkins will constantly review and update \nplug-in vulnerabilities. The Security  Advisory is a list of vulnerab ilities and security issues that \nare identified and highlighted in a report within Jenkins. The report includes a description of the vulnerability, security risks posed, severities, versions it  affects, workarounds, and any \npossible solutions.\nThe eight key forms of security for Jenkins fall under three categories:\n1. Basic setup:\n– Controller Isolation– Access Control\n2. Build Behavior:\n– Access control for builds– Securing builds– Handling environment variables\n3. Use",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 288,
            "total_chunks": 337
          }
        },
        {
          "id": "fc3834df-78ea-49e1-8bed-2ca923873cba",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " Build Behavior:\n– Access control for builds– Securing builds– Handling environment variables\n3. User Interface:\n– Cross-Site Request Forgery (CSRF) Protection– Markup formatter– Rendering user content\n4.7.2  Source-to-image (S2I)\nThe Source-to-image (S2I) framework makes it easier to write images that take application source code as an input and output a new image that runs the assembled application as output.\nUsing S2I for constructing consis tent container images offers a primary benefit in terms of \ndeveloper convenience. Build image authors need to understand two key concepts to get the best performance out of S2I - The build process and S2I scripts.\nS2I creates images that are ready-to-run by injecting the source code into a specific container \nthat prepares it for execution.\n\nChapter 4. Application developme nt in a trusted environment 97S2I can be used to control what permissions and  privileges are available to the builder image \nbecause the build is launched in a single co nt",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 289,
            "total_chunks": 337
          }
        },
        {
          "id": "62ab6996-4235-405e-b20c-d1f0f61ab1eb",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " and  privileges are available to the builder image \nbecause the build is launched in a single co ntainer. In parallel with platforms like Red Hat \nOpenShift, S2I can enable admins to tightly control what privileges developers have at build time.\n4.7.3  GitHub Actions\nGitHub Actions is a continuous integration an d continuous delivery (CI/CD) platform that \nallows developers to automate their build, test, and deployment pipeline. Workflows are created that build and test every pull request to the repository or deploy merged pull requests to production.\nGitHub Actions can be configured into a workfl ow to be triggered when an event occurs in a \nrepository, such as a pull request being opened, or an issue being created. The workflow contains one or more jobs, which can run in sequential order or in parallel. Each job runs inside its own virtual machine runner, or inside a container. Each job has one or more steps that either run a script that you define or run an action, which is a reusa",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 290,
            "total_chunks": 337
          }
        },
        {
          "id": "ebe54305-83c2-454d-873c-dd12bc377a88",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ob has one or more steps that either run a script that you define or run an action, which is a reusable extension that can \nsimplify the workflow.\nA workflow is a configurable automated process that can run multiple jobs. Workflows are \ndefined by a YAML file that is checked in to  a repository and will run when triggered by an \nevent, manual intervention, or at a defined schedule.\nStore sensitive values as secrets and never as plain text in workflow files. Secrets can be \nconfigured at the organization, repository, or environment level, and can allow storage of sensitive information in GitHub.\nSecrets use Libsodium sealed boxes  so that they are encrypted before reaching GitHub. This \noccurs when the secret is submitted by using the UI or through the REST API. This client-side encryption helps minimize the risks that are related to accidental logging such as exception logs and request logs, within GitHub's infrastructu re. After the secret is uploaded, GitHub can \nthen decrypt it, so ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 291,
            "total_chunks": 337
          }
        },
        {
          "id": "b17995bf-ec10-498c-9cd8-1c6e8d9dd32c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ogs, within GitHub's infrastructu re. After the secret is uploaded, GitHub can \nthen decrypt it, so that it can be injected into the workflow runtime.\nThere are certain proactive steps and good practices to follow to help ensure that secrets are \nredacted and to limit other risks associated with secrets:\n/SM590000Never use structured data as a secret.\n/SM590000Register all secrets used within workflows.\n/SM590000Audit how secrets are handled.\n/SM590000Use credentials that are minimally scoped.\n/SM590000Audit and rotate registered secrets.\n/SM590000Consider requiring review  for access to secrets.\n\n98 Applying Data Protection and Confidentiality in a Hybrid Cloud Environment\n\n© Copyright IBM Corp. 2024. 99Appendix A. Client contract setup sample \nfiles \nThis appendix contains the followi ng sample files from snippets shown in “Client  setup steps” \non page 60.\n/SM590000“Sample YAML file with literal scalars” on page 100\n/SM590000“Sample YAML file with double-quoted scalars” on page 101\n",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 292,
            "total_chunks": 337
          }
        },
        {
          "id": "d3b7b87d-4193-43b9-9d41-bf950767e0c6",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ith literal scalars” on page 100\n/SM590000“Sample YAML file with double-quoted scalars” on page 101\n/SM590000“Sample script for certificate or key files” on page 103A\n\n100 Applying Data Protection and Confident iality in a Hybrid Cloud EnvironmentSample YAML file with literal scalars\nSee Example A-1 for a complete example of YAML file with literal (|) scalars. \nExample A-1   YAML file with literal (|) scalars\nenv: |\ntype: env\n   logging:      syslog:         hostname: ${HOSTIP} # eg 10.0.0.8 or ${HOSTNAME}         port: 6514         server: |            -----BEGIN CERTIFICATE-----            MIIEuDCCAyCgAwIBAgIUBR9g6L5hivov7eNT00HSXW39oD0wDQYJKoZIhvcNAQEL            BQAwXzELMAkGA1UEBhMCVVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFDASBgNVBAcM            C0xvcyBBbmdlbGVzMQwwCgYDVQQKDANJQk0xFzAVBgNVBAMMDmNhLmV4YW1wbGUu            b3JnMB4XDTIzMDEyNzE4MzkzMFoXDTI1MDExNjE4MzkzMFowXzELMAkGA1UEBhMC            VVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFDASBgNVBAcMC0xvcyBBbmdlbGVzMQww            CgYDVQQKDANJQk0xFzAVB",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 293,
            "total_chunks": 337
          }
        },
        {
          "id": "f5e212db-e3dc-4cab-a47b-003d21f3533a",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "   VVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFDASBgNVBAcMC0xvcyBBbmdlbGVzMQww            CgYDVQQKDANJQk0xFzAVBgNVBAMMDmNhLmV4YW1wbGUub3JnMIIBojANBgkqhkiG            9w0BAQEFAAOCAY8AMIIBigKCAYEAsgnjL9ZxtdlQf1IAwEWkcPXQEHA47LY03BO7            c5jai9vRSGxa16hj6SUOidzKj4lK7w9REhxnaihkk1P8zfF9Eld/FvMKANpdaniO            JUFT/FXzlwG1c+R+h3ZTv4LiEi9SzWwkbNBOhMN5YM7nCOWNxQBHptisNvdRqb6V            idQfneU0aesmmu61IDeB54wpsZXiRXB7M2fpx3c/9ppFYSCGWpC0Bibu12r6kg85            EykR4zjEQiVg4U9m4zchQE73Ljl+pDHaGuJlMlfKgPKwAdmPEGK8IBctb3WbuBff            ojmgRerlAEajDlDD0IQE4s+dX/F+bZCZiJrfpHz8CAs6i/GPqVR1U2jP7zaAJqgs            KNfWm0CXcLDsdbxgGNU8ECzVKw9pWPcG4A4bxiznZpgWXwoBHmnbqriZe1KEAjRM            fGCGJI3xyODCsT/64zGIwX1ajpn9b6cWnAGoXrjOc5HSXnPGyiRDc/7jmmSXcOKd            Cd3SaQu/ahNHXjr5X1dQ7XrzgY/1AgMBAAGjbDBqMA4GA1UdDwEB/wQEAwIChDAP            BgNVHRMBAf8EBTADAQH/MEcGA1UdEQRAMD6CEnNlcnZlci5leGFtcGxlLm9yZ4IX            c2VydmVyLmV4YW1wbGUub3JnOjY1MTSCCTkuMjAuNy45MocECRQHXDANBgkqhkiG            9w0BAQsFA",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 294,
            "total_chunks": 337
          }
        },
        {
          "id": "d0d52d58-cc4b-4602-be7a-1556dc2a3120",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "4IX            c2VydmVyLmV4YW1wbGUub3JnOjY1MTSCCTkuMjAuNy45MocECRQHXDANBgkqhkiG            9w0BAQsFAAOCAYEAfLdjI2j0XbVqbZcQNBIPbyL1fhsNX0I0NvPRU9tpA1M41+Ur            dCFNQKivai7jKsATzDZZVG0Adprki7YFqUocZ/NpwC6GHFETrTCICaVrjUuF2ecg            iwXLZIiHWV+qOAbycG9VoUw7eRVKarfbmd31YwZ1il27e44V1mjVd4gdJvV6a4H6            5lm3fobogin9535lnJuZjHQNQ7cbN1hrKOkNrvZpSfe5lZ6EFUZHk3S/Wdux31rz\n            1cQQAhpFfJL84KDvM/cBISKGDutgAADCb9jtH8q+ow19n7R1ff10r4/9G7CA9mv6\n            llJf8+P8Y22CGDvez3YPs3Dt7AnZe7bEEpQg1EDlgVq+WzbyEcmmaVwJcj5o7ann            ifkUIrmJOiACtDlklQXYIiZcyg8zfLIcFYkf+PHpvtYF8c5Av4DS2YFPwtc5OoLz            xUOI54V6pJZBc60aL9vNyxBAKj/cQEQKrLtUGr1vjPTOFycNW5yQAAhDL6o6E+3S            uTEClFsd2QMaCJuQ            -----END CERTIFICATE-----         cert: |            -----BEGIN CERTIFICATE-----            MIID0zCCAjsCFFS5goaaDyhsJsUHv5WooqDg9gqGMA0GCSqGSIb3DQEBCwUAMF8x            CzAJBgNVBAYTAlVTMRMwEQYDVQQIDApDYWxpZm9ybmlhMRQwEgYDVQQHDAtMb3Mg            QW5nZWxlczEMMAoGA1UECgwDSUJ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 295,
            "total_chunks": 337
          }
        },
        {
          "id": "5dadf2c9-85ce-4062-815e-c6fa7a711eee",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "JBgNVBAYTAlVTMRMwEQYDVQQIDApDYWxpZm9ybmlhMRQwEgYDVQQHDAtMb3Mg            QW5nZWxlczEMMAoGA1UECgwDSUJNMRcwFQYDVQQDDA5jYS5leGFtcGxlLm9yZzAe            Fw0yMzAxMzAxNjIzMDFaFw0yNTAxMjkxNjIzMDFaMG0xCzAJBgNVBAYTAlVTMQsw            CQYDVQQIDAJDQTEUMBIGA1UEBwwLTG9zIEFuZ2VsZXMxDDAKBgNVBAoMA0lCTTEQ            MA4GA1UECwwHU3lzdGVtczEbMBkGA1UEAwwSY2xpZW50LmV4YW1wbGUuY29tMIIB            IjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvQoaZ9z2ZU0sKCoJ+lTyzI7v            N3Mhc2QOsSYBWxrQIFUt4WW1pinXXOqlo4iPRnQsQPzhkN8blZrgI2SFk1N8IdK8            JHFcO9yVWEKmnxNeIOgiOvjrk3nSTkDH7GZyZe0p0d+Dbk671P4cKoxi32JgSK2i            FelZnYrgELiZFWbIZfKuy4YzWF0BSIPN2GZfp2IFmjzAyDpasc5ucL4ulI8jqBVd            FXzpygkBnadWNrmD57LyVgUK+aez0+JXkSOBL9XiiDjIDvSNFuSzUdZqHBBJsdNI            I2AF+e1a6JRgisK0AOU5m6Jfemnu6e+oHToY07vEUiRtueWg99Y1C0+zdnsdgwID\n\nAppendix A. Client contract setup sample files 101            AQABMA0GCSqGSIb3DQEBCwUAA4IBgQBN8StXpYafIHj6E7Knlq30kn5OIUriZSez\n            /XUm1wibmqVV/oh5YSQhMYrhJZIyX6onRl1sj2",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 296,
            "total_chunks": 337
          }
        },
        {
          "id": "d8ccc2a9-4ef6-4fb9-9336-b21757704b3c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "3DQEBCwUAA4IBgQBN8StXpYafIHj6E7Knlq30kn5OIUriZSez\n            /XUm1wibmqVV/oh5YSQhMYrhJZIyX6onRl1sj2BHxQ7HDnZpChwo8RvYvTQE6EF2            uhAQxAd0V+pQ7U/oMTF5JzFK8YnhXWsTqJ11SI/FR5A1dLWne9B6y8FcBzEnATHV            Ly8aJJCsI7zFWwzfQZn7WL1SKDkY8f7sZaWJAfch9auoqQH+AVvoQCK7sT26dzpI            hskWkj0cwrSgmtU2XSERsO9mPCyvBCFeXO8Ig13XSyKblh4qb++zeUsS8xb4EN27            hL/YOoBH3nVcpWW5EhVU5RJfbStrp1kGT7a6CRl4LyVSDNzPX0zEtUT958fdoXNV            rkPh/xre3lCxV8pTC7BGG81kCMfDqHdKKwcp8HJpJfXc/aQ220vh2JevXNSFQtIP            2idwz4A+jn+FEVWXAKB6jOS4XBoEdNh/8nSBiJLJ6bI0c4Rz2KLprKNZ4y43toI4            +nQ1PgTmc+x6hmiSZJuV0OnGVEDGxA0=            -----END CERTIFICATE-----         key: |            -----BEGIN RSA PRIVATE KEY-----            MIIEogIBAAKCAQEAvQoaZ9z2ZU0sKCoJ+lTyzI7vN3Mhc2QOsSYBWxrQIFUt4WW1            pinXXOqlo4iPRnQsQPzhkN8blZrgI2SFk1N8IdK8JHFcO9yVWEKmnxNeIOgiOvjr            k3nSTkDH7GZyZe0p0d+Dbk671P4cKoxi32JgSK2iFelZnYrgELiZFWbIZfKuy4Yz            WF0BSIPN2GZfp2IFmjzAyDpasc5ucL4ulI8jqBV",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 297,
            "total_chunks": 337
          }
        },
        {
          "id": "118a63f7-563b-4e12-8439-fbecb85e20c4",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "p0d+Dbk671P4cKoxi32JgSK2iFelZnYrgELiZFWbIZfKuy4Yz            WF0BSIPN2GZfp2IFmjzAyDpasc5ucL4ulI8jqBVdFXzpygkBnadWNrmD57LyVgUK            +aez0+JXkSOBL9XiiDjIDvSNFuSzUdZqHBBJsdNII2AF+e1a6JRgisK0AOU5m6Jf            emnu6e+oHToY07vEUiRtueWg99Y1C0+zdnsdgwIDAQABAoIBAGUllL9iEUxeoHPV            GoWhElC1YfWvWoSdUuciSLNwg0/pg6UIgMsptBv5SStZFCBEZLFZHXAgkGfZ+He3            f2k20EJguV5ecVVuT1JsRy7yc/jze+1F5vZ/xjEspEwu/KLg5PFwwKgy5HmwhW2W            tAiGYLJChWln9BVBi2Ym/3HeDvfMmBSv4JkgVwZvjlQddlpP2ljMiZdKjblvdU45            6QUYsv0kz2WyQPOlb23B5Yy5cUmRwutYacFgqkPbTmdjbpAAwr1nYi8uIVqYn47h            72dwV9RfiqKUGKb05UmfwonQYwHecpqFIG4jlkKGjY5MeIcecgYYaq9WpoCUgk5h            Ba0fT+ECgYEA67Oo5rdlE4kGV6b4XvtIyWAsKGD5Ofavyxj9VjCSeLb10UC4vUp3            kTu42EJ1UVGN8TYKpz3sAqkT+tVTrC663KLD3bdRWf0fP6BkvHTshWP03nnWwjTN            RNjAroojjDKfSLI/Qn2BPAQm2QsJFRa5ZYMl7WMAjuZu+VOz1z+uphsCgYEAzVG2            zTZ5668AU0FU7eHjJfDdBYhiEzIjgme3zFKg3tINvlyStyLKURkxtKUmliw8Tdhk            4as3YUu0ky0Dz+XopN8IBhwBKwG",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 298,
            "total_chunks": 337
          }
        },
        {
          "id": "9b5a866a-840e-4b74-968e-f26343875fc1",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "5668AU0FU7eHjJfDdBYhiEzIjgme3zFKg3tINvlyStyLKURkxtKUmliw8Tdhk            4as3YUu0ky0Dz+XopN8IBhwBKwGizXMHP9BYfxrIy7yOVj9Lp2WSORvjwoB7nGxJ            lWwyrer2O2tNKMx6sGynJvfgSFh9AvTddldwfLkCgYAtDnsLH6PCyD7eIpz4CzEu            zaOjVGZQHkgcmvpSr5ZQXSlAhw7JoKKasL/1Fz81/FEV+y6uKbgkCg43tO/5yjUO            WE7440I54ZlHoHGhVPihxynYHZJgLZfPwV+T/fQtqL+qNejB3RwHTQPgGavyzBVE            wn1Nk89XgdVU9Bs82n+YYQKBgBuwN1y5SfvUn9CacN+bpMxLDSNf3woDqvI9FnZB            dlxWK3BOf6Ke2HXTVfashuWdlYxR8FjWhCNk2Dc4zNjOgm8pfKWGRUoNcG0QZBvg            9u49KHMBPJi49HTgp7V342EpfoH7wHicHMGDfC1LLR6hZLJCFNCWgPKArGsnpm39            ILhRAoGARwIM8o27pBymXcoyQDSqDgobXwKs9qqDxgYqXOJudPm8F39AG/Ww3FCx            f3t+UjvqAWQEL5bt+kGeBlrIbidgr2zuQ5mlEHCRchE1GI4QcTmGSBqW7KZJpGy2            BAsiTpe4uyh4+Vk7gXqt9+pfdF7aFiWphDERYJVFUFoizytAYjY=            -----END RSA PRIVATE KEY-----\nSample YAML file with double-quoted scalars\nSee Example A-2 for a complete example of YAML file with double-quoted (\"\") scalars.\nExample A-2   YAML f",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 299,
            "total_chunks": 337
          }
        },
        {
          "id": "0ab1f5ad-ca84-4652-833c-ac460a60dafa",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "xample A-2 for a complete example of YAML file with double-quoted (\"\") scalars.\nExample A-2   YAML file with double-quoted (\"\") scalars\nenv: |\ntype: env\n   logging:      syslog:         hostname: ${HOSTIP} # eg 10.0.0.8 or ${HOSTNAME}         port: 6514         server: \"-----BEGIN CERTIFICATE-----\\nMIIFCTCCAvECFEp7wJLz4jNStIsVH2dUeHDN26ZyMA0GCSqGSIb3DQEBCwUAMEEx\\nCzAJBgNVBAYTAlVTMRkwFwYDVQQKDBBMb2dzdGFzaCBUZXN0IENBMRcwFQYDVQQD\\nDA5jYS5leGFtcG\n\n102 Applying Data Protection and Confident iality in a Hybrid Cloud EnvironmentxlLm9yZzAeFw0yMzAxMDUxNjU0MTNaFw0yNDAxMDUxNjU0MTNa\\nMEExCzAJBgNVBAYTAlVTMRkwFwYDVQ\nQKDBBMb2dzdGFzaCBUZXN0IENBMRcwFQYD\\nVQQDDA5jYS5leGFtcGxlLm9yZzCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCCAgoC\\nggIBANe7PR4XaTXtF6h3FhWe/R4BSTVylXWopA51+ppcJ3BOMPjmRMNJ3tFAFE3h\\nF4d0RHBNJOZF0+ogT0ZEseTe4mqJXk3RgfMSrLaymNgzaefD67uhQ9ZzznE3kIXe\\nmzh/A8aDwhaUMifIKxekisrmpvjDwUJtaSs3pb27W+cOmzAPZ3cmOs09tELLY134\\nf52sp0ZqFSOgvCwcdt88PFVMm2rrFgwxP2gLgOkZL4OsM9sQykYEPR28unS+P90V\\nqnYPy27xqJNss4OdZCJrjkS7",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 300,
            "total_chunks": 337
          }
        },
        {
          "id": "66eac131-eb5d-4383-ab37-99e2ba569a76",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "tELLY134\\nf52sp0ZqFSOgvCwcdt88PFVMm2rrFgwxP2gLgOkZL4OsM9sQykYEPR28unS+P90V\\nqnYPy27xqJNss4OdZCJrjkS7lv2PbBxSoFjDq/yLjnDV8khW2+6w0MFRvamoKL34\\nn/XoXN6VCathSxcwvXg0x3wwTxa5Hevb0iziNGXHjZ9bXt+8bnu/Bhsa7KwaoUt9\\nrJJeMy0KNsdQyWhMJE904YKm9Eo/S92rrcNWzmzBIV0iecOHc24iw3SIXOnoAKNY\\n1GtDOQChSEeb7en25s1fjWTqIDyDOktWjp9DXu1ips9YDb7GKZ7raOoQnsPkGrRE\\nKOWClkWQ4qIXJ9LH73ytR1h8+AsGyInaan5ehnz7JC5SFhE96wPzJDaXCKNHBP/e\\ntfwQ0BTbgO6z8gPE8JlPGXTmdf9YF5NxMd4oJA7u7Y6x2y4KIRYacrcevDxe/lFk\\n843MwiYU2atYgqgFK07BIOHNvqv93WiqXy8WAolSmMoJ/eqdAgMBAAEwDQYJKoZI\\nhvcNAQELBQADggIBAFkQpmW3T50eI5AhAOzN6duxQtjDuIE4AhcQaejIVFu8R9H4\\nGKw8WQo1DO7jaefRK7BFy68u8Cacgyn6btCoA0AMuKYyt1StM4Jzf2ZxWrox0Tl+\\nUW5RJFP8HoIBQutMtgHaY3hWZJ4Jvcg6y7kroMynZnsV3jbK0/GmthtUYonjCpCH\\nuC1rEp/0Gkp9BPnrY6cgyRdDbgmDo3YMqmUh8BqTGLEi+F45K/PEN502kUBcMJTY\\nvpWVfgMz7nQhN4temIlQQDs8gu3LBt9lxomuMXtYkTq245LfXdtbPPkrwjvbIKzM\\nFasa1PkTmK23cXLpRWfNUu/JHChpCl27Yg8ScTm6GV/eKhJbtku8ExvLWgHAITGi\\n5Rhh2Pl//Jh4szzTL34IY5bPqSXMrqUB3vFzND5ybmWrwo0i2CyLS+gKgGqzz0Xt\\n",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 301,
            "total_chunks": 337
          }
        },
        {
          "id": "69908022-bcb9-4e4c-bd42-8660f60f638a",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "Yg8ScTm6GV/eKhJbtku8ExvLWgHAITGi\\n5Rhh2Pl//Jh4szzTL34IY5bPqSXMrqUB3vFzND5ybmWrwo0i2CyLS+gKgGqzz0Xt\\nmvQ6XCiq7EsTdNLlX1ZDWjias12EIyCVbWrmxzFsR/Ji8XQqUoKK+QXhdsQ/uA4H\\nn72jGuWsjhRAKE7WyI2i1H+TPRZ+K6VaKeS2aikC3p2JCU4VxrP2jSdWhdYYwEgL\\nC/mjDXOjxbr6TIOtrxQQSBplTuRz8yNabrPB6G2xN+e70qpB1KT5w2ee1RH7M1o+\\nUoeDoSwqneVvONAjQn/0KKL0Y7P2BURHjJeWsLtFmyUZVlkqlosg8P7Nabrj\\n-----END CERTIFICATE-----\\n\"         cert: \"-----BEGIN CERTIFICATE-----\\nMIIFETCCAvkCFBhx5DuYtRzCxRx8Bo+WIS2LFI2uMA0GCSqGSIb3DQEBCwUAMEEx\\nCzAJBgNVBAYTAlVTMRkwFwYDVQQKDBBMb2dzdGFzaCBUZXN0IENBMRcwFQYDVQQD\\nDA5jYS5leGFtcGxlLm9yZzAeFw0yMzAxMDUxNjU1MzZaFw0yNDAxMDUxNjU1MzZa\\nMEkxCzAJBgNVBAYTAlVTMR0wGwYDVQQKDBRMb2dzdGFzaCBUZXN0IENsaWVudDEb\\nMBkGA1UEAwwSY2xpZW50LmV4YW1wbGUub3JnMIICIjANBgkqhkiG9w0BAQEFAAOC\\nAg8AMIICCgKCAgEArY+N+3IEYrIQpdMPT6xMqksSS43g2+44EdYiPNonP2KjEUdG\\n/g57CBnIOaUfZyvg3Fc9ROBMhqYa6CGKCd8Yec7mL4c97tS9wBtc6I0eEBmXAeZz\\nCGy6/1HtScZ/mAHw9rshgwF1Si/j9R4NA6ZepmGvoQMdUOGJJHhSsEfovVoRR5D/\\nVO1Urpu4LYnz7Zwo+/QEzJbUmSVN52/tNWjgTlH",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 302,
            "total_chunks": 337
          }
        },
        {
          "id": "5ccbcfef-56e2-442f-8876-881a9f1dc84c",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "1HtScZ/mAHw9rshgwF1Si/j9R4NA6ZepmGvoQMdUOGJJHhSsEfovVoRR5D/\\nVO1Urpu4LYnz7Zwo+/QEzJbUmSVN52/tNWjgTlHFCOdQ6aZCkwBK2DCa3b2NFWSO\\nvfhhl/4GLLeYZ2hG2f9+W+L6slLMWwpMqywY9bUXn+NROcYPoLX222saH6nY74+0\\n+B9Qk4BOlNqhN493FCoXq2uzGmYb0igOx+o8UVc7gcozbhvREgW4RRa4XgamonSW\\nCccM5sQFHFSxaGZXokpE2GJtQT0wv/pim4Ku0XEIQKmZGejC1dw26fbG+CWDWPXs\\nYEmf4S5Z5exjSiQCPL2QawCXgGEJNkiUMj5ld3jeb2kY221IKb+uSE6waNTts4nO\\nRa9DveHrUKrNqq33yoEZvj4K/QZ9px9km7o0BhR+oMPAYI/YODgNOhQLSR3q2n1C\\njd+baFfg4Sb8L2OUTyW4Kd2Ok9rkCk9W/8T/YKlrBFoSrNHtPhKIi6FZLkrodn1g\\n8+lPo3E80Gn5hUZdgsIZ7Y+c2qcUAVu8X/otFaWm8FCmIDyz+ZKm/YgaX4UCAwEA\\nATANBgkqhkiG9w0BAQsFAAOCAgEACg4PxboxM01cO3pmhTfvwetBvICz8GOuAq3f\\nLWYFXcZmnMHqwDZKOx20a03XfcBaWhyF9XHdCugziEMXTdfKxGwFsIUxQIbDBT4N\\nBNCcLTXiTEdtjvXxm0TnM5QdPOE36EsI+O4YT8w+C5nlKuNMtsxsJe+bxEfBi2PS\\nJ0vU1bO+4m9p0SDc3h39jb+FLrAnqez2QbT2maby8A8wahunAMWY+ZUkQYoWpilf\\nRkGpiLKlkJ95HCYzmt7IeddH5+ZBuG+Sx4SMwCynn64J/UafNW0XV36dzeLSla59\\nvQCmWAurjAqa8fqepdvNI4I/JxVfeCQwkrZEos0gec+D7qOupfHk3Zyr6G5Zn8kS\\nYRU8HpRIRH4KvsO",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 303,
            "total_chunks": 337
          }
        },
        {
          "id": "4b4db126-3243-4425-ad41-ced88e97941d",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "fNW0XV36dzeLSla59\\nvQCmWAurjAqa8fqepdvNI4I/JxVfeCQwkrZEos0gec+D7qOupfHk3Zyr6G5Zn8kS\\nYRU8HpRIRH4KvsObTNIrW5Z/qbfWAFSTC3q0eflaVLWsrXfSGvBDVqlxO3arhv2q\\nra6tcD7MQOBO226i+v3aL9qJ3viWhIQTvONm7D8U+/WryrChBOHVCQ2M3AZQQLeC\\nqSkJ60wFx8jEqLj9ELWuTMuHYg5lhMZFyLI8iWOvGRPmgTUZKNH74LF1ujIEuMBx\\nE7LBWRGNx2lD0f2aYUdv+qWA8m1ETPyKYme6oUM+kDlf6sstMgahN7zT8jj/W2KD\\ndG+yzHk5G06lSQzXGbec3bi2WOpWHJ1J/kTQ8Af1HuJr4UjQmin8fLW6n06diySA\\nBSHYGWk=\\n-----END CERTIFICATE-----\\n\"         key: \"-----BEGIN PRIVATE KEY-----\\nMIIJQQIBADANBgkqhkiG9w0BAQEFAASCCSswggknAgEAAoICAQCtj437cgRishCl\\n0w9PrEyqSxJLjeDb7jgR1iI82ic/YqMRR0b+DnsIGcg5pR9nK+DcVz1E4EyGphro\\nIYoJ3xh5zuYvhz3u1L3AG1zojR4QGZcB5nMIbLr/Ue1Jxn+YAfD2uyGDAXVKL+P1\\nHg0Dpl6mYa+hAx1Q4YkkeFKwR+i9WhFHkP9U7VSum7gtifPtnCj79ATMltSZJU3n\\nb+01aOBOUcUI51DppkKTAErYMJrdvY0VZI69+GGX/gYst5hnaEbZ/35b4vqyUsxb\\nCkyrLBj1tRef41E5xg+gtfbbaxofqdjvj7T4H1CTgE6U2qE3j3cUKhera7MaZhvS\\nKA7H6jxRVzuByjNuG9ESBbhFFrheBqaidJYJxwzmxAUcVLFoZleiSkTYYm1BPTC/\\n+mKbgq7RcQhAqZkZ6MLV3Dbp9sb4JYNY9exgSZ/hL",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 304,
            "total_chunks": 337
          }
        },
        {
          "id": "1cd71c84-69ac-4808-8b9f-e8287554d6ed",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "RVzuByjNuG9ESBbhFFrheBqaidJYJxwzmxAUcVLFoZleiSkTYYm1BPTC/\\n+mKbgq7RcQhAqZkZ6MLV3Dbp9sb4JYNY9exgSZ/hLlnl7GNKJAI8vZBrAJeAYQk2\\nSJQyPmV3eN5vaRjbbUgpv65ITrBo1O2zic5Fr0O94etQqs2qrffKgRm+Pgr9Bn2n\\nH2SbujQGFH6gw8Bgj9g4OA06FAtJHerafUKN35toV+DhJvwvY5RPJbgp3Y6T2uQK\\nT1b/xP9gqWsEWhKs0e0+EoiLoVkuSuh2fWDz6U+jcTzQafmFRl2Cwhntj5zapxQB\\nW7\n\nAppendix A. Client contract setup sample files 103xf+i0VpabwUKYgPLP5kqb9iBpfhQIDAQABAoICACsovIzfgHmuf/dMcc1FMldS\\njb0eDeGC7ox47FCniw\nT3GUfNqri4jx2nk6PKDPIR9ju0sfaztDPzkFNTK8lioeqA\\nabs97Ue7vWfNJiBqHySvyF5fmRFqQGIHVHN5GfeJ3Aru49l4/lqxaAVnMKNMttK3\\nDf6DEMIxI3JfPWi6qQSVJiDezK+oyNsWvAkO+gqHP6XPu3XIuBtRLHs12Q3kA4tW\\nSCH7q6I+huWZOANkqs4jObctJ1XUMyihsZVjHlHwm1XQc/KTkfXQIyMsf349XAOV\\nwccvtt4gA3jaZwWPL5LaIKkJ2l2tI9NaH7BiYZ64XUs1YGdvQ7130MlEztAlzlOe\\n9M4tkvdELLcvyByEsY3JaObWe/N/RPk3vom4EP/XF+dTnIXRO0nLDP5Kwzj1Cpwb\\nRh7Jp6dmfOpBMLKtb5iEKUROPjGJT+jKORhWaTwo4zmqj6EHp1Z2cUeZdvZomQWM\\nb75xNoJyBKooLAafdGWO0ADR1nbK56+RpbF07/xHHdWR1SuJE6vppkQ33WOsLMJb\\nCo141AG+5NRPe5bn5KH9KrgsJTNPpl",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 305,
            "total_chunks": 337
          }
        },
        {
          "id": "230464c7-e140-45bf-8ffd-1bde3fd2178f",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "WM\\nb75xNoJyBKooLAafdGWO0ADR1nbK56+RpbF07/xHHdWR1SuJE6vppkQ33WOsLMJb\\nCo141AG+5NRPe5bn5KH9KrgsJTNPplhNfq5KGE+xb+gfIH71KuxMgHafBp2Ng432\\njGr4ZfBJy/w8cS0jLWrzAPEvz1ZmhIEGNeiOs78QO6efeT4fCAohw7qQur23K8YB\\nTyVFdUaq63ndFq3kqBGtAoIBAQDZPs86SyDwvLttaLlgpE8z+XgxLRWZRwPCQ+yV\\nAhJKaehxyavAPkp+k5f1EaAL9g20ZCzMA3N8imsEe8zvbrDuR/xBCIUGppmQnD/E\\nCUQk4Un63znNZdJ+h7cn/kysi7D0od9oHgYxI3oj0VhPNkdwm4GSJ8lvXrL7RD/f\\ncXFKrzIOmdkOJY1DydtRAS772MKXVURxaFZ3kePFETloqiqgBIaKt1gw9uSrtkg8\\necW3NVQDI521Q/uNYQaqA2AKGmLXC9Cg4GCfxseaiLxWaEsd+cOAzzoU8v+8nPRo\\ntVdKPEg//b0TJArxh4ZQVOxogLVbgW2JoY9gsaslqZpQaRbLAoIBAQDMhcBVG8vO\\nqsmqv2qV0+251KLt5Y2hfU5k3OONsIAQl1a7sKOY4eXiVpKzT3J/emZeLsQmHnw2\\nRdIXqjaVjH5jNADV9tHsQZ2KA0qV2JO/VK7fQtjV8XIaHh/gIAXXerSyLdh7rdHp\\ng+xfHaDB1kKUmZR6MtB87hlQ6ngBI49/7xgJReTeee2oj4sN10ALVi51XyNfZ+FV\\nQu8D2VP6ssn70qvempzLaP70ZNj2eES7KK8XEm7x7Stv0ptZl7n+E+OqZ1i4OVcF\\n7UCRBCrmDYWYCLmajmR04zyBeppJfSRbH3y3mXBeNwmlFqmQz5NVNDg/YkcPbM4O\\nakCX7ZXPH0jvAoIBAFhZx/NgLIRbbSpAxet8x01O7sepGzib/fZao3OyRPgIfGUS\\nbIwhiT",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 306,
            "total_chunks": 337
          }
        },
        {
          "id": "af2e7c0e-526f-4886-992d-ad3f249023fe",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "BeNwmlFqmQz5NVNDg/YkcPbM4O\\nakCX7ZXPH0jvAoIBAFhZx/NgLIRbbSpAxet8x01O7sepGzib/fZao3OyRPgIfGUS\\nbIwhiTBTHCCpy1ox9j7f4qwR1zzWGlHXe3AAp2ow0nEsYtVimd+K/A/g6NrK2Mhz\\nUlGrUGDvFtjn/gzKPuwujOoOE9yWHg1FDVIhtAoi5B4pmi116PpxNjzMKRQDjisL\\n/I9ZTEs+Y7hc79uyuujK36vzj/7O0UALEjrzwaQUUxdFG1PGhRckadpWd8dbo9An\\nAvN+M2a7B/fKqZtSQdJNVsqmlgVE1VaOt3G4tpv5QL45CNkOPl1Zw7h1z4s8WvHT\\nYrrPFLhHsqMm9oJFnfwZ9g9cKjBb8Uu+3yhGpOMCggEAeHzfZwReGB2zev0TvLrC\\nlTS426/dtWKN2YvsHt/5Qkz2EtKoPnvuo13fRPWr/X/NaPTiJ5bUFGEjuT9Ustu2\\n5ZiQWXz0BNxPBCyWNxsFR7WK5AqMldWNI+fVXYNgDabDZyjtHUe0n35RtWNN/oPM\\na6DiwO7ItqDKl0nacslRU8w2e9gKUirAoQoXoIrLtyIJcqoeu6kGLeWly72v5MSJ\\ni+p7yEOL1aXAdZgn3WPTEfOQ2uXIKIxRh6oqTSi+sPlkqVIDCVz2cI5p+ETdRPR4\\nXK3fMjdq5RWt4pWo6VxpG6m8HqmtckO4UeK8+IvhP1PpQyYRuPuflQxxi0+zbvb+\\nTwKCAQAtxUAS8r+AP3Uufi9DvujI5z3+mWqZiM5Mxg8OJq0qNPE8V6gfrSspEgDt\\nHWF8TUNoATWLCCak1u9ImBqiPZMH9WfRXaLSofrFJsVTFt+5ZeT6QMnc0RnBZakL\\nvJMX9rKkb98leIRfCwzlnBQ84IFM41e0F15+853aIibpBAI7BEfTvJ8Eg/m20w1H\\nrPRP1j6GYhpkAIm2+TVx6DFY/JO6JM1i0tzHv7zihSeji0lw",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 307,
            "total_chunks": 337
          }
        },
        {
          "id": "82ebb221-faae-4e79-a405-eb39a6198ab6",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "RfCwzlnBQ84IFM41e0F15+853aIibpBAI7BEfTvJ8Eg/m20w1H\\nrPRP1j6GYhpkAIm2+TVx6DFY/JO6JM1i0tzHv7zihSeji0lwBMKJ7M0TRXz1dJeR\\n3GsDlD7mKwLVaBBKQ1Uxh1zYbiaUzVst1S2Wdvt13f89IV4Mmmuq2v1Uz4je7pDB\\nhJITxResgCTR2aD0nMzF8egEKJoY\\n-----END PRIVATE KEY-----\\n\"\nSample script for certificate or key files\nThis simple sample script can be  used to output certificate or key files into double-quoted \nscalar values for YAML f iles. See Example A-3.\nExample A-3   Script for certificate or key files\n~# cat yaml_doublequoted_input.sh \n#!/bin/bash\nsed 's/$/\\\\n/' $1 | tr -d '\\n'\necho \"\"\nSee Example A-4 for how to use the script.\nExample A-4   Making use of the script\n~# cat ca.pem \n-----BEGIN CERTIFICATE-----MIIEuDCCAyCgAwIBAgIUBR9g6L5hivov7eNT00HSXW39oD0wDQYJKoZIhvcNAQELBQAwXzELMAkGA1UEBhMCVVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFDASBgNVBAcM\n\n104 Applying Data Protection and Confident iality in a Hybrid Cloud EnvironmentC0xvcyBBbmdlbGVzMQwwCgYDVQQKDANJQk0xFzAVBgNVBAMMDmNhLmV4YW1wbGUu\nb3JnMB4XDTIzMDEyNzE4MzkzMFoXDTI1MDExNj",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 308,
            "total_chunks": 337
          }
        },
        {
          "id": "38dc19e6-61e6-47c1-8320-9ce7a5935e84",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "vcyBBbmdlbGVzMQwwCgYDVQQKDANJQk0xFzAVBgNVBAMMDmNhLmV4YW1wbGUu\nb3JnMB4XDTIzMDEyNzE4MzkzMFoXDTI1MDExNjE4MzkzMFowXzELMAkGA1UEBhMCVVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFDASBgNVBAcMC0xvcyBBbmdlbGVzMQwwCgYDVQQKDANJQk0xFzAVBgNVBAMMDmNhLmV4YW1wbGUub3JnMIIBojANBgkqhkiG9w0BAQEFAAOCAY8AMIIBigKCAYEAsgnjL9ZxtdlQf1IAwEWkcPXQEHA47LY03BO7c5jai9vRSGxa16hj6SUOidzKj4lK7w9REhxnaihkk1P8zfF9Eld/FvMKANpdaniOJUFT/FXzlwG1c+R+h3ZTv4LiEi9SzWwkbNBOhMN5YM7nCOWNxQBHptisNvdRqb6VidQfneU0aesmmu61IDeB54wpsZXiRXB7M2fpx3c/9ppFYSCGWpC0Bibu12r6kg85EykR4zjEQiVg4U9m4zchQE73Ljl+pDHaGuJlMlfKgPKwAdmPEGK8IBctb3WbuBffojmgRerlAEajDlDD0IQE4s+dX/F+bZCZiJrfpHz8CAs6i/GPqVR1U2jP7zaAJqgsKNfWm0CXcLDsdbxgGNU8ECzVKw9pWPcG4A4bxiznZpgWXwoBHmnbqriZe1KEAjRMfGCGJI3xyODCsT/64zGIwX1ajpn9b6cWnAGoXrjOc5HSXnPGyiRDc/7jmmSXcOKdCd3SaQu/ahNHXjr5X1dQ7XrzgY/1AgMBAAGjbDBqMA4GA1UdDwEB/wQEAwIChDAPBgNVHRMBAf8EBTADAQH/MEcGA1UdEQRAMD6CEnNlcnZlci5leGFtcGxlLm9yZ4IXc2VydmVyLmV4YW1wbGUub3JnOjY1MTSCCTkuMjAuNy45MocECRQHXDANBgkqhkiG9w0BAQsFAAOCAYEAfLdjI2j0XbVqbZcQNBIPbyL1fh",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 309,
            "total_chunks": 337
          }
        },
        {
          "id": "8f7a4f58-7b23-4244-b9fc-1f7fa419fab2",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "VyLmV4YW1wbGUub3JnOjY1MTSCCTkuMjAuNy45MocECRQHXDANBgkqhkiG9w0BAQsFAAOCAYEAfLdjI2j0XbVqbZcQNBIPbyL1fhsNX0I0NvPRU9tpA1M41+UrdCFNQKivai7jKsATzDZZVG0Adprki7YFqUocZ/NpwC6GHFETrTCICaVrjUuF2ecgiwXLZIiHWV+qOAbycG9VoUw7eRVKarfbmd31YwZ1il27e44V1mjVd4gdJvV6a4H65lm3fobogin9535lnJuZjHQNQ7cbN1hrKOkNrvZpSfe5lZ6EFUZHk3S/Wdux31rz1cQQAhpFfJL84KDvM/cBISKGDutgAADCb9jtH8q+ow19n7R1ff10r4/9G7CA9mv6llJf8+P8Y22CGDvez3YPs3Dt7AnZe7bEEpQg1EDlgVq+WzbyEcmmaVwJcj5o7annifkUIrmJOiACtDlklQXYIiZcyg8zfLIcFYkf+PHpvtYF8c5Av4DS2YFPwtc5OoLzxUOI54V6pJZBc60aL9vNyxBAKj/cQEQKrLtUGr1vjPTOFycNW5yQAAhDL6o6E+3SuTEClFsd2QMaCJuQ-----END CERTIFICATE-----~#~# ./yaml_doublequoted_input.sh ca.pem -----BEGIN CERTIFICATE-----\\nMIIEuDCCAyCgAwIBAgIUBR9g6L5hivov7eNT00HSXW39oD0wDQYJKoZIhvcNAQEL\\nBQAwXzELMAkGA1UEBhMCVVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFDASBgNVBAcM\\nC0xvcyBBbmdlbGVzMQwwCgYDVQQKDANJQk0xFzAVBgNVBAMMDmNhLmV4YW1wbGUu\\nb3JnMB4XDTIzMDEyNzE4MzkzMFoXDTI1MDExNjE4MzkzMFowXzELMAkGA1UEBhMC\\nVVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFDASBgNVBAcMC0xvcyBBbmd",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 310,
            "total_chunks": 337
          }
        },
        {
          "id": "c3c8010f-bde8-4818-a42a-329b879f68de",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "zkzMFoXDTI1MDExNjE4MzkzMFowXzELMAkGA1UEBhMC\\nVVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFDASBgNVBAcMC0xvcyBBbmdlbGVzMQww\\nCgYDVQQKDANJQk0xFzAVBgNVBAMMDmNhLmV4YW1wbGUub3JnMIIBojANBgkqhkiG\\n9w0BAQEFAAOCAY8AMIIBigKCAYEAsgnjL9ZxtdlQf1IAwEWkcPXQEHA47LY03BO7\\nc5jai9vRSGxa16hj6SUOidzKj4lK7w9REhxnaihkk1P8zfF9Eld/FvMKANpdaniO\\nJUFT/FXzlwG1c+R+h3ZTv4LiEi9SzWwkbNBOhMN5YM7nCOWNxQBHptisNvdRqb6V\\nidQfneU0aesmmu61IDeB54wpsZXiRXB7M2fpx3c/9ppFYSCGWpC0Bibu12r6kg85\\nEykR4zjEQiVg4U9m4zchQE73Ljl+pDHaGuJlMlfKgPKwAdmPEGK8IBctb3WbuBff\\nojmgRerlAEajDlDD0IQE4s+dX/F+bZCZiJrfpHz8CAs6i/GPqVR1U2jP7zaAJqgs\\nKNfWm0CXcLDsdbxgGNU8ECzVKw9pWPcG4A4bxiznZpgWXwoBHmnbqriZe1KEAjRM\\nfGCGJI3xyODCsT/64zGIwX1ajpn9b6cWnAGoXrjOc5HSXnPGyiRDc/7jmmSXcOKd\\nCd3SaQu/ahNHXjr5X1dQ7XrzgY/1AgMBAAGjbDBqMA4GA1UdDwEB/wQEAwIChDAP\\nBgNVHRMBAf8EBTADAQH/MEcGA1UdEQRAMD6CEnNlcnZlci5leGFtcGxlLm9yZ4IX\\nc2VydmVyLmV4YW1wbGUub3JnOjY1MTSCCTkuMjAuNy45MocECRQHXDANBgkqhkiG\\n9w0BAQsFAAOCAYEAfLdjI2j0XbVqbZcQNBIPbyL1fhsNX0I0NvPRU9tpA1M41+Ur\\ndCFNQKivai7jKsATzDZZVG0Adprki7Y",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 311,
            "total_chunks": 337
          }
        },
        {
          "id": "1e020568-a75b-498b-9041-8e2253504433",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "G\\n9w0BAQsFAAOCAYEAfLdjI2j0XbVqbZcQNBIPbyL1fhsNX0I0NvPRU9tpA1M41+Ur\\ndCFNQKivai7jKsATzDZZVG0Adprki7YFqUocZ/NpwC6GHFETrTCICaVrjUuF2ecg\\niwXLZIiHWV+qOAbycG9VoUw7eRVKarfbmd31YwZ1il27e44V1mjVd4gdJvV6a4H6\\n5lm3fobogin9535lnJuZjHQNQ7cbN1hrKOkNrvZpSfe5lZ6EFUZHk3S/Wdux31rz\\n1cQQAhpFfJL84KDvM/cBISKGDutgAADCb9jtH8q+ow19n7R1ff10r4/9G7CA9mv6\\nllJf8+P8Y22CGDvez3YPs3Dt7AnZe7bEEpQg1EDlgVq+WzbyEcmmaVwJcj5o7ann\\nifkUIrmJOiACtDlklQXYIiZcyg8zfLIcFYkf+PHpvtYF8c5Av4DS2YFPwtc5OoLz\\nxUOI54V6pJZBc60aL9vNyxBAKj/cQEQKrLtUGr1vjPTOFycNW5yQAAhDL6o6E+3S\\nuTEClFsd2QMaCJuQ\\n-----END CERTIFICATE-----\\n:~#  \nThe contents of ca.pem in Example A-2 on page 101 is the literal scalar example in \nExample A-1 on page 100. After running the command ./yaml_doublequoted_input.sh \nca.pem  the contents have been modified as such that it can be copied and pasted inside a \n\nAppendix A. Client contract setup sample files 105double-quote (\"\") in a YAML file as can be seen in the double-quoted scalar example in \nExample A-2 on page 101",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 312,
            "total_chunks": 337
          }
        },
        {
          "id": "9c39dd4b-e6dd-4b86-8690-2089a7dff8a7",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "e (\"\") in a YAML file as can be seen in the double-quoted scalar example in \nExample A-2 on page 101.\n\n106 Applying Data Protection and Confident iality in a Hybrid Cloud Environment\n\n© Copyright IBM Corp. 2024. 107Appendix B. Creating a Hyper Protect Virtual \nServer for VPC \nThis appendix provides a walkthrough that shows how to create a Hyper Protect Virtual \nServer (HPVS) instance by using the IBM Cloud Virtual Private Cloud (VPC) user interface (UI).B\n\n108 Applying Data Protection and Confident iality in a Hybrid Cloud EnvironmentUsing the IBM Cloud VPC UI\nIn this section we sh ow the steps that are needed to cr eate a HPVS instance  using the IBM \nCloud VPC UI.\n1. Login to IBM Cloud at https://cloud.ibm.com/login . \n2. From the icons in the left panel, select VPC infrastructure. See Figure B-1.\nFigure B-1   Login to IBM Cloud\n3. Click Virtual server instances  and click Create. See Figure B-2.\nFigure B-2   Virtual server instance for VPC\n\n\nAppendix B. Creating a Hyper Protect Virt",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 313,
            "total_chunks": 337
          }
        },
        {
          "id": "19d1b6d1-6160-4bd4-8fc3-f9a289e058ff",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "Figure B-2.\nFigure B-2   Virtual server instance for VPC\n\n\nAppendix B. Creating a Hyper Protect Virtual Server for VPC 1094. Select the location of your choice, then provide a name for your virtual server. See \nFigure B-3.\nFigure B-3   Select a location\n\n\n110 Applying Data Protection and Confident iality in a Hybrid Cloud Environment5. Under the Image and Profile heading, for the CentOS image, click Change image . See \nFigure B-4.\nFigure B-4   Change image\n\n\nAppendix B. Creating a Hyper Protect Virtual Server for VPC 1116. On the next screen select  IBM Z, LinuxONE, s390 architecture . See Figure B-5.\n7. Select the switch under Confidential computing. \n8. Select the desired image.\nFigure B-5   Select architecture and confidential computing\n\n\n112 Applying Data Protection and Confident iality in a Hybrid Cloud Environment9. Click Save.\n10.Select the profile and under the Data volumes heading, select Create . See Figure B-6. \nFigure B-6   Select a profile and storage\n11.Input the contents",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 314,
            "total_chunks": 337
          }
        },
        {
          "id": "ac9aba63-9027-4112-93f4-a08661814c1b",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ng, select Create . See Figure B-6. \nFigure B-6   Select a profile and storage\n11.Input the contents of user_data.yam l prepared in 4.2.4, “Prepare user_data.yaml” on \npage 73 to “User data” field. See Figure B-7.\n12.Click Create virtual server .\nFigure B-7   Create a virtual server\n\n\nAppendix B. Creating a Hyper Protect Virtual Server for VPC 11313.Monitor the serial console to see the boot process of the virtual server instance. See \nFigure B-8.\nFigure B-8   Open serial console\n14.View the messages in the serial console. See Figure B-9. If there are any errors, the \nvirtual server instance shuts down. \nFigure B-9   Virtual server boot\n\n\n114 Applying Data Protection and Confident iality in a Hybrid Cloud Environment15.Monitor IBM Log Analysis to see if the container workload is started successfully. See \nFigure B-10.\nFigure B-10   Check the container workload\n\n\n© Copyright IBM Corp. 2024. 115Appendix C. Additional examples for HPSB \nand HPVS\nThis appendix contains the co mplete exampl",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 315,
            "total_chunks": 337
          }
        },
        {
          "id": "ecd747c5-275d-4637-8943-5ceb2f3a432e",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "4. 115Appendix C. Additional examples for HPSB \nand HPVS\nThis appendix contains the co mplete examples of  the Hyper Protect Secure Build (HPSB) \nprocess from Example 4-19 on page 76 and HPVS instance verify ing disk (volume) \nencryption from Example 4-38 on page 88.\n/SM590000“Hyper Protect Secure Build log” on page 116\n/SM590000“How to verify disk (volume) encryption with HPL13000I” on page 118C\n\n116 Applying Data Protection and Confident iality in a Hybrid Cloud EnvironmentHyper Protect Secure Build log\nThe ./build.py log --log build --env  command can be used to  view the HPSB log. See \nExample C-1.\nExample C-1   HPSB log\n$ ./build.py log --log build --env sbs-config.jsonINFO:__main__:2023-07-25 08:59:36,446  build_task               INFO    starting a buildINFO:__main__:2023-07-25 08:59:36,447  build_task               INFO    cleaning up the local github repo and the github access credentialINFO:__main__:2023-07-25 08:59:36,447  clean_up                 INFO    github_dir=paynow-w",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 316,
            "total_chunks": 337
          }
        },
        {
          "id": "2a1edfea-1cec-4e3a-a8f6-5052565b00c5",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "redentialINFO:__main__:2023-07-25 08:59:36,447  clean_up                 INFO    github_dir=paynow-websiteINFO:__main__:2023-07-25 08:59:36,448  build_task               INFO    cloning a github repoINFO:__main__:2023-07-25 08:59:36,448  clone_github_repo        INFO    github_host=github.comINFO:__main__:2023-07-25 08:59:36,448  clone_github_repo        INFO    github_dir=paynow-websiteINFO:__main__:2023-07-25 08:59:36,626  run                      INFO    run: Cloning into 'paynow-website'...INFO:__main__:2023-07-25 08:59:37,010  clone_github_repo        INFO    image_tag=v3-f29b1abINFO:__main__:2023-07-25 08:59:37,011  build_task               INFO    building a docker containerINFO:__main__:2023-07-25 08:59:37,011  build_docker_image       INFO    github_dir=paynow-websiteINFO:__main__:2023-07-25 08:59:37,025  run                      INFO    run:             environment-variable.INFO:__main__:2023-07-25 08:59:37,025  run                      INFO    run:  INFO:__main__:2023-07-25 ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 317,
            "total_chunks": 337
          }
        },
        {
          "id": "2f78a4b9-bf3c-41ee-8741-eb21eadeb514",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "O:__main__:2023-07-25 08:59:37,025  run                      INFO    run:  INFO:__main__:2023-07-25 08:59:37,129  run                      INFO    run: Sending build context to Docker daemon  2.842MBINFO:__main__:2023-07-25 08:59:37,131  run                      INFO    run: Step 1/7 : FROM node:19\nINFO    run: 5ac921848b31: Waiting\nINFO:__main__:2023-07-25 08:59:37,393  run                      INFO    run: 86b7a0ecd4be: WaitingINFO:__main__:2023-07-25 08:59:37,717  run                      INFO    run: e4000487deec: Verifying Checksum INFO:__main__:2023-07-25 08:59:37,719  run                      INFO    run: e4000487deec: Download completeINFO:__main__:2023-07-25 09:00:12,822  run                      INFO    run: e4000487deec: Pull completeINFO:__main__:2023-07-25 09:00:18,052  run                      INFO    run: Digest: sha256:92f06fc13bcc09f1ddc51f6ebf1aa3d21a6532b74f076f224f188bc6b9317570INFO:__main__:2023-07-25 09:01:16,173  run                      INFO    run: Status: Down",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 318,
            "total_chunks": 337
          }
        },
        {
          "id": "4cc3e346-21a8-491a-b790-dfc0a77797e9",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "bc6b9317570INFO:__main__:2023-07-25 09:01:16,173  run                      INFO    run: Status: Downloaded newer image for node:19INFO:__main__:2023-07-25 09:01:16,175  run                      INFO    run:  ---> f2e8386523b1INFO:__main__:2023-07-25 09:01:16,175  run                      INFO    run: Step 2/7 : WORKDIR /appINFO:__main__:2023-07-25 09:01:29,718  run                      INFO    run: Step 3/7 : COPY app/package*.json ./\n\nAppendix C. Additional examples for HPSB and HPVS 117INFO:__main__:2023-07-25 09:01:29,878  run                      INFO    run: Step \n4/7 : RUN npm install INFO:__main__:2023-07-25 09:01:33,744  run                      INFO    run: added 65 packages, and audited 66 packages in 3s INFO:__main__:2023-07-25 09:01:35,142  run                      INFO    run: Step 5/7 : COPY app/ . INFO:__main__:2023-07-25 09:01:35,450  run                      INFO    run:  ---> 1ed7cc739746 INFO:__main__:2023-07-25 09:01:35,450  run                      INFO    run: Ste",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 319,
            "total_chunks": 337
          }
        },
        {
          "id": "12591d28-f449-4d5a-a6d4-f66229f54365",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "  ---> 1ed7cc739746 INFO:__main__:2023-07-25 09:01:35,450  run                      INFO    run: Step 6/7 : EXPOSE 8443INFO:__main__:2023-07-25 09:01:35,572  run                      INFO    run: Step 7/7 : CMD npm startINFO:__main__:2023-07-25 09:01:35,594  run                      INFO    run:  ---> Running in fcba8cd7f8acINFO:__main__:2023-07-25 09:01:35,678  run                      INFO    run: Removing intermediate container fcba8cd7f8acINFO:__main__:2023-07-25 09:01:35,679  run                      INFO    run: Successfully built 9cae137191b0INFO:__main__:2023-07-25 09:01:35,683  run                      INFO    run: Successfully tagged devuser/samplepaymentsystem:latestINFO:__main__:2023-07-25 09:01:35,684  run                      INFO    run: return code = 0INFO:__main__:2023-07-25 09:01:35,748  run                      INFO    run: return code = 0INFO:__main__:2023-07-25 09:01:35,749  build_task               INFO    pushing a container to a docker hub INFO:__main__:2023-07-",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 320,
            "total_chunks": 337
          }
        },
        {
          "id": "5a71bdb4-8e5a-48af-a3ad-b5134b3b84b3",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ":35,749  build_task               INFO    pushing a container to a docker hub INFO:__main__:2023-07-25 09:01:35,860  run                      INFO    run: INFO:__main__:2023-07-25 09:01:35,860  run                      INFO    run: Login SucceededINFO:__main__:2023-07-25 09:01:35,861  run                      INFO    run: return code = 0INFO:__main__:2023-07-25 09:01:35,910  run                      INFO    run: The push refers to repository [docker.io/devuser/samplepaymentsystem]INFO:__main__:2023-07-25 09:01:35,922  run                      INFO    run: fb4fa3257dd1: PreparingINFO:__main__:2023-07-25 09:01:35,923  run                      INFO    run: fda0660b571f: WaitingINFO:__main__:2023-07-25 09:01:36,422  run                      INFO    run: ef13dc0a223f: Mounted from library/nodeINFO:__main__:2023-07-25 09:01:37,211  run                      INFO    run: ef13dc0a223f: PushedINFO:__main__:2023-07-25 09:01:38,885  run                      INFO    run: v3-f29b1ab: digest: sha256:",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 321,
            "total_chunks": 337
          }
        },
        {
          "id": "36a0183e-f619-4185-bfee-fcb6faf8c8e6",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": ":__main__:2023-07-25 09:01:38,885  run                      INFO    run: v3-f29b1ab: digest: sha256:d10e26e72a2f83a3fdf8a6a79da5b88f1b6747ce0af9309749afc55295973bd8 size: 2839INFO:__main__:2023-07-25 09:01:38,888  run                      INFO    run: Signing and pushing trust metadataINFO:__main__:2023-07-25 09:01:39,289  run                      INFO    run: Finished initializing \"docker.io/devuser/samplepaymentsystem\"INFO:__main__:2023-07-25 09:01:39,388  run                      INFO    run: Successfully signed docker.io/devuser/samplepaymentsystem:v3-f29b1abINFO:__main__:2023-07-25 09:01:39,390  run                      INFO    run: return code = 0INFO:__main__:2023-07-25 09:01:39,448  run                      INFO    run: The push refers to repository [docker.io/devuser/samplepaymentsystem]INFO    run: ef13dc0a223f: Layer already exists\n\n118 Applying Data Protection and Confident iality in a Hybrid Cloud EnvironmentINFO:__main__:2023-07-25 09:01:40,152  run                      I",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 322,
            "total_chunks": 337
          }
        },
        {
          "id": "1e7f0945-413b-4ec0-9a04-0d03c2c2ea09",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ality in a Hybrid Cloud EnvironmentINFO:__main__:2023-07-25 09:01:40,152  run                      INFO    run: \nlatest: digest: sha256:d10e26e72a2f83a3fdf8a6a79da5b88f1b6747ce0af9309749afc55295973bd8 size: 2839INFO:__main__:2023-07-25 09:01:40,154  run                      INFO    run: Signing and pushing trust metadataINFO:__main__:2023-07-25 09:01:40,398  run                      INFO    run: Successfully signed docker.io/devuser/samplepaymentsystem:latestINFO:__main__:2023-07-25 09:01:40,399  run                      INFO    run: return code = 0INFO:__main__:2023-07-25 09:01:40,400  build_task               INFO    extracting an image keyid and keyINFO:__main__:2023-07-25 09:01:40,400  build_task               INFO    docker contrust value:https://notary.docker.ioINFO:__main__:2023-07-25 09:01:40,400  build_task               INFO    entered dctINFO:__main__:2023-07-25 09:01:40,400  extract_image_key_id     INFO    keyid=60340916db8868c4db38f99a9928829332b341ca5313f548ec25cc015102c",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 323,
            "total_chunks": 337
          }
        },
        {
          "id": "e23bd6b6-10b0-4458-8a54-b5d08f88ec1e",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "extract_image_key_id     INFO    keyid=60340916db8868c4db38f99a9928829332b341ca5313f548ec25cc015102c140INFO:__main__:2023-07-25 09:01:40,400  extract_image_key_id     INFO    publickey=LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJsVENDQVR1Z0F3SUJBZ0lRV2RmQStQcm9tVjRwd2RVS01KYnFqREFLQmdncWhrak9QUVFEQWpBeE1TOHcKTFFZRFZRUURFeVprYjJOclpYSXVhVzh2WVdKb2FYSmhiV3N2YzJGdGNHeGxjR0Y1YldWdWRITjVjM1JsYlRBZQpGdzB5TXpBM01qVXdPVEF4TXpoYUZ3MHpNekEzTWpJd09UQXhNemhhTURFeEx6QXRCZ05WQkFNVEptUnZZMnRsCmNpNXBieTloWW1ocGNtRnRheTl6WVcxd2JHVndZWGx0Wlc1MGMzbHpkR1Z0TUZrd0V3WUhLb1pJemowQ0FRWUkKS29aSXpqMERBUWNEUWdBRWplMmFPYTVPYUE4UHJFUThHNTgybTZxWmlJaEFvYWo2bDZaaThsaDFwM01VbTBLNAp4TVBJcytZNHA2TzVzeE9tRFpFaW9SbmJOeU1NRDJrN05zNXVLYU0xTURNd0RnWURWUjBQQVFIL0JBUURBZ1dnCk1CTUdBMVVkSlFRTU1Bb0dDQ3NHQVFVRkJ3TURNQXdHQTFVZEV3RUIvd1FDTUFBd0NnWUlLb1pJemowRUF3SUQKU0FBd1JRSWdZUnZObW1nRkg1dTBSNnlENUhxcDFTcW9zM2k5cVczbWxRWVlJN2oyZXJVQ0lRRFZGNjhXbXo0RQplUXExeVJvaHIwZXpZck52OEh4eXQvUS9CMDBlQUY1NzlRPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 324,
            "total_chunks": 337
          }
        },
        {
          "id": "58df764e-0780-4b8c-b11c-48c2d77bbdf7",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "RRFZGNjhXbXo0RQplUXExeVJvaHIwZXpZck52OEh4eXQvUS9CMDBlQUY1NzlRPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=INFO:__main__:2023-07-25 09:01:40,400  build_task               INFO    generating a config fileINFO:__main__:2023-07-25 09:01:40,903  digest                   INFO    digest=636a6f000b503de6ac9f65c9c226b20d6ee72d513ed578547433f3a093061df0INFO:__main__:2023-07-25 09:01:41,284  build_task               INFO    completed a buildINFO:__main__:\nHow to verify disk (volum e) encryption with HPL13000I\nUser data volumes in the HPVS instance are encrypted with Li nux Unified Key Setup (LUKS) \nencryption. The encryption status can be verified by checking the messages in the log. Example C-2 shows verification of disk (volume) encryption. \nExample C-2   Verifying disk (volume) encryption\nconmon[991]: HPL13000I...verify-disk-encryption: Return value for disk-encrypt: 0verify-disk-encryption: Executed cmd: ('lsblk', '-b', '-n', '-o', 'NAME,SIZE')verify-disk-encryption: Return value: 0verify-disk-enc",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 325,
            "total_chunks": 337
          }
        },
        {
          "id": "831929ef-5537-4094-a775-8fa178f86a31",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " cmd: ('lsblk', '-b', '-n', '-o', 'NAME,SIZE')verify-disk-encryption: Return value: 0verify-disk-encryption: Stdout: vda                                           107374182400verify-disk-encryption: ??vda1                                          4292870144verify-disk-encryption: ??vda2                                        103079215104\n\nAppendix C. Additional examples for HPSB and HPVS 119verify-disk-encryption:   ??luks-4089f10e-284b-43aa-ac29-37203fb046c9 103062437888\nverify-disk-encryption: vdb                                                 391168verify-disk-encryption: vdc                                                  45056verify-disk-encryption: List of volumes greater than or equal to 10GB are: ['/dev/vda']verify-disk-encryption: Updated Volumes list: ['/dev/vda2']verify-disk-encryption: Executed cmd: ('lsblk', '/dev/vda2', '-b', '-n', '-o', 'NAME,MOUNTPOINT')verify-disk-encryption: Return value: 0verify-disk-encryption: Stdout: vda2verify-disk-encryption: ??luks-4089f10e-2",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 326,
            "total_chunks": 337
          }
        },
        {
          "id": "9d06e863-0a9f-488d-b397-13c75c434816",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "yption: Return value: 0verify-disk-encryption: Stdout: vda2verify-disk-encryption: ??luks-4089f10e-284b-43aa-ac29-37203fb046c9 /verify-disk-encryption: Boot volume is /dev/vda2verify-disk-encryption: Volume /dev/vda2 has mount point /verify-disk-encryption: List of mounted volumes are: ['/dev/vda2']verify-disk-encryption: Verifying the boot disk /dev/vda2 is encrypted or notverify-disk-encryption: Executed cmd: ('lsblk', '/dev/vda2', '-b', '-n', '-o', 'NAME,TYPE')verify-disk-encryption: Return value: 0verify-disk-encryption: Stdout: vda2                                        partverify-disk-encryption: ??luks-4089f10e-284b-43aa-ac29-37203fb046c9 cryptverify-disk-encryption: Executed cmd: ('cryptsetup', 'isLuks', '/dev/vda2')verify-disk-encryption: Return value: 0verify-disk-encryption: Executed cmd: ('cryptsetup', 'luksDump', '/dev/vda2')verify-disk-encryption: Return value: 0verify-disk-encryption: HPL13003I: Checked for mount point /, LUKS encryption with 1 key slot foundverify-disk",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 327,
            "total_chunks": 337
          }
        },
        {
          "id": "df9224c1-f754-44d0-bf51-896b30534fb9",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "k-encryption: HPL13003I: Checked for mount point /, LUKS encryption with 1 key slot foundverify-disk-encryption: HPL13001I: Boot volume and all the mounted data volumes are encrypted\n\n120 Applying Data Protection and Confident iality in a Hybrid Cloud Environment\n\n© Copyright IBM Corp. 2024. 121Appendix D. Encryption keys explained\nKey generation is the process of creating encryption keys that can be used to encrypt and \ndecrypt sensitive data that needs to be protected from unauthorized access. The kind of keys that need to be generated depends on the algorithm used for encryption. Symmetric-key algorithms, such as AES, use a single key for encryption an d decryption. A symmetric- or \nPublic-Key algorithms, such as ECC, use a public-key to encrypt and the corresponding \nprivate-key to decrypt. The keys can be used to generate and verify signatures over data to prove their originality. This is especially useful with asymmetric keys in which the private key \nis used to generate the sign",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 328,
            "total_chunks": 337
          }
        },
        {
          "id": "1f9ea5b0-9476-4e13-a1f5-d1848daac813",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "his is especially useful with asymmetric keys in which the private key \nis used to generate the signatures and the public  key is sufficient to verify the signatures.\nThe keys are created with algorithms designed to ensure that each key is unique and \nunpredictable. The starting point is usually a seed that is used to generate a series of random \nnumbers to increase the statistical randomness of the algorithm used. A seed can be generated in software, but it can be embedded in hardware, which makes it secure and helps Hardware Security Modules (HSMs) generate random numbers. The generated random numbers can be used to generate encryption keys.\nThe same seed, if it is used with the same algorithm, generates the same encryption key. So, \nit is vitally important to protect the seed.\nFor more details on how seeds are used in confidential computing with Hyper Protect Virtual \nServer (HPVS), see IBM Cloud: Securing your data .\nFor further information on random number generation see 4.3.4, “R",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 329,
            "total_chunks": 337
          }
        },
        {
          "id": "72f8596c-edd2-470d-a8ce-9fca01abcaaf",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "ee IBM Cloud: Securing your data .\nFor further information on random number generation see 4.3.4, “Random number \ngeneration” on page 80.\nThis appendix covers the following topics:\n/SM590000“What is a master key (MK)” on page 122\n/SM590000“What are data encryption keys (DEKs)” on page 122\n/SM590000“What are key encryption  keys (KEKs)” on page 123\n/SM590000“Using and protecting keys” on page 123\n/SM590000“How encryption keys are created using GREP11” on page 124D\n\n122 Applying Data Protection and Confident iality in a Hybrid Cloud EnvironmentWhat is a master key (MK)\nA Hardware Security Module (HSM) is a physic al device that is used to protect and manage \ncryptographic keys. Before an HSM can be deployed, it must be initialized in a secure environment with a master key. Because all other keys generated by the HSM are derived from the Master Key, it is important to ensure that the master key is not compromised by any one individual. Hence, the master key is often divided into master ke",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 330,
            "total_chunks": 337
          }
        },
        {
          "id": "da1bb498-11e8-4c47-bed4-a183abf5c138",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " key is not compromised by any one individual. Hence, the master key is often divided into master key-parts with each part being owned by different individuals within an organization. See Figure D-1.\nFigure D-1   Multiple master key parts\nAfter the master key is stored in an HSM through the initialization process, the HSM is ready to derive a Data Encryption Key (DEK) or Key Encryption Keys (KEKs). See Figure D-2.\nFigure D-2   HSM initialization process\nWhat are data encryption keys (DEKs)\nA data encryption key is a type of key that is designed to encrypt and decrypt data. DEKs are \ngenerally provided by the application that owns the data being encrypted. Data is encrypted and decrypted with the help of the same DEK. Hence, a DEK must be stored for at least a specified duration for decrypting the generated cypher text. \n\n\nAppendix D. Encryption keys explained 123There are four levels in a DEK lifecycle:\n1. The key is created using the crypto module of the encryption engine.\n2. The key ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 331,
            "total_chunks": 337
          }
        },
        {
          "id": "9be82337-a32e-4e50-8789-adda7b975112",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "a DEK lifecycle:\n1. The key is created using the crypto module of the encryption engine.\n2. The key is then provided to a key vault and to various other encryption engines.3. The key is used for encrypting and decrypting data.4. The key is then suspended, terminated, or destroyed. \nA DEK can be customized to expire during a particular time frame to prevent data from being \ncompromised. Under such circumstances, it should be used once more for decrypting the data and then the resulting clear text is re-keyed, which means it is encrypted by using a new key.\nWhat are key encryption keys (KEKs)\nAs the name suggests, a key encryption key is used with envelope encryption  to protect a \nDEK from unauthorized use. Envelope encryption is the practice of encrypting plain text data with a data key and then encrypting the data key under another key. \nEnvelope encryption reduces the network load for the application or the cloud service as only \nthe request and fulfillmen t of the much smaller data ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 332,
            "total_chunks": 337
          }
        },
        {
          "id": "36667335-9236-4a87-bafa-bb61c417c9e1",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "the application or the cloud service as only \nthe request and fulfillmen t of the much smaller data key through KMS must go over the \nnetwork.\nUsing and protecting keys\nIf the key-generation is performed within an HSM, the seed stays in the HSM at all times. If the seed is generated within a confidential  computing enclave or Trusted Execution \nEnvironment (TEE), encryption keys generated can be used without exposing them to external threats. To achieve the best security, the technologies can be combined.\nWith a Hyper Protect Crypto Services (HPCS) instance connected to an HPVS instance that \nis protected with Secure Execution for Linux,  a simple yet secure signing setup can be \nestablished. This setup can be used to securely generate and use a certificate authority (CA).\nThe whole process can be built into containe rs running within the H PVS instance. First the \nHPCS instance is used to privately generate a new intermediate key pair. This is wrapped within the HSM and exported into ",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 333,
            "total_chunks": 337
          }
        },
        {
          "id": "ffc72b90-86ad-4bd7-86d9-e62ae769e8db",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "to privately generate a new intermediate key pair. This is wrapped within the HSM and exported into the HPVS, where it is stored into an encrypted data volume. This wrapped key can be used only with the HSM backing the HPCS instance. To use this secure key, the container generates a CA certif icate signing request (CSR). This is then in \nturn signed using an external signer with a well-known root certificate. Alternatively the CSR can be self-signed to produce a new root certificate. The resulting certificate can be used to establish a trusted certificate chain on t he leaf certificates to be generated next.\nIn the next step, the intermediate key pair can be used to sign CSRs by itself. This further \nexpands the certificate chain to leaf certificates to be used for means such as TLS, code signing, and so on. Each of these leaf cert ificate keys can either be generated in the \napplication where it is used or within the secure environment.\n\n124 Applying Data Protection and Confident iali",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 334,
            "total_chunks": 337
          }
        },
        {
          "id": "c817bca4-31b5-4886-ae03-8cab0723f2fd",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": " where it is used or within the secure environment.\n\n124 Applying Data Protection and Confident iality in a Hybrid Cloud EnvironmentHow encryption keys are created using GREP11 \nSecrets like seeds, encryption keys, and certificates are required for applications to perform \ntasks that they are designed to do. Also, the secrets must be protected from unauthorized use.\nWhen you work with a confidenti al computing enclave like HPVS,  the stateless nature of the \nGREP11 (EP11) service offered by HPCS can be used effectively to keep the secrets in a tamper-proof secure  enclave of HPVS. This is  illustrated in Figure D-3.\nFigure D-3   Tamper-proof secure enclave of HPVS\n\n\nISBN DocISBNSG24-8555-00\nISBN DocISBNSG24-8555-00\nISBN DocISBNSG24-8555-00\n(0.1”spine)\n0.1”<->0.169”\n53<->89 pages(0.2”spine)\n0.17”<->0.473”\n90<->249 pages(1.5” spine)\n1.5”<-> 1.998”\n789 <->1051 pages\n(1.0” spine)\n0.875”<->1.498”\n460 <-> 788 pages\n(0.5” spine)\n0.475”<->0.873”\n250 <-> 459 pagesApplying Data Protection and Co",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 335,
            "total_chunks": 337
          }
        },
        {
          "id": "e454a9d5-6135-4ff1-96ba-5344f76cb468",
          "doc_id": "12b4be16-46b7-4277-a787-5335bdea322d",
          "content": "498”\n460 <-> 788 pages\n(0.5” spine)\n0.475”<->0.873”\n250 <-> 459 pagesApplying Data Protection and Conf identiality in a Hybrid Cloud IBM Hyper Protect Platform:\nApplying Data Protection and IBM Hyper Protect Platform:\nApplying Data Protection and \nApplying Data Protection and Confidentiality in a Hybrid Cloud Environment\n\nISBN DocISBNSG24-8555-00\nISBN DocISBNSG24-8555-00\n(2.0” spine)\n2.0” <-> 2.498”\n1052 <-> 1314 pages(2.5” spine) \n2.5”<->nnn.n” \n1315<-> nnnn pagesIBM Hyper Protect \nPlatform:\nIBM Hyper Protect Platform:\nApplying Data Protection and \nConfidentiality in a Hybrid Cloud \n\n\n\nibm.com /redbooksPrinted in U.S.A .Back cover\nISBN 0738461490SG24-8555-00\n®\n\n",
          "metadata": {
            "source": "sg248555.pdf",
            "chunk_index": 336,
            "total_chunks": 337
          }
        }
      ]
    },
    {
      "id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
      "url": "",
      "title": "sg248574.pdf",
      "content_chunks": [
        {
          "id": "6d8a192a-2ca5-4a27-b821-29212f1a9501",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "RedbooksData and AIFront cover\nSimplify Your AI Journey: \nUnleashing the Power of AI with IBM watsonx.ai\nDeepak Rangarao\nPhillip GerrardCharley BellerCarl BrokerDaniele ComiLakshmana EkambaramShuvanker GhoshKaren MedhatPayal PatelMatthew Price\nShirley ShumMark Simmonds\n\n\n\nIBM Redbooks\nSimplify Your AI Journey: Un leashing the Power of AI \nwith IBM watsonx.ai\nJanuary 2025\nSG24-8574-00\n\n© Copyright International Bu siness Machines Corp oration 2025. All rights reserved.\nNote to U.S. Government Users Restricted Rights -- Use, duplication or disclosure re stricted by GSA ADP Schedule\nContract with IBM Corp.First Edition (January 2025)\nThis edition applies to Version 2, Release 1, Modification x of IBM watsonx.ai.Note: Before using this information and the product it supports, read the information in “Notices” on \npage vii.\n\n© Copyright IBM Corp. 2025. iiiContents\nNotices  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 0,
            "total_chunks": 313
          }
        },
        {
          "id": "1567f5df-e112-4c90-810d-398985825829",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\nTrademarks  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\nForeword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\nAuthors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .x\nNow you can become a published author, too!  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiiComments welcome. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xii\nStay connected to IBM Redbooks . . . . . . . . . . . . . . . . . . . . ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 1,
            "total_chunks": 313
          }
        },
        {
          "id": "3a0b203e-81f9-4221-994d-85786329c5eb",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " . . . . . . . . . . . . xii\nStay connected to IBM Redbooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii\nChapter 1.  Competing with artificial intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.1  Competing with AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . 2\n1.2  Challenges in building and deploying AI models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.2.1  Technical considerations for building and deploying AI models . . . . . . . . . . . . . . . 5\n1.3  Opportunities around using AI on trusted data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.3.1  Enhancing decision-making with accurate insights. . . . . . . . . . . . . . . . . . . . . . . . . 61.3.2  Driving operational efficiency  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.3.3  Accelerating innovation. . . ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 2,
            "total_chunks": 313
          }
        },
        {
          "id": "d3b09033-8438-461c-9b50-18359b365ef0",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.3.3  Accelerating innovation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.3.4  Enhancing governance and compliance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71.3.5  Unlocking new revenue streams. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.3.6  Transforming industries with AI and trusted data . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.4  Improving AI model reliability  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.4.1  Enabling cross-enterprise collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1.4.2  Enhancing real-time decision-making . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1.4.3  Scaling AI-driven ecosystems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 3,
            "total_chunks": 313
          }
        },
        {
          "id": "9383cd22-8f81-4bac-b28a-1f9f1b88a09e",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "-driven ecosystems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101.4.4  Driving sustainability and environmental, social, and governance goals  . . . . . . . 10\n1.4.5  Personalizing customer experiences  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.5  Creating new AI-enabled products and services. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\nChapter 2.  Introducing IBM watsonx.ai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.1  Overview of watsonx.ai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.1.1  Key capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.1.2  The watsonx.ai architecture  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142.1.3  watsonx.ai empowering IBM Software offerings . . . . . . . . . . . . . . .",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 4,
            "total_chunks": 313
          }
        },
        {
          "id": "238b1ae3-e294-4f8b-9a0b-fe5ace67b187",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " . . . . . . . . 142.1.3  watsonx.ai empowering IBM Software offerings . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.1.4  Benefits of using watsonx.ai for businesses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.2  Synergy between watsonx.ai and other components in the watsonx platform . . . . . . . 15\n2.2.1  Synergy between watsonx.ai and watsonx.data . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.2.2  Synergy between watsonx.ai and watsonx.governance . . . . . . . . . . . . . . . . . . . . 16\n2.3  Business impact of these synergies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nChapter 3.  Tools for diverse data science teams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n3.1  Key personas for watsonx.ai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.1.1  Data scientists. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 5,
            "total_chunks": 313
          }
        },
        {
          "id": "feabdafa-8546-44c4-884d-f2660ba166f4",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": ".1  Data scientists. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 8\n3.1.2  Machine learning engineers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.1.3  AI engineers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 9\n3.2  Low-code, no-code, and full-code tools  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n3.2.1  No-code, low-code, and full-code tools on IBM watsonx.ai. . . . . . . . . . . . . . . . . . 20\nChapter 4.  Building and using artificial intelligence models  . . . . . . . . . . . . . . . . . . . . 27\n\niv Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai4.1  Prerequisites and assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n4.2  How to use this chapter. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 6,
            "total_chunks": 313
          }
        },
        {
          "id": "3ecf73c3-3f54-44a8-8ae7-495e81ec9aea",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " 28\n4.2  How to use this chapter. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 8\n4.3  Building and using AI models in watsonx.ai  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n4.3.1  Overview of the watsonx.ai platform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284.3.2  Key features and capa bilities  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n4.4  Getting started with watsonx.ai: Setting up the environment  . . . . . . . . . . . . . . . . . . . . 29\n4.5  Data preparation and ingestion for AI model building . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n4.5.1  Understanding the importance of data in AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n4.5.2  Preparing and cleaning data: data quality considerations. . . . . . . . . . . . . . . . . . . 35\n4.5.3  Handling missing data, outliers, and bias . . . . . . . . . . . . .",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 7,
            "total_chunks": 313
          }
        },
        {
          "id": "d62f1282-83f2-43dd-82a7-9169805ecd86",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " . . . . . . . . . . . 35\n4.5.3  Handling missing data, outliers, and bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354.5.4  Ingesting data into watsonx.ai Studio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n4.5.5  Connecting to data repositories and cloud services . . . . . . . . . . . . . . . . . . . . . . . 36\n4.6  Building AI models in watsonx.ai. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n4.6.1  Choosing the right model for your use case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n4.6.2  Model creation workflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n4.7  Deploying AI models in watsonx.ai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n4.7.1  watsonx.ai Studio deployments  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n4.8  watsonx.ai LLM deployment . . . . . .",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 8,
            "total_chunks": 313
          }
        },
        {
          "id": "b5550e9d-94ba-4a58-a6df-bf5c4fff8f96",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n4.8  watsonx.ai LLM deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.8.1  Model packaging and exporting  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.9  Operationalizing machine learning and LLM models  . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.9.1  Calling ML models by using API calls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.9.2  Calling Prompt Lab LLM models by using API ca lls . . . . . . . . . . . . . . . . . . . . . . . 52\n4.9.3  IBM watsonx Assistant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n4.10  Additional information and where to go next . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4.10.1  Additional support and documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554.10.2  watsonx.ai A",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 9,
            "total_chunks": 313
          }
        },
        {
          "id": "e58234bb-ead9-4580-896e-2544416921d0",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554.10.2  watsonx.ai API reference  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.10.3  watsonx.ai data pipeline and orchestration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nChapter 5.  Advanced capabilities of watsonx.ai  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n5.1  Prompt engineering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n5.1.1  Prompting techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585.1.2  Importance of system tokens  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n5.1.3  Model-specific peculiarities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n5.1.4  How watsonx.ai supports prompt engineering  . . . . . . . . . . . . . . . .",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 10,
            "total_chunks": 313
          }
        },
        {
          "id": "baa21d52-f7f3-436e-9637-30f18654b335",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " . . . . . . . 59\n5.1.4  How watsonx.ai supports prompt engineering  . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n5.2  Multitask prompt tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1\n5.2.1  Prompt tuning parameters  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 625.2.2  Interdependencies and holistic tuning strategies  . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5.3  Fine-tuning  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n5.3.1  Challenges with fine-tuning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645.3.2  How watsonx.ai addresses fine-tuning challenges . . . . . . . . . . . . . . . . . . . . . . . . 65\n5.4  InstructLab  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n5.4.1  Advanta",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 11,
            "total_chunks": 313
          }
        },
        {
          "id": "485cbae9-b62e-4d12-b73d-443f8dd30f83",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n5.4.1  Advantages of InstructLab  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 705.4.2  How to use InstructLab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n5.4.3  InstructLab on watsonx.ai Software-as-a-Service. . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.4.4  InstructLab use case examples  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\nChapter 6.  Artificial intelligence agents  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n6.1  What makes an AI agent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n6.2  Why AI agents are needed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n6.3  Multiple AI agents . . . . . . . . . . . . . . . . . . . . . . . . . .",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 12,
            "total_chunks": 313
          }
        },
        {
          "id": "0bbcd1c9-20f2-4622-bc00-a2335189c3ef",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": ". . . . . . . . . . . 94\n6.3  Multiple AI agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . 95\n6.4  AI agents on watsonx.ai  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1006.5  AI agents use case examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\nChapter 7.  Use cases  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n\n Contents v7.1  Using RAG to aid a medical school admissions office  . . . . . . . . . . . . . . . . . . . . . . . . 110\n7.1.1  The challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n7.1.2  The solution  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n7.1.3  Special considerations  . . . . . . . . . . . . . . . . . . . . . . . . . . .",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 13,
            "total_chunks": 313
          }
        },
        {
          "id": "b69cdb82-ff1c-4f8d-9aef-828a67b02214",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": ". . . . . . 110\n7.1.3  Special considerations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n7.2  Embedding workflow automation to streamline recommendations . . . . . . . . . . . . . . . 111\n7.2.1  The challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n7.2.2  The solution  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1127.2.3  Special considerations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\nAbbreviations and acronyms  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\nRelated publications  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\nIBM Redbooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 14,
            "total_chunks": 313
          }
        },
        {
          "id": "a3359cbd-41d0-4450-81d0-143ae74564aa",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\nOnline resources  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\nHelp from IBM  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . 118\n\nvi Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai\n\n© Copyright IBM Corp. 2025. viiNotices\nThis information was developed for prod ucts and services offered in the US . This material might be available \nfrom IBM in other languages. However, you may be required  to own a copy of the product or product version in \nthat language in order to access it. \nIBM may not offer the products, services, or features di scussed in this document in other countries. Consult \nyour local IBM representative for information on the produc ts and services currently available in your area. Any \nreference to an IBM produ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 15,
            "total_chunks": 313
          }
        },
        {
          "id": "3174ad0b-d331-4895-a2a5-84823331a071",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ation on the produc ts and services currently available in your area. Any \nreference to an IBM product, program, or service is not intended to state or imply that only that IBM product, \nprogram, or service may be used. Any functionally equi valent product, program, or service that does not \ninfringe any IBM intellectual property right may be used instead. However, it is t he user’s responsibility to \nevaluate and verify the operation of any non-IBM product, program, or service. \nIBM may have patents or pending patent applications covering subject matter described in this document. The \nfurnishing of this document does not grant you any license to these patents. You can send license inquiries, in \nwriting, to:\nIBM Director of Licensing, IBM Corporation, North Castle Drive, MD-NC119, Armonk, NY 10504-1785, US \nINTERNATIONAL BUSINESS MACHINES CORPORATIO N PROVIDES THIS PUBLICATION “AS IS” \nWITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED \nTO, THE IMPLIE",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 16,
            "total_chunks": 313
          }
        },
        {
          "id": "7c445fdc-3810-4ec9-9a0d-ffd49207f761",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "\nWITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED \nTO, THE IMPLIED WARRANTIES OF NON-INFR INGEMENT, MERCHANTABILITY OR FITNESS FOR A \nPARTICULAR PURPOSE. Some jurisdictions do not allow disclaimer of express or implied warranties in \ncertain transactions, therefore, this statement may not apply to you. \nThis information could include technical inaccuracies or  typographical errors. Changes are periodically made \nto the information herein; th ese changes will be incorporated  in new editions of the publication. IBM may make \nimprovements and/or changes in the product(s) and/or the program(s) described in this publication at any time \nwithout notice. \nAny references in this information to non-IBM websites are provided for convenience only and do not in any \nmanner serve as an endorsement of those websites. The materials at those websites are not part of the \nmaterials for this IBM product and use of those websites is at your own risk. \nIBM may use or di",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 17,
            "total_chunks": 313
          }
        },
        {
          "id": "844019ea-cf8c-4240-9bb8-bd2d13e28495",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "he \nmaterials for this IBM product and use of those websites is at your own risk. \nIBM may use or distribute any of the information you provide in any way it believes appropriate without \nincurring any obligation to you. \nThe performance data and c lient examples cited are presented fo r illustrative purposes only. Actual \nperformance results may vary depending on specific configurations and operating conditions. \nInformation concerning non-IBM products was obtained from the suppliers of those products, their published \nannouncements or other publicly available sources. IBM has not tested those products and cannot confirm the \naccuracy of performance, co mpatibility or any other clai ms related to non-IBM pr oducts. Questions on the \ncapabilities of non-IBM products should be addr essed to the suppliers of those products. \nStatements regarding IBM’s future direction or intent are subject to change or withdrawal without notice, and \nrepresent goals and objectives only. \nThis information",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 18,
            "total_chunks": 313
          }
        },
        {
          "id": "dc623979-1034-4054-b1f5-5674e267dac8",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " to change or withdrawal without notice, and \nrepresent goals and objectives only. \nThis information contains exam ples of data and reports used in daily business operations. To illustrate them \nas completely as possible, the exam ples include the names of individual s, companies, brands, and products. \nAll of these names are fictitious and any similarity to  actual people or business enterprises is entirely \ncoincidental. \nCOPYRIGHT LICENSE:\nThis information contai ns sample application prog rams in source language, which illustrate programming \ntechniques on various operating platforms. You may co py, modify, and distribute these sample programs in \nany form without payment to IBM, for the purposes of developing, using, marketing or distributing application programs conforming to the application programming interface for the operating platform for which the sample \nprograms are written. These examples have not been th oroughly tested under all conditions. IBM, therefore, \ncannot guar",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 19,
            "total_chunks": 313
          }
        },
        {
          "id": "e5491040-80ef-42d1-ad6c-41398c1c338c",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": ". These examples have not been th oroughly tested under all conditions. IBM, therefore, \ncannot guarantee or im ply reliability, serviceability, or function of  these programs. The sample programs are \nprovided “AS IS”, without warranty of any kind. IBM sha ll not be liable for any damages arising out of your use \nof the sample programs. \n\nviii Simplify Your AI Journey: Unleashi ng the Power of AI with IBM watsonx.aiTrademarks\nIBM, the IBM logo, and ibm.com are trademarks or regi stered trademarks of International Business Machines \nCorporation, registered in many jurisdictions worldwide. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on the web at “Copyright \nand trademark information” at https://www.ibm.com/legal/copytrade.shtml  \nThe following terms are trademarks or registered trademarks of International Business Machines Corporation, \nand might also be trademarks or registered trademarks in other countri",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 20,
            "total_chunks": 313
          }
        },
        {
          "id": "941a9b0a-dbbb-4dcb-9f36-cade77bcb3c1",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "siness Machines Corporation, \nand might also be trademarks or registered trademarks in other countries. \nCloudant®\nCognos®\nDataStage®\nDb2®IBM®\nIBM API Connect®\nIBM Cloud®IBM Instana™\nIBM Spectrum®\nIBM Watson®\nInformix®InfoSphere®\nInstana®\nNetezza®Orchestrate®\nRedbooks®\nRedbooks (logo) ®\nSPSS®Turbonomic®\nz/OS®\nThe following terms are trademarks of other companies:\nThe registered trademark Linux® is used pursuant to a sublicense from the Linux Foundation, the exclusive \nlicensee of Linus Torvalds, owner of  the mark on a worldwide basis.\nMicrosoft, and the Windows logo are trademarks of Microsoft Corporation in the United States, other \ncountries, or both.\nJava, and all Java-based trademarks and logos are trademarks or registered trademarks of Oracle and/or its \naffiliates.\nRed Hat, Fedora, OpenShift, are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in \nthe United States and other countries.\nRStudio, and the RStudio logo are registered trademarks of RStudio, I",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 21,
            "total_chunks": 313
          }
        },
        {
          "id": "a7a62aeb-c980-4e42-bc7c-f5a95597d9fb",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ed States and other countries.\nRStudio, and the RStudio logo are registered trademarks of RStudio, Inc.\nOther company, product, or service names may be trademarks or service marks of others. \n\n\n© Copyright IBM Corp. 2025. ixForeword\nThis IBM Redbooks® publication is part of a trilogy that positions and explains IBM watsonx, \nwhich is IBM’s strategic artificial intelligence (AI) and data platform. Each book focuses on \none of the three main components of the watsonx platform:\n/SM590000IBM watsonx.ai: A next-generation enterprise studio for AI builders to train, validate, tune, \nand deploy both traditional machine learning (ML) and new generative AI (gen AI) capabilities that are powered  by foundation models (FMs).\n/SM590000IBM watsonx.data: A fit-for-purpose data store that is built on an open lakehouse \narchitecture, and is optimized for different and governed data and AI workloads.\n/SM590000IBM watsonx.governance: A set of AI Governance capabilities that e nables trusted AI \nworkflow",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 22,
            "total_chunks": 313
          }
        },
        {
          "id": "f6ab0569-8c85-4c4c-b0cb-d3667bb7a034",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "590000IBM watsonx.governance: A set of AI Governance capabilities that e nables trusted AI \nworkflows, which help organizations implement and comply with ever-changing industry and government regulations.\nOrganizations have long recognized the value that IBM Redbooks publications provide in \nguiding them with best practices, frameworks, clear explanations, and use cases as part of their solution evaluations and implementations.\nThis trilogy of books was poss ible because of close collabo ration among many skilled and \ntalented authors that were selected from IBM Technical Sales, IBM Development, IBM Expert Labs, IBM Client Success Management, and consul ting services organizations to use their \ndiverse skills, experiences, and technical knowledge across the watsonx platform.\nThanks to the authors, contributors, reviewers, and the IBM Redbooks team for their \ndedication, time, and effort in making this publication a valuable asset that organizations can use as part of their journey to A",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 23,
            "total_chunks": 313
          }
        },
        {
          "id": "a264b082-1801-430c-8ce2-b9f167c3e4b4",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "in making this publication a valuable asset that organizations can use as part of their journey to AI.\nThanks to Mark Simmonds and Deepak Rangarao for taking the lead in shaping this request \ninto yet another successful IBM Redbooks project.\nSteve Astorino, IBM General Manager - Development, Data, AI, and Sustainability\nPreface\nIBM watsonx is IBM’s strategic AI and data platform. This book focuses on watsonx.ai , one of \nthe three main components of the platform. IBM watsonx.ai is a next-generation enterprise studio that you can use to train, validate (test), tune, and deploy both traditional ML and new gen AI capabilities, which are powered by FMs through an open and intuitive user interface (UI). This AI studio provides a range of FMs, training and tuning tools, and a cost-effective infrastructure that facilitates the entire data and AI lifecycle, from data preparation through model development, deployment, and monitoring. Th e studio also includes an FM library that \nprovides IBM® c",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 24,
            "total_chunks": 313
          }
        },
        {
          "id": "b5e2774d-9ba3-4fa9-a11c-776c7cd02f7c",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "velopment, deployment, and monitoring. Th e studio also includes an FM library that \nprovides IBM® curated and trained FMs. FMs use a large, curated set of enterprise data that is backed by a robust filtering and cleansing process, and with an auditable data lineage. These models are trained on language and other modalities, such as code, time-series data, tabular data, geospatial data, and IT events data. \n\nx Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiHere are some examples of the model categories:\n/SM590000fm.code: Models that automatically generate code for developers through a \nnatural-language interface to boost developer productivity and enable the automation of many IT tasks.\n/SM590000fm.NLP: A collection of large language models (LLMs) for specific or industry-specific \ndomains that use curated data to help mitigate bias and quickly make domains customizable by using client data.\n/SM590000fm.geospatial: Models that are built on climate and remote se",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 25,
            "total_chunks": 313
          }
        },
        {
          "id": "e2996dc5-0440-4f86-a2e1-5c6d970ae318",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "mizable by using client data.\n/SM590000fm.geospatial: Models that are built on climate and remote sensing data to help \norganizations understand and plan for changes in natural disaster patterns, biodiversity, \nland use, and other geophysical processe s that might impact their businesses\nThe watsonx.ai studio builds on  Hugging Face open-source libraries, which offer thousands of \nHugging Face open models and datasets. Users can leverage the power of IBM Granite LLMs, along with the latest Mistral, Llama, and other third-party LLMs. It is part of IBM's commitment to deliver an open ecosystem approach that enables users to leverage the best models and architecture for their unique business needs.\nThis IBM Redbooks publication provides a broad understanding of watsonx.ai concepts, its \narchitecture, and the services that are available with the product. Also, several common use cases and scenarios are include d that should help you better u nderstand the capabilities of \nthis product. Cod",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 26,
            "total_chunks": 313
          }
        },
        {
          "id": "c5f19c97-64be-4819-a38c-24e16baadf82",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "enarios are include d that should help you better u nderstand the capabilities of \nthis product. Code samples of common scenarios are available at this GitHub repository . For \nmore examples, which include using Instructlab and AI agents, see this GitHub repository .\nThis publication is for watsonx customers who se ek best practices and real-world examples of \nhow to best implement their solu tions while optimizing the value of their existing and future \ntechnology, AI, data, and skills investments.\nAuthors\nThis book was produced by a team of specialists from around the world:\nDeepak Rangarao  is an IBM Distinguished Engineer and CTO who is responsible for \nTechnical Sales-Cloud Paks. He leads the technical sales team that helps organizations modernize their technology landscape with IBM Cloud Paks. He has broad, cross-industry experience in the data warehousing and analytics space from building analytic applications at large organizations and performing technical pre-sales for start-u",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 27,
            "total_chunks": 313
          }
        },
        {
          "id": "967c576f-2fcc-4849-975a-6b720e8c6930",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "building analytic applications at large organizations and performing technical pre-sales for start-ups and large enterprise software vendors. Deepak has co-authored several books on many topics, such as OLAP analytics, change data capture, data warehousing, and object storage. He is a regular speaker at technical conferences. He is a certif ied technical specialist in Red Hat OpenShift, \nApache Spark, Microsoft SQL Server, and web development technologies.Note:  Here are the other books in the trilogy:\n/SM590000Simplify Your AI Journey: Ensuring Trustworthy AI with IBM watsonx.governance , \nSG24-8573\n/SM590000Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.data,  \nSG24-8570\n\n Foreword xiPhillip Gerrard  is a Project Leader for the International Technical Support Organization \nworking out of Beaverton, Oregon. As part of IBM for over 15 years, he has authored and contributed to hundreds of technical documents that were published to IBM.com, and worked directly with",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 28,
            "total_chunks": 313
          }
        },
        {
          "id": "c4faf1e7-ac36-40f3-84af-9cc613c93990",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "tributed to hundreds of technical documents that were published to IBM.com, and worked directly with IBM's largest customers to resolve cr itical situations. As a team lead and subject \nmatter expert (SME) for the IBM Spectrum® Protect support team, he is experienced in leading and growing international teams of talented IBM employees by developing and implementing team processes,  and creating and de livering education. Ph illip holds a degree \nin computer science and business admini stration from Oregon State University.\nCharley Beller  is a Principal Data Scientist and IBM Master Inventor. He  works with clients \nas the Worldwide Solution Engineering Lead for watsonx.ai and AI Assistants within Technology Expert Labs. Charley has been working with IBM language technologies since \njoining the Watson group in 2014. He is an inventor with over 100 patents, and holds a PhD in Cognitive Science.\nCarl Broker  is an AI Architect at IBM who specializes in enterprise gen AI solutions. With a",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 29,
            "total_chunks": 313
          }
        },
        {
          "id": "d3149b0e-c739-487f-8a72-9a713431f7fb",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ience.\nCarl Broker  is an AI Architect at IBM who specializes in enterprise gen AI solutions. With a \nbackground in both gen AI and traditional data science, Carl leads design sessions and develops proof-of-concepts for clients. Before this role, he worked as an AI Engineer and Data Scientist at IBM, focusing on predictive modeling and AI-driven solutions. Carl holds a Master of Science degree from Johns Hopkins University. \nDaniele Comi  is a Data Scientist, AI Engineer, and Software Engineer at IBM Italy, with over \n3 years of experience in data analytics, ML, and deep learning (DL). His expertise spans the entire spectrum of AI, from architectural design to scientific research, with a focus on ML, reinforcement learning (RL), and DL. Daniele holds a master's degree in Computer Science \nEngineering, with a specialty in AI frameworks and models. At IBM, Daniele has been a key member of the AI and gen AI team in Italy, where he has designed and implemented complex AI and gen AI archite",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 30,
            "total_chunks": 313
          }
        },
        {
          "id": "bec92392-f78b-46a5-aac1-ac4fc22fd23d",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "the AI and gen AI team in Italy, where he has designed and implemented complex AI and gen AI architectures for many industr y applications. His technical expertise also \nincludes Fully Homomorphic Encrypted AI, which enables secure AI solutions that help ensure data privacy. \nLakshmana Ekambaram  is an IBM Senior Technical Leader with over 30 years of \nexperience in database development, advanc ed analytics, and building hybrid cloud \nsolutions. He is part of the IBM Expert La bs SWAT organization where he leads the data \nfabric and trusted AI journey for customers worldwide. He has developed many IBM certification courses and co-authored books about data science, AI, and data fabric. \nKaren Medhat is a Customer Success Manager Architect in the UK and the youngest \nIBM Certified Thought Leader Level 3 Techni cal Specialist. She is the Chair of the \nIBM Technical Consultancy Group and an IBM Academy of technology member. She holds a MSc degree with honors in Engineering in AI and Wirele",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 31,
            "total_chunks": 313
          }
        },
        {
          "id": "88f239e3-4445-4913-a57e-7144862c386a",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "IBM Academy of technology member. She holds a MSc degree with honors in Engineering in AI and Wireless Sensor Networks from the Faculty of Engineering, Cairo University, and a BSc de gree with honors in Engineering from the \nFaculty of Engineering, Cairo University. She co-creates curriculum and exams for different \nIBM professional certificates. She also crea ted and co-created cour ses for the IBM Skills \nAcademy in various areas of IBM technologies. She serves on the review board of international conferences and journals in AI and wireless communication. She also is an IBM Inventor who is experienced in creating applications architecture and leading teams of different scales to deliver customers' projects successfully. She frequently mentors IT professionals to help them def ine their career goals, learn ne w technical skills, or acquire \nprofessional certifications. She has authored publications on cloud, IoT, AI, wireless networks, microservices architecture, and blockchain.\n\nxii ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 32,
            "total_chunks": 313
          }
        },
        {
          "id": "0e853936-f3be-4b84-a22c-44083c95ac7d",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "publications on cloud, IoT, AI, wireless networks, microservices architecture, and blockchain.\n\nxii Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiPayal Patel works in Data and AI Technical Content Development at IBM, where she creates \ntechnical learning materials for sellers, IBM Busi ness Partners, and clients to enable them to \nget the most value out of IBM Data and AI products and solutions. She has worked in various roles at IBM, which include marketing analytics and as a Solutions Architect in IBM Technology Expert Labs, with a focus on Data and AI. She has worked in various technical roles across the financial services, insura nce, and technology industries. She holds a \nBachelor of Science degree in  Information Science from UNC Chapel Hill, and a Masters in \nAnalytics degree from North Carolina State University.\nMatthew Price  is a Senior watsonx Client Success Man ager with 20 years of experience in IT \nand 10 years of experience focusing on Wats on ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 33,
            "total_chunks": 313
          }
        },
        {
          "id": "34d5003a-6116-4f70-9b05-c7f4477e190b",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " Success Man ager with 20 years of experience in IT \nand 10 years of experience focusing on Wats on technologies. His previous experience \nincludes writing the base code that went on to become the IBM Watson® Assistant for Citizens application, which is IBM’s no-charge offering that was released during 2020 to help business and government agencies navigate the pandemic. His previous publications \ncentered on application migration and the cloud.\nShirley Shum is a Senior Software Engineer for the IBM Fusion team. She has worked as a \ntechnical lead on IBM Storage products, such as IBM Storage Insights and Fusion. Her areas of expertise include Kafka, complex event proc essing, backup and restore, and AI solutions, \nsuch as watsonx.ai and InstructLab on the Red Hat OpenShift platform. \nMark Simmonds  is a Program Director with IBM Data and AI. He writes extensively on AI, \ndata science, and data fabrics, and holds multiple author recognition awards. He previously worked as an IT architect",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 34,
            "total_chunks": 313
          }
        },
        {
          "id": "e05ec61e-3edf-44a4-9300-5e75d973c03a",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " data fabrics, and holds multiple author recognition awards. He previously worked as an IT architect leading complex infrastructure design and corporate technical architecture projects. He is a member of the British Computer Society, holds a bachelor's degree in Computer Science, is a published author, and a prolific public speaker.\nNow you can become a published author, too!\nHere’s an opportunity to  spotlight your skills , grow your career, and become a published \nauthor—all at the same time! Join an IBM Redbooks residency project and help write a book in your area of expertise, while honing your experience using leading-edge technologies. Your efforts will help to increase pr oduct acceptance and customer satisfaction, as you expand \nyour network of technical contacts and relationships. Residencies run from two to six weeks in length, and you can participate either in person or as a remote resident working from your home base.\nFind out more about the residency program, browse the re",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 35,
            "total_chunks": 313
          }
        },
        {
          "id": "07e200a3-3664-46b9-85f0-828469cbbc1e",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "emote resident working from your home base.\nFind out more about the residency program, browse the residency index, and apply online at:\nibm.com/redbooks/residencies.html\nComments welcome\nYour comments are important to us!\nWe want our books to be as helpful as possible . Send us your comments about this book or \nother IBM Redbooks publications in one of the following ways:\n/SM590000Use the online Contact us  review Redbooks form found at:\nibm.com/redbooks\n/SM590000Send your comments in an email to:\nredbooks@us.ibm.com\n\n Foreword xiii/SM590000Mail your comments to:\nIBM Corporation, IBM Redbooks\nDept. HYTD Mail Station P0992455 South RoadPoughkeepsie, NY 12601-5400\nStay connected to IBM Redbooks\n/SM590000Find us on LinkedIn:\nhttps://www.linkedin.com/groups/2130806\n/SM590000Explore new Redbooks publications, residencies, and workshops with the IBM Redbooks \nweekly newsletter:\nhttps://www.redbooks.ibm.com/subscribe\n/SM590000Stay current on recent Redbooks publications with RSS Feeds:\nhttps:",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 36,
            "total_chunks": 313
          }
        },
        {
          "id": "b2654f7d-2e85-4ba8-a0cd-e82b3db9977b",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "books.ibm.com/subscribe\n/SM590000Stay current on recent Redbooks publications with RSS Feeds:\nhttps://www.redbooks.ibm.com/rss.html\n\nxiv Simplify Your AI Journey: Unleashi ng the Power of AI with IBM watsonx.ai\n\n© Copyright IBM Corp. 2025. 1Chapter 1. Competing with artificial \nintelligence\nIn today's fast-paced digital landscape, artificial intellig ence (AI) has emerged as a \ngame-changer that is revolutionizing the way bus inesses operate, innovate, and compete. As \nAI technologies continue to advance and becom e increasingly ubiquitous, organizations are \nfaced with the daunting task of competing with AI-driven rivals while also leveraging AI to stay ahead of the competition. This chapter delves into the world of competing with AI by exploring the challenges, opportunities, and strategies that organizations can employ to remain competitive in an AI-dominated market.\nThe following topics are described in this chapter:\n/SM5900001.1, “Competing with AI” on page 2\n/SM5900001.2, “Challe",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 37,
            "total_chunks": 313
          }
        },
        {
          "id": "a15e0114-23f7-4da1-81f9-b9d365840011",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ics are described in this chapter:\n/SM5900001.1, “Competing with AI” on page 2\n/SM5900001.2, “Challenges in building and deploying AI models” on page 4\n/SM5900001.3, “Opportunities around using AI on trusted data” on page 5\n/SM5900001.4, “Improving AI mo del reliability” on page 8\n/SM5900001.5, “Creating new AI-enabled pr oducts and services” on page 111\n\n2 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai1.1  Competing with AI\nTo harness the competitive advantage of using AI, organizations must first understand the AI \nlandscape and the various types of AI that exis t. Organizations must also be aware of the \nvarious AI technologies that are being used to drive innovation and competitiveness, which include the following ones:\n/SM590000Machine learning (ML): ML represents a pi votal subset of AI where algorithms are \ndeveloped and trained to recognize patterns and make data-driven predictions or decisions. Unlike traditiona l programming, ML systems learn iterati",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 38,
            "total_chunks": 313
          }
        },
        {
          "id": "b9757d69-29d2-41e9-b997-27a486e2fa7d",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "make data-driven predictions or decisions. Unlike traditiona l programming, ML systems learn iteratively from data, \nimproving performance with ex perience. These systems rely on vast datasets to develop \nstatistical models that enable predictions across diverse applications, such as anomaly detection, natural language processing (NLP), and image recognition. The training process involves feeding labeled (supervi sed learning) or unlabeled (unsupervised \nlearning) data to the algorithm. Over time, the system refines its parameters to minimize errors and maximize predictive accuracy. ML algorithms are central to AI's practical applications, and they drive everything from recommendation systems to fraud detection in modern business ecosystems.\n/SM590000Deep learning (DL): DL is an advanced branch of ML that employs artificial neural \nnetworks that are modeled after the human brain's structure and functioning. Unlike traditional ML, which often depends on manual feature engineering, DL au",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 39,
            "total_chunks": 313
          }
        },
        {
          "id": "c9351eba-6186-4d9a-9a25-8aea09cf4a61",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ure and functioning. Unlike traditional ML, which often depends on manual feature engineering, DL automates the extraction of complex features from raw data through multiple layers of interconnected neurons. DL excels in handling unstructured data such as images, audio, and text, making it instrumental in solving tasks like computer vision, spee ch recognition, and language \ntranslation. For example, convolutional n eural networks (CNNs) specialize in image \nprocessing by identifying spatial hierarchies in pixels, and recurrent neural networks (RNNs) and transformers tackle sequential data with unparalleled efficiency. By leveraging high-performance computing and large datasets, DL approximates nonlinear functions to enable machines to solve intricate, high-dimensional problems.\n/SM590000Unsupervised learning: Unsupervised learning focuses on deriving patterns and \nstructures from unlabeled datasets. This method trains algorithms to identify inherent groupings, clusters, or association",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 40,
            "total_chunks": 313
          }
        },
        {
          "id": "2b4e90f4-2c60-4feb-ba17-f74f9b8a5a36",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "led datasets. This method trains algorithms to identify inherent groupings, clusters, or associations in data without human-provided annotations. Common techniques include clustering algorithms, such  as k-means and hier archical clustering, \nand dimensionality reduction methods, such as Principal Component Analysis (PCA) and t-SNE. Applications of unsupervised learning are vast, ranging from customer segmentation in marketing to anomaly detection in cybersecurity. These systems are \nparticularly valuable in exploratory data a nalysis, where insights emerge from raw data \nwithout prior assumptions. By uncovering hidden relationships, unsupervised learning enhances your understanding of data and informs decision-making processes.\n/SM590000Reinforcement learning (RL): RL is a paradigm of ML where algorithms learn optimal \nbehaviors by interacting with an environment and receiving feedback in the form of rewards or penalties. RL systems employ agents that act based on policies, and aim to",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 41,
            "total_chunks": 313
          }
        },
        {
          "id": "e2a8d021-56b8-4504-aa5d-6b808fcb3608",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "in the form of rewards or penalties. RL systems employ agents that act based on policies, and aim to maximize cumulative rewards over time. Core to RL are concepts such as the exploration-exploitation tradeoff, Markov deci sion processes (MDPs), and value functions. \nTechniques such as Q-learning  and Deep Q-Networks (DQNs) extend RL’s capabilities to \nenable applications in robotics, game play ing, and autonomous vehicles. RL's emphasis \non learning through trial and error aligns it closely with real-world problem-solving, where dynamic environments require adaptive strategies.\n\nChapter 1. Competing wit h artificial intelligence 3/SM590000Foundation models: Foundation models (FMs) represent a transformative leap in AI and \nML, characterized by their scala bility, versatility, and ability to genera lize across tasks. \nThese models, such as Granite, are pre-trained on vast and diverse datasets, enabling them to adapt to specific applications with minimal fine-tuning. Unlike traditional ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 42,
            "total_chunks": 313
          }
        },
        {
          "id": "96f60587-3f78-47d1-afe3-52c73d1544e3",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "asets, enabling them to adapt to specific applications with minimal fine-tuning. Unlike traditional ML \nmodels that are tailored to single tasks, FMs leverage their pre-trained knowledge to excel in multiple domains. This adaptability is achieved throug h transfer learning, where the \nmodel's pre-trained weights are fine-tuned on domain-specific data sets. FMs empower organizations to reduce the cost and complexity of training AI systems while achieving state-of-the-art performance in tasks like natural language understanding, summarization, and multimodal reasoning.\nLeveraging AI for co mpetitive advantage\nTo thrive in an AI-driven landscape, organizati ons must strategically harness AI technologies \nto drive innovation, enhance operational efficiency, and improve decision-making. Key areas of focus include the following ones:\n/SM590000AI-powered automation: Automation that is fueled by AI enables the running of repetitive \nand high-volume tasks with speed and precision,  such as appl",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 43,
            "total_chunks": 313
          }
        },
        {
          "id": "4f998ac1-e19e-48ed-a099-cd254b527916",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " AI enables the running of repetitive \nand high-volume tasks with speed and precision,  such as applications in Robotic Process \nAutomation (RPA), intelligent document processing, and automated workflows. By offloading routine operations, organizations can redirect human resources to strategic and creative endeavors, which foster innovation and competitive differentiation.\n/SM590000AI-driven analytics: AI-powered analytics transform raw data into actionable insights, \nwhich equips businesses to make data-informed decisions. Predictive analytics, which is powered by ML, enables forecasting trends and identifying potential challenges, and prescriptive analytic s suggests optimal courses of action. These ca pabilities enable \norganizations to stay ahead in dynamic market s by responding proactively to opportunities \nand risks.\n/SM590000AI-based innovation: AI acts as a catalyst for innovation to enable organizations to \nconceptualize and deliver groundbreaking products, services, and busi",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 44,
            "total_chunks": 313
          }
        },
        {
          "id": "e3e45f13-dcd1-417f-b97a-1d83dfbf49ad",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "on to enable organizations to \nconceptualize and deliver groundbreaking products, services, and business FMs. From personalized healthcare solutions to autonomous logistics systems, AI's potential to redefine industries is immense. By embedding AI in their innovation processes, companies can create unique value propositions that resonate with customers and stakeholders alike.\nBy embracing these AI strategies, organizations can position themselves as leaders in the era \nof digital transformation. As AI  continues to evolve, its syner gy with trusted data will unlock \nunprecedented op portunities, which will re shape the competitive landscape and drive \nsustainable growth.\n\n4 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai1.2  Challenges in building and deploying AI models\nBuilding and deploying AI models is a complex and challenging endeavor that requires \nexpertise, resources, and infrastructure. Despite the potential benefits of AI, many organizations struggle",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 45,
            "total_chunks": 313
          }
        },
        {
          "id": "0d3414f4-8bdf-49e3-8e31-79fb0b11f9bb",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "se, resources, and infrastructure. Despite the potential benefits of AI, many organizations struggle to overcome the numerous hurdles that are associated with AI model development and deployment. Here are some of the key challenges:\n/SM590000Data quality and availability: AI  models require vast amounts of high-quality,  relevant, and \ndiverse data to learn, train, and validate. However, many organizations face challenges in collecting, processing, and integrating data from disparate sources, which can lead to issues with data quality, consistency, an d availability. Furthermo re, data privacy and \nsecurity concerns can limit access to sensitive data, which can hinder the development of accurate and reliable AI models.\n/SM590000Model complexity and in terpretability: As AI models be come increasingly complex, they \ncan be difficult to interpret and understand, which makes it challenging to identify biases, errors, or flaws in the decision-making process. The lack of transparency and \ne",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 46,
            "total_chunks": 313
          }
        },
        {
          "id": "6ac01544-e587-4158-ad8f-c37fa47e177e",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "to identify biases, errors, or flaws in the decision-making process. The lack of transparency and \nexplainability in AI models can lead to mistrust, regulat ory issues, and reputational \ndamage. Moreover, the complexity of AI models can make it difficult to integrate them with existing systems, processes, and infrastructure.\n/SM590000Talent acquisition and retention: The development and deployment of AI models require \nspecialized skills and expertise, which in clude data science, ML, and software \nengineering. However, the demand for AI talent far exceeds the supply, which can lead to challenges in acquiring and retaining top talent. Furthermore, the constant evolution of AI technologies means that professionals must continually update their skills to remain relevant, which can add to the talent acquisition and retention challenges.\n/SM590000Infrastructure and scalability:  AI models require significan t computational resources, \nmemory, and storage to process and analyze large datase",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 47,
            "total_chunks": 313
          }
        },
        {
          "id": "c99b8616-4e18-4c4b-b8f7-9ba7a89cd4ff",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "quire significan t computational resources, \nmemory, and storage to process and analyze large datasets. However, many organizations lack the necessary infrastructure to support the development and deployment of AI models, which can lead to  issues with scalability, performance, and \nreliability. Furthermore, the inte gration of AI models with existing systems and processes \ncan be complex, requiring si gnificant investment in infrastructure and architecture.\n/SM590000Bias and fairness: AI models can perpetuate and amplify existing biases and inequalities if \nthey are trained on biased data or designed with a particular world view. The lack of diversity and inclusion in AI development t eams can exacerbate these issues, which can \nlead to unfair outcomes and reputational damage. Furthermore, the identification and mitigation of bias in AI models can be challe nging, which requires significant expertise and \nresources.\n/SM590000Regulatory compliance: The development and deployment of AI ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 48,
            "total_chunks": 313
          }
        },
        {
          "id": "f1d06769-833c-4c09-ad10-184cc5a2a80b",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "cant expertise and \nresources.\n/SM590000Regulatory compliance: The development and deployment of AI models are subject to \nvarious regulations and laws, which include data protection, intellectual property, and anti-discrimination legislation. However, the rapidly evolving nature of AI technologies can make it challenging to ensure regulatory compliance, particular ly in industries with strict \nregulations, such as healthcare and finance.\n/SM590000Model maintenance and updates: AI models require continuous maintenance and updates \nto help ensure that they remain accurate, reliable, and relevant. However, the process of updating AI models can be complex and require significant resources and expertise. Furthermore, the integration of updated AI models with existing systems and processes can be challenging, which can lead to issues with compatib ility and performance.\n\nChapter 1. Competing wit h artificial intelligence 5/SM590000Explainability and transpar ency: The lack of explainability",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 49,
            "total_chunks": 313
          }
        },
        {
          "id": "a9ad59f9-c069-4367-ac29-15b277fc7b59",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "wit h artificial intelligence 5/SM590000Explainability and transpar ency: The lack of explainability a nd transparency in AI models \ncan make it challenging to understand the decision-making process, which can lead to \nmistrust and reputational damage. Furthermore, the identification and mitigation of errors or biases in AI models can be difficult, which can require expertise and resources.\n/SM590000Cybersecurity: AI models can be vulnerable to cyberthreats, such as data poisoning, \nmodel hijacking, and adversarial attacks. The identification and mitigation of these threats can be challenging and require expertise and resources. Furthermore, the integration of AI models with existing security systems and processes can be complex, which can lead to issues with compatib ility and performance.\n1.2.1  Technical considerations fo r building and depl oying AI models\nWhen building and deploying AI models, there are several technical considerations that must \nbe accounted for:\n/SM590000Data pr",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 50,
            "total_chunks": 313
          }
        },
        {
          "id": "276518ed-8a75-442d-b548-3c3e5e7a2a1a",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " AI models, there are several technical considerations that must \nbe accounted for:\n/SM590000Data preprocessing: Ensuring that the data that is used to train and test the model is \naccurate, complete, and relevant.\n/SM590000Model selection: Choosing the most suitable algorithm and model architecture for the \nproblem being addressed.\n/SM590000Hyper-parameter tuning: Optimizing the mode l's hyper-parameters to achieve the best \npossible performance.\n/SM590000Model evaluation: Evaluating the model's perfor mance by using metric s such as accuracy, \nprecision, and recall.\n/SM590000Model deployment: Deploying the model in a production-ready environment, such as a \ncloud-based API or a containerized application.\n/SM590000Model monitoring: Continuously monitoring the model's performance and updating it as \nnecessary to help ensure that it  remains accurate  and relevant.\nIn addition to these technical considerations, organizations must also consider the following \nitems:\n/SM590000Data governa",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 51,
            "total_chunks": 313
          }
        },
        {
          "id": "3490f639-6639-4ea4-bcad-13029c287874",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "chnical considerations, organizations must also consider the following \nitems:\n/SM590000Data governance: Establishi ng policies and procedures for data management, which \ninclude data quality, security, and compliance.\n/SM590000Model governance: Establishing policies and procedures for model development, \ndeployment, and maintenance, which include model validation, testing, and updating.\n/SM590000Infrastructure governance: Establishing po licies and procedures for infrastructure \nmanagement, which include infrastructure provisioning, scaling, and security.\nBy being conscious of these technical consider ations and establishing effective governance \npolicies and procedures, organizations can help ensure the successful development and deployment of AI models that drive business value and competitiveness.\n1.3  Opportunities around using AI on trusted data\nAI thrives on data. However, the effectiveness of AI systems is not solely dependent on the volume of data but also on its quality and t",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 52,
            "total_chunks": 313
          }
        },
        {
          "id": "1b77d95d-c905-4e11-9609-3b6bdeb82563",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ectiveness of AI systems is not solely dependent on the volume of data but also on its quality and trustworthiness. Trusted data (data that is accurate, consistent, secure, and compliant) serves as the foundation for reliable AI-driven insights. Organizations today are exploring opportunities to harness AI on trusted data to drive operational efficiency, enhance decision-maki ng, and unlock new revenue streams. This \nsection explores the myriad possibilities that AI unlocks when it operate s on a foundation of \nhigh-quality, trusted data.\n\n6 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai1.3.1  Enhancing de cision-making with accurate insights\nThe fusion of AI and trusted data is reshapin g decision-making processes across industries, \nand enabling organizations to derive precise and actionable insights. This transformation is \ncritical in domains where de cisions significantly impact outcomes, such as healthcare, \nfinance, and supply chain management. By leve ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 53,
            "total_chunks": 313
          }
        },
        {
          "id": "91b753eb-c351-4b27-aa0a-90e0c180c9cb",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "s significantly impact outcomes, such as healthcare, \nfinance, and supply chain management. By leve raging high-quality, trusted data, AI systems \ncan identify patterns, predict outcomes, and provide recommendations that drive superior decisions.\nFor example, in healthcare, AI systems that are powered by trusted clinical and patient data \nenhance diagnostic precision, predict pati ent outcomes with remarkable accuracy, and \nsuggest personalized treatment plans that are tailored to individual needs. In the financial sector, AI models that are trained on trusted datasets excel in detecting fraudulent activities, assessing credit risks, and automating sophisticated trading strategies that are based on dynamic market trends. These examples illustrate how truste d data amplifies the reliability \nand impact of AI-driven decision-making,  minimizing risks and maximizing outcomes.\nOne of the most transformative applications of AI on trusted data is in improving \ndecision-making. High-quality d",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 54,
            "total_chunks": 313
          }
        },
        {
          "id": "757d97dc-3502-4364-9592-5e818b0c536f",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "t transformative applications of AI on trusted data is in improving \ndecision-making. High-quality data enables AI algorithms to deliver precise and actionable \ninsights, which are beneficial in industries like healthcare, finance, and supply chain management, where even minor errors in decision-making can have significant consequences.\n/SM590000Healthcare: Trusted data enables AI systems  to accurately predict patient outcomes, \nsuggest personalized treatment plans, and enhance diagnostic accuracy.\n/SM590000Finance: In financial services, AI models that are trained on trusted data can detect fraud, \nassess credit risks, and automate trading stra tegies based on market predictions. \n1.3.2  Driving operational efficiency\nAI's ability to automate  and optimize complex processes is  magnified when it is built on a \nfoundation of trusted data. By eliminating inefficiencies and reducing the need for human \nintervention, organizations can achieve unpr ecedented levels of operational efficien",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 55,
            "total_chunks": 313
          }
        },
        {
          "id": "39ef38bf-dc7b-414b-9342-c8c11dd85067",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "eed for human \nintervention, organizations can achieve unpr ecedented levels of operational efficiency.\nIn the manufacturing and energy sectors, predictive maintenance systems leverage trusted \nsensor data to foresee equipment failures, which enable preemptive interventions that \nminimize downtime and reduce maintenance costs. Similarly, AI-powered customer service platforms, which are underpinned by reliable customer interaction data, provide accurate, context-aware responses that deliver personalized experiences while alleviating the workload of human agents. These advancements highlight the transformative potential of combining AI with trusted data to streamline operations across industries.\nWhen AI is applied to trusted data, it automates complex processes, which reduce the need \nfor human intervention, and improves efficiency:\n/SM590000Predictive maintenance: In the manufacturing and energy sectors, AI systems use trusted \nsensor data to predict equipment failures, which minimize ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 56,
            "total_chunks": 313
          }
        },
        {
          "id": "7cef327a-14be-42cf-8cf1-8ebf58bfa526",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "d energy sectors, AI systems use trusted \nsensor data to predict equipment failures, which minimize downtime and optimizes maintenance schedules.\n/SM590000Customer service automation: AI-powered chatbots, which are fueled by reliable customer \ndata, provide accurate responses and deliver personalized experiences, which reduce the burden of human agents.\n\nChapter 1. Competing wit h artificial intelligence 71.3.3  Accelerating innovation\nThe convergence of AI and trusted data is a catalyst for innovation, which unlocks hidden \npatterns and opportunities that were previously  inaccessible. By analyzing vast amounts of \nhigh-quality data, AI systems empower organizations to develop groundbreaking products and solutions.\nFor example, in product development, companies use AI to analyze customer feedback, \nmarket trends, and usage data th at is extracted from trusted sources to create offerings that \nresonate with consumer preferences. In the realm of scientific rese arch, AI accelerates \ndis",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 57,
            "total_chunks": 313
          }
        },
        {
          "id": "551f8f73-0175-4dae-baed-5670ca6f26d9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " that \nresonate with consumer preferences. In the realm of scientific rese arch, AI accelerates \ndiscovery processes by interpreting extensive experimental datasets, which lead to advancements in fields such as drug develop ment and material science. Trusted data \nenhances the accu racy of these insights and helps ensure the repr oducibility of  outcomes, \nwhich drive sustained innovation.\nThe combination of AI and trusted data fosters innovation by uncovering hidden patterns and \nopportunities that were previously inaccessible:\n/SM590000Product development: Companies leverage AI to analyze customer feedback and market \ntrends from trusted datasets, which helps the companies to design products that align with consumer preferences.\n/SM590000Research and development: In scientific rese arch, AI accelerates discovery by analyzing \nlarge volumes of trusted experimental data, which can lead t o breakthroughs in areas \nsuch as d rug discovery and material science.\n1.3.4  Enhancing gover nanc",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 58,
            "total_chunks": 313
          }
        },
        {
          "id": "76dc99d8-b1f0-4549-abab-5d842856f94a",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " o breakthroughs in areas \nsuch as d rug discovery and material science.\n1.3.4  Enhancing gover nance and compliance\nIn an era where regulatory landscapes are bec oming increasingly stringent, trusted data \nplays a pivotal role in helping ensure that AI systems operate within legal and ethical frameworks. Governance and co mpliance initiatives are fortified by AI systems that \ncontinuously monitor operations, detect anomalies, and flag potential risks.\nFor example, compliance monitoring AI tool s analyze operational data to help ensure \nadherence to industry regulations and standa rds, which reduce the risk of noncompliance \npenalties. Also, the usage of diverse and repres entative trusted datasets mitigates biases in \nAI model training, which foster fairness and ethi cal outcomes in critical applications such as \nhiring or loan approvals. Trusted data serves as a cornerstone for responsible AI development and deployment.\nTrusted data helps ensure that AI systems operate within legal a",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 59,
            "total_chunks": 313
          }
        },
        {
          "id": "d60975f5-063f-4fa1-9dea-237053f7db39",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ible AI development and deployment.\nTrusted data helps ensure that AI systems operate within legal and ethical boundaries, which \ncritical factors in maintain ing customer trust and avoiding regulatory penalties:\n/SM590000Compliance monitoring: AI models can continuously analyze operational data to help \nensure adherence to regulations by flagging any potential compliance risks.\n/SM590000Bias mitigation: Trusted data, when diverse and representative, helps train AI models that \nare fair and unbiased, which helps ensure ethical outcomes in areas like hiring or loan approvals.\n1.3.5  Unlocking new revenue streams\nThe monetization of trusted data through AI-driven services and products has emerged as a significant avenue for revenue generation. Or ganizations across sectors are capitalizing on \nthis synergy to create innovative business models.\n\n8 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiFor example, in telecommunications and retail, companies offer AI-power",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 60,
            "total_chunks": 313
          }
        },
        {
          "id": "89b87e17-4e61-4f41-915e-1da8b6c106ee",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "wer of AI with IBM watsonx.aiFor example, in telecommunications and retail, companies offer AI-powered insights or \nanalytics as services to their partners and c lients, which transform data into a valuable asset. \nMoreover, trusted customer data enables hyper-targeted marketing campaigns, which enhance conversion rates and foster customer loyalty. By harnessing the power of trusted data, organizations can unlock untapped revenue opportunities while delivering value to stakeholders.\nOrganizations are monetizing their trusted data through AI-driven services and products:\n/SM590000Data monetization: Companies in sectors such as telecommunications and retail generate \nnew revenue by offering AI-powered insights or analytics as a service to their partners and \nclients.\n/SM590000Personalized marketing: AI leverages trusted customer data to deliver hyper-targeted \nmarketing campaigns that increase conversion rates and customer loyalty.\n1.3.6  Transforming industrie s with AI and trusted data",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 61,
            "total_chunks": 313
          }
        },
        {
          "id": "0f03050e-fc86-453a-90b8-a2ec69b968b6",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ease conversion rates and customer loyalty.\n1.3.6  Transforming industrie s with AI and trusted data\nThe integration of AI with trusted data is revolutionizing industries in unique and profound \nways. Retailers use transaction and customer data to fuel recommendation engines, which enhance sales and customer satisfaction. In agriculture, AI models analyze environmental and crop data to optimize farming practices an d maximize yields. Similarly, in the energy \nsector, AI systems leverage consumption and grid data to predict demand, optimize distribution, and enhance energy efficiency. These transformative applications underscore the versatility and impact of trusted data-driven AI across diverse domains.\nDifferent industries are leveraging AI and trusted data in unique ways:\n/SM590000Retail: Trusted transaction and customer data power recommendation engines that boost \nsales and improve customer experiences.\n/SM590000Agriculture: AI models analyze trusted environmental and crop data to ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 62,
            "total_chunks": 313
          }
        },
        {
          "id": "c44270a7-d004-4f7e-9606-8fc28b1d52c6",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ustomer experiences.\n/SM590000Agriculture: AI models analyze trusted environmental and crop data to optimize farming \npractices and increase yield.\n/SM590000Energy: AI systems use trusted consumption and grid data to predict demand and \noptimize energy distribution.\n1.4  Improving AI model reliability\nThe reliability and interp retability of AI models  are enhanced when they are tr ained and \nvalidated on trusted data. High-quality data helps ensure that AI systems deliver consistent and accurate outputs, which foster stakeholder trust a nd facilitate br oader adoption.\nFor example, explainable AI (XAI) models rely  on trusted data to generate transparent and \ninterpretable insights, which address concerns  about the “black-box” nature of AI. Also, \ntrusted data simplifies model auditing and debugging by enabling the identification of inconsistencies and anomalies, which leads to continuous performance improvements. By prioritizing data quality, organizations can overcome one of the pr",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 63,
            "total_chunks": 313
          }
        },
        {
          "id": "bc4b0f86-d7d2-4619-9400-2bd3e6d0f59c",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ous performance improvements. By prioritizing data quality, organizations can overcome one of the primary challenges of scaling AI systems.\n\nChapter 1. Competing wit h artificial intelligence 9Trusted data enhances t he reliability and interpre tability of AI models, addressing one of the \nmajor challenges in deploying AI at scale:\n/SM590000Explainability and trust: AI m odels that are trained on hi gh-quality data provide more \nconsistent and interpretable outputs, which enable stakeholders to trust and adopt AI-driven solutions.\n/SM590000Model auditing and debugging: Trusted data helps identify inconsistencies and anomalies \nin AI predictions, which make it easier to debug models and improve their performance over time.\n1.4.1  Enabling cross-enterprise collaboration\nTrusted data serves as a bridge for collaboratio n across organizational silos and with external \npartners. This capability enhance s operational transparency and fosters innovation by \nenabling seamless data sharing and ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 64,
            "total_chunks": 313
          }
        },
        {
          "id": "ec85e613-0154-4d79-9ea9-b5c2b6f064fb",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ty enhance s operational transparency and fosters innovation by \nenabling seamless data sharing and integration.\nEnterprises can leverage trusted data infrastructures to break down silos so that \ncross-functional teams can work collaboratively on shared objectives. Secure data exchange mechanisms further facilitate partnerships  across geographies, which helps ensure \ncompliance with data privacy regulations and fosters rust among stakeholders. Such collaborative ecosystems are critical for driving comprehensive digital transformation initiatives.\nAI on trusted data enables organizations to collaborate more effectively across departments \nand even with external partners:\n/SM590000Data sharing across silos: Enterprises can break down data silos and enable \ncross-functional collaboration to enhance operational transparency.\n/SM590000Secure data exchange: Trusted data infrastructures help ensure that shared data between \npartners or across geographies remain secure and compliant.\n1.4.2  E",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 65,
            "total_chunks": 313
          }
        },
        {
          "id": "6ce7ab8c-097c-4b55-9274-282df528e30c",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "nsure that shared data between \npartners or across geographies remain secure and compliant.\n1.4.2  Enhancing real -time decision-making\nThe ability to make real-time de cisions is a cornerstone of mo dern business strategies, and \ntrusted data is a key enabler of  this capability. By processing and analyzing data  streams in \nreal time, AI systems empower organizations to act swiftly and effectively.\nIn industries like finance, dynamic pricing models use real-time trusted data to optimize \nstock-pricing strategies based on demand and in ventory levels. Financia l institutions employ \nAI systems to perform instant risk assessments to mitigate fraud and help ensure secure \ntransactions. These applicatio ns demonstrate how trusted da ta enhances the agility and \nresponsiveness of AI-driven decision-making processes.\nReal-time analytics that are powered by trusted data enable businesses to make faster, more \ninformed decisions:\n/SM590000Dynamic pricing: In e-commerce or travel industries, ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 66,
            "total_chunks": 313
          }
        },
        {
          "id": "1ca01bdf-9579-4c19-8509-080701eb2dc9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ake faster, more \ninformed decisions:\n/SM590000Dynamic pricing: In e-commerce or travel industries, AI leverages real-time trusted data to \noptimize pricing strategies based on demand and inventory levels.\n/SM590000Real-time risk assessment: Fi nancial institutions use AI to perform instant risk \nassessments for transactions to hel p mitigate fraud or credit risks.\n\n10 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai1.4.3  Scaling AI-driven ecosystems\nLarge-scale AI implementations  depend on the robustness and reliability of trusted data \necosystems. By building scalable infrastructures that integrate trusted data with advanced AI capabilities, organizations can unlock the full potentia l of AI-driven solutions.\nAI as a Service (AIaaS) platforms exemplify this integration by offering modular services such \nas ML models, NLP, and predictive analytics that are powered by trusted data. In parallel, the integration of IoT devices with AI systems generates vast volu",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 67,
            "total_chunks": 313
          }
        },
        {
          "id": "764ff5de-c320-4833-8251-a17a150fdadb",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "red by trusted data. In parallel, the integration of IoT devices with AI systems generates vast volumes of real-time data to enable actionable insights in sectors like logistics, healthcare, and smart cities. These scalable \necosystems are the foundation for sustained growth and innovation.\nTrusted data serves as the backbone for large- scale AI implementations, fostering robust AI \necosystems:\n/SM590000AI as a Service (AIaaS): Companies are building scalable platforms where trusted data \npowers modular AI services like ML models, NLP, and predictive analytics.\n/SM590000Integration with IoT: IoT devices generate vast amounts of data. Trusted IoT data enables \nAI systems to deliver real-time analytics for industries like logistics, healthcare, and smart cities.\n1.4.4  Driving sustainability and envi ronmental, social, an d governance goals\nAI-powered sustainabilit y initiatives are gain ing momentum, with trus ted data playing a \ncentral role in achieving environmental, social, and gove",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 68,
            "total_chunks": 313
          }
        },
        {
          "id": "6c72cc8b-617a-46a8-a743-e9a5433595fe",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ng momentum, with trus ted data playing a \ncentral role in achieving environmental, social, and governance (ESG) objectives. By analyzing environmental and operational datasets, AI systems help organizations optimize resource usage, reduce carbon footprints, and enhance supply chain transparency.\nFor example, sustainabilit y analytics tools use trusted data to identify inefficiencies and \nrecommend strategies for improv ing energy efficiency. Similarl y, AI systems provide visibility \ninto supply chains to enable organizations to address waste and improve sustainability \npractices. By aligning AI initiatives with ESG goals, organizations can demonstrate their commitment to responsible and ethical operations.\nAI, combined with trusted data, helps organizations meet ESG objectives:\n/SM590000Sustainability analytics: AI mode ls analyze trusted environmen tal and operational data to \noptimize resource usage and reduce carbon footprints.\n/SM590000Supply chain transparency: Truste d data pro",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 69,
            "total_chunks": 313
          }
        },
        {
          "id": "93daa044-644c-4471-b5fc-62737bd6ede0",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "e resource usage and reduce carbon footprints.\n/SM590000Supply chain transparency: Truste d data provides visibility into the supply chain so that AI \ncan identify inefficiencies, reduce waste, and improv e sustainability practices.\n1.4.5  Personalizing customer experiences\nCustomer-centric in dustries are leveraging AI's abilit y to deliver highly personalized \nexperiences, which is a capability that is ro oted in truste d data. By analyzing customer \nbehavior, preferences, and interactions, AI systems create tailored experiences that enhance satisfaction and loyalty.\nFor example, adaptive AI systems use real-time trusted data to modify services dynamically \nto help ensure relevance and engagement. Behavioral analytics enable companies to predict customer needs, which reduce churn and fosters long-term relationships. Trusted data empowers organizations to elevate customer experiences to new heights.\n\nChapter 1. Competing with artificial intelligence 11Customer-centric in dustries are ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 70,
            "total_chunks": 313
          }
        },
        {
          "id": "b631599c-9b27-415e-90fa-3c29e27be576",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " new heights.\n\nChapter 1. Competing with artificial intelligence 11Customer-centric in dustries are capitalizing on AI's ab ility to deliver highly personalized \nexperiences through trusted data:\n/SM590000Adaptive AI systems: Real-time, trusted cu stomer data enables AI systems to adapt and \ntailor services to create more engaging user experiences.\n/SM590000Behavioral analytics: Trusted behavioral data enables companies to predict customer \npreferences, which reduce churn and increase satisfaction.\n1.5  Creating new AI-enabled products and services\nTrusted data serves as the foundation for dev eloping innovative AI-enabled products and \nservices that redefine industries. By harnessin g historical and real-time data, organizations \ncan anticipate needs and deliver solutions proactively.\nFor example, proactive support systems leverage AI to predict and address issues before \nthey escalate, which enhances cu stomer satisfaction and operational efficiency. Custom AI \nsolutions, which are t",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 71,
            "total_chunks": 313
          }
        },
        {
          "id": "68e14dfe-3354-4326-8e85-71d01ec51ca6",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " which enhances cu stomer satisfaction and operational efficiency. Custom AI \nsolutions, which are tailo red to specific market niches, furt her demonstrate the transformative \npotential of trusted data. As organizations continue to invest in data governance and quality, the opportunities for creating AI-driven innovations will expand.\nBy aligning AI initiatives with business objectives and prioritizing trusted data infrastructures, \norganizations can unlock unparalleled levels of  efficiency, innovation, and growth. As the \ncomplexity of data landscapes continues to evolve, the role of trusted data in enabling AI to achieve its full potential becomes increasingly indispensable.\nTrusted data opens doors to innovations that can redefine industries:\n/SM590000Proactive support systems: AI systems, which are trained on historical  and real-time data, \npredict customer or machine needs before problems occur to offer preemptive solutions.\n/SM590000Custom AI solutions: Organizations use their",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 72,
            "total_chunks": 313
          }
        },
        {
          "id": "bb30ce37-0125-4a4d-a7e2-4ec11f97c7d9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " problems occur to offer preemptive solutions.\n/SM590000Custom AI solutions: Organizations use their proprietary trusted data to build AI products \nthat are tailored to niche market requirements.\nBy continuously investing in trusted data infr astructures and aligning AI initiatives with \nbusiness goals, organizations can unlock new levels of efficiency, growth, and innovation.AI and trusted data offers a powerful opportunity to drive growth, innovation, and operational excellence. Organizations that prioritize data governance, ensure data quality, and build AI systems on trusted data are better positioned to harness these opportunities. As the volume \nand complexity of data continue to grow, the role of trusted data in enabling AI to deliver its full potential will become more critical.\n\n12 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai\n\n© Copyright IBM Corp. 2025. 13Chapter 2. Introducing IBM watsonx.ai\nIBM watsonx is IBM’s next-generation platform that is de",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 73,
            "total_chunks": 313
          }
        },
        {
          "id": "f58dedfe-d12c-43d8-823b-59c711b5b7c5",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "25. 13Chapter 2. Introducing IBM watsonx.ai\nIBM watsonx is IBM’s next-generation platform that is designed to help businesses accelerate \ntheir journey into artificial in telligence (AI)-driven insights, de cision-making, and automation. \nThe platform offers a comprehensive suite of t ools and services that are tailored to simplify \nand streamline the development and deployment of AI solutions. It is built on three foundational pillars:\n/SM590000IBM watsonx.data: A scalable data lakehouse that is designed for efficient and secure \ndata management to enable hybrid cloud deployments and optimize data for AI workloads.\n/SM590000IBM watsonx.governance: Provides robust governance to help ensure that AI models \nremain ethical, transparent, and compliant with regulatory standards. It also helps businesses monitor and mitigate AI-related risks.\n/SM590000IBM watsonx.ai: A cutting-edge AI development and deployment environment. It supports \nthe full lifecycle of AI, from model training and fine-",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 74,
            "total_chunks": 313
          }
        },
        {
          "id": "43fb9e10-e80f-463a-88df-ab5d2fde168e",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ent and deployment environment. It supports \nthe full lifecycle of AI, from model training and fine-tuning to deployment and monitoring.\nThe seamless integration of these components enables enterprises to leverage trusted data \n(watsonx.data), enforce governance and ethical standards (watsonx.governance), and develop AI-powered solutions (watsonx.ai). Together, they form a comprehensive ecosystem for deploying enterprise-grade AI at scale.\nThe following topics are described in this chapter:\n/SM5900002.1, “Overview of watsonx.ai” on page 14\n/SM5900002.2, “Synergy between watsonx.ai and other components in the watsonx platform” on \npage 15\n/SM5900002.3, “Business impact of these synergies” on page 162\n\n14 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai2.1  Overview of watsonx.ai\nwatsonx.ai serves as the core engine of the watsonx platform, which is focused on the rapid \ndevelopment and deployment of AI models. Its architecture is designed to support various AI wo",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 75,
            "total_chunks": 313
          }
        },
        {
          "id": "0be59b7e-d625-45f2-86ba-7198eb1dbf1a",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "apid \ndevelopment and deployment of AI models. Its architecture is designed to support various AI workloads, such as traditional machine learning  (ML), deep learning (DL), and generative AI \n(gen AI).\n2.1.1  Key capabilities\nHere are the key capab ilities of watsonx.ai:\n/SM590000Foundation models (FMs): watsonx.ai provides access to pre-trained FMs, such as large \nlanguage models (LLMs) and vision models, wh ich can be fine-tuned for specific business \nneeds.\n/SM590000Machine learning operations (MLOps): The platform integrates tools for version control, \nmodel monitoring, and deployment to streamline AI lifecycle management.\n/SM590000Multi-cloud compatibility: Su pports hybrid and multi-cl oud environments so that \nbusinesses can run AI workloads wherever they see fit.\n/SM590000Extensibility: Developers can br ing their own models or inte grate open-source frameworks \nlike PyTorch and TensorFlow.\n2.1.2  The watsonx.ai architecture\nThe watsonx.ai architecture components include the fo",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 76,
            "total_chunks": 313
          }
        },
        {
          "id": "79320fc5-f911-44a5-aec8-cf4185fb10cc",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "TensorFlow.\n2.1.2  The watsonx.ai architecture\nThe watsonx.ai architecture components include the following items:\n/SM590000Model Studio: An interface for training, fine-tuning, and deploying models.\n/SM590000Inference Engine: Optimized for running AI models in production environments to help \nensure low latency and high scalability.\n/SM590000Integration Layer: Enables seamless integration with watsonx.data for real-time data \naccess and watsonx.governance for compliance.\n2.1.3  watsonx.ai empoweri ng IBM Software offerings\nThe watsonx.ai FMs are being infused throughout all of IBM's major software offerings. The \nFMs are as follows:\n/SM590000IBM watsonx Code Assistant: Uses gen AI so  that developers can automatically generate \ncode by using a natural-language prompt.\n/SM590000IBM watsonx AIOps Insights: Includes FMs for code and natural language processing \n(NLP) to provide greater visibility in to performance acro ss IT environments.\n/SM590000IBM watsonx Assistant and IBM watsonx Or",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 77,
            "total_chunks": 313
          }
        },
        {
          "id": "b8021c5d-99a3-48ec-bfda-42c6e1334ef5",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ibility in to performance acro ss IT environments.\n/SM590000IBM watsonx Assistant and IBM watsonx Orchestrate®: Boosted by an NLP FM, IBM's \ndigital labor products enhance employee prod uctivity and customer service experiences.\n/SM590000Environmental Intelligence  Suite: powered by th e IBM geospatial FM, \nIBM EIS Builder Edition creates tailored solutions that address and mitigate environmental risks.\n\nChapter 2. Introducing IBM watsonx.ai 152.1.4  Benefits of using watsonx.ai for businesses\nAdopting watsonx.ai offers several key advantages for enterprises looking to harness the \npower of AI:\n/SM590000Accelerated AI development: watsonx.ai simplifies the development process with \npre-trained FMs and built-in tools for traini ng and deployment. Businesses can achieve \nfaster time-to-value by reducing the complexity of building AI from scratch.\n/SM590000Enhanced productivity and efficiency: Through automation of repetitive tasks, watsonx.ai \nenables teams to focus on high er-value acti",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 78,
            "total_chunks": 313
          }
        },
        {
          "id": "1a3415ac-2715-4717-9cbf-051853958e76",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "cy: Through automation of repetitive tasks, watsonx.ai \nenables teams to focus on high er-value activities. gen AI capabilities can handle complex \nprocesses, which improve the mean time to resolution for IT incidents and streamlining customer service.\n/SM590000Scalable and cost-efficient: watsonx.ai support for hybrid cloud and open architecture \nprovides cost flexibility. Businesses can choose the most economical deployment \nenvironment and scale AI workloads as needed.\n/SM590000Trust and governance: With watsonx.governance tightly integrated, watsonx.ai helps \nensure that AI models operate transparently and ethically. Businesses can meet regulatory compliance standards, which mitigate risks that are associated with biased or unexplainable AI decisions.\n/SM590000Business innovation: watsonx.ai enables companies to explore new AI-driven \nopportunities, such as personalizing customer experiences, optimizing supply chains, and driving data-driven decision-making.\nwatsonx.ai is a transfo",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 79,
            "total_chunks": 313
          }
        },
        {
          "id": "53162363-4a2d-4eee-b59f-e9e3769506ea",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "eriences, optimizing supply chains, and driving data-driven decision-making.\nwatsonx.ai is a transformative tool that empowers businesses to unlock the full potential of AI. \nBy integrating seamlessly with watsonx.data and watsonx.governance, it offers a unified platform that combines innovation, efficiency, and compliance. Organizations adopting watsonx.ai can expect to gain a competitive edge through smarter automation, better decision-making, and faster scaling of AI solutions.\n2.2  Synergy between watsonx.a i and other components in the \nwatsonx platform \nThis section covers the following topics:\n/SM590000Synergy between watsonx.ai and watsonx.data\n/SM590000Synergy between watsonx.ai and watsonx.governance\n2.2.1  Synergy between wa tsonx.ai and watsonx.data\nwatsonx.ai and watsonx.data streamline the development and deployment of AI models by \nensuring that AI systems are powe red by high-quality, trusted data. Here is how their synergy \ncreates value:\n/SM590000Efficient AI model de",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 80,
            "total_chunks": 313
          }
        },
        {
          "id": "85591352-6002-4515-b1c3-5459d64ef3b7",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "high-quality, trusted data. Here is how their synergy \ncreates value:\n/SM590000Efficient AI model development: watsonx.data provides a robust and scalable data \nlakehouse that is optimized for AI workloads.  This lakehouse helps ensure that watsonx.ai \nhas instant access to vast amounts of clean, organized, and queryable data, which accelerates training and fine-tuning of AI models.\n/SM590000Real-time data for AI: watson x.data facilitates real-time da ta streaming, which enables \nwatsonx.ai to build and run AI models on up-to-date information. This approach enables dynamic AI use cases, such as predictive maintenance and fraud detection.\n\n16 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai/SM590000Hybrid and multi-cloud flexibility: Both wats onx.ai and watsonx.data  support deployment \nacross hybrid and multi-clou d environments, which help en sure scalability and cost \nefficiency while keeping data sovereignty intact.\n/SM590000Unified data governance: With  w",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 81,
            "total_chunks": 313
          }
        },
        {
          "id": "285e187c-d9eb-48d2-be87-1262ac6d8080",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "nd cost \nefficiency while keeping data sovereignty intact.\n/SM590000Unified data governance: With  watsonx.data acting as th e backbone, organizations can \nensure data integrity, enhance data sharing, and maintain a single source of truth for AI models that are developed in watsonx.ai.\n2.2.2  Synergy between wats onx.ai and watsonx.governance\nThe relationship between watsonx.ai and watsonx.governance helps ensure that AI models \nare high-performing, compliant, ethical, and transparent. Here is how they complement each other:\n/SM590000Ethical AI deployment: watsonx.governance provides guardrails for AI models that are \ndeveloped and deployed through watsonx.ai. These guardrails include bias detection, \nexplainability, and compliance trac king to help ensure  that AI decisions are fair and aligned \nwith regulatory standards.\n/SM590000Lifecycle management and monitoring: watsonx.governance tracks the entire lifecycle of \nAI models by monitoring performance, drift, and adherence to governa",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 82,
            "total_chunks": 313
          }
        },
        {
          "id": "7fec3391-e956-4b96-bcfe-8ac662e47947",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "tracks the entire lifecycle of \nAI models by monitoring performance, drift, and adherence to governance policies. This approach enables watsonx.ai users to maintain the integrity of deployed models over time.\n/SM590000Transparency and a uditability: Models that are built  on watsonx.ai benefit from the \nwatsonx.governance robust repo rting and audit capabilities, which provide stakeholders \nwith clear insights into how AI models make decisions, which help ensure trustworthiness.\n/SM590000Risk mitigation and remediation: If there are anomalies or breaches in governance \npolicies, watsonx.governance en ables quick remediation. Th is capability is crucial for \nmission-critical applications where trust in AI outputs is paramount.\n2.3  Business impact of these synergies\nBy leveraging the combined strengths of watsonx.ai, watsonx.data, and watsonx.governance, enterprises can achieve the following goals:\n/SM590000Help ensure that their AI models are trained on trusted, compliant data.\n/SM5900",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 83,
            "total_chunks": 313
          }
        },
        {
          "id": "c6413a8c-ed86-4a5d-a586-7e74409c12e7",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ing goals:\n/SM590000Help ensure that their AI models are trained on trusted, compliant data.\n/SM590000Maintain high performance and ethical standards across AI deployments.\n/SM590000Accelerate innovation while minimizing the risks that are associated with AI adoption.\nThis holistic approach empowers organizations to maximize ROI on their AI investments and \ngain a competitive edge in their industries.\n\n© Copyright IBM Corp. 2025. 17Chapter 3.Tools for diverse data science \nteams\nData science teams today are di verse in terms of skill sets, ba ckgrounds, and experiences. \nThese teams are also diverse in terms of the types of solutions that are implemented. Common roles of data science teams are data scientists, machine learning (ML) engineers, and artificial intelligence (AI) engineers. Therefore, different types of tools and solutions are needed to support data science teams. \nThis chapter describes a few of the key personas for IBM watsonx.ai and how the different \ntypes of tools that",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 84,
            "total_chunks": 313
          }
        },
        {
          "id": "15594ecb-fe03-45ee-a934-e6cf9b699efd",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "er describes a few of the key personas for IBM watsonx.ai and how the different \ntypes of tools that are available on watsonx.ai support these individuals in their day-to-day \nwork. \nThe following topics are described in this chapter:\n/SM5900003.1, “Key personas for watsonx.ai” on page 18\n/SM5900003.2, “Low-code, no-code, and full-code tools” on page 203\n\n18 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai3.1  Key personas for watsonx.ai \nMany organizations are building data science te ams. Depending on the level of data science \nmaturity within the organization, the types of ro les and the experience of individuals in these \nroles can vary. Common roles in data science te ams include data analysts, data scientists, \nML engineers, and AI engineers. Other roles in organizations that can benefit from IBM watsonx.ai include, but are not limited to, data science leaders, directors of enterprise architecture, and line-of-business users. This section goes through a fe",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 85,
            "total_chunks": 313
          }
        },
        {
          "id": "39bce1ed-08ad-4520-809d-28dd3d3d3b03",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "rs, directors of enterprise architecture, and line-of-business users. This section goes through a few of these key personas by providing an overview of each ro le’s responsibilities, common challenges that \nindividuals in this role often fa ce, and how IBM watsonx.ai is designed to enable individuals in \nthese roles. \n3.1.1  Data scientists\nData scientists use data to solve proble ms and improve decision making within an \norganization. Depending on th e team and organ ization, their respon sibilities include data \ncollection, model development, data analysis, communication of findings, and providing recommendations. Data scientists often have a background in mathematics, specifically \nstatistics and linear algebra, and programming.\nIndividuals in these roles often face challenges  that are related to a lack of self-service \naccess to the correct, or accurate data, for cleaning, transforming, and generating insights. They also lack integrated tools across the model lifecycle, and depend",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 86,
            "total_chunks": 313
          }
        },
        {
          "id": "ec904c49-6329-4077-bffe-f234116164f9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ing, and generating insights. They also lack integrated tools across the model lifecycle, and depending on their level of experience and skill set, they might lack experi ence with creating models by using certain \ntools or programming languages.\nIBM watsonx.ai addressees this situation with vari ous tools within the platform to provide data \nscientists with the flexibility that  they need to build  models. These tools include no-code, \nlow-code, and full-co de solutions such as AutoAI, IBM SPSS® Modeler, and Jupyter \nNotebooks. watsonx.ai provides many tools to select from because the correct tool to use varies based on factors such as the indivi duals’ level of expertise and the project \nrequirements.\nData scientists often collaborate with others in their team, such as business stakeholders, \ndata science leaders, or fellow data scientists. watsonx.ai enables storing and sharing of assets among users within an organization through projects. In watsonx.ai, \nprojects  are \ncollaborativ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 87,
            "total_chunks": 313
          }
        },
        {
          "id": "680ca56e-31ff-4869-8405-1056a3149945",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ets among users within an organization through projects. In watsonx.ai, \nprojects  are \ncollaborative workspaces where individuals can work with and share data and other assets to \naccomplish a specific goal.\n3.1.2  Machine learning engineers\nML engineers work with data scientists and other IT experts, such as software developers to automate and move ML models into production. Typically, ML engineers have a background in computer science, mathematics, statistics, or software engineering. ML engineers are responsible for the data science pipeline, which can include sourcing and preparing data, building and training models, deploying models to production, and maintaining and improving existing ML systems. \nCommon challenges that are faced by ML engi neers include difficulty in defining short and \nlong-term goals, difficulty in scaling ML models , general lack of support for the services that \nare used by various teams, an d incompatibility between tools.\n\nChapter 3. Tools for diverse dat",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 88,
            "total_chunks": 313
          }
        },
        {
          "id": "452fdf25-7bb7-4c42-9324-1dc18bbe2136",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "at \nare used by various teams, an d incompatibility between tools.\n\nChapter 3. Tools for diverse data science teams 19IBM watsonx.ai helps overcome these challenges by providing various no-code, low-code, \nand full-code tools, which are compatible with each one. Also, with IBM Watson Machine Learning on IBM watsonx.ai, the model deployment process is simplified through capabilities \nsuch as deployment spaces. Regardless of the tool on watsonx.ai that is used to develop the model, ML engineers can deploy the model, which makes it simpler to deploy and manage ML assets. \n3.1.3  AI engineers\nAI engineers are responsible for developing, maintaining, and implementing AI systems. Common tasks that are performed by AI engineers include building and maintaining AI systems, and tuning AI models. AI engineers generally have a background in computer science, mathematics, software engineering, or programming.\nBecause the AI engineer role is new to many organizations, a common challenge many \nteams",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 89,
            "total_chunks": 313
          }
        },
        {
          "id": "efc60334-13fd-4254-bccf-6834261d1ed8",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ogramming.\nBecause the AI engineer role is new to many organizations, a common challenge many \nteams and individuals in this ro le face involves having varying levels of technical skills and \nexperience to implement AI solutions. Also, other challenges include selecting and fine-tuning models for a specific use case, and managing the cost to implement and maintain an AI solution. \nIBM watsonx.ai helps AI engineers overcome these challenges in a few ways, starting with \nthe foundation model (FM) library that is provid ed by IBM. This library includes a diverse \nselection of AI models, such as the following ones:\n/SM590000IBM trained models (the Granite and Slate model series) \n/SM590000IBM selected open-source models through Hugging Face\n/SM590000Third-party models such  as llama and mixtral \nTeams can choose among many different FMs for their use case, and choose a model that is \nbest suited for their use case. Teams are not locked into one specific vendor or model series. Also, for mo",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 90,
            "total_chunks": 313
          }
        },
        {
          "id": "d04c66a1-e72d-425f-89e6-0e8c6e3b2e05",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ited for their use case. Teams are not locked into one specific vendor or model series. Also, for more advanced AI engineers, th ey can upload and deploy their own FMs. \nWith watsonx.ai, there are different ways for AI engineers to work with models. The Prompt \nLab tool in watsonx.ai enables AI engineers to write effective prompts (by using a GUI) to deploy to FMs for inferencing. For individu als who have more programming experience and \ntechnical expertise, there is also the programmatic alternative to the Prompt Lab, where users can prompt FMs by using the Python library or REST API. \nYou can use the tuning studio in watsonx.ai to tune a smaller FM to improve its performance. \nAI engineers can tune a smaller FM to achieve comparable results to larger models in the same model family, which can lead to reduced inference costs in the long term. \n\n20 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai3.2  Low-code, no-code, and full-code tools\nBecause the demand for",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 91,
            "total_chunks": 313
          }
        },
        {
          "id": "c2679fdd-2ba0-4c54-a9ce-3d7dc2b5fda5",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "he Power of AI with IBM watsonx.ai3.2  Low-code, no-code, and full-code tools\nBecause the demand for data scientists has grown over the years, finding experienced data \nscientists is difficult for many organizations. For organizations that are newer to data science, \nthey find that experienced data scientists often request higher salaries or tend to seek opportunities with interesting and advanced problems to solve.\nTherefore, it is important for organizations to find ML and AI platforms that support individuals \nwith varying skill sets. There are many different types of tools for implementing data science, \nmachine learning operations (MLOps), and generative AI (gen AI) solutions. At a high level, they can fall into one of three categories: no-code, low-code, or full-code. \n/SM590000With no-code tools, users can create soluti ons and applications without writing code. The \ntool provides a GUI and includes “drag-and-drop” features so that users can build models with little to no prog r",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 92,
            "total_chunks": 313
          }
        },
        {
          "id": "8a24f906-7058-418e-802c-f6f2ef00e837",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " a GUI and includes “drag-and-drop” features so that users can build models with little to no prog ramming knowledge. \n/SM590000Low-code tools provide a visual approach to  development so that users can generate \nsolutions, such as models, with minimal ha nd-coding. Like no-code tools, these tools \ntypically provide a GUI and include “drag-and-drop” features. Low-code tools enable users with minimal coding experience, such as citize n data scientists or business analysts to \nquickly build and implement a solution.\n/SM590000With full-code tools, users can write their own code to develop solutions. These tools are \ntypically leveraged by experien ced data scientists. Full-code to ols enable greater flexibility \nand more customization, but require deeper programming knowledge and expertise. \n3.2.1  No-code, low-code, and fu ll-code tools on  IBM watsonx.ai\nWhen to use a no-code, lo w-code, or full-code tool varies by project requirements, team skill \nset, and the time and cost that a spec",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 93,
            "total_chunks": 313
          }
        },
        {
          "id": "333bf337-baee-475b-8f09-126e82750b46",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "or full-code tool varies by project requirements, team skill \nset, and the time and cost that a specific solution has for an organization. IBM watsonx.ai provides data scientists, ML e ngineers, and AI engin eers with the flexibility of choose the right \ntool for a use case by providing various no-code, low-code, and full-code tools as part of the overall platform. \nNo-code solutions on IBM watsonx.ai\nThis section highlights a few of the no-code tools that are available on the IBM watsonx.ai \nplatform. \nAutoAI\nAutoAI is a no-code tool that data scientists can use to develop and prototype models quickly, \nwithout requiring the user to code or have programming knowledge. AutoAI helps data scientists and data science teams compare the resu lts of multiple models efficiently, thus \nproviding teams with the opportunity to save time and money.\nFigure 3-1 on page 21 shows an AutoAI experiment on watsonx.ai. \n\nChapter 3. Tools for diverse data science teams 21Figure 3-1   AutoAI experiment on ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 94,
            "total_chunks": 313
          }
        },
        {
          "id": "b12a84ca-d2fd-4275-89ba-4857b69bf25f",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "n watsonx.ai. \n\nChapter 3. Tools for diverse data science teams 21Figure 3-1   AutoAI experiment on IBM watsonx.ai\nAlthough AutoAI is a no-code tool, the ML pipelines that are generated by an AutoAI \nexperiment can be exported as a notebook. Therefore, more experienced data scientists can view the code “under the hood” and make modifications and updates to the underlying code. \nFigure 3-2 shows an example notebook that was created from a pipeline that was generated \nfrom an AutoAI experiment. \nFigure 3-2   Pipeline notebook that wa s generated from an AutoAI experiment\n\n\n22 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiAutoAI for Retrieval-Augmented Generation \nRetrieval-Augmented Generation (RAG) is an AI framework for improving the quality of large \nlanguage model (LLM)-generated responses by grounding the model in external sources of knowledge to supplement the LLM’s internal representation of information. The key benefits of RAG solutions include reducing ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 95,
            "total_chunks": 313
          }
        },
        {
          "id": "c16fbc98-4b49-4f75-ad5b-95a23dbe93c1",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "he LLM’s internal representation of information. The key benefits of RAG solutions include reducing the chance that an LLM leaks sensitive data or ‘hallucinate’ incorrect or misleading information, and reducing the need for users to continuously train the model on new data, thus resulting in lower computational and financial costs. For more information about RAG, see What is Retrieval-Augmented Generation?\nMany organizations are implementing RAG solutions  as part of their gen AI efforts. Although \nRAG has many benefits, there are challenges too, such as creating a robust and scalable pipeline, and the time to deliver and implement RAG solutions. \nAutoAI for RAG is a tool that was created by IBM. Similar to AutoAI, AutoAI for RAG is \nintended to help AI engineers quickly build RAG solutions. With AutoAI for RAG, AI engineers can quickly build and test multiple  RAG pipelines without writing code. From the pipelines that \nare generated, the AI engineer can assess the performance of each",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 96,
            "total_chunks": 313
          }
        },
        {
          "id": "14a72bf8-4286-4fb0-8598-4128279a6d94",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ing code. From the pipelines that \nare generated, the AI engineer can assess the performance of each pipeline, select the best pipeline for their team and project, and deploy it into a production or non-production environment.\nFigure 3-3 shows an AutoAI for RAG experiment.\nFigure 3-3   AutoAI for RAG experiment on IBM watsonx.ai \nFor more information about AutoAI for RAG, view the documentation and demo video at \nCreating a RAG experiment (fast path) (Beta) .\nSynthetic Data Generator \nThe Synthetic Data Generator is a no-code tool that you can use to generate tabular data for \nmodel training. Users have two options to genera te synthetic data by using the graphical flow \neditor in the Synthetic Data Generator tool:\n/SM590000Generate synthetic tabular data based on production data, with the goal of masking and \nmimicking this data. \n/SM590000Generate synthetic data from a custom data sc hema that is defined by the user by using \nvisual flows and modeling algorithms. \n\n\nChapter 3. Tools ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 97,
            "total_chunks": 313
          }
        },
        {
          "id": "b2a6ce5c-9920-4859-acf7-fefdc0f92ca9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ema that is defined by the user by using \nvisual flows and modeling algorithms. \n\n\nChapter 3. Tools for diverse data science teams 23Figure 3-4 shows a view of the Synthetic Data Generator interface on watsonx.ai. \nFigure 3-4   Synthetic Data Generator on IBM watsonx.ai\nData Refinery\nData Refinery is a no-code tool that you can use to prepare and visualize data without writing \nany code. With this tool, you can prepare the da ta for analysis by applying operations such as \nfilters and aggregations, and you can generate visualizations such as pie charts and bar charts to extract insights and share findings with stakeholders. \nFigure 3-5 shows an example of a Data Refinery flow on watsonx.ai. \nFigure 3-5   Data Refinery flow on IBM watsonx.ai\n\n\n24 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiPrompt Lab\nPrompt Lab is a tool in IBM watsonx.ai that  you can use to experiment with prompting \ndifferent FMs, and create and share effective prompts to submit to deploye",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 98,
            "total_chunks": 313
          }
        },
        {
          "id": "3124d899-3dee-4ad2-ad32-44168379ac1a",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "xperiment with prompting \ndifferent FMs, and create and share effective prompts to submit to deployed FMs for inferencing. Prompt Lab is a no-code tool that provides AI engineers with three different edit \nmodes for prompt editi ng: Chat, Structured, and  Freeform. This flexib ility enables novice and \nexperienced AI engineers to get the most out of Prompt Lab and watsonx.ai. The Chat mode enables users to converse with a FM of their choice. The Structured mode is great for novice users because it helps them create effective pr ompts by providing defined fields, and also by \nproviding sample templates to build on. The Freeform mode is great for more experienced AI engineers who know how to format a prompt; with this option, users submit prompts in plain text. \nFigure 3-6 shows the Chat mode in Prompt Lab on watsonx.ai.\nFigure 3-6   Prompt Lab Chat on IBM watsonx.ai\nLow-code solutions on IBM watsonx.ai\nThis section highlights a few of the low-code tools that are available on the IBM wat",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 99,
            "total_chunks": 313
          }
        },
        {
          "id": "9a711ed4-ca45-427f-b7e4-5d6e0e5556ae",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "IBM watsonx.ai\nThis section highlights a few of the low-code tools that are available on the IBM watsonx.ai \nplatform.\nSPSS Modeler\nSPSS Modeler is a low-code tool that you can use to  transform data and build models with \nlittle to no-programming experience. You can drag-and-drop various nodes onto the canvas to create a flow to perform various tasks, such as importing data, merging data, and creating models. \nFigure 3-7 on page 25 shows an exampl e SPSS Modeler flow in watsonx.ai. \n\n\nChapter 3. Tools for diverse data science teams 25Figure 3-7   SPSS Modeler flow on IBM watsonx.ai\nFull-code solutions on IBM watsonx.ai\nThis section highlights a few of the full-code tools that are available on the IBM watsonx.ai \nplatform. \nJupyter Notebook editor\nThe Jupyter Notebook editor is a full-code tool  that is available on IBM watsonx.ai. Jupyter \nNotebooks enable more experienced data scientists and developers to write and run code directly on the IBM watsonx.ai platform. Teams can build mor",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 100,
            "total_chunks": 313
          }
        },
        {
          "id": "1e3f3c1c-eb82-4d61-8abd-a0900e0cd7ae",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ts and developers to write and run code directly on the IBM watsonx.ai platform. Teams can build more customized and flexible solutions. \nFigure 3-8 shows an example of a Jupyter Notebook on the watsonx.ai platform. \nFigure 3-8   Jupyter Notebook on IBM watsonx.ai\n\n\n26 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiRStudio\nRStudio, like the Jupyter Notebook editor, is  a full-code tool that is available on \nIBM watsonx.ai. RStudio enables individuals with  programming experience in R to visualize \ndata, create models, and build solutions by using the R programming language on the IBM watsonx.ai platform. \nFigure 3-9 shows the RStudio interface on the watsonx.ai platform. \nFigure 3-9   RStudio on IBM watsonx.ai\nProgrammatic alternative to Prompt Lab\nIBM watsonx.ai has a Python library and REST API that you can use to prompt FMs. This \napproach is an alternative to the GUI in the Prompt Lab that is used to prompt FMs. This option is great for users who have more ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 101,
            "total_chunks": 313
          }
        },
        {
          "id": "9b318c86-5456-4fc4-bf80-4a95c861668e",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " the GUI in the Prompt Lab that is used to prompt FMs. This option is great for users who have more programming or technical experience, and for projects and teams that might require an alternative to the Prompt Lab.\nFor more information about using the REST API or Python library to prompt FMs, see Coding \ngenerative AI solutions .\n\n\n© Copyright IBM Corp. 2025. 27Chapter 4.Building and using artificial \nintelligence models\nThis chapter serves as a resource for setting up , building, fine-tuning, and deploying artificial \nintelligence (AI) models within  the IBM watsonx.ai ecosystem. By exploring key features, and \nbest practices, it aims to empower users (begin ners or experienced practitioners) to harness \nthe power of AI for developing effective business solutions and enhancing their AI initiatives.\nThe following topics are described in this chapter:\n/SM5900004.1, “Prerequisites and assumptions” on page 28\n/SM5900004.2, “How to use this chapter” on page 28\n/SM5900004.3, “Building and",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 102,
            "total_chunks": 313
          }
        },
        {
          "id": "a52360af-9a36-4ca2-9290-612d3b2695d3",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "sumptions” on page 28\n/SM5900004.2, “How to use this chapter” on page 28\n/SM5900004.3, “Building and using AI models in watsonx.ai” on page 28\n/SM5900004.4, “Getting started with watsonx.ai: Setting up the environment” on page 29\n/SM5900004.5, “Data preparation and ingestion for AI model building” on page 34\n/SM5900004.6, “Building AI models in watsonx.ai” on page 38\n/SM5900004.7, “Deploying AI models in watsonx.ai” on page 40\n/SM5900004.8, “watsonx.ai LLM deployment” on page 45\n/SM5900004.9, “Operationalizing machine learning and LLM models” on page 50\n/SM5900004.10, “Additional information and where to go next” on page 544\n\n28 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai4.1  Prerequisites and assumptions\nIn this chapter, it is assumed that you have met the following prerequisites:\n/SM590000An active IBM Cloud® account: You have an ac tive IBM Cloud account. If you do not have \none, see watsonx.ai .\n/SM590000Familiarity with Jupyter Noteboo ks: You understa",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 103,
            "total_chunks": 313
          }
        },
        {
          "id": "6969c0bf-2540-4784-9b71-642f11ac4f69",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "If you do not have \none, see watsonx.ai .\n/SM590000Familiarity with Jupyter Noteboo ks: You understand how to na vigate and work with Jupyter \nNotebooks for data exploration and model development.\n/SM590000Proficiency in Python: You have basic to intermediate knowledge of Python programming \nbecause many examples, scripts, and workfl ows in this chapter use Python code.\n/SM590000Cloud computing concepts: You have a basic und erstanding of cloud computing principles, \nincluding concepts such as APIs, data storage, computing resources, and cloud-based environments.\n/SM590000Knowledge of large language models (LLMs): A general understanding of LLMs, their \ncapabilities, and use cases is  beneficial for build ing and fine-tuning AI models within \nwatsonx.ai.\n/SM590000You read Chapters 1 - 3 of this book.\n4.2  How to use this chapter\nThis chapter is structured to help users of varying expertise levels by providing a general \napproach to understanding, building, deploying, and optimizing AI ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 104,
            "total_chunks": 313
          }
        },
        {
          "id": "dcc39321-2c71-4269-aed7-d3c62ef412b1",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "se levels by providing a general \napproach to understanding, building, deploying, and optimizing AI models by using the watsonx.ai platform. \nFor beginners, start with the overview sections to familiarize yourself with the platform's \nfeatures and capabilities. Progress through the chapter sequentially, beginning with \nenvironment setup and basic concepts before delving into more comp lex topics, such as \nmodel building an d optimization. \nFor experienced users, you can go directly to sp ecific chapters of in terest, such as model \noptimization techniques, deployment strategies, or advanced configurations. Each section is modular and provides targeted information and best practices that you can apply immediately to your projects. \nThroughout the chapter, you find practical examples, hands-on exercises, and links to more \nresources to help ensure a well-rounded learning experience. \n4.3  Building and using AI models in watsonx.ai\nThis section covers the following topics:\n/SM590000Overvi",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 105,
            "total_chunks": 313
          }
        },
        {
          "id": "ec5014f6-d5d6-4ae9-ac82-64136c712590",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "Building and using AI models in watsonx.ai\nThis section covers the following topics:\n/SM590000Overview of the watsonx.ai platform\n/SM590000Key features and capabilities\n4.3.1  Overview of the watsonx.ai platform\nwatsonx.ai is an enterprise-grade AI studio that you use to streamline the development, \ntraining, tuning, and depl oyment of AI models. It  includes generative AI  (gen AI) capabilities \nthat are powered by foundation models (FMs).\n\nChapter 4. Building and using artificial intelligence models 294.3.2  Key features and capabilities\nHere are the key features a nd capabilities of watsonx.ai:\n/SM590000Foundation models: Access various powerful, low-cost, and fit-for-purpose models, such \nas the IBM Granite series and other LLMs for tasks such as content generation, summarization, and classification.\n/SM590000Data preparation: Use tools for refining and visualizing data to help ensure high-quality \ninputs for model training. \n/SM590000Model development: Build machine learning (ML) ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 106,
            "total_chunks": 313
          }
        },
        {
          "id": "c0a3cd78-9398-4324-9d70-a80e1a1673f4",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "e high-quality \ninputs for model training. \n/SM590000Model development: Build machine learning (ML) models by using open-source \nframeworks with options for code-based, automated, or visual data science approaches.\n/SM590000Prompt Lab: Experiment with gen AI prompts to enable tasks like question answering, \ncontent generation, summarization, text classification, and data extraction.\n/SM590000Tuning Studio: Fine-tune FMs to customize outputs for specific use cases to enhance \nmodel performance and accuracy.\n/SM590000InstructLab: At the time of writing, this feature is planned to be integrated into watsonx.ai \nin the future.\n4.4  Getting started with watsonx .ai: Setting up the environment\nTo set up the watsonx.ai environment, you must have an IBM Cloud account. To register for \nan account, see Create an IBM Cloud account .\nAt the time of writing, here are the high-level steps to provision watsonx.ai in a \nSoftware-as-a-Service (SaaS) environment:\n1. Set up your IBM Cloud account.2. Crea",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 107,
            "total_chunks": 313
          }
        },
        {
          "id": "7c567c8d-cedf-4372-bc34-3c7f47ad64bc",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " watsonx.ai in a \nSoftware-as-a-Service (SaaS) environment:\n1. Set up your IBM Cloud account.2. Create a Project in watsonx:\na. Log in to https://dataplatform.cloud.ibm.com/login\nb. Expand the ‘hamburger’ navigation menu, as shown in Figure 4-1.\nFigure 4-1   watsonx navigation menu icon\nc. Select Projects → View all projects , as shown in Figure 4-2.\nFigure 4-2   View all projects menu\n\n\n30 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aid. Click New project + , as shown in Figure 4-3.\nFigure 4-3   New project+ menu\ne. Enter the project name and, if app licable, upload lo cal files. Click Create , as shown in \nFigure 4-4.\nFigure 4-4   Create menu\n3. Provision watsonx.ai Studio:\na. Log in to https://cloud.ibm.com/ .\nb. To add services from the catalog, use the search box that is shown in Figure 4-5.\nFigure 4-5   watsonx.ai utilities search box\nc. Enter “watsonx.ai Studio” and choose the studio from the catalog, as shown in \nFigure 4-6.\nFigure 4-6   wastonx.ai uti",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 108,
            "total_chunks": 313
          }
        },
        {
          "id": "da5e0ff7-554e-4d67-b1b3-c3be1e941c74",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "Studio” and choose the studio from the catalog, as shown in \nFigure 4-6.\nFigure 4-6   wastonx.ai utilities list\nd. Select a pricing plan ( Lite or Professional ), and then click Create .\nFigure 4-7   watsonx.ai pricing plan selection\n4. Generate a watsonx.ai API key:\na. Go to https://cloud.ibm.com/iam/apikeys .\nb. Click Create , as shown in Figure 4-8.\nFigure 4-8   Create API key menu option\n\n\nChapter 4. Building and using artificial intelligence models 31c. Enter the relevant information, and then click Create, as shown in Figure 4-9.\nFigure 4-9   Create IBM Cloud API key\nd. After the API key is successf ully created, copy or downlo ad the key and save it locally, \nas shown in Figure 4-10\nFigure 4-10   API key creation\n5. Open watsonx.ai Studio and associate the watsonx.ai service:\na. In the upper left of your screen, click the four horizontal lines, as shown in Figure 4-11\nFigure 4-11   watsonx.ai studio menu icon\n\n\n32 Simplify Your AI Journey: Unleashing the Power of AI with IBM wat",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 109,
            "total_chunks": 313
          }
        },
        {
          "id": "11c06a16-dbc1-4b45-a683-07c66f6d0032",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "  watsonx.ai studio menu icon\n\n\n32 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aib. Select Resource list , as shown in Figure 4-12.\nFigure 4-12   watsonx.ai studio Resource list option\nc. Expand AI / Machine Learning , as shown in Figure 4-13.\nFigure 4-13   AI / Machine Learning menu option\nd. Click the watsonx.ai Studio  record, as shown in Figure 4-14.\nFigure 4-14   watsonx.ai Studio record icon\ne. Click View full details , as shown in Figure 4-15.\nFigure 4-15   watsonx.ai studio details\nf. Select Launch in → IBM watsonx , as shown in Figure 4-16.\nFigure 4-16   watsonx.ai Launch in option\nFigure 4-17 shows the Welcome to watsonx window.\nFigure 4-17   Welcome to watsonx window\n\n\nChapter 4. Building and using artificial intelligence models 33g. Click the + in the  Projects table.\nh. Enter a project name, for example, test_project , as shown in Figure 4-18.\nFigure 4-18   Define details window\ni. Select Storage (if required).\nj. Click Create .\nk. Select Chat and",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 110,
            "total_chunks": 313
          }
        },
        {
          "id": "857cabb3-f30e-4765-bc01-79768e6a6911",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "e 4-18   Define details window\ni. Select Storage (if required).\nj. Click Create .\nk. Select Chat and build prompts with foundations models .\nl. Click Associate service  to associate a Watson ML servic e to the project, as shown in \nFigure 4-19.\nFigure 4-19   Clicking Associate service \nm. Select the displayed Machine Learning  service and click Associate , as shown in \nFigure 4-20.\nFigure 4-20   watsonx machine learning: Associate service\n\n\n34 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ain. Go to the Assets  tab, and then click New asset , as shown in Figure 4-21.\nFigure 4-21   watsonx projects: All assets\no. Select Chat and build prompts with foundation models , as shown in Figure 4-22. \nFigure 4-22   Chat and build pr ompts with foundation models tile\n4.5  Data preparation and ingestion for AI model building\nThis section describes the following topics:\n/SM590000Understanding the importance of data in AI\n/SM590000Preparing and cleaning data: data quality con",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 111,
            "total_chunks": 313
          }
        },
        {
          "id": "95222ba5-3521-4158-898b-1f8d697083c9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "000Understanding the importance of data in AI\n/SM590000Preparing and cleaning data: data quality considerations\n/SM590000Handling missing data, outliers, and bias\n/SM590000Ingesting data into watsonx.ai Studio\n/SM590000Connecting to data repositories and cloud services\n4.5.1  Understanding the importance of data in AI\nData is the backbone of AI. It shapes the accuracy, effectiveness, and reliability of AI models. \nHigh-quality data enables AI to learn patterns, make predictions, and deliver meaningful \ninsights. Understanding the importance of data in AI goes beyond mere volume; it involves ensuring data accuracy, consistency, and fairness. Poor data quality, bias, or gaps can lead to flawed models, incorrect predictions, and poten tial ethical issues. Therefore, this section \nfocuses on the robust data preparation, cleaning, and validation that is essential for building AI systems that are trustworthy, transparent,  and impactful in real-world applications.\n\n\nChapter 4. Building and u",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 112,
            "total_chunks": 313
          }
        },
        {
          "id": "e32fb9eb-aaed-45aa-afc6-fd2f6c10deb0",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "are trustworthy, transparent,  and impactful in real-world applications.\n\n\nChapter 4. Building and using artificial intelligence models 354.5.2  Preparing and cleaning data: data quality considerations\nData quality is a critical factor in the success of  AI models because it directly influences their \nperformance and accuracy. Here are some key considerations:\n/SM590000Accuracy: Ensuring that data is correct, consistent, and error-free is vital for creating \nreliable AI models. Inaccurate data can lead to faulty predictions and flawed decision-making.\n/SM590000Completeness: AI models rely on comprehensive datasets to learn effectively. Missing \ndata can skew results, which cause incomplete or biased predictions.\n/SM590000Consistency: Data must be consistent across different sources and formats to help ensure \nthe AI model functions as expected.\n/SM590000Relevance: Data should be pertinent to the problem the AI aims to solve. Irrelevant data \nmight introduce noise and reduce model effec",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 113,
            "total_chunks": 313
          }
        },
        {
          "id": "020d69f0-ac70-4108-b965-0e3105ec9fbd",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "t to the problem the AI aims to solve. Irrelevant data \nmight introduce noise and reduce model effectiveness.\n/SM590000Bias and fairness: Addressing data biases and ensuring fairness are critical for building \nethical and unbiased AI systems. Give careful attention to the diversity and representation in data to avoid discrimination and ensure balanced outcomes.\nHigh data quality maximizes th e accuracy, transparency, an d applicability of AI systems, \nwhich ultimately enhances their value and impact.\n4.5.3  Handling missing data, outliers, and bias\nHandling missing data, outliers, and bias is cruc ial for ensuring the accuracy and fairness of \nAI models. Here is how each one is managed: \n/SM590000Handling missing data:\n– Imputation techniques: Missing data can be filled by using st atistical techniques such \nas mean, median, or mode imputation, or more complex methods like k-nearest neighbors (KNNs) or predictive models.\n– Data removal: Sometimes, records with significant missing value",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 114,
            "total_chunks": 313
          }
        },
        {
          "id": "747ae819-9897-4ca3-ae16-8faa9e3f55d4",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "hbors (KNNs) or predictive models.\n– Data removal: Sometimes, records with significant missing values are removed if they \nare unlikely to add useful information.\n– Domain knowledge: Input from domain experts can help determine whether missing \ndata should be treated differently based on context. \n/SM590000Handling outliers:\n– Detection: Outliers are identified by usi ng methods like Z-scores, Interquartile Range \n(IQR), or visualization techniques like boxplots.\n– Treatment: Once detected, outliers can be managed by removal, transformation (for \nexample, log transformation), or capping values based on acceptable thresholds.\n– Contextual consideration: Not all outliers are problematic; they might represent \ngenuine data points. Understanding their impact is crucial before deciding on a course of action.\n/SM590000Addressing bias:\n– Data auditing: Systematic review of datasets to identify sources of bias, such as \nunderrepresentation of specific groups. \n– Data balancing: Techniques like",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 115,
            "total_chunks": 313
          }
        },
        {
          "id": "c9a67b06-5dff-4b91-9a3c-04d417e06868",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "sources of bias, such as \nunderrepresentation of specific groups. \n– Data balancing: Techniques like oversampling, undersampling, or generating synthetic \ndata (for example, SMOTE) can help balance datasets.\n\n36 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai– Algorithmic bias mitigation: Algorithms can be fine-tuned by using fairness constraints \nor reweighting schemes to minimize bias during model training.\n– Regular monitoring: Bias can emerge or c hange over time, which requires continuous \nassessment and model updates to maintain fairness and inclusivity.\nBy handling missing data, outliers, and bias, AI models can provide more accurate, reliable, \nand ethical outcomes, which ultimately increa se their utility and impact across diverse \napplications.\n4.5.4  Ingesting data into watsonx.ai Studio\nThis section describes the following topics:\n/SM590000Supported data formats and sources\n/SM590000Manually uploading data to watsonx.ai Studio\nSupported data formats",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 116,
            "total_chunks": 313
          }
        },
        {
          "id": "42b9fbbb-ee0c-4cba-aaf2-fc886fa4647f",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ata formats and sources\n/SM590000Manually uploading data to watsonx.ai Studio\nSupported data formats and sources\nwatsonx.ai Studio supports seamless ingestion of diverse data formats and sources to \nfacilitate efficient model devel opment and deployment. Suppor ted formats incl ude structured \ndata (CSV, JSON, and Parquet), semi-structured data (XML and Avro), and unstructured data (plain text and PDF). Data can be sourced from cloud storage platforms (such as AWS S3 and IBM Cloud Object Storage), databases (such as PostgreSQL and MySQL), APIs, and on-premises file systems.\nThe ingestion process is op timized for scalability and ca n handle large datasets while \nensuring data integrity and co mpatibility with downstream AI workflows. Integration with \nwatsonx.data and watsonx.governance tools enables a secure, governed data pipeline, which enhances traceabilit y and compliance.\nManually uploading data to watsonx.ai Studio\nYou can upload files through the watsonx.ai Studio interface by ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 117,
            "total_chunks": 313
          }
        },
        {
          "id": "a8868079-532d-48c4-80ea-a3d68852af90",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "uploading data to watsonx.ai Studio\nYou can upload files through the watsonx.ai Studio interface by dragging the files there, as \nshown in Figure 4-23. \nFigure 4-23   watsonx Studio data upload window\n4.5.5  Connecting to data re positories and cloud services\nwatsonx.ai provides robust connectivity options to integrate with various data repositories and \ncloud services, which help ensure smooth data access for AI model development. Users can \nconnect to cloud storage solutions such as IBM Cloud Object Storage, AWS S3, Azure Blob Storage, and Google Cloud Storage, and traditional databases like PostgreSQL, MySQL, and MongoDB.\nFigure 4-24 on page 37 shows the watsonx cloud services connections window.\n\n\nChapter 4. Building and using artificial intelligence models 37Figure 4-24   watsonx cloud services connections \nAt the time of publication, here are the re positories and services that are supported:\n/SM590000Amazon Redshift\n/SM590000Amazon S3\n/SM590000Apache Cassandra\n/SM590000Apache De",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 118,
            "total_chunks": 313
          }
        },
        {
          "id": "f618678b-6d65-438d-9a0d-4e5dbea99cfc",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " supported:\n/SM590000Amazon Redshift\n/SM590000Amazon S3\n/SM590000Apache Cassandra\n/SM590000Apache Derby\n/SM590000Apache HDFS\n/SM590000Apache Hive\n/SM590000Apache Impala\n/SM590000Apache Kafka\n/SM590000Box\n/SM590000DataStax Enterprise\n/SM590000Denodo\n/SM590000Dremio\n/SM590000Dropbox\n/SM590000Elasticsearch\n/SM590000Google BigQuery\n/SM590000Google Cloud Pub/Sub\n/SM590000Google Cloud Storage\n/SM590000Google Locker\n/SM590000Greenplum Database\n/SM590000IBM Cloud Data Engine\n/SM590000IBM Cloud Object Storage\n/SM590000IBM Cloudant®\n/SM590000IBM Cognos® Analytics\n/SM590000IBM Data Virtualization Manager for IBM z/OS®\n/SM590000IBM DataStage® for Cloud Pak for Data\n/SM590000IBM Db2®\n/SM590000IBM Informix®\n/SM590000IBM InfoSphere® DataStage\n/SM590000IBM Match 360\n/SM590000IBM MQ\n/SM590000IBM Netezza® Performance Server\n/SM590000IBM Planning Analytics\n/SM590000IBM watsonx.data\n/SM590000MariaDB\n/SM590000Microsoft Azure Blob Storage\n/SM590000Microsoft Azure Cosmos DB\n/SM590000Microsoft Azure Data Lake",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 119,
            "total_chunks": 313
          }
        },
        {
          "id": "e3fee6ce-3037-4406-9bc3-c164c5ebefa1",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "00Microsoft Azure Blob Storage\n/SM590000Microsoft Azure Cosmos DB\n/SM590000Microsoft Azure Data Lake Storage\n\n\n38 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai/SM590000Microsoft Azure Databricks\n/SM590000Microsoft Azure Files\n/SM590000Microsoft Azure SQL Database\n/SM590000Microsoft Azure Synapse Analytics\n/SM590000Microsoft Power BI\n/SM590000Microsoft SQL Server\n/SM590000Milvus\n/SM590000MongoDB\n/SM590000MySQL\n/SM590000Oracle Database\n/SM590000PostgreSQL\n/SM590000Presto\n/SM590000Salesforce API\n/SM590000SAP ASE\n/SM590000SAP IQ\n/SM590000SAP S/4HANA\n/SM590000SingleStoreDB\n/SM590000Snowflake\n/SM590000Tableau\n/SM590000Teradata database\n/SM590000Vertica\nwatsonx.ai supports secure connection protoc ols, which include API-based integrations, \nJDBC/ODBC drivers, and file transfer mechanisms like SFTP. With built-in authentication and access controls, watsonx.ai helps ensure data  security while main taining flexibility for \nenterprise-scale data workflows.  By streamli",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 120,
            "total_chunks": 313
          }
        },
        {
          "id": "785d35c3-0d51-46b5-9834-476aa415d3a4",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ure data  security while main taining flexibility for \nenterprise-scale data workflows.  By streamlining access to da ta repositories and services, \nthe platform empowers teams to leverage their existing data infrastructure efficiently.\n4.6  Building AI models in watsonx.ai\nThis section describes the following topics:\n/SM590000Choosing the right model for your use case\n/SM590000Model creation workflow\n4.6.1  Choosing the right model for your use case\nwatsonx.ai supports many model types to meet diverse business needs: \n/SM590000Supervised models  for predictive tasks by using labeled data.\n/SM590000Unsupervised models  for discovering patterns in unlabeled data.\n/SM590000Reinforcement learning  (RL) models for decision-making through rewards and penalties.\n/SM590000Large language models  (LLMs) for natural language understanding and generation. \nPretrained LLMs streamline AI development, which enables faster implementation and powerful insights across applications.\nIn addition to its n",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 121,
            "total_chunks": 313
          }
        },
        {
          "id": "d60f8b92-d9a8-411e-a293-9bccc6fec034",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " which enables faster implementation and powerful insights across applications.\nIn addition to its native capa bilities, watsonx.ai embraces flexibility with the IBM Bring Your \nOwn Model (BYOM) feature, which enables users to integrate and fine-tune their own LLMs within the platform for customized solutions. Furthermore, watsonx.ai supports integration with Hugging Face, which enables access to a vast library of pretrained models and tools. This collaboration accelerates development by leveraging open-source innovations while maintaining watsonx.ai enterprise-grade secu rity and scalability.\n\nChapter 4. Building and using artificial intelligence models 394.6.2  Model creation workflow\nThis section describes the workflow of model creation.\nModel selection and configuration\nSelecting the appropriate AI model is key to achieving your business objectives, and \nwatsonx.ai provides the flexib ility to support both LLM and non-LLM models. Whether your \nneeds involve predictive analytics, pa",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 122,
            "total_chunks": 313
          }
        },
        {
          "id": "51b485c0-42cb-4220-84aa-f0e06e8514e4",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "b ility to support both LLM and non-LLM models. Whether your \nneeds involve predictive analytics, pattern discovery, decision-making, or natural language \nprocessing (NLP), watsonx.ai helps ensure th at the correct tools are at your fingertips.\nFor non-LLM tasks, the platform accommodates models such as regression, classification, \nclustering, and RL, which enable a wide range of traditional ML applications.\nWhen working with LLMs, watsonx.ai offers various pretrained models in different sizes, from \nlightweight options for tasks that require efficiency and speed to larger, more complex models that are ideal for nuanced language understanding and generation. Choosing the correct size depends on your specific use case, with sm aller models excelling in cost-effective, \nlower-latency scenarios and larger models delivering superior accuracy and depth for intricate applications.\nWith watsonx.ai, you can confidently match the model type and size to your project’s unique \nrequirements to hel",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 123,
            "total_chunks": 313
          }
        },
        {
          "id": "c5c20fef-da4d-494e-885c-4852039f836c",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": ".ai, you can confidently match the model type and size to your project’s unique \nrequirements to help ensure scalability, efficiency, and impact.\nTraining the model\nTraining your AI model is a crucial step in ta iloring it to your specific business needs. \nwatsonx.ai provides powerful tools and workflows for training both non-LLM models and LLM models, which help ensure flexibility and pr ecision at every stage of development.\nNon-LLM models\nFor traditional ML tasks, watson x.ai offers robust training ca pabilities that leverage tools like \nwatsonx.ai Studio and AutoAI to streamline and enhance the development process.\nwatsonx.ai Studio\nwatson.ai Studio provides a collaborative env ironment for data scientists, developers, and \nanalysts to prepare data, build, and train ML models. With features like Jupyter Notebooks, Python libraries, and model monitoring, wats on.ai Studio is desi gned for flexibility and \nscalability to accommodate pr ojects of any complexity.\nFor more information a",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 124,
            "total_chunks": 313
          }
        },
        {
          "id": "61f755fb-9c55-4d21-8482-765199af369a",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " for flexibility and \nscalability to accommodate pr ojects of any complexity.\nFor more information about watsonx.ai Studio, see IBM Watson Studio .\nAutoAI\nIf you want o accelerate the development process, AutoAI automates key stages of ML, which \ninclude feature engineering, algorithm selection, and hyperparameter optimization. It simplifies the model-building process to make it  accessible to users with varying technical \nexpertise while still deliverin g highly accurate results.\nFor more information about AutoAI, see IBM AutoAI .\nLLM models\nwatsonx.ai provides advanced features for trai ning and fine-tuning LLMs to deliver seamless \ncustomization and performance.\n\n40 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiPrompt Lab\nPrompt Lab is an environment for creating and testing prompts that are tailored to specific \ntasks. With Prompt Lab, users can interact with  pretrained LLMs, evaluate their outputs, and \nrefine instructions for optimal results, all withou",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 125,
            "total_chunks": 313
          }
        },
        {
          "id": "01bf091f-fa91-4b7a-b4cb-59d4a65070a1",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "h  pretrained LLMs, evaluate their outputs, and \nrefine instructions for optimal results, all without extensive coding expertise.\nWithin Prompt Lab, you can explore and train various training LLM training methodologies, \nsuch as Zero-Shot, Few-Shot, Multi-Shot, and Retrieval-Augmented Generation (RAG). \nFor more information about Prompt Lab, see Prompt Lab .\nTuning Studio\nFor deeper customization, Tuning Studio enables fine-tuning of LLMs on your proprietary \ndata, which helps ensure that the model adapts  to your specific dom ain while maintaining \nhigh performance. This feature is ideal for organizations seeking more targeted insights and applications from their AI. \nFor more information about Tuning Studio, see Tuning Studio .\nInstructLab\nAt the time of writing, InstructLab is not available. \nInstructLab will revolutionize the training proc ess by enabling users to craft task-specific \ninstructions, which further enhance the prec ision of LLMs in generating accurate and \nactionable ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 126,
            "total_chunks": 313
          }
        },
        {
          "id": "c41fd85d-70b1-4177-8d48-bf2e4f1248a9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " \ninstructions, which further enhance the prec ision of LLMs in generating accurate and \nactionable outputs. This tool simplifies the proc ess of aligning model behavior with unique \nbusiness objectives.\nWith these comprehensive training tools, watsonx.ai empowers users to harness the full \npotential of their models, whether refining traditional ML algorithms or unleashing the power of cutting-edge LLMs.\n4.7  Deploying AI models in watsonx.ai\nThis section explores the process of deploying AI models in the watsonx.ai platform. It provides a detailed overview of two major deployment options: Studio and Prompt Lab. From deploying models as APIs for real-time infe rence to batch processing workflows, the \nwatsonx.ai platform helps ensure flexibility and scalability. Readers gain insights into selecting \nthe most appropriate deployment strategy for thei r use case while also learning best practices \nto optimize performance and reliab ility in production environments.\n4.7.1  watsonx.ai Studi",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 127,
            "total_chunks": 313
          }
        },
        {
          "id": "6e24f318-4730-4b82-b314-6c0224fd676c",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ctices \nto optimize performance and reliab ility in production environments.\n4.7.1  watsonx.ai Studio deployments\nDeploying models in watsonx.ai St udio involves several key steps to help ensure that your AI \nassets are effectively managed and operational. Here is a concise guide to help you through \nthe process:\n1. Create a deployment space: Begin by estab lishing a deployment space within the studio. \nThis space serves as a collaborative environment where you can manage and deploy your AI assets. To set up a deployment space, go to the Deployment Space section in the Studio interface and follow the prompts to create a space (Figure 4-25 on page 41).\n\nChapter 4. Building and using artificial intelligence models 41Figure 4-25   watsonx.ai studio projects: Deployments\n2. Promote or import your model: When your deployment space is ready, add your trained \nmodel to it. If your model is in a project, promote it to the deployment space. Alternatively, you can import models that are trained ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 128,
            "total_chunks": 313
          }
        },
        {
          "id": "12517f00-3c6d-4253-b43e-7b4d04bad9ff",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " project, promote it to the deployment space. Alternatively, you can import models that are trained externally by uploading them directly into the deployment space. Ensure that the model files are in a compatible format and that any necessary dependencies are addressed. \na. Click the three dots, and then click Promote to space  (Figure 4-26).\nFigure 4-26   Model interface window\nb. Enter deployment_test  in to the Name  field, select Production  under Deployment \nstage, and then click Create  (Figure 4-27).\nFigure 4-27   Create a deployment spaceNote:  To help ensure that your trained model is in the right format and compressed, see \nAdding a model by using UI .\n\n\n42 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai3. Create the deployment: With your model in the deployment space, initiate the deployment \nprocess:\na. Go to the Deployments window (Figure 4-28).\nFigure 4-28   Deployments window\na. Select deployment_test\nb. Click the generated model’s  service (Figu",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 129,
            "total_chunks": 313
          }
        },
        {
          "id": "7ca6444d-e740-416d-9d68-6de3b3f54077",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "re 4-28   Deployments window\na. Select deployment_test\nb. Click the generated model’s  service (Figure 4-29).\nFigure 4-29   Services\nc. Click New deployment , which opens the window that is shown in Figure 4-30 on \npage 43.\n\n\nChapter 4. Building and using artificial intelligence models 43Figure 4-30   Create a deployment\nd. Under Deployment type, select either  Online or Batch , and enter \nregression_model_deployment  under Name  and regression_model_service  under \nServing name. Click Create .\nOnline deployment is ideal for real-time processing, where the model handles input \ndata and provides immediate predictions. Batch deployment is suitable for processing \nlarge datasets in bulk, which generate predictions for a collection of inputs at scheduled intervals or on-demand. \n\n\n44 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai4. Test the deployment: After deployment, validate the model’s functions by going to the \nnewly created deployment and clicking regressio",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 130,
            "total_chunks": 313
          }
        },
        {
          "id": "331c74c8-29bd-44c2-be4b-461809713cc8",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ent, validate the model’s functions by going to the \nnewly created deployment and clicking regression_model_deployment , as shown in \nFigure 4-31. \nFigure 4-31   Deployment overview\nwatsonx.ai Studio provides multiple ways to test the deployment, such as the Test tab and \ncode snippets (Figure 4-32).\nFigure 4-32   Deployment testing\n\n\nChapter 4. Building and using artificial intelligence models 45By following these steps, you can effectively deploy and manage your AI models within \nwatsonx.ai Studio, which helps ensure that the models are ready for production use and \ncapable of delivering valuable insights. \n4.8  watsonx.ai LLM deployment\nDeploying models in watsonx.ai St udio involves several key steps to help ensure that your AI \nassets are effectively managed and operational. This section provides a concise guide to help you through the process:\n4.8.1  Model packaging and exporting\nTo package and export a model, complete the following steps:\n1. Go to the Prompt Lab window, and if y",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 131,
            "total_chunks": 313
          }
        },
        {
          "id": "ba090bdb-2c20-4f02-a4ce-efe2b69f81e2",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "o package and export a model, complete the following steps:\n1. Go to the Prompt Lab window, and if you do not already have a prompt set up, populate it \nby using the example that is shown in Figure 4-33. In this lab, you use a text classification prompt. \nFigure 4-33   watsonx Prompt Lab\n\n\n46 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai2. Generate a response by the prompt. Keep the decoding method as Greedy , and set the \nmax tokens to 5 to produce Positive and Negative text only (Figure 4-34).\nFigure 4-34   Model parameters\n3. Click Generate , which tests the prompt. Then, click the View code icon (Figure 4-35).\nFigure 4-35   View code icon\n4. Copy the code to a notepad application (Figure 4-36).\nFigure 4-36   Code example\n\n\nChapter 4. Building and using artificial intelligence models 47The code (Figure 4-37) is an example of a REST call that starts the model. watsonx.ai \nalso provides a Python API for model invoca tion, which you review later in this lab. ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 132,
            "total_chunks": 313
          }
        },
        {
          "id": "3e3dad3e-8236-4576-b474-75a07b393219",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": ". watsonx.ai \nalso provides a Python API for model invoca tion, which you review later in this lab. \nThe header of the REST request includes the URL where the model is hosted and a \nplaceholder for the authentication token. At the time of writing, all users share a single model inference endpoint. In the future, IBM plans to provide dedicated model endpoints.\nSecurity is managed by the IBM Cloud authentication token, which is described later in \nthis section. \nThe body of the request contains the entire prompt. \nFigure 4-37   Curl command example\n5. At the end of the request, you specify the model parameters and the project ID, as shown \nin Figure 4-38 . \nFigure 4-38   Curl command details\n\n\n48 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiTo look up the project ID, select Project → General → Manage  in the watsonx.ai project, \nas shown in Figure 4-39.\nFigure 4-39   Projects Manage view\n6. Save the newly configured prompt as a notebook. Select Standard noteboo",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 133,
            "total_chunks": 313
          }
        },
        {
          "id": "8b5137e6-a373-4507-a0a1-857e5dd61851",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "39   Projects Manage view\n6. Save the newly configured prompt as a notebook. Select Standard notebook as the \nasset type and then select Save as , as shown in Figure 4-40.\nFigure 4-40   Notebook save icon\n7. You will now create an authentication token. Open the na vigation menu (four horizontal \nbars) in the upper left of the watsonx interface and select Access (IAM) , as shown in \nFigure 4-41 on page 49.\n\n\nChapter 4. Building and using artificial intelligence models 49Figure 4-41   Access (IAM) menu item\n8. Select API Keys → Create . Give the token a name and save it in a notepad (Figure 4-42). \nYou use the token in a Python notebook.\nFigure 4-42   API keys\n9. Go to your watsonx project and open the notebook that you saved in step 8 (Figure 4-43). \nFigure 4-43   watsonx Studio projects overview\n\n\n50 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai10.Review the sample notebook. \nThis notebook acts as a cli ent application that starts the deployed LLM with a Pyth",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 134,
            "total_chunks": 313
          }
        },
        {
          "id": "3f6668bb-47d9-4dd4-855a-a609715df75b",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "mple notebook. \nThis notebook acts as a cli ent application that starts the deployed LLM with a Python SDK. \nYou use the notebook as a client for simplicity of testing during this lab.\nEnterprise client applications can be implemented in Python, Java, .NET, and many other \nprogramming languages. LLMs that are deployed in watsonx.ai can be started either with REST calls or the Python SDK.\nRun the notebook to test the LLM with your prompts.\n4.9  Operationalizing machine learning and LLM models\nNow that you have a machine learning (ML) model or LLM built, you now enter the operational \nphase. Much like how traditional application development created the need for formal DevOps tools and systems, so too have AI models created the need for ModelOps. ModelOps is the practice of enabling the deployment and management of models throughout the application development and deploymen t lifecycle with the goal of operat ionalizing models in production. \nAs a joint endeavor with traditional DevOps, M",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 135,
            "total_chunks": 313
          }
        },
        {
          "id": "bec6cce7-d74b-4870-bda1-7526576ef6a5",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " the goal of operat ionalizing models in production. \nAs a joint endeavor with traditional DevOps, ModelOps takes the feedback and measurements that are taken in the DevOps lifecycle to iterate on the training, testing and deploying stages of th e ModelOps lifecycle. \nKey stages in the ModelOps lifecycle include governance, monitoring, deployment of \ninfrastructure, and m odel versioning. IBM offers variou s tools to help facilitate these \nprocesses, each of which has its own place in the lifecycle. Examples of such tools include \nthe following ones: \n/SM590000IBM watsonx.gov  helps govern and monitor model key performance indicators (KPIs).\n/SM590000IBM Instana ®™ helps monitor LLM performance, responsiveness, and throughput at the \napplication level.\n/SM590000IBM Turbonomic ® can help dynamically scale up and down infrastructure as the workload \nagainst your models changes.\n/SM590000IBM API Connect ® provides a GUI wizard to create AI-aware APIs and products, plus \nintegration with A",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 136,
            "total_chunks": 313
          }
        },
        {
          "id": "e4aebab3-e8c0-4d8e-9a0b-a44269c261db",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "M API Connect ® provides a GUI wizard to create AI-aware APIs and products, plus \nintegration with AI services to forward requests and manage responses.\nTo demonstrate how these models can be deployed into existing applicat ions, we briefly walk \nthrough how to call these models, and integrate them into the DevOps and ModelOps lifecycle. \n4.9.1  Calling ML mode ls by using API calls\nWhen you have built a model, your ML model is live and ready to perform as an inference \nendpoint. This endpoint is your gateway to interact with the model so that you can send data and receive predictions in return. Here, we walk through how you can use it effectively. \nSecuring your API key \nBefore making any calls to your endpoint, yo u need an API key for se cure access. For more \ninformation about generating this key, see 4.8, “watsonx.ai LLM deployment” on page 45. Here is a quick overview: \n1. Go to your deployed model in watsonx.ai Studio.\n2. Go to the Access  tab under the Deployment settings. \n3. ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 137,
            "total_chunks": 313
          }
        },
        {
          "id": "94d2c00b-9b0c-44a1-95c5-00eb42bad276",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ur deployed model in watsonx.ai Studio.\n2. Go to the Access  tab under the Deployment settings. \n3. Generate your API key and store it securely. (You use it to authenticate your requests.) \n\nChapter 4. Building and using artificial intelligence models 51Making an API call\nWith your API key in hand, you are ready to communicate with your model. Figure 4-44 shows \nan example of a simple POST request (by using a cURL command) to send input data and \nretrieve predictions.\nFigure 4-44   watsonx.ai regr ession model deployment tool\nNote:  The watsonx.ai user interface (UI) prepopulates different ways to call the model, \nsuch as cURL, Java, JavaScript, Python, Scala, and others. \n\n\n52 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiVisualizing the process\nFigure 4-45 shows a simple way to visua lize what occurs during an API call.\nFigure 4-45   API call visualization\nYour application sends input data to the endpoint; the model processes the data; and the \nresults are s",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 138,
            "total_chunks": 313
          }
        },
        {
          "id": "d84c3a0f-8a21-40af-8796-727e055589ae",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "r application sends input data to the endpoint; the model processes the data; and the \nresults are sent back as predictions. Whether you are handling real-time data (through online deployment) or batch processing, this streamlined interaction helps ensure that you can make the most of your deployed model.\n4.9.2  Calling Prompt Lab LLM  models by using API calls\nCalling an LLM model from Prompt Lab is sim ilar to calling an LLM model from watsonx.ai’s \nStudio; the only difference is where to find the code within the UI to do so. To accomplish this task, complete the following steps:\n1. Go to Prompt Lab, and then build your prompt in either the Chat , Structured , or Freeform  \ntab. \n2. In the upper right, click the View code  icon (Figure 4-46).\nFigure 4-46   View code icon\nThe Prompt Lab automatically creates everything that you need to copy and paste your \nprompt into the application of your choosing (in either cURL, Node.js, or Python), as shown in Figure 4-47 on page 53. \n\n\nChapter ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 139,
            "total_chunks": 313
          }
        },
        {
          "id": "307c0ac7-17a2-493f-84f3-f2c22937df0e",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " your choosing (in either cURL, Node.js, or Python), as shown in Figure 4-47 on page 53. \n\n\nChapter 4. Building and using artificial intelligence models 53Figure 4-47   watsonx.ai Prompt Lab code window \n4.9.3  IBM watsonx Assistant\nOperationalizing AI and ML models by using IBM technology helps ensure seamless \ndeployment and management across enterprise environments while delivering measurable business outcomes. IBM watsonx.ai provides a robust platform for building, fine-tuning, and deploying AI models to enable data scientists to leverage pre-trained models or create custom solutions. Once models are developed, they can be containerized and deployed by using Red Hat OpenShift, which is the IBM enterprise Kubernetes platform, which helps ensure scalability, high availabilit y, and integration with an existi ng infrastructure. IBM Watson \nStudio simplifies model lifecycle management by providing end- to-end capabilities for version \ncontrol, testing, and collaboration. Real-time moni",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 140,
            "total_chunks": 313
          }
        },
        {
          "id": "d0a163c2-9018-42ee-9236-ba6abe321071",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " providing end- to-end capabilities for version \ncontrol, testing, and collaboration. Real-time monitoring is enabled through IBM Instana Observability so that teams can track KPIs, detect anomalies, and maintain model health in \nproduction environments.Note:  Include your bearer access token. For more information about this token within the \nIBM ecosystem, see Generating a bearer token .\n\n\n54 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiFigure 4-48 shows an overview of IBM watsonx Assistant.\nFigure 4-48   IBM watsonx Assistant overview\nEnsuring the ongoing success of operational AI and ML solutions requires integrating \ngovernance, automation, and business alignment. IBM watsonx.governance enforces responsible AI principles by providing tool s for bias detection, lineage tracking, and \ncompliance management, which help organizations meet regulatory requirements and ethical standards. Automated deployment pipelines with IBM DevOps for AI streamline continuous",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 141,
            "total_chunks": 313
          }
        },
        {
          "id": "9dc86c99-22a9-4651-ac10-136f9eccba57",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "s and ethical standards. Automated deployment pipelines with IBM DevOps for AI streamline continuous integration and continuous delivery (CI/CD), which enables rapid updates and retraining to address data drift or evolving business needs. Feedback loops that are powered by IBM Watson Discovery facilitate continuous improvement by analyzing real-world user \ninteractions to enhance model performance. By leveraging IBM’s AI and hybrid cloud capabilities, organizations can operationalize AI and ML soluti ons effectively, which drive \ninnovation while helpin g ensure reliability and trustworthiness.\nFor more information about tools that are related to operationalizing your models, see 4.10.3, \n“watsonx.ai data pipeline and orchestration” on page 55.\n4.10  Additional information and where to go next\nIn this chapter, you le arned the following things:\n/SM590000How to set up your environment, which includes IBM Cloud accounts and project \nconfiguration.\n/SM590000The key features of watsonx.ai,",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 142,
            "total_chunks": 313
          }
        },
        {
          "id": "1a7b58fc-5144-462a-a1af-2225144c47d5",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ich includes IBM Cloud accounts and project \nconfiguration.\n/SM590000The key features of watsonx.ai, such as FMs, Prompt Lab, and Tuning Studio.\n/SM590000The importance of data quality, cleaning, and ingestion for AI model development.\n/SM590000Building and training AI models, which include traditional ML and LLMs, by using tools like \nAutoAI, Tuning Studio, and InstructLab.\n/SM590000Deploying AI models as APIs for real-time or batch processing and integrating them with \nenterprise systems.\n\n\nChapter 4. Building and using artificial intelligence models 554.10.1  Additional support and documentation\nwatsonx.ai has extensive support and documentat ion to help users maximize the platform's \ncapabilities. The IBM watsonx Documentation Portal  offers a comprehensive collection of \nresources, which include detailed user guides, tutorials, API references, and best practices. Whether you are starting or looking to optimize your AI workflows, the portal helps ensure that you have the guidance t",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 143,
            "total_chunks": 313
          }
        },
        {
          "id": "b489a3d2-fdf8-445e-a341-acb8d390dc4b",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "rting or looking to optimize your AI workflows, the portal helps ensure that you have the guidance that you need to succeed.\nHighlights include th e following resources:\n/SM590000Getting Started Guides: Step-by-step instructions for onboarding and initial setup.\n/SM590000Model Development Resources: In-depth documentation about training, fine-tuning, and \ndeploying both LLM and non-LLM models.\n/SM590000Troubleshooting and FAQs: Solutions to common issues and tips for resolving challenges \nefficiently.\n/SM590000Integration Guidance: Instructions for incorporating watsonx.ai into existing workflows and \nleveraging tools, such as Hugging Face and BYOM.\nThis rich repository of knowledge empowers users at every skill level to confidently build, \ndeploy, and scale AI solutions with watsonx.ai.\n4.10.2  watsonx.ai API reference\nFor comprehensive guidan ce about using watsonx. ai capabilities, the IBM watsonx API \nDocumentation  offers detailed information about avail able APIs, including endpo",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 144,
            "total_chunks": 313
          }
        },
        {
          "id": "ac403896-6d70-448e-ac29-d9baf34b83f8",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "e IBM watsonx API \nDocumentation  offers detailed information about avail able APIs, including endpoints, request \nparameters, and response structures. This resource is essential for developers that want to integrate watsonx.ai into their applications by  providing clear instructions and examples to \nfacilitate seamless implementa tion. Whether you are working with FMs, performing text \ninference, or managing deployments, this docum entation serves as a valuable reference to \nhelp ensure effective and efficient usage of watsonx.ai features.\n4.10.3  watsonx.ai data pipeline and orchestration\nFor more information about data pipelining and orchestration, see the following resources:\n/SM590000IBM Orchestration Pipelines:\nhttps://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-orchestrati\non-overview.html?context=wx\n/SM590000IBM Seismic: Introducing AI Agent Orchestration (IBMid required):\nhttps://ibm.seismic.com/Link/Content/DCbHf2RCFTPf3G9Cc2PBGggJWfGV\n/SM590000Instana: \nhttps",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 145,
            "total_chunks": 313
          }
        },
        {
          "id": "47b9b442-99d3-472b-b017-edf95dabfe6e",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "equired):\nhttps://ibm.seismic.com/Link/Content/DCbHf2RCFTPf3G9Cc2PBGggJWfGV\n/SM590000Instana: \nhttps://www.ibm.com/products/instana/generative-ai-monitoring\n/SM590000Turbonomic:\nhttps://community.ibm.com/community/user/aiops/blogs/cheuk-hung-lam/2024/05/28/\nturbonomic-tackles-gpus-for-genai-workloadshttps://www.ibm.com/case-studies/ibm-big-ai-models-turbonomic\n\n56 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai\n\n© Copyright IBM Corp. 2025. 57Chapter 5.Advanced capabilities of \nwatsonx.ai\nwatsonx.ai embodies IBM’s extensiv e expertise in artificial in telligence (AI) and foundation \nmodels (FMs), which combine advanced AI research with practical tools to make large language models (LLMs) efficient and versatile across many applications. Underlying watsonx.ai is its integration of FMs that are tuned to accelerate and optimize business operations. These models are positioned to perform at the intersection of language understanding, structured data processing, and ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 146,
            "total_chunks": 313
          }
        },
        {
          "id": "bb5668b1-7d8f-4e4b-bfbc-b3d787e03ae8",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ositioned to perform at the intersection of language understanding, structured data processing, and knowledge retrieval, which enhance the ability to extract, refine, and use va st amounts of unstructured data.\nThe platform's capabilities ex tend beyond simple text processing to include complex \ninteractions between structured and unstructured data sources, which enable the model to draw relevant information and learn domain-specific knowledge. For example, watsonx.ai supports both general- purpose and highly specialized mode l architectures, which facilitate \nthe design of task-optimized LLMs that serve nuanced business needs while ensuring data privacy and regulatory complia nce. Its multi-modal capabilit ies enable seamless handling of \ndiverse data types (such as text, image, and audio inputs) and applications to traverse disparate data landscapes cohesively, which achi eves a high level of contextual relevance \nand adaptability.\nThe watsonx.ai platform has several capabilities to ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 147,
            "total_chunks": 313
          }
        },
        {
          "id": "7efb4d9e-8337-4628-a227-be7a5aed2e5b",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "evel of contextual relevance \nand adaptability.\nThe watsonx.ai platform has several capabilities to support advanced use cases such as \nprompt engineering, multi-task prompt tuning, and fine-tuning. \nThe following topics are described in this chapter:\n/SM5900005.1, “Prompt engineering” on page 58\n/SM5900005.2, “Multitask prompt tuning” on page 61\n/SM5900005.3, “Fine-tuning” on page 64\n/SM5900005.4, “InstructLab” on page 675\n\n58 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai5.1  Prompt engineering\nPrompt engineering within the watsonx.ai ecosystem serves as an essential component in \nharnessing the full potential of language models. By precisely framing prompts, users can guide LLM responses to ward relevance and co herence, which greatly enhances the utility of \ngenerated outputs. The process of prompt engineering in watsonx.ai is highly nuanced, and it involves detailed adjustments to phrasing, context, and iterative feedback mechanisms to yield wanted output",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 148,
            "total_chunks": 313
          }
        },
        {
          "id": "74be21a7-c6e7-4030-be00-6ac004d4a4f9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " detailed adjustments to phrasing, context, and iterative feedback mechanisms to yield wanted outputs consistently. Prompt engineering plays a pivotal role in directing LLMs to perform specific tasks with high precision, a ta sk that requires linguistic adjustments and a \ndeep understanding of the underlying model dynamics.\nThe reason why prompt engineering is important is because it is a way to make generalist \nmodels (LLMs) perform a specific task. Without quality informat ion, well-defin ed instructions, \nand a clear set of examples, the model might misbehave and hallucinate in various ways.\nPrompt engineering is about writing something in a better, clearer, and cleaner form. It also \ninvolves using specific system tokens that are for only the model that is used and, if available, a series of examples that provide better information to the model so that it can perform as intended. This approach is explored more in-depth in this section.\n5.1.1  Prompting techniques\nThere are three ma",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 149,
            "total_chunks": 313
          }
        },
        {
          "id": "0bcde0a5-5e05-4e7c-a377-952f62b2144b",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "s approach is explored more in-depth in this section.\n5.1.1  Prompting techniques\nThere are three main prompting techniques: \n/SM590000Zero-shot prompting\n/SM590000One-shot prompting\n/SM590000Few-shot prompting\nThese techniques are not learning techniques, but prompting techniques only, which improve \nmodel performances at inference-time without modifying the original model.\nThese techniques leverage the model's existing capabilities with out requiring fine-tuning or \nparameter updates, which make them lightweight and adaptable solutions for various use cases. This section provides an explanation of each prompting technique, and when it is best to use each approach.\n/SM590000Zero-shot prompting: Relies solely on the model's pre-trained knowledge to generate \nresponses without providing any task-specific examples in the prompt. Instead, the input typically includes clear instructions or a well-defined query that guides the model to perform the task, for example, asking a model to summar",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 150,
            "total_chunks": 313
          }
        },
        {
          "id": "bf481712-ba90-4a7a-8293-adf3fd640e60",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " well-defined query that guides the model to perform the task, for example, asking a model to summarize a paragraph or translate a sentence into another language without providi ng prior examples. The effectiveness of this \napproach hinges on the clarity and precision of the prompt and the model's inherent ability \nto generalize across diverse tasks.\n/SM590000One-shot prompting: In this approach, a single example of the task is embedded within the \nprompt, alongside the query or instructions. This example serves as a reference for the \nmodel to infer the behavior. By including a single demonstration, one-shot prompting can \nenhance performance for tasks that require nuanced or domain-specific understanding because it provides a concre te context for the model to interpret the instructions.\n\nChapter 5. Advanced cap abilities of watsonx.ai 59/SM590000Few-shot prompting: Expands on this concept by incorporating multiple examples of the \ntask in the prompt. The additional examples provide ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 151,
            "total_chunks": 313
          }
        },
        {
          "id": "a528dce7-1a30-4f28-a027-0cbf95ad00cb",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "cept by incorporating multiple examples of the \ntask in the prompt. The additional examples provide a richer context and help the model \nbetter understand complex patterns or subtle variations in the task. Few-shot prompting is useful for tasks that require multi-step reasoning, handling of ambiguous inputs, or understanding domain-specific jargon. However, it demands careful prompt construction to balance informativeness and brevity because excessive length can lead to token limitations or diminished performance.\nFigure 5-1   LLM prompting method types overview\n5.1.2  Importance of system tokens\nIn addition to zero-shot, one-shot, and few-shot prompting techniques, system tokens  (also \nknown as system-level instructions  or control tokens) play a critical role in crafting effective \nprompts. These tokens provide metadata or guidance to steer the behavior of the language model at a higher level, often defining the co ntext, tone, or expected behavior of the model \nduring inference. Im",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 152,
            "total_chunks": 313
          }
        },
        {
          "id": "b1656d04-279e-4cb4-afb5-7b31c2c6efdc",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "er level, often defining the co ntext, tone, or expected behavior of the model \nduring inference. Importantly, the implementati on and interpretation of these tokens can vary \nacross different models, making their effective use model specific.\nSystem tokens enable users to establish a “role” or context for the model, shaping its \nresponses beyond what is specified in the natural language prompt. \nBy embedding these tokens, users can accomplish the following goals:\n/SM590000Control output behavior: Ensure the order of prompt completions between the main \nprompt areas of System, Assistant, and User\n/SM590000Reduce ambiguity: Guide the model's interpretation of the task, especially in contexts \nwhere instructions alone might be misinterpreted.\n/SM590000Enhance few-shot learning: When combined with example-based prompting, system \ntokens can provide an overarching framework that amplifies the impact of the examples.\n5.1.3  Model-specific peculiarities\nDifferent language models interpret an",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 153,
            "total_chunks": 313
          }
        },
        {
          "id": "8e1eda9b-54c2-45ec-b9a6-839209735537",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "e impact of the examples.\n5.1.3  Model-specific peculiarities\nDifferent language models interpret and use system tokens in unique ways due to their architecture and pre-training data. The best practices for incorporating system tokens are to understand model documentation. Because the behavior of system tokens is model-dependent, consulting the model's technical documentation is essential to understanding how tokens are implemented and what variations are supported.\n\n\n60 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai5.1.4  How watsonx.ai suppo rts prompt engineering\nRegarding prompt engineering, the simplest way to interact with LLMs is extensive but \npeculiar. Fortunately, the watsonx.ai platform enables prompt engineering by providing a series of tools for its usage. Figure 5-2 shows a series of these tools in the Prompt Lab section of watsonx.ai.\nFigure 5-2   watsonx.ai Prompt Lab dashboard\nPrompt Lab is the main area to access and interact with LLMs. Here,",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 154,
            "total_chunks": 313
          }
        },
        {
          "id": "df41b695-75f3-4e5e-bda8-4c1f8d2e1f82",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " watsonx.ai Prompt Lab dashboard\nPrompt Lab is the main area to access and interact with LLMs. Here, it is possible to interact with models and perform prompt engineering techniques. In Prompt Lab, users have three modes to select from to interact with LLMs:\n/SM590000Chat mode: Provides a simplisti c, multimodal chatbot-like inte raction with capabilities such \nas memory and document understanding. It is good for model interaction and for simple system prompt definition and a zero-shot-prompting approach.\n/SM590000Structured mode: Provides a way to set up your prompt to create a particular prompt \nengineering setting within the main model Instructions and Examples, where the examples are the input/output series that is provided in a few-shot-prompting setting. It automatically applies the system tokens for a specific model to best fit the one/few-shot-prompting setting.\n/SM590000Freeform mode: Provides more advanced users with the option to be free of crafting their \nraw prompt by usin",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 155,
            "total_chunks": 313
          }
        },
        {
          "id": "5dfa03a1-fdae-4431-98c9-89398acd467b",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " mode: Provides more advanced users with the option to be free of crafting their \nraw prompt by using all the possible prompt  engineering capabilitie s that leverage raw \nsystem tokens. Although this mode leverages the power of prompt engineering and freedom, it requires specific skills in unde rstanding system tokens, what they are for a \nspecific model, and how to use them.\nAlthough prompt engineering provides a faster way to the objective, it is not always the best \ntool to use or the most capable tool that is  available. As task difficulty and complexity \nincreases, watsonx.ai can use more advanced model enhancing techniques. We explore these techniques in the following sections of this chapter. \n\n\nChapter 5. Advanced cap abilities of watsonx.ai 615.2  Multitask prompt tuning\nMultitask prompt tuning  within watsonx.ai builds on traditional prompt engineering by \nimplementing adaptive mechanisms to refine the prompt’s interpretative accuracy over time. This approach differs fundame",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 156,
            "total_chunks": 313
          }
        },
        {
          "id": "8c27ed62-3c08-444e-acea-eec0bce20195",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "e mechanisms to refine the prompt’s interpretative accuracy over time. This approach differs fundamentally from prompt engineering because it modifies prompt content and continuously aligns the model’s interpretative layers with domain-specific expectations. In essence, prompt tuning enables models to retain learned adjustments across sessions, which support consistency and reduce  the need for extensive re-engineering.\nPrompt tuning leverages techniques such as embedding adjustments and parameter scaling \nto influence the model’s internal state and guide responses within boundaries. Using the watsonx.ai dynamic configuration settings, de velopers can set up continuous tuning \nprocesses that adapt prompts based on evolving  business contexts, which lead to a finely \ncalibrated model that re flects current operation al realities. This c apability enables rapid \nadaptation without costly retraining, and the watsonx.ai architecture permits this tuning to take place seamlessly, which enabl",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 157,
            "total_chunks": 313
          }
        },
        {
          "id": "dfa77957-bbb7-4e55-97c8-820d0101a8cd",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "etraining, and the watsonx.ai architecture permits this tuning to take place seamlessly, which enables real-time adjustments as new data is ingested or as user preferences change.\nIn this setting, it is not the LLM that is modified. Instead, a dedicated, smaller LLM is trained to \ngenerate the best possible prompt adjustment for each prompt in the input. The smaller LLM leverages the system tokens that are available for each LLM on watsonx.ai and produces new, compatible virtual tokens to enhance the performances. The smaller LLM is trained by using a loss function that accounts for the resulting response from the immutable (in this setting) generative LLM model that you want to improve, and adapts its weights to create better prompts through a \ntunable soft prompt , as shown in Figure 5-3.\nFigure 5-3   Prompt tuning overview \nWith prompt tuning, you create a model that automatizes the prompt engineering task, which makes it dynamically adaptive to new, incoming inputs over time. The k",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 158,
            "total_chunks": 313
          }
        },
        {
          "id": "af6234db-3476-482e-99d9-a3857f71045f",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "rompt engineering task, which makes it dynamically adaptive to new, incoming inputs over time. The key benefit of prompt tuning is performing tuning in ways that are better than what experts can do for certain tasks, that is, the best token leading to a successful completion of the input task. Prompt tuning can do this task because it leverages 100 - 10000 examples to learn whic h token is the best one \nto add to a starting pre-engineered prompt to minimize the loss of the generative model. \n\n\n62 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiThe watsonx.ai platform provides a simplistic way of using prompt tuning, as shown in \nFigure 5-4.\nFigure 5-4   Prompt tuning in watsonx.ai Tuning Studio\n5.2.1  Prompt tuning parameters\nIn watsonx.ai Tuning Studio, you can use multitask prompt tuning by leveraging various prompt tuning parameters. The process of optimizing hyperparameters for prompt tuning, such as batch size, the number of epochs, the learning rate, and a",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 159,
            "total_chunks": 313
          }
        },
        {
          "id": "aeddb675-b9bb-4944-b17a-9068c951f80e",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "yperparameters for prompt tuning, such as batch size, the number of epochs, the learning rate, and accumulation steps, plays a critical role in achieving task-specific adaptation and helping ensure effective usage of LLMs. Each of these parameters impacts the training process in unique ways by influencing the generalization ability, st ability, computational ef ficiency, and pe rformance of th e fine-tuned \nprompts.\n/SM590000\nBatch size  refers to the number of training samples that are processed simultaneously \nduring each forward and backward pass through the model. It is a fundamental factor in determining the balance between computational efficiency and the quality of gradient updates. Larger batch sizes tend to stabiliz e gradient updates by averaging over more \nsamples, which enable faster convergence.  However, they often require significant \ncomputational resources and might overlook fine -grained variations in  the dataset, which \nmight potentially limit the prompt's ability t",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 160,
            "total_chunks": 313
          }
        },
        {
          "id": "afda34cd-5f08-4475-bdd4-25830a7209be",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "look fine -grained variations in  the dataset, which \nmight potentially limit the prompt's ability to address nuanced tasks.  Conversely, smaller \nbatch sizes enable greater granularity in gradi ent computations, which is advantageous for \nsmall datasets or specific tasks. However, small batches introduce noisier gradient updates, which require more iterations to c onverge effectively. To optimize batch size, \npractitioners should aim for a balance that satisfies computat ional feasibility while meeting \nthe requirements of the task. Dynamic batch sizing (adjusting the batch size during training) can further stabilize the learning process and e nhance overall effectiveness.\n/SM590000The \nnumber of epochs  represents the total number of complete passes that the training \nalgorithm makes through the dataset. This parameter directly influences how thoroughly the model explores the data to refine its prompt parameters. A higher number of epochs allows the model to capture in tricate patter",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 161,
            "total_chunks": 313
          }
        },
        {
          "id": "29ef26a2-786c-4a39-8a2f-8d7e65dc0c5d",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "efine its prompt parameters. A higher number of epochs allows the model to capture in tricate patterns, which improv es task-specific adaptation. \nHowever, this approach  comes with the risk of overfitting, especially with smaller datasets, \nwhich reduce the generalization of the prompt s. Conversely, a lower number of epochs \nminimizes the risk of overfitting but might lead to underoptimized prompts that fail to leverage the model's full potential. To strike the right balance, monitor validation loss and apply early stopping criteria to help ensure that training halts before overfitting occurs. \nUsing pre-trained checkpoints can also reduc e the need for extensive epochs because \nthese starting points encapsulate foundatio nal knowledge that accelerates convergence.\n\n\nChapter 5. Advanced cap abilities of watsonx.ai 63/SM590000The learning rate  governs the size of the updates that are made to prompt parameters \nduring each optimization step. It influences the speed and stability of th",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 162,
            "total_chunks": 313
          }
        },
        {
          "id": "142b2610-a9c7-4a21-b68e-1aceb7ddcc85",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ade to prompt parameters \nduring each optimization step. It influences the speed and stability of th e training process. \nA high learning rate expedites convergenc e, which reduces training time but risks \novershooting optimal solutions, and can lead to suboptimal performance or even divergence. Conversely, a low learning rate enables a more precise exploration of the \nparameter space, which increases the likelihood of finding an optimal solution at the cost of prolonged training. Effective strategies include employing learning rate schedules, such as cosine decay or step-based decay, which adjust the learning rate dynamically during training. Warm-up strategies, where the learning rate gradually increases at the start of training, can also mitigate in itial instability and improve overall training robustness.\n/SM590000The concept of \naccumulation steps  addresses memory constraints by enabling gradient \naccumulation across several mini-batches before updating the model’s parameters. T",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 163,
            "total_chunks": 313
          }
        },
        {
          "id": "d90c183d-35f4-40f6-99af-f50b5516a05b",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "nabling gradient \naccumulation across several mini-batches before updating the model’s parameters. This approach effectively simulates larger batch sizes without exceeding hardware memory limits, which make it valuab le for memory-constrained environments. Accumulation steps \nsmooth gradient updates by averaging across multiple mini-batches, which improve stability at the cost of increased training time . Optimizing this parame ter involves selecting \nan accumulation step size that balances memory efficiency with the effective batch size. Combining this approach with batch size tuning can further optimize resource usage and enhance performance.\n5.2.2  Interdependencies an d holistic tuning strategies\nThese hyperparameters are interdependent be cause changes in one can influence the \nbehavior of others. For example, increasing the number of epochs without modifying the \nlearning rate might lead to overfitting, and combining a high batch size with too few epochs might result in undertrai",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 164,
            "total_chunks": 313
          }
        },
        {
          "id": "2da9693e-234a-46b5-beee-9fc18e655d09",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "t lead to overfitting, and combining a high batch size with too few epochs might result in undertrained prompts. To navigate these interdependencies, practitioners can employ regularization techniques such as data augmentation to counteract overfitting in high-epoch scenarios. Gradient c lipping can also be used to prev ent instability during training, \nparticularly when high accumulation steps are involved.\nPerformance metrics, which include task-spec ific measures like accuracy or F1-scores, \nshould guide the evaluation of prompt tuning effectiveness. Monitoring loss convergence and gradient stability help ensure that the chosen hy perparameters lead to tangible \nimprovements.\nCarefully calibrating batch size, the number of epochs, the learning rate, and accumulation \nsteps enables precise optimization of prompt tuning, which unlocks the full potential of LLMs for specific tasks. By managing these para meters holistically, practitioners can achieve \nperformance gains while balancing ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 165,
            "total_chunks": 313
          }
        },
        {
          "id": "8e44966d-a855-4bf9-94d1-669214f40d4b",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "naging these para meters holistically, practitioners can achieve \nperformance gains while balancing computational efficiency and resource constraints.\n\n64 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai5.3  Fine-tuning\nFine-tuning within the watsonx.ai ecosys tem represents the next layer of model \nspecialization, where foundation LLMs undergo retraining on domain-specific datasets to enhance accuracy and relevance for specific applications. Fine-tuning goes beyond prompt adjustments by modifying the model’s weights to encode new knowledge or adapt to complex industry-specific language structures, terminologies, and operational protocols. This process is beneficial for industries that require high precision in terminology and context, such as \nhealthcare, finance, and legal services. Fi ne-tuning within the watsonx.ai ecosystem \nexemplifies a sophisticated approach to model specialization, which enables foundation LLMs \nto adapt to domain-specific needs through",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 166,
            "total_chunks": 313
          }
        },
        {
          "id": "4c12dda8-e319-489b-926f-e120499febee",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ch to model specialization, which enables foundation LLMs \nto adapt to domain-specific needs through retr aining. Unlike prompt tuning, which focuses on \nlightweight modifications to steer model behavior, fine-tuning directly alters the model’s weights to encode new knowledge or align with complex industry-specific language structures and terminologies.\n5.3.1  Challenges with fine-tuning\nTo fully appreciate the significan ce of watsonx.ai capabilities, it  is essential to understand the \ninherent complexity of fine-tuning in general. At its core, fine-tuning involves retraining a model’s internal weights on care fully curated datasets to refine its understanding of specific \nterminologies, language patterns, or operational protocols. This process differs from lightweight techniques like prompt tuning, wh ich adjusts model behavior externally without \naltering its core structure. Fine-tuning, by contrast, modifies the model itself, embedding new knowledge directly in to its architecture",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 167,
            "total_chunks": 313
          }
        },
        {
          "id": "add07297-ace1-4ab0-9f86-a832a1a14ac0",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ing, by contrast, modifies the model itself, embedding new knowledge directly in to its architecture. \nWhile this approach enables unparalleled precis ion and customization, it introduces many \nchallenges. Here are some common challenges with fine-tuning:\n/SM590000Data management: One of the primary difficulties with fine-tuning. Fine-tuning demands \ndatasets that are relevant and meticulously prepared, which includes ensuring that the data is formatted correctly, free from bias, and representative of the target domain. Even determining the appropriate size of the dataset requires careful consideration because too little data risks underfitting, and too much can lead to overfitting or unnecessary computational burdens. For example, in watsonx.ai, datasets are limited to 200 MB for JSON or JSONL files, or up to 10,000 examples when sourced from connected data stores. These constraints are carefully balanced to optimize efficiency without sacrificing \nperformance, but managing these para",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 168,
            "total_chunks": 313
          }
        },
        {
          "id": "efa60513-1311-4015-a7a2-cee480819043",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " carefully balanced to optimize efficiency without sacrificing \nperformance, but managing these parameters manually would be daunting for most users.\n/SM590000Precise calibration of numerous hyperparameters: These hyperparameters include the \nlearning rate, batch size, number of training epochs, and strategies for regularization, among others. Each of these parameters is in terdependent, which means that altering one \ncan have cascading effects on others. Achieving the optimal configuration often involves extensive trial-and-error or the usage of advanced hyperparameter optimization techniques like Bayesian search . This complexity is compo unded when working with large \nmodels, which can have billions of parameters, which requir e significant computational \nresources and expertise to manage effectively.\n\nChapter 5. Advanced cap abilities of watsonx.ai 65/SM590000Computationally demanding workloads requiring a high-performance architecture: Large \nmodels can have billions of  parameter",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 169,
            "total_chunks": 313
          }
        },
        {
          "id": "148e5f53-10ed-4d7a-8715-939a8472e07e",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "g workloads requiring a high-performance architecture: Large \nmodels can have billions of  parameters, which require a significant amount of \ncomputational resources. Even with the right data and parameters in place, fine-tuning remains computationally demanding. Training these large LLMs requires access to high-performance hardware, such as multi-GPU, along with robust memory and storage capabilities. For organizations without dedicated AI infrastructure, thes e requirements are \noften prohibitive.\n/SM590000Ensuring stability during the tr aining process: Large-scale op timization algorithms are \nprone to issues like exploding or vanishing gradients, so achieving convergence without diverging from the optimal solution requires careful tuning and monitoring.\n5.3.2  How watsonx.ai addr esses fine-tuning challenges\nBy automating the complexities of fine-tuning, watsonx.ai transforms what was once a \nlabor-intensive and technically demanding process into an accessible, streamlined experie",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 170,
            "total_chunks": 313
          }
        },
        {
          "id": "d04ff779-8684-4edc-8f5b-54b3ced265a2",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "as once a \nlabor-intensive and technically demanding process into an accessible, streamlined experience. This approach lowers the barrier to entry for organizations looking to adopt AI and enables experienced practitioners to focus on higher-level strategic goals rather than \ngetting bogged down in technical minutiae. With its combination of cutting-edge technologies, managed infrastructure, and user-centric design, watsonx.ai empowers businesses to harness the full potential of fine-tuning, which unlocks new levels of precision, efficiency, and innovation in AI-driven solutions. \nwatsonx.ai provides the following features:\n/SM590000Hardware and resource allocation automation: The platform’s automation begins with its \nability to manage hardware and resource allocation seamle ssly. Users do not need to \nworry about provisioning servers, config uring GPUs, or scaling their setups to \naccommodate large datasets or models. Instead, watsonx.ai handles these tasks behind the scenes, helping",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 171,
            "total_chunks": 313
          }
        },
        {
          "id": "85128b77-3334-454d-9a12-814ab94f8fae",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "mmodate large datasets or models. Instead, watsonx.ai handles these tasks behind the scenes, helping ensure that every fine-tuning operation runs on optimized configurations. \n/SM590000Supervised Fine-Tuning Trainer (SFTTrainer): At the heart of watsonx.ai fine-tuning \ncapabilities is the SFTTr ainer, which is a powerful tool that is developed in collaboration \nwith Hugging Face. This framework simplifies the optimization of model weights by automating key aspects of the training process, which includes the application of advanced learning rate schedules and warm-u p strategies. These techniques are crucial \nfor maintaining stab ility during training, particularl y when dealing with complex or \nhigh-dimensional data. By leveraging SFTTrainer, watsonx.ai helps ensure that models converge rapidly and reliably without the need for extensive manual intervention. \n/SM590000Low-rank adaptation (LoRA) and quantized low-rank adaptation (QLoRA): In addition to \nSFTTrainer, watsonx.ai incorporat",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 172,
            "total_chunks": 313
          }
        },
        {
          "id": "0b8b9cad-2813-4cb0-8e99-56186ce7202c",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " (LoRA) and quantized low-rank adaptation (QLoRA): In addition to \nSFTTrainer, watsonx.ai incorporates cutting-edge techniques like LoRA and QLoRA. These methods represent a paradigm shift in fine-tuning efficiency. Rather than retraining all of a model’s parameters, LoRA focuses on fine-tuning small modula r blocks of weights \nwhile freezing most the model. This approach reduces the computational and memory requirements of the process, which makes fine-tuning accessible even on resource-constrained hardware. QLoRA goes a step further by lowering the precision of certain parameters during training, which further optimizes performance without compromising accuracy. These innovations enable watsonx.ai to deliver results faster and with fewer resources than traditional approaches. \n/SM590000Tuning Studio integration: Another key advantage of watsonx.ai is its integration with the \nTuning Studio, which provides access to a library of pre-configured model templates. These templates enable u",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 173,
            "total_chunks": 313
          }
        },
        {
          "id": "16a963cd-e715-432c-bbee-93ad8dd56ad8",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "udio, which provides access to a library of pre-configured model templates. These templates enable users to build on pre-existing architectures that are optimized for specific tasks or domains. This approach eliminates the need to design custom models from scratch, which reduces the time and expertise that are required to initiate fine-tuning projects. \n\n66 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai/SM590000Custom FMs: For organizations with unique requirements, watsonx.ai also supports the \nimport of custom FMs if they have fewer th an 20 billion parameters . This approach opens \nthe possibility of automatically fine-tuning tons of available models on Hugging Face and \non watsonx.ai. This flexibility helps ensure that the pla tform can accommodate a wide \nrange of use cases and industries. \n/SM590000Monitoring and optimization: Throughout the fine-tuning process, watsonx.ai provides \nrobust tools for monitoring and optimization.  Real-time performance tra",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 174,
            "total_chunks": 313
          }
        },
        {
          "id": "0fdee61a-5917-431e-856b-966d4dd0cd05",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ocess, watsonx.ai provides \nrobust tools for monitoring and optimization.  Real-time performance tracking enables \nusers to assess key metrics, such as validat ion loss and gradient stability, which helps \nensure that the model improves as expect ed. The platform also employs advanced \nearly-stopping mechanisms to prevent overfitting  by halting training when further iterations \nwould yield diminishing returns. These features  enhance the efficiency of the process and \nimprove the quality of the final model. \nFigure 5-5 shows the watsonx.ai fine-tuning process.\nFigure 5-5   Prompt fine-t uning pipeline process overview\nIn conclusion, watsonx.ai revolutionizes the traditionally complex and resource-intensive \nprocess of fine-tuning LLMs by introducing a seamlessly automated, highly efficient, and \nscalable solution. By leveraging advanced tool s like the SFTTrainer, innovative techniques \nsuch as LoRA and QLoRA, and a robust Tuning Studio, watsonx.ai enables businesses to achieve unpara",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 175,
            "total_chunks": 313
          }
        },
        {
          "id": "776f5d8f-3abe-42b8-969b-4b22f440c888",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "\nsuch as LoRA and QLoRA, and a robust Tuning Studio, watsonx.ai enables businesses to achieve unparalleled levels of cu stomization and precision in thei r AI solutions. Its ability to \nmanage every aspect of the fine-tuning lifecycle (from data preparation and parameter optimization to resource allocation and model monitoring) removes significant technical barriers, which democratize access to AI specialization for organizations of all sizes. \nThis comprehensive platform empowers businesses to tailor FMs to their unique \ndomain-specific needs, whether in healthcare, fi nance, legal services, or other fields that \nrequire high precision. By doing so, the platform enhances the relevance and accuracy of AI applications and accelerates time-to-value while reducing the costs that are associated with traditional fine-tuning methods. watsonx.ai stands as a testament to IBM's commitment to innovation and accessibility by providing a robu st foundation for bu sinesses to unlock the \ntransforma",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 176,
            "total_chunks": 313
          }
        },
        {
          "id": "50b00a95-cbe9-4d90-b80f-560daa1af709",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "vation and accessibility by providing a robu st foundation for bu sinesses to unlock the \ntransformative potential of AI with confidence and ease.\n\n\nChapter 5. Advanced cap abilities of watsonx.ai 675.4  InstructLab\nInstructLab represents a groundbreaking shift in the way LLMs are fine-tuned by making the \nprocess more accessible, flexible, and efficient. At its core, InstructLab leverages a unique combination of community-driven input, synthetic data generation (SDG), and iterative training methodologies to refine LLMs in a way that dramatically lowers the barriers to entry for fine-tuning ta sks (see Figure 5-6). This process ma kes it simpler for developers and \nsubject matter experts (SMEs) to improve model outputs. It also accelerates the fine-tuning cycle and reduces the computational overhead that is traditionally associated with customizing models.\nFigure 5-6   The InstructLab large language  model development kit functions overview\nThe InstructLab approach to fine-tuning is he",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 177,
            "total_chunks": 313
          }
        },
        {
          "id": "fc48f37c-746d-43a7-bbb0-e937d81802d3",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "rge language  model development kit functions overview\nThe InstructLab approach to fine-tuning is heavily grounded in the concept of \ntaxonomy-driven knowledge curation. Taxonomies , in this context, are structured frameworks \nof concepts and relationships that organize information into logical categories and subcategories. These taxonomies are built collaboratively by SMEs, and they serve as the foundation for the knowledge that is used to tune the model. For example, if a business wanted to fine-tune an LLM on customer support for a specific industry, an SME in that industry would work with a taxonomy that represents common questions, issues, and \nterminology that are relevant to the field. By formalizing domain knowledge in a taxonomy, InstructLab helps ensure that the model fine-tuning process is precise, efficient, and contextually relevant.\nWhat makes this approach powerful is that the knowledge that is curated in these taxonomies \nis used to generate synthetic data. Unlike tradi",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 178,
            "total_chunks": 313
          }
        },
        {
          "id": "76d1fca0-6b77-4b26-adc3-50627eef9759",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " the knowledge that is curated in these taxonomies \nis used to generate synthetic data. Unlike traditio nal fine-tuning methods that rely heavily on \nvast amounts of manually labeled training data, InstructLab uses SDG to produce the training examples that are needed to adjust model behavior. This task is accomplished by feeding the curated taxonomies into  the system (composed of Knowl edge and Skills taxo nomies), which \ncontains question-answer pairs and other types of data. \n\n\n68 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiAn example of Skills taxonomies is provided in  Figure 5-7, where the Skill that is defined here \nis made to make a model learn to better interpret particular tables.\nFigure 5-7   Skill Taxonomy example\nThis data generation process is not random: It follows predefined patterns based on the taxonomy’s structure, which helps ensure that the synthetic examples are highly relevant to \nthe domain. This synthetic data serves as a proxy for r",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 179,
            "total_chunks": 313
          }
        },
        {
          "id": "0a56b6e6-1255-4a2b-b328-ca8280093d93",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "e synthetic examples are highly relevant to \nthe domain. This synthetic data serves as a proxy for real-world data, which enables InstructLab to adapt to niche topics or specialized knowledge areas without requiring the collection of large-scale, expensive datasets. The synthetic data can also be customized and controlled by the SME, which means that they can influence the generation process to help ensure that the model is trained  on the most important or crit ical examples. The ability to \ngenerate synthetic training data directly fr om taxonomies is what makes InstructLab so \nefficient: It drastically reduces the need for large-scale manual data curation and opens. \nFurthermore, it solves the ever-existing problem of not having enough data for fine-tuning a model that is tailored for a particular business need in the generative AI (gen AI) era. \nThe InstructLab approach is built around iterative feedback and instruction training, which are \ntwo techniques that further streamline th",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 180,
            "total_chunks": 313
          }
        },
        {
          "id": "354b150a-efbd-47fd-95d7-6ba50b2997c6",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "nd iterative feedback and instruction training, which are \ntwo techniques that further streamline the fine-tuning process. Instruction training involves providing LLMs with explicit in structions (similar to how humans learn new tasks) about how \nto generate responses based on the synthetic data. This process is highly flexible because the SMEs can continually tweak the instructions and the knowledge base based on the evolving needs of the model and the domain it is being trained on. This process uses a 2-phase approach with a replay that serves the pur pose of ensuring high diversity and quality \nin the synthetically generated in struction-tuning data set while ensuring training stability. It \nalso prevents catastrophic forgetting, which is a common situation that happens in the FM fine-tuning process when it is not well controlled. \nOnce the initial synthetic data is generated from the curated taxonomy, the InstructLab \nsystem trains the model by providing it with a series of questio",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 181,
            "total_chunks": 313
          }
        },
        {
          "id": "98300b75-b981-4bf4-af5b-92b1a24d202d",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " curated taxonomy, the InstructLab \nsystem trains the model by providing it with a series of questions and answers that align with the knowledge base. The process is iterative, which means that the model does not undergo a single round of training and then stop. Instead, as new feedback is gathered from the model’s performance, the data is refined, and further training is conducted. This iterative feedback loop is crucial in guiding the model toward better understanding, more accurate outputs, and more aligned responses to specific use cases.\n\n\nChapter 5. Advanced cap abilities of watsonx.ai 69The power of iterative feedback lies in its ability to refine th e model's responses over time, \nwhich enhances its accuracy and applicability to real-world prob lems. SMEs can continuously \nassess the model’s performance in relation to specific tasks or topics, and helps ensure that the model becomes progressively better at understanding the subtleties of the domain and generating more precise, ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 182,
            "total_chunks": 313
          }
        },
        {
          "id": "384c5021-25ad-4258-b8c8-ae17c7b3c3d6",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "mes progressively better at understanding the subtleties of the domain and generating more precise, cont extually relevant responses.\nAnother key feature of InstructLab is its model-neutral and open-source nature. Unlike \nproprietary fine-tuning solutions that are often tightly coupled with specific models or platforms, InstructLab enables users to contribute to the fine-tuning of various LLMs regardless of their underlying architecture. InstructLab is built on the premise that fine-tuning should be an open, community-driven process. It supports a wide range of open-source LLMs from repositories like Hugging Face, which enables users to choose the model that best suits their needs, and even enabling them to experiment with models from different frameworks. The open-source nature of InstructLab also means that users have full transparency into the fine-tuning process and the abilit y to modify it as needed. An yone from hobbyists to industry \nexperts can contribute to improving the syst",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 183,
            "total_chunks": 313
          }
        },
        {
          "id": "9f57c6f5-34aa-438c-a69e-9415e8e57ffc",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "odify it as needed. An yone from hobbyists to industry \nexperts can contribute to improving the system, whether by adding new taxonomies, adjusting training data, or developing new  techniques for SDG. This approach makes \nInstructLab a true community-driven initiative that is always evolving based on the needs and contributions of its users.\nFigure 5-8 shows a community-driven InstructLab fine-tuning process example.\nFigure 5-8   Community-driven Inst ructLab fine-tuning process example\nUsing InstructLab involves several key steps, each of which is designed to make the process \nas efficient and accessible as possible:\n1. Users download a base model from a supported repository, such as Hugging Face, and \ninitialize the InstructLab co mmand-line interface (CLI). \n2. Once the environment is set up, the user creates a knowledge base, which is stored in a \nstructured directory that follows the taxonomy format. This knowledge base is populated with question-answer pairs, references to exter",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 184,
            "total_chunks": 313
          }
        },
        {
          "id": "fd5ffef9-4443-47c3-9950-464df74b23df",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "he taxonomy format. This knowledge base is populated with question-answer pairs, references to external documents (for example, PDFs or markdown files), and metadata to describe the knowledge, such as the domain and relevant attributes. \n3. Now, you generate synthetic data based on the knowledge base. This synthetic data is \nused to train the model, with the InstructLab system automatically generating examples that adhere to the structure of the taxonomy. The SDG process is designed to produce high-quality training samples by following the patterns and relationships that are defined in the taxonomy, which reduces the time and cost of manual data creation and helps ensure that the model receives high-quality, domain-specific examples. \n\n\n70 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai4. Once the synthetic data is generated, the model is trained by using the InstructLab training \nframework. This training can be done on a local machine or in a cloud environment",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 185,
            "total_chunks": 313
          }
        },
        {
          "id": "590419f3-7ece-46d9-bac2-09aad5083ef9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "tructLab training \nframework. This training can be done on a local machine or in a cloud environment, with the flexibility to use GPUs to accelerate the process. \n5. After training is complete, the model undergoes a testing phase to ensure that it performs \nas expected. InstructLab enables users to run tests that evaluate how well the model answers questions and generates responses, which help identify areas for further improvement. \n6. The final stage is deploying the trained model, which can be done through the InstructLab \nserving tools. Once the model is deployed, users can interact with it through the CLI or through an application interface, which e nables them to assess  how well it handles \nreal-world queries and performs in live environments.\nFigure 5-9 shows a high-level view of the InstructLab pipeline.\nFigure 5-9   High-level overview of the InstructLab pipeline\n5.4.1  Advantages of InstructLab\nThe InstructLab methodology provides numerous advantages over traditional fine-tu",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 186,
            "total_chunks": 313
          }
        },
        {
          "id": "12ea7f14-05bf-4400-8d06-b0dd0c0ffd04",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ges of InstructLab\nThe InstructLab methodology provides numerous advantages over traditional fine-tuning \ntechniques:\n/SM590000Synthetic data generation (SDG):  Facing and solving an older pr oblem of data availability, \nInstructLab offers within its automatic capabilities a process to enhance, in cardinality and \nvariability, a training dataset fo r LLM fine-tuning by using SDG, which is integr ated into its \ncore. \n/SM590000Cost and complexity reduction: One of the most significant benefits of InstructLab is its \nability to drastically reduce the cost and comp lexity of fine-tuning. By relying on SDG \nrather than manually labeled datasets, InstructLab eliminates one of the most time-consuming and expensive aspects of model customization. \n/SM590000Agile development: The iterative feedback loops and the ability to work collaboratively with \nSMEs enable the fine-tuning process to be far more agile, with models refined and improved continuously based on real-time insights. \n\n\nChapter 5.",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 187,
            "total_chunks": 313
          }
        },
        {
          "id": "b1c0730a-9188-471d-aa1d-91bfbeaee967",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "more agile, with models refined and improved continuously based on real-time insights. \n\n\nChapter 5. Advanced cap abilities of watsonx.ai 71/SM590000Flexible model selection and customization: InstructLab is model-neutral, which means \nthat users can select and fine-tune various LLMs based on their specific needs. Whether it is a general-purpose model like Granite or a more specialized compatible model that is found on Hugging Face for a particular domain, InstructLab enables users to adapt and improve the model that best fits their use case. \n/SM590000Broader audience participation: The open-sourc e nature and simplicity of the workflow in \nInstructLab make it possible for people without deep machine learning (ML) expertise to participate in model development and fine-tuning. This approach is a significant step toward democratizing AI and ensuring that more organizations, regardless of size or expertise, can harness the power of  LLMs for their specific needs.\nThe InstructLab innovati",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 188,
            "total_chunks": 313
          }
        },
        {
          "id": "4005d02f-121e-4280-9139-6a629f41d441",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "size or expertise, can harness the power of  LLMs for their specific needs.\nThe InstructLab innovative approach to mo del fine-tuning (leveraging taxonomy-driven \nknowledge curation, SDG, and iterative training) marks a transformative shift in how LLMs can be customized and applied across a wide ra nge of domains. By reducing the barriers to \nentry, lowering computational costs, and enabling highly specialized, domain-specific training, InstructLab is positioning itself as a key enabler of accessible, efficient, and scalable AI development. This open-source, community-driv en initiative is paving the way for a new \ngeneration of AI practitioners to fine-tune and enhance LLMs without requiring extensive \ntechnical expertise, which expands the scope and impact of AI in real-world applications.\n5.4.2  How to use InstructLab\nInstructLab is a model-neutral, open-source AI proj ect that facilitates contributions to LLMs. It \nis a new community-based approach to build truly open-source LLMs. ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 189,
            "total_chunks": 313
          }
        },
        {
          "id": "1eafbd49-4aa9-44fd-87e2-08acc6f7e6b4",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "tates contributions to LLMs. It \nis a new community-based approach to build truly open-source LLMs. InstructLab uses a synthetic-data-based alignment tuning method to train LLMs. The InstructLab tuning method is driven by manually created taxonomies. In structLab provides a process for optimizing and \ntuning LLMs by collecting knowledge and skills as par t of a taxonomy tree.\nTo start the InstructLab process, ilab must be installed. You can download it from its official \nrepository .\nilab is a CLI tool that you can use to perform the following actions:\n/SM590000Download a pre-trained LLM.\n/SM590000Chat with the LLM.\n/SM590000Add new knowledge and skills to the pre-train ed LLM by adding in formation to the \ncompanion taxonomy repository.\nAfter you add kno wledge and skills to the taxonomy, yo u can perform the following actions:\n/SM590000Use ilab to generate new synthetic training data based on the changes in your local \ntaxonomy repository.\n/SM590000Retrain the LLM with the new traini",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 190,
            "total_chunks": 313
          }
        },
        {
          "id": "b8d9df6f-e11c-4e0a-8ac7-2052dc7d594c",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ased on the changes in your local \ntaxonomy repository.\n/SM590000Retrain the LLM with the new training data.\n/SM590000Chat with the retrained LLM to see the results.\n\n72 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiFigure 5-10 shows the ilab flow of commands, which show how to start processing data for \nsynthetic data generation and the fine-tuning process.\nFigure 5-10   The ilab flow of commands\nBefore you begin working with ilab, ensure that your system meets the following \nrequirements:\n/SM590000Operating system: Linux (tested on Fedora), or macOS with  Apple M1/M2/M3 chipsets.\n/SM590000Disk space: Minimum of 250 GB. 500 GB is recommended for complete workflows.\n/SM590000Python Version 3.10 or 3.11. At the time of writing, Python 3.12+ is unsupported due to \ndependency constraints.\n/SM590000C++ compiler: Ensure that a modern GCC version or equivale nt is installed.\nIf you use Python environment management tools, ensure that they build libraries that are \ni",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 191,
            "total_chunks": 313
          }
        },
        {
          "id": "70cb7fb4-4f6d-428b-b762-8838b432c234",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "talled.\nIf you use Python environment management tools, ensure that they build libraries that are \nimplemented in C by including flags during Python compilation. For example, when using \npyenv , you use the following string:\nPYTHON_CONFIGURE_OPTS=\"--enable-framework\" pyenv install 3.11.5\n\n\nChapter 5. Advanced cap abilities of watsonx.ai 73To install the required tool s, run the following command:\nsudo dnf install gcc gcc-c++ make git python3.11 python3.11-devel\nilab can be installed with various configurations, depending on your hardware and preferred \naccelerators. Different hardware setups often require specific steps to optimize performance and ensure compatibility with the chosen accelerators, such as  Apple Metal, AMD ROCm, or \nNVIDIA CUDA.\nIf a GPU is available, you can leverage more processing power by using the following \ncommands for initialization:\npython3.11 -m venv --upgrade-deps venv\nsource venv/bin/activatepip cache remove llama_cpp_pythonCMAKE_ARGS=\"-DGGML_CUDA=on -DGGML",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 192,
            "total_chunks": 313
          }
        },
        {
          "id": "40cb60ca-1b47-4d64-a06e-6e1e6d39c0ed",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "deps venv\nsource venv/bin/activatepip cache remove llama_cpp_pythonCMAKE_ARGS=\"-DGGML_CUDA=on -DGGML_NATIVE=off\" pip install 'instructlab[cuda]'pip install vllm@git+https://github.com/opendatahub-io/vllm@v0.6.2\nAfter you install ilab, proceed with the first initialization by running the following command:\nIlab config init\nDuring initialization, ilab prompts you to perform specif ic tasks that influence how the \nenvironment is configured. By following these prompts, you can tailor ilab to meet your \nneeds by setting up essential components lik e the taxonomy repository, model paths, and \ntraining profiles. For example, selecting a training profile helps ens ure compatibility with your \nhardware, whether it uses CPUs or GPUs, to provide the best performance and resource optimization for your setup:\n1. Clone the taxonomy repository, either interactively or by specifying a path with the \n--taxonomy-path  flag.\n2. Specify the path to your model. By default, it uses a quantized Granite model",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 193,
            "total_chunks": 313
          }
        },
        {
          "id": "69a7076d-2b2b-4eed-81c2-add34c15bd75",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "axonomy-path  flag.\n2. Specify the path to your model. By default, it uses a quantized Granite model .\n3. Select a training profile. For systems without dedicated GPUs, choose No Profile (CPU, \nApple Metal, AMD ROCm) .\nAfter initialization, the directories that are shown in Table 5-1 are created.\nTable 5-1   ilab directory overview and details\nDirectory Description\n~/.cache/instructlab/models/ Contains downloaded models.\n~/.local/share/instructlab/datasets/ Stores the dataset outputs that are \ngenerated during workflows.\n~/.local/share/instructlab/taxonomy/ Contains skill and knowledge data from the \ntaxonomy repository.\n~/.local/share/instructlab/checkpoint\ns/Contains model checkpoints from the \ntraining process.\n~/.config/instructlab/config.yaml The configuration file  that is generated \nduring initialization.\n\n74 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiAfter you install the InstructLab CLI on your syst em, start by downloading the base model that \nyou",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 194,
            "total_chunks": 313
          }
        },
        {
          "id": "b74fc79f-b22c-48a3-a0ad-7468cbd41341",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "After you install the InstructLab CLI on your syst em, start by downloading the base model that \nyou want to train. The foundation of using InstructLab effectively is access to its models. The ilab CLI simplifies this process by offering robu st integration with repositories like Hugging \nFace or OCI. It provides authentication mechanisms, such as token-based access for Hugging Face, and features like repository specification and download acceleration. These capabilities help ensure se cure and efficient downlo ads of pre-tr ained models.\nTo quickly get started, download compact pre-trained versions of the following models:\n/SM590000granite-7b-lab-GGUF\n/SM590000merlinite-7b-lab-GGUF\n/SM590000Mistral-7B-Instruct-v0.2-GGUF\nTo initiate the download of a m odel, can run the following command:\nilab model download –repository <MODEL-ID>\nWhen this command runs, the ilab CLI interacts with the designated repositories to fetch the \nselected models. By default, the models are stored locally in t",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 195,
            "total_chunks": 313
          }
        },
        {
          "id": "5fcbe588-add1-4799-9a9f-c32c4675efc1",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "esignated repositories to fetch the \nselected models. By default, the models are stored locally in the ~/.cache/instructlab/models/  directory, which helps ensure efficient reuse because the \ndownloaded models do not need to be fetched again for future operations unless explicitly removed or updated.\nYou can download a non-default LLM from Hugging Face. If a Hugging Face token is required, \ncan add it by running ilab model download –repository <MODEL-ID> , but add the token after \nthe argument -hf-token .\nYou can use OCI-compliant repositories. To do so, log in to the registry and use the following \ncommand:\nilab model download -rp docker://<MODEL_ID> -rl latest\nOnce the models are downloaded, they can be served locally for inference. ilab supports \nserving both default and custom models if the system prerequisi tes are met. To serve models \nlocally, ensure that the system has sufficient hardware resources, which include at least 8 GB of RAM and, for GPU-accelerated serving, an  NVIDIA",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 196,
            "total_chunks": 313
          }
        },
        {
          "id": "b57c743c-4f67-4c36-9051-b8f169c07e5a",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " hardware resources, which include at least 8 GB of RAM and, for GPU-accelerated serving, an  NVIDIA GPU with CUDA support. If multiple \nilab clients attempt to connect to the same server, the first client connects successfully, and \nthe others create temporary servers, which require more resources. To prevent conflicts, manage the connections.\nTo serve the model, run the following command, which provides a URL for API interaction:\nIlab model serve –model-path <MODEL_PATH>\nIt is possible to interact with a served model directly within ilab by using the following \ncommand, with optional personalization of inference parameters, such as temperature:\nIlab model chat –model <MODEL_PATH> [-temperature <VALUE>]\nNow, you can start personalizing the mo del by adding new skills and knowledge.\nTo train an open source  model with InstructLa b, create knowledge and skills in the taxonomy \ndirectory. When you initialized the ilab CLI, it automatically cloned the InstructLab taxonomy \nrepository , wh",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 197,
            "total_chunks": 313
          }
        },
        {
          "id": "b4bdbc48-f51e-4add-a360-b3c50928acf6",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "When you initialized the ilab CLI, it automatically cloned the InstructLab taxonomy \nrepository , which is the source of truth for your model training.\nIn the context of skill contributions, the requi red content is smaller in volume compared to \nknowledge contributions. A complete skill addition to the taxonomy tree can be represented by a few lines in a qna.yaml  file (short for “questions and answers”) and an attribution.txt  \nfile to cite sources. \n\nChapter 5. Advanced cap abilities of watsonx.ai 75To make a valid skills contribution,  the pull request must include a qna.yaml  file with key-value \nentries that contain at least five question-and-answer pairs and an attribution.txt  file that \nlists the sources that are used. The taxonomy structure serves multiple purposes: selecting the relevant subset for data generation, ensu ring interpretability for contributors and \nmaintainers, and forming part of the prompt for the LLM when generating synthetic samples.\nEach qna.yaml  file mu",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 198,
            "total_chunks": 313
          }
        },
        {
          "id": "7f721cd3-9f5a-4e55-9b34-5d02dce7113b",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "and forming part of the prompt for the LLM when generating synthetic samples.\nEach qna.yaml  file must adhere to a standa rd structure with specific keys:\n/SM590000version : Must be set to 2 (required).\n/SM590000task_description : A description of the skill (required).\n/SM590000created_by : The GitHub username of the contributor (required).\n/SM590000seed_examples : A collection of key-value entries with at least five examples (required for \nnew files, although older files may contain fewer examples).\n/SM590000context : Provides relevant information for gr ounded skills, which guide the model’s \nprocessing (not used  for ungrounded skills).\n/SM590000question : The model's input query (required).\n/SM590000answer : The expected response (required).\nThe taxonomy tree also categorizes skills as  either grounded (req uiring context) or \nungrounded (not requ iring context). For example,  a grounded skill might be \ngrounded/linguistics/grammar, wh ile an ungrounded skill might be \nlinguistics/",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 199,
            "total_chunks": 313
          }
        },
        {
          "id": "4e9bef2f-9b2f-46c7-aa4e-5a9dca9fb676",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "nded skill might be \ngrounded/linguistics/grammar, wh ile an ungrounded skill might be \nlinguistics/writing/poetry/haiku. The qna.yaml  file is always in the fi nal node of the taxonomy \npath. Importantly, there is a limit on the content length in question-answer pairs to ensure model compatibility; co ntributions should not exceed appr oximately 2,300 words for these \npairs. By adhering to these gui delines, contributors can maintain  consistency and utility within \nthe skill taxonomy framework\n.\nTo make the qna.yaml  files faster for humans to read, it is best practice to specify version  \nfirst, which is followed by task_description , then created_by , and finally seed_examples . In \nseed_examples , it is a best practice to specify context  first (if applicable), followed by question  \nand answer . \nExample 5-1 shows an example of a qna.yaml file.\nExample 5-1   A qna.yaml file\nversion: 2\ntask_description: <string>created_by: <string>seed_examples:  - question: <string>    answer: | ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 200,
            "total_chunks": 313
          }
        },
        {
          "id": "4d1a95e8-5145-4013-9f39-4b2997d40ab2",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": ": 2\ntask_description: <string>created_by: <string>seed_examples:  - question: <string>    answer: |      <multi-line string>  - context: |      <multi-line string>    question: <string>    answer: |      <multi-line string>  ...\nCreate an attribution.txt  file that includes the sources of your information, which can be \nself-authored sources.\n\n76 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiKnowledge contributions differ from skills by focusing on answering fa ctual, data-driven, or \nreference-based questions, which are often su pported by documents like textbooks, technical \nmanuals, encyclopedias, journals, or maga zines. Although knowle dge and skills share \nsimilarities in their taxonomy structures, kno wledge nodes include additional elements to \naccommodate their document-based nature.\nFor contributors that use InstructLab 0.21.0 or later, knowledge contributions can include PDF \nfiles as valid document types, but earlier versions accept only markdown f",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 201,
            "total_chunks": 313
          }
        },
        {
          "id": "91cee1e5-ef2a-4f24-bce6-d637736761f5",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ibutions can include PDF \nfiles as valid document types, but earlier versions accept only markdown formats. Each knowledge node in the taxonomy tree contains a qna.yaml  file that is simila r in structure to the \none that is used for skills, but with additional fields to support knowledge-specific attributes. Notably, all knowledge submissions must be in a Git repository, such as one hosted on GitHub, and the qna.yaml  file must reference this repository:\n/SM590000Submit the most current version of the document.\n/SM590000Contributions must be text-based. Images are ignored.\n/SM590000Avoid using tables in your markdown freeform contributions.\nThe qna.yaml  file for knowledge contributions must follow a specific format and include the \nfollowing fields:\n/SM590000version : The version of the qna.yaml  file format, which is set to 3.\n/SM590000created_by : The GitHub username of the contributor.\n/SM590000domain : The category of the knowledge.\n/SM590000seed_examples : A collection of key-va",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 202,
            "total_chunks": 313
          }
        },
        {
          "id": "f4b2a84d-58a2-4d48-950f-23e1c27e0fcc",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "or.\n/SM590000domain : The category of the knowledge.\n/SM590000seed_examples : A collection of key-value entries.\n/SM590000context : A chunk of information from the knowledge document. Each qna.yaml  file must \ninclude at least five context blocks, with a maximum of 500 words per block.\n/SM590000questions_and_answers : Holds the questions and answers based on the context. Each \ncontext block requires a minimum of three question-and-answer pairs, each with a maximum word count of 250 words.\n–question : A question for the model.\n–answer : The corresponding answer.\n/SM590000document_outline : An overview of the document being submitted.\n/SM590000document : The source document for the knowledge contribution.\n/SM590000repo: The URL of the repository that contains the knowledge files.\n/SM590000commit : The SHA of the commit in the repository for the knowledge files.\n/SM590000patterns : A list of glob patterns specifying the files in the repository. Patterns starting with \n*, such as *.md, mus",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 203,
            "total_chunks": 313
          }
        },
        {
          "id": "3fdcb45e-67b3-4b84-ab60-5e23b054e538",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "f glob patterns specifying the files in the repository. Patterns starting with \n*, such as *.md, must be quoted (\" *.md\") to comply with YAML rules.\nBy adhering to these guidelines, knowledge c ontributions maintain a structured, accessible \nformat that aligns with the taxonomy framework and supports efficient integration into the system.\n\nChapter 5. Advanced cap abilities of watsonx.ai 77When working with YAML files (f or both skills and knowledge), it is crucial to adhere to \nspecific formatting rules to help ensure correctness and avoid parser errors. Indentation and spacing play a role, and YAML requires \ntwo spaces for each level of indentation ( tabs must not \nbe used  under any circumstances). Also, avoid traili ng spaces at the end of lines because they \ncan lead to issues during processing. For entries in seed_examples , each example begins with \na - placed before the first field, such as question  or context . Subsequent keys within the \nsame example should not include the -.",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 204,
            "total_chunks": 313
          }
        },
        {
          "id": "9720628c-44a0-47bf-811a-61405d583c21",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "d, such as question  or context . Subsequent keys within the \nsame example should not include the -. Pay attention to sp ecial characters like \" and ', which \nmust be escaped by using a backslash ( \\). To simplify handling these characters, YAML \nenables the use of the | character at the start of a value, which disables special character \ninterpretation and supports multi-line strings. For example, lines that start with | are followed \nby an indented block that contains the string 's content. To avoid unexpected YAML parser \nbehavior, it is a best practice to quote all values  by using double quotation marks ( \"). This \napproach prevents values such as Yes or No from being interpreted as Boolean types ( True or \nFalse ). For more information about managing multi-line strings and YAML nuances, see the \nyaml-multiline.info  file.\nNow, after creating a YAML file for skills and knowledge, as shown in Figure 5-7 on page 68, \nyou can validate your new data. Use the ilab taxonomy diff  comman",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 205,
            "total_chunks": 313
          }
        },
        {
          "id": "ab2e027c-04ca-441b-9935-94ed8c532289",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " shown in Figure 5-7 on page 68, \nyou can validate your new data. Use the ilab taxonomy diff  command to help ensure that \nilab is registering your new knowledge or skills  and that your cont ributions are properly \nformatted. This command displays any new or modified YAML files within your taxonomy tree. You can also validate your entire taxonomy by performing a diff against an empty base by using the --taxonomy-base=empty  argument.\nAfter validation, it is possible to start the Synthetic Data Generation (SDG) pipeline. To \ngenerate a synthetic da taset based on newly a dded knowledge or skill sets in the taxonomy \nrepository, run the ilab data generate  command. Before proceeding, ensure the existing \nmodel to which you are adding skills or knowle dge is still running. Al ternatively, you can \ninitiate the serv er by using the ilab data generate  command by specifyi ng a fully qualified \nmodel path with the --mode l flag. At the time of writing, the full CLI pipelin e supports only \n",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 206,
            "total_chunks": 313
          }
        },
        {
          "id": "b83cdbf1-6852-4bb0-b288-666ef4afbd2b",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "d \nmodel path with the --mode l flag. At the time of writing, the full CLI pipelin e supports only \nMixtral and Mistral Instruct Family models as the teacher model. For the simple pipeline, Merlinite 7b Lab is the only supported teacher model due to the specific model prompt templates that it uses. There is  a plan to expand compatibility in  the future, and on watsonx.ai \n(as described in Chapter 6 , “Artificial intelligence  agents” on page 87).\nTo start generation, run the following command:\nilab data generate [--pipeline full --gpus <NUM_OF_GPUS> --model <MODEL_PATH>\nOptionally, you can start SDG by using GPUs wh en they are available. You can specify the \nteacher model that is used (the default one for the ilab CLI is Merlinte-7B).\nAfter generation finished, the synthetic dataset consists of two files in the \n~/.local/share/instructlab/datasets  directory: \n/SM590000skills_train_msgs_*.jsonl\n/SM590000knowledge_train_msgs_*.jsonl\nYou can run the generate step against a different mo",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 207,
            "total_chunks": 313
          }
        },
        {
          "id": "b36712a1-7426-48c4-b4e1-5694ee9ba1d8",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "s_*.jsonl\n/SM590000knowledge_train_msgs_*.jsonl\nYou can run the generate step against a different model through a compatible API, such as \nthe one that is created by the ilab model serve or any remote or locally hosted LLM (through \nollama, LM Studio, or others). Run the following command:\nilab data generate --endpoint-url http://localhost:8000/v1\nNow that the curated dataset for a fine-tuning is ready, the fine-tuning process can be started.\n\n78 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiThe InstructLab model train has three pipelines: simple , full, and accelerated . The default \nis full.\n/SM590000simple  uses an SFTTrainer on Linux and MLX on MacOS. This type of training takes \nroughly an hour and produces the lowest fidelity model but should indicate whether your data is being picked up by the training process.\n/SM590000full uses a custom training loop and data proc essing functions for the Granite family of \nmodels. This loop is optimized for CPU and M",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 208,
            "total_chunks": 313
          }
        },
        {
          "id": "9d04720a-129e-4f72-ac33-e17f0002205f",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "d data proc essing functions for the Granite family of \nmodels. This loop is optimized for CPU and MPS function. Use --pipeline=full  with \n--device=cpu  (Linux) or - -device=mps  (MacOS). You can also use --device=cpu  on a \nMacOS machine. However, MPS is optimized for better performance on these systems.\n/SM590000accelerated  uses the instructlab-training  library, which suppor ts GPU-accelerated \nand distributed training. The full loop and data processing functions are either pulled directly from or based on of the work in this library.\nTo limit training time, you can adjust the num_epoch  parameter in the config.yaml  file. The \nmaximum number of epochs for running the InstructLab end-to-end workkflow is 10.\nThe following command shows how to start the automatic fine-tuning process with the \npreviously generated dataset. Furthermore, it can specify more than the pipeline, such as the \ndevice that you want the model to be trained on (CPU, MPS, or GPU).\nIlab model train [--pipeline <",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 209,
            "total_chunks": 313
          }
        },
        {
          "id": "ef1cddaf-e7c3-4c57-bab2-03bf6abf18e9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " \ndevice that you want the model to be trained on (CPU, MPS, or GPU).\nIlab model train [--pipeline <PIPE_ID> --device <DEVICE_ID> --data-path \n<DATA_PATH>]\nThis training step can potentially take from several minutes to several hours to complete, \nwhich depends on the available computing resources.\nAfter the fine-tuning pipeline completes, it is possible to verify the quality of the new model \nand whether the genera ted dataset with the defined skills and k nowledge produced good \nresults. To thoroughly test and evaluate a newly trained model by using InstructLab, first run a series of commands that are designed to assess the performance and accuracy of the model after training. The testing process involves using the ilab model test  command to obtain \noutput from the model before and after the traini ng process. With this output, you can see how \nwell the model performs based on its previous state and after it has undergone the enhancements from the training process. The results from ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 210,
            "total_chunks": 313
          }
        },
        {
          "id": "cf49e809-6e63-4fc1-80ca-01a99a096d1e",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "vious state and after it has undergone the enhancements from the training process. The results from this test show the effectiveness of your training and provide insight into areas where further improvement might be needed.\nWhen the model is tested, you can use the ilab model evaluate  command to run the model \nthrough a set of predefined benchmarks to evaluate its performance across various categories. At the time of writing, there are four primary benchmarks that are supported by InstructLab: \n/SM590000Multitask Language Understanding (MMLU)\n/SM590000MMLUBranch\n/SM590000MTBench\n/SM590000MTBenchBranch. \nThese benchmarks assess different aspects of a model's capabilities, su ch as its knowledge \nand skills: \n/SM590000MMLU evaluates the model’s general knowledge on a wide range of topics. \n/SM590000MMLUBranch compares the model’s performance to the performance of a base model to \nidentify improvemen ts in knowledge. \n/SM590000MTBench evaluates how well the model applies its knowledge in",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 211,
            "total_chunks": 313
          }
        },
        {
          "id": "f4c83e68-2cdc-42d8-97bd-50cb6d7defc9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " improvemen ts in knowledge. \n/SM590000MTBench evaluates how well the model applies its knowledge in multi-turn conversations.\n/SM590000MTBenchBranch assesses improvements or r egressions in specific skill areas when \ncompared to a base model. \n\nChapter 5. Advanced cap abilities of watsonx.ai 79For each benchmark, the evaluation generates detailed reports, which show scores and \nidentify areas where the model performs well and areas that need further work. For example, the MMLU report provides a score for various s ubjects, such as abstract algebra, anatomy, \nand business ethics, which indicate how the model performs in each area. A typical output for MMLU looks like a series of subject categories with a score 0.0 - 1.0, with higher scores indicating better performance in the respective topics. \nRunning MMLUBranch involves evaluating your model's contributions compared to a base \nmodel. The evaluation outputs a score for both the base model and the newly trained model, along with a rep",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 212,
            "total_chunks": 313
          }
        },
        {
          "id": "493676fe-c9c9-4f81-88d4-be9716bb1e03",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "The evaluation outputs a score for both the base model and the newly trained model, along with a report on the improvements or regressions that are observed. For example, you might see that the model improved in one area, like “tonsils,” from a score of 0.74 to 0.78, which indicates that your  training enhanced the model’s ab ility in that particular knowledge \ndomain. \nMTBench and MTBenchBranch follow a similar structure, but they focus on testing the \nmodel’s skills rather th an just knowledge. MTBench evaluate s the model's ability to perform in \nmulti-turn dialogs, which provide a score for each turn in the conversation, such as turn one and turn two. MTBenchBranch compares your model’s skill performance to  a base model, \nwhich provides a detailed breakdown of areas wh ere your model improved or regressed, and \nhighlighting any skills that sh owed no significan t change. This deta iled feedback helps \npinpoint specific areas where more fine-tuni ng might be necessary to enhance th",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 213,
            "total_chunks": 313
          }
        },
        {
          "id": "76746039-a911-4a88-886d-ac2982cca5a9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "led feedback helps \npinpoint specific areas where more fine-tuni ng might be necessary to enhance the model's \nabilities. For each benchmark, it is important to help ensure that  the model that is evaluated is \nin a supported format, either safetensors or GGUF. Using models directly from Hugging Face without downloading them is not supported.\nAlso, while using models for MMLU and MMLUBranch evaluations, GGUF models are not \nsupported at the time of writing. When running MTBench and MTBenchBranch, it is a best practice to use the \nPrometheus-8x7b-v2.0  model as the judge model, but you can use a \ndifferent judge model. You can download the Prometheus model by running the ilab model \ndownload  command for local use in these evaluations.\nThe entire process of running these evaluations can take from several minutes to several \nhours, which depend on the size of the model and the dataset that is used. Be prepared to allocate enough time for the evaluations to complete, especially wh en work",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 214,
            "total_chunks": 313
          }
        },
        {
          "id": "5f8283fc-f35a-4d17-be57-9004c40df7d1",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " is used. Be prepared to allocate enough time for the evaluations to complete, especially wh en working with large \ndatasets or multiple training epochs. The results from these evaluations provide a \ncomprehensive view of the mode l's strengths and weak nesses, which offer valuable insights \nto guide further refinement and optimization of the model.\nAfter the process ends and the results are good enough for the specified use case, a new \nfine-tuned model with new synthetic data in GGUF format is available in the ilab specified \nfolder location in a GGUF format. Apart from the usage on ilab, it is possible to deploy it on \nhyperscalers such as watsonx.ai run time by using the Bring Your Own Model (BYOM) function, which fully enables a true, at-scale, enterprise-level fine-tuning process of FMs.\n5.4.3  InstructLab on watsonx.ai Software-as-a-Service\nAt the time of writing, Inst ructLab has demonstr ated its usability pr imarily through the ilab \nCLI. This method enables users to interact",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 215,
            "total_chunks": 313
          }
        },
        {
          "id": "e725ef24-5e4f-4fdf-9e31-3de24bd0f161",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " demonstr ated its usability pr imarily through the ilab \nCLI. This method enables users to interact with InstructLab features and functions in a straightforward and efficient manner. However, the development team is working on integrating InstructLa b with watsonx.ai. This upcoming integration will enh ance the user \nexperience by providing a dedicated user interf ace (UI). This UI will streamline the entire \nprocess, which will make it mo re accessible and intuitive for users who might not be \ncomfortable with CLI operations. This development is expected to open new possibilities for a \nbroader range of user by facilitating simpler access to powerful InstructLab tools and features.\n\n80 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiFigure 5-11 shows the interface for tuning a generative LLM by using InstructLab. This \ninterface is designed to provide users with an intuitive platform for refining language models \nto meet specific needs and requ irements. It al",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 216,
            "total_chunks": 313
          }
        },
        {
          "id": "705068e1-9728-4b51-b01f-2a9f02f8d77d",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " an intuitive platform for refining language models \nto meet specific needs and requ irements. It also illustrates the starting point of the overall \nprocess: the addition of  skills and knowledge. \nFigure 5-11   Tuning Studio interface\nUsers begin by selecting the FM , and then proceed to integrat e various skills and knowledge \nareas into the model. The interface guides users through detailed steps, which include specifying configurations. Throughout this process, users can monitor their progress and make necessary adjustments to ensure that the model's performance aligns with outcomes.\nMoreover, InstructLab offers advanced features such as feedback, performance metrics, and \ntroubleshooting tools to enhance the tuning ex perience. This comprehensive approach helps \ncreate an accurate and efficient LLM, with continuous improvement and adaptation to evolving linguistic patterns and user needs.\nFigure 5-12 on page 81 illustrates a detailed tax onomy tree of skill and knowledge files th",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 217,
            "total_chunks": 313
          }
        },
        {
          "id": "91d26f7f-9c2f-4977-8700-d9bdf6b16f88",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " needs.\nFigure 5-12 on page 81 illustrates a detailed tax onomy tree of skill and knowledge files that \nare managed within Tuning Studio on watsonx.ai for InstructLab. The UI shows a comprehensive and organized view of the hier archical structure of  skills and knowledge \nareas. Each node in the taxo nomy tree represents a distinct  category, which facilitates \nnavigation and access to specific training data.\n\n\nChapter 5. Advanced cap abilities of watsonx.ai 81Figure 5-12   Taxonomy tree of skill and knowle dge files that are managed in Tuning Studio on \nwatsonx.ai for InstructLab\nOne of the key functions of this UI is its abilit y to visualize and mainta in version history for \neach training run, which includes the train ed models, the associated skills and knowledge \ntaxonomy, grounding data, and the generated synthetic data. By tracking the evolution of these elements over time, users can effectively monitor the progression and improvements that are made with each iteration. This ve",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 218,
            "total_chunks": 313
          }
        },
        {
          "id": "0ebf7194-e935-48df-b001-a8491e565bae",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " can effectively monitor the progression and improvements that are made with each iteration. This version control mechanism is essential for helping ensure the reproducibility and relia bility of model training processe s. Also, it will be possible to \nadd knowledge and ingest multiple data formats, such as PDFs, docx, HTML, MD, and more.\nThe Tuning Studio capability to  handle such a vast array of data types and their versions \nenables meticulous fine-tuning and enhances the overall model development lifecycle. Through this meticulous documentation and ma nagement, users can draw insights from past \ntraining runs, identify best practices, and apply learned lessons to future projects.\n\n\n82 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiFigure 5-13 shows the progress map during the tuning phase of InstructLab on watsonx.ai.\nFigure 5-13   Progress map during the tu ning phase of InstructLab on watsonx.ai\nBefore commencing the SDG and fine-tuning process, conduct ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 219,
            "total_chunks": 313
          }
        },
        {
          "id": "04a350c6-e0eb-48a9-8658-23b0347a89d8",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " ning phase of InstructLab on watsonx.ai\nBefore commencing the SDG and fine-tuning process, conduct a peer review of the submitted \nskills and knowledge files. This  review involves project mem bers and SMEs to help ensure \ncomprehensive assessment and validation. If any modifications are identified as necessary during this review phase, new branches can be created within the version control system to incorporate these changes without disrupting the main development line.\nAfter you incorporate the feedback and necessary adjustments, the fine-tuning process \ncommences. This step involves leveraging pre-trained models and adjusting them to specific requirements by training on customized data sets. The goal is to enhance the model's \nperformance in targeted areas to help ensure that it meets the specif ications and accuracy \nlevels.\nOn successful fine-tuning, the newly optimized model can be exported in the GGUF format, \nwhich is a versatile and widely supported format  for deploying quan",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 220,
            "total_chunks": 313
          }
        },
        {
          "id": "88c79cd2-10f9-49ef-9c82-6721b2ca7e86",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "e exported in the GGUF format, \nwhich is a versatile and widely supported format  for deploying quantized FMs. Alternatively, \nthe model can be directly deployed onto the watsonx.ai run time environment. This deployment establishes a seamless large language model operations (LLMOps) pipeline within watsonx.ai for InstructLab, which enables automated monitoring, maintenance, and iterative improvement of the model.\nBy integrating these processes within the watsonx.ai ecosystem, you help ensure continuous \ndelivery and operational efficiency of AI solutions that align with best practices in modern ML workflows.\nThe fine-tuning process that is shown in Fi gure 5-14 on page 83 shows the intricate and \nmethodical approach that is adopted by InstructLab within the watsonx.ai framework. This process is characterized by a 2-phase, fine-tuning methodology that is augmented with a replay buffer, which helps ensure enhanced performance and accuracy of the models.\n\n\nChapter 5. Advanced cap abilitie",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 221,
            "total_chunks": 313
          }
        },
        {
          "id": "045f95ac-0f91-4399-839f-8074382baf08",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ich helps ensure enhanced performance and accuracy of the models.\n\n\nChapter 5. Advanced cap abilities of watsonx.ai 83Figure 5-14   InstructLab fine-tuning process with  real-time performance on the 2-phase fine-tuning \nprocess with replay buffer\nIn the initial phase, the pre-trained models undergo a rigorous training regimen by using \ntargeted datasets that are pertinent to the specif ic knowledge areas that are identified within \nthe taxonomy tree. This phase focuses on adapting the general-purpose models to the specialized requirements of the InstructLab projects, which hone their performance to meet the specifications. The second pha se uses the skills and continues  catastrophic forgetting by \nusing a dedicated replay buffer while fi ne-tuning the skills in  the second phase. \nThe replay buffer mechanism plays a crucial role in this fine-tuning process. By systematically \nstoring and replaying past experiences (training data and model states) during the training sessions, the buff",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 222,
            "total_chunks": 313
          }
        },
        {
          "id": "a8685e4f-e7bb-4099-93ba-bdb814c48770",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "d replaying past experiences (training data and model states) during the training sessions, the buffer helps ensure that the models continuously learn and adapt from the previous run. This approach mitigates catastroph ic forgetting and reinforces learning from \nhigh-value data points, which improves th e models' robustness and generalization \ncapabilities. Figure 5-14 encapsu lates a holistic and dynamic ap proach to model fine-tuning \nwithin the watsonx.ai ecosystem, which highlight s the interplay of ad vanced methodologies \nand real-time performance monitoring to achieve superior gen AI solutions.\nInstructLab will be available on watonx.ai as Software-as-a-Serv ice (SaaS), and it will be an \nimportant addition for gen AI on  an enterprise level. It will ena ble a pool of resources to \nfine-tune and improve generative LLMs.\n\n\n84 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai5.4.4  InstructLab use case examples\nIntroducing InstructLab on watsonx.ai heralds a",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 223,
            "total_chunks": 313
          }
        },
        {
          "id": "e0d50291-509a-4d84-89be-dff7770ea718",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "h IBM watsonx.ai5.4.4  InstructLab use case examples\nIntroducing InstructLab on watsonx.ai heralds a new era of advancements in gen AI, \nparticularly in the realm of enterprise-level ap plications. This innovative platform integrates \nseamlessly within the watsonx.ai ecosystem , which enables a dynamic and robust \nenvironment for the fine-tuning and deployment of LLMs. As an SaaS offering, InstructLab provides an unparalleled suite of tools and methodologies to optimize model performance through a meticulously structured 2-phase, fine-tuning process.\nThe InstructLab capabilities ar e impactful across many use cases, and they empower \norganizations to tailor AI solutions to their unique requirements. By leveraging the advanced features of Instru ctLab, users can achieve superior accura cy, efficiency, and scalability in their \nAI-driven initiatives. \nHere are some of the first prominent examples of use cases where InstructLab has \ndemonstrated significant improvements in its early life:",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 224,
            "total_chunks": 313
          }
        },
        {
          "id": "4a7c98a7-3364-49cd-89ee-27b33ebbab7e",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "xamples of use cases where InstructLab has \ndemonstrated significant improvements in its early life:\n/SM590000Emergency medical services use case: A large hospital is looking to automate the \nprocessing of emergency medical records to  improve the efficiency and accuracy of \ncritical tasks. The system should be capable of completing the following tasks:\n– Case classification: Assigning priority le vels such as green (low urgency), orange \n(medium urgency), or red (high urgency) flags to cases based on their severity.\n– Peer medical review recommendations: Identifying cases that might require further \nreview by a medical peer to help ensu re proper oversight and decision-making.\n– Clinical compliance and guideline deviation: Detecting discrepancies  in medical reports \ncompared to established clinical guidelines, which enhance compliance and quality assurance.\n– Knowledge: The system will rely  on hospital compliance da ta, which includes clinical \nguidelines and historical patient reco",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 225,
            "total_chunks": 313
          }
        },
        {
          "id": "2c311736-27f8-41d8-b470-653cf36f0bea",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " rely  on hospital compliance da ta, which includes clinical \nguidelines and historical patient records to perform its tasks.\n– Skill requirements:\n Case classification to prio ritize emergency scenarios.\n Identifying deviations from clinical guide lines to help ensure regulatory compliance.\n/SM590000Call transcript processing use case: A large North American telecommunications \ncompany requires an automated system to process and summarize incoming customer support call transcripts. The system must extract and organize information based on a predefined set of 80 questions. Examples include “Did the customer want to upgrade their plan?” or “Did the customer report bandwidth issues?”\n– Knowledge: The primar y data source will be call transcripts, which contain varied \nhuman expressions and conversational styles.\n– Skill requirements: Interpreting and anal yzing natural languag e, which includes \nunderstanding diverse writing styles and linguistic nuances to extract relevant insights fr",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 226,
            "total_chunks": 313
          }
        },
        {
          "id": "6acec7a2-fbcb-4211-b78b-87bbecd89faa",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ncludes \nunderstanding diverse writing styles and linguistic nuances to extract relevant insights from conversations.\n\nChapter 5. Advanced cap abilities of watsonx.ai 85/SM590000Personalized retail recommendations use case: A retailer aims to deploy a personalized \nrecommendation engine that suggests in-stock products that are tailored to a user’s dietary preferences, which include offering recommendations based on food allergies, nutritional goals, or ingredient restrictions.\n– Knowledge: The engine must use detailed product nutritional information and inventory \ndata to ensure accurate and timely recommendations.\n– Skill requirements:\n Classification of ingredients and their alignment with dietary preferences. Recommending products that match user profiles while considering inventory \navailability.\n– Agent capabilities:\n Understanding and analyzing inventory data in real time. Interpreting customer preferences and purchase history to generate meaningful \nsuggestions.\n/SM590000Int",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 227,
            "total_chunks": 313
          }
        },
        {
          "id": "bbf6c629-29cc-4ffb-b0dd-7e18002d9f51",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "rpreting customer preferences and purchase history to generate meaningful \nsuggestions.\n/SM590000Intelligent auto claims processi ng use case: An insurance prov ider requires a solution to \nanalyze images of auto accidents and sugges t insurance coverage recommendations that \nare based on the claimant's active policy. The system should improve the speed and accuracy of claims processing.\n– Knowledge: The system needs access to policy details, which include terms, coverage \nlimits, and exclusions.\n– Skill requirements:\n Classification of accident severity by analyzing damage in submitted images.\n Matching severity with appropriate coverage based on the policy.\n– Agent capabilities:\n Accessing and analyzing driver history to help ensure an accurate policy \napplication.\n Interpreting active insurance policies to provide recommendations that are aligned \nwith coverage terms.\n\n86 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai\n\n© Copyright IBM Corp. 2025. 87Chap",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 228,
            "total_chunks": 313
          }
        },
        {
          "id": "acfb2f37-b467-4343-ba37-85b973d6efff",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai\n\n© Copyright IBM Corp. 2025. 87Chapter 6. Artificial intelligence agents\nArtificial intelligence (AI) agents will soon become  pivotal in modern co mputing by serving as \nautonomous entities that can perceive their environment, reason about their goals, and \nperform actions to achieve wanted outcomes. These agents will be central to many AI \nsystems, from personal assistants and autono mous vehicles to advanced simulations and \ndecision-making tools. The vers atility and capability of AI agen ts arise from their ability to \noperate independently while adapting to dynamic and often unpredictable environments. They combine advanced algorithms with AI and generative AI (gen AI) models, which enable \nthem to make intelligent decisions, ad apt to changes , and optimize outcomes. \nThis chapter delves into the intricacies of AI agents by exploring their fundamental \ncharacteristics, the motivation behind their development, and t",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 229,
            "total_chunks": 313
          }
        },
        {
          "id": "4fe83035-6b3d-498e-8481-9ccb28934fc7",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ents by exploring their fundamental \ncharacteristics, the motivation behind their development, and their applications across various domains. Through a detailed ex ploration, readers will understa nd why AI agents represent a \nparadigm shift in how computational systems interact with and influence their surroundings.\nThe following topics are described in this chapter:\n/SM5900006.1, “What makes an AI agent” on page 88\n/SM5900006.2, “Why AI agents are needed” on page 94\n/SM5900006.3, “Multiple AI agents” on page 95\n/SM5900006.4, “AI agents on watsonx.ai” on page 100\n/SM5900006.5, “AI agents use case examples” on page 1076\n\n88 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai6.1  What makes an AI agent\nAn AI agent  can be formally defined as a computational  entity that is equipped with sensors to \nperceive its environment and use its actuators to interact with it. The core of an agent lies in its ability to reason and dec ide, which is driven by algorithms that ena",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 230,
            "total_chunks": 313
          }
        },
        {
          "id": "ec7d4536-8f87-4c1f-8d58-0ce37e098913",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "e core of an agent lies in its ability to reason and dec ide, which is driven by algorithms that enable it to  analyze inputs, \npredict outcomes, and choose actions that are aligned with specif ic objectives. Unlike \ntraditional software systems, which operate on predefined instructions, AI agents exhibit autonomy and flexibility with dynam ic execution flows, which en able them to handle complex \nscenarios with minimal human intervention. This autonomy stems from their design \nprinciples, which often draw from cognitive sciences, game theory, and control systems, \nenabling agents to emulate human-like decision -making processes. Furthermore, each AI \nagent can be designed for differ ent levels of complexity and interaction. This adaptability \nmakes AI agents indispensable in fields such as robotics and natural language processing (NLP), where complex interactions with dynamic environments are required.\nFigure 6-1 shows a schematic view of an AI agent.\nFigure 6-1   Schematic view of an",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 231,
            "total_chunks": 313
          }
        },
        {
          "id": "91e13ddb-fa60-4734-a4f2-2900a322fe42",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ts are required.\nFigure 6-1 shows a schematic view of an AI agent.\nFigure 6-1   Schematic view of an AI agent\nAgents, in the context of AI, can be conceptualized as sophisticated orchestrators of \nintelligence that can address intricate and multi-faceted problems. They achieve these goals by leveraging the reas oning capabilities of large languag e models (LLMs), formulating \ndetailed plans to resolve challenges, and running these plans by using a diverse set of tools. \nAn agent operates as a cohesive system, and its architecture can typically be broken down into four key modules:\n/SM590000The agent core\n/SM590000Search and memory modules\n/SM590000Planning modules\n/SM590000Tools\nEach of these components plays a critical role in ensuring th e agent's function s, adaptability, \nand effectiveness.\nThe \nagent core  stands as the central coordination hub of the agent. It is often described as \nthe “decision-making nucleus” due to its pivotal role in governing the agent's logic, behavior, an",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 232,
            "total_chunks": 313
          }
        },
        {
          "id": "b6021333-9d09-4bda-8ad6-11670800fed8",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " \nthe “decision-making nucleus” due to its pivotal role in governing the agent's logic, behavior, and overall strategy. This module is responsible for synthesizing inputs, determining appropriate actions, and managing the interactions between other components. \n\n\nChapter 6. Artificial intelligence agents 89To design a robust and efficient agent core, you must define several foundation aspects that \nserve as the blueprint for the agent’s behavior: \n/SM590000The general goals of the agent must be es tablished. These goals act as the guiding \nprinciples that dictate the agent’s actions  and responses, which help ensure that its \noperations align with the overarching objectives that it is designed to achieve. Without clear goals, the agent risks becoming unfocused or inefficient. \n/SM590000Another critical aspect of the agent core is the explicit definition of the tools that are \navailable for execution, which involves cr eating a comprehensive “user manual” that \ncounts and describes all ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 233,
            "total_chunks": 313
          }
        },
        {
          "id": "636a159a-3cc7-4a38-afbe-745d3a9ee265",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "or execution, which involves cr eating a comprehensive “user manual” that \ncounts and describes all tools at  the agent's disposal. Each t ool’s capabilities, limitations, \nand specific use cases should be outlined to enable the agent to allocate resources effectively and run tasks with precision. \n/SM590000Furthermore, the a gent core must provide detailed guidance about t he utilization of \nplanning modules. These modules are instrumental in enabling the agent to adapt dynamically to varying scenarios by selecting the most suitable planning strategy based on the context. This adaptability is key to ensuring th e agent’s effectiv eness in complex \nand unpredictable environments.\nMemory integration  is another cornerstone  of the agent core. The memory system is \ndesigned to maintain and use relevant information from prior interactions or external research, which enhances the agent’s ability to g enerate accurate and context-aware \nresponses. This integration requires dynamic managemen",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 234,
            "total_chunks": 313
          }
        },
        {
          "id": "51bf029c-b07f-4734-8198-ba6cb345dff9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "lity to g enerate accurate and context-aware \nresponses. This integration requires dynamic management of memory items to help ensure that only the most pertinent data is referenced during inference. Also, an optional persona definition can be incorporated into the agent core to influence the agent’s tone, preferences, and behavioral nuances. By defining a persona, the agent can be tailored to exhibit specific \ncharacteristics that align with its intended use case or audience, adding a layer of uniqueness to its interactions. \nMemory modules  are integral to the agent’s ability to maintain contextual \nawareness and continuity. These modules are responsible for storing and managing information that supports the agent’s operations. Memory can be categorized into two main types:\n/SM590000Short-term memory\nShort-term memory focuses on capturing the agent’s immediate actions, thoughts, and \nobservations during ongoing interactions, which include data that is retrieved from vector searches, o",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 235,
            "total_chunks": 313
          }
        },
        {
          "id": "c16e3785-5d2d-49e6-a63e-0ef48436a07d",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "servations during ongoing interactions, which include data that is retrieved from vector searches, outputs from API calls, and results from database queries. Short-term memory \nis essential for helping ensure that the agent can respond effectively to the immediate \ncontext of a user’s query.\n/SM590000Long-term memory\nIn contrast,  long-term memory serves as a repository for information that is accumulated \nover extended periods, which include summariz ed logs of prior interactions, personal \ndetails and preferences of the user, and other contextual information that might influence the agent’s behavior. Long-term memory enables the agent to maintain a consistent and personalized approach in its interactions, which enhance user satisfaction and engagement. For example, in a conversational agent, long-term memory enables the retention of user preferences and past conversations, which create a more seamless and intuitive experience.\nWhen solving intricate problems, agents that are powered ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 236,
            "total_chunks": 313
          }
        },
        {
          "id": "7ca98aa2-5659-4370-a627-a7774af24588",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " a more seamless and intuitive experience.\nWhen solving intricate problems, agents that are powered by LLMs are adept at navigating \ncomplexity by employing a combination of advanced methodologies that resembles planning an execution flow. One such methodology is \ntask and question decomposition . This approach \ninvolves breaking down compound queries into smaller, more manageable sub-questions that can be addressed sequentially. \n\n90 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiFor example, to answer the question, “Will the te mperature tomorrow be higher or lower than \nthe historical average?” the agent must decomp ose it into subquestions such as identifying \nthe location, determining the forecasted temperature for the specified location, and retrieving the historical average temperature for comp arison. By addressing each sub-question \nindividually, the agent can construct a comprehensive and accurate response. \nAnother essential technique is the usage of",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 237,
            "total_chunks": 313
          }
        },
        {
          "id": "bc4ec9cf-93ab-456f-b28d-a3a314830290",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "nt can construct a comprehensive and accurate response. \nAnother essential technique is the usage of \nreflection and critique frameworks . These \nmethodologies, which include well-established prompting strategies such as ReAct , \nReflexion , Chain of Thought , and Graph of Thought , are designed to enhance the reasoning \ncapabilities of the agent. By  incorporating elements of evidence-based reasoning and \niterative self-critique, these frameworks enable the agent to refine its execution plans and improve the quality of its responses. Reflection  techniques enable the agent to evaluate the \nplausibility and coherence of its answers, which help  ensures that its outputs meet high \nstandards of reliab ility and relevance.\nClassification and the impl ementation of guardrails  are extra mechanisms that enhance the \nagent’s decision-making process. By classifyi ng questions and queries, the agent can filter \nsearch results, identify relevant sub-agents, or even deny a response if necessary.",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 238,
            "total_chunks": 313
          }
        },
        {
          "id": "cfcad261-a1ef-4f95-a0f7-10089cc18f3a",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "gent can filter \nsearch results, identify relevant sub-agents, or even deny a response if necessary. Guardrails, which serve as a specialized form of classification , act as safeguards to help ensure that the \nagent’s outputs adhere to predefined ethical and operational guidelines. These mechanisms are valuable in scenarios where precision, safety, and compliance are paramount.\nThe \ntools  that are employed by an agent are another critical aspect of its function. These tools \nare executable workflows that enable the agent to perform specific tasks. Analogous to specialized third-pa rty APIs, tools provide the agent with targeted capabilities that extend its \nproblem-solving abilities. Exam ples of tools include Retrie val-Augmented Generation (RAG) \npipelines for generating context-aware answers, code interpreters for handling complex programming tasks,  and APIs for retrieving information from the internet. Also, utility APIs \nsuch as weather services or instant messaging platforms ca",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 239,
            "total_chunks": 313
          }
        },
        {
          "id": "bf140cb6-ea7f-46dd-855a-9e85236c9861",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "on from the internet. Also, utility APIs \nsuch as weather services or instant messaging platforms can be integrated to address domain-specific needs. The versatility and ef fectiveness of an agent are enhanced by its \nability to leverage these specialized tools.\nGoing one step forward, agents that rely solely on text inputs face inher ent limitations in their \nability to interact with and ana lyze diverse data formats. \nMulti-modal agents  address this \nlimitation by incorporating the ability to process and reason ov er various input types, which \ninclude images, audio files, an d structured datasets. This capability expands the range of \napplications for agents, which enable them to ta ckle tasks that require visual analysis, speech \nprocessing, or combined reasoning across multiple modalities. For example, a multi-modal agent can analyze an image to extract relevant features, process an accompanying audio file for contextual information, and synthesize this  data with text-based inpu",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 240,
            "total_chunks": 313
          }
        },
        {
          "id": "31f033de-b009-4a6b-964e-478b8a0f54e3",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "n accompanying audio file for contextual information, and synthesize this  data with text-based inputs to deliver a \ncomprehensive solution.\nBy following the main core modules of an AI agent and through thoughtful design and \ncontinuous refinement, agents are poised to become indispensable tools in the ever-expanding landscape of AI.\nAn AI agent, as illustrated in Fi gure 6-2 on page 91, consists of a centralized planning system \nthat is supported by a general-purpose generative LLM. This LLM serves as the core orchestrator, which can devise a structured pl an of actions to address user queries. Its \ndecision-making process is enhanced by a memory module, which maintains contextual information and is continuously updated based on past actions and outcomes, which help ensure adaptive and dynamic responses.\n\nChapter 6. Artificial intelligence agents 91Figure 6-2   High-level view at the core of an AI agent\nThe planning system interacts with an I/O communication layer to run the planned a",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 241,
            "total_chunks": 313
          }
        },
        {
          "id": "24915139-89e5-4d36-a92d-fb234d9f052f",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "re of an AI agent\nThe planning system interacts with an I/O communication layer to run the planned actions \neffectively. This layer acts as a bridge between the LLM and an extensive Tool Library, which enables the agent to pe rform specialized tasks beyond its intrinsic capabilities. The tools \ninclude functions such as code execution, do cument retrieval (RAG), calculations, weather \nqueries, time and location services, and AI and gen AI models, among others. The module of communication is a crucial part of an agent because it sets the formatting of the I/O communication in a standardized way to enable fast and transparent invocation of tools and output comprehension.\nThe LLM at the core of this system is typically a large foundation model (FM) that is optimized \nfor high performance across diverse benchmar ks and tasks. For example, in the context of \nthe IBM watsonx.ai platform, this  role can be fulfilled by models  such as Mistral Large, Llama \n3.3 70B, or Llama 3.1 405B, which ar",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 242,
            "total_chunks": 313
          }
        },
        {
          "id": "9d33d280-1067-4b38-8b66-73fd67d48024",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " role can be fulfilled by models  such as Mistral Large, Llama \n3.3 70B, or Llama 3.1 405B, which are know n for their robust capab ilities in generative \nreasoning and planning.\nBehind the scenes, an agent has its own calle d system prompt, which is the usual system \nprompt that can be set for any generative LLM. It describes in detail the main task for which a generative LLM should adhere to. The scheme behind the main prompt solution is shown in Figure 6-3.\nFigure 6-3   Logical prompt schema definition for agents\n\n\n92 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiThis system prompt is a structured set of instructions that encapsulates the core task, and \ndefines the role of the LLM, the expected output schema, and the instructions that the model should follow.\nAs shown in Figure 6-3 on page 91, the system prompt integrates multiple critical elements:\n/SM590000Memory: Provides contextual data or historical  interactions that the LLM can leverage to \nmaintain",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 243,
            "total_chunks": 313
          }
        },
        {
          "id": "5ec9f1fa-a74e-4a0e-a76f-cbcdf665f2b4",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "0Memory: Provides contextual data or historical  interactions that the LLM can leverage to \nmaintain continuity and relevance in responses. This approach helps ensure that the agent adapts dynamically to ongoing tasks or user needs.\n/SM590000Role definition: Specifies the persona or role that the agent assumes (for example, a \ncustomer support assistant, a data scientist, or a creative writer), and aligns the behavior and tone of the LLM with the intended use case. \nExample 6-1 shows an example of a role prompt definition:\nExample 6-1   Defining a role for the agent\n#Role\nAs a helpful assistant, your role is to provide users with actionable insights from their data files.\n/SM590000Instructions: Detailed gu idelines about how the LLM should process inputs, interpret user \nqueries, and generate outputs. These instructions help ensure that the model stays on task and adheres to the wanted operational framework.\nExample 6-2 show an example of the instruction prompt definition.\nExample 6-2 ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 244,
            "total_chunks": 313
          }
        },
        {
          "id": "1e36d71e-2e8e-4991-9c0a-1e8d74e8a1f2",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "perational framework.\nExample 6-2 show an example of the instruction prompt definition.\nExample 6-2   Outline instructions for the agent\n# Instructions\nThe user can see only the Final Answer. All answers must be provided there. Functions must be used to retrieve factual or historical information to answer the message. If the user suggests using a function that is not available, answer that the function is not available.\n/SM590000Output schema: A predefined structure that dictates how the LLM should format its \nresponses, which may include JSON formats, bullet points, or other data representations \nthat are required by downstream tools or workflows. This prompt is important for the \noverall communication and interaction in the multiple steps in an agentic execution flow.\nExample 6-3 shows an example of the output schema prompt definition.\nExample 6-3   Defining the output schema that the agent should follow\n#Output schema (also known as the agent’s control logic)\nYou communicate only in",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 245,
            "total_chunks": 313
          }
        },
        {
          "id": "c77c68e7-a729-4f4e-8222-dd1a05d33b6c",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "agent should follow\n#Output schema (also known as the agent’s control logic)\nYou communicate only in instruction lines. The format is: \"Instruction: expected output\". Only use these instruction lines and must not enter empty lines or anything else between instruction lines. Skip the instruction lines Function Name, Function Input, Function Caption, and Function Output if no function calling is required. Message: User's message. You never use this instruction line. Thought: A single-line step-by-step plan of how to answer the user's message. You can use the available functions that are defined above. This instruction line must be immediately followed by Function Name if one of the available functions that are defined above needs to be called, or by Final Answer. Do not provide the answer here. Function Name: Name of the function. This instruction line must be immediately followed by Function Input. Function Input: Function parameters. An empty object is a valid parameter. \n\nChapter 6. A",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 246,
            "total_chunks": 313
          }
        },
        {
          "id": "e0e60cde-597e-4b5e-9463-d5b7b1db568e",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ion Input. Function Input: Function parameters. An empty object is a valid parameter. \n\nChapter 6. Artificial intelligence agents 93Function Output: Output of the function in JSON format. Thought: Continue your \nthinking process. Final Answer: Answer the user or ask for more information or clarification. It must always be preceded by Thought.\nAlso, the agent can enhance its functions with tools, which ex tend the LLM's capabilities by \nintegrating external APIs or specialized modules. Each tool includes the following items:\n/SM590000A tool description: An explanation of its purpose and function.\n/SM590000A tool schema: Specifications for how the LLM should interact with the tool, which includes \ninput and output formats.\nExample 6-4 shows the example parsed output after a certain user request regarding an \nexplanation of a certain dataset file occurs.\nExample 6-4   Agent output example\nThought: The user wants to know what the file \"CSV- bill of materials.csv\" is about. I can use the Py",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 247,
            "total_chunks": 313
          }
        },
        {
          "id": "23872272-a2a5-41bd-85f8-19b7edb3c5b7",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "hought: The user wants to know what the file \"CSV- bill of materials.csv\" is about. I can use the Python tool to read the file and provide a summary. Function Name: Python Function Input: {\"language\":\"python\",\"code\":\"import pandas as pd\\n\\ndf = pd.read_csv('CSV-billmaterials.csv')\\nprint(df.head()\",\"inputFiles\":{\"file_670d56abb912d7771371652e\":\"CSV- bill of materials.csv\"}} Function Output: The code ran successfully. Standard output: ‘’’QUANTITY ... PART 0 3700 ... 144EC8-14101-20 1 5500 ... 096EUF-T4101D20 2 45000 ... 004ZTF-41Z01M20 3 4334 ... NaN 4 1564 ... NaN \n[5 rows x 4 columns] ‘’’\nBy combining these components, the system prompt orchestrates the interaction between the \nLLM and external resources, which help ensure consistency, precision, and task alignment. \nThis modular approach enables AI agents to handle complex workflows and adapt to diverse applications.\nThis standard agent architecture can generate and run plans, and iteratively refine its \napproach by leveraging feedba",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 248,
            "total_chunks": 313
          }
        },
        {
          "id": "4e5aa952-61a5-4a89-a8d6-b430d1453b6a",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "t architecture can generate and run plans, and iteratively refine its \napproach by leveraging feedback and memory updates. The modular design of agents, which is supported by a versatile Tool  Library, enables sca lability and adaptabilit y for a wide range of \nuse cases, and with the usage of agents on watsonx.ai, the scaling to enterprise-grade solutions is once again possible.\n\n94 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai6.2  Why AI agents are needed\nThe necessity for AI agents stems from the growing complexity of tasks and environments \nthat surpass the capabilit ies of conventional software systems.  Traditional systems often falter \nin scenarios requiring adaptive, context-aware de cision-making, especially when dealing with \nvast datasets, uncertain outcomes, and real-time constraints. In domains like logistics, healthcare, and finance, where decision-making must balance multiple variables simultaneously, the utility of AI agents becomes evid ent. ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 249,
            "total_chunks": 313
          }
        },
        {
          "id": "82318df0-8501-4879-bf05-6c8564ecfa66",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "n-making must balance multiple variables simultaneously, the utility of AI agents becomes evid ent. These agents excel in scenarios \ndemanding real-time responses, such as aut onomous vehicles navigating dynamic traffic \nconditions or virtual assistants managing multifaceted user requests. \nBeyond efficiency, AI agents also bring about significant cost savings by automating repetitive \ntasks, reducing human error, and improving o perational accuracy. Thei r ability to adapt from \nexperience by using a memory module and to adapt to novel situations is crucial in addressing problems where predefined solutions are inadequate. For example, reinforcement learning (RL) techniques enable AI agents to optimize behavior over time, which refines their \ndecision-making capabilities with minimal hum an input. By acting as adaptive problem \nsolvers, AI agents provide a bridge between theoretical AI research and practical, real-world enterprise-grade applications.\nFigure 6-4 shows the gen AI journey",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 250,
            "total_chunks": 313
          }
        },
        {
          "id": "75578d1f-1778-406a-8f38-cdb8649c89c0",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "esearch and practical, real-world enterprise-grade applications.\nFigure 6-4 shows the gen AI journey for AI agents.\nFigure 6-4   Generative AI journey for AI agents\nAgents that are powered by LLMs represent the next frontier in driving productivity gains for \nenterprises. As businesses increasingly rely on AI to streamline operations and elevate \ncustomer experiences, agents offer a revolutionary step forward by automating complex, multi-step tasks that previously required human intervention. Unlike traditional LLMs, which excel at handling FAQs or supporting nuanced informational queries through RAG approaches, agents bring advanced capabilities to orches trate and run high-value workflows. \nThis ability to handle complex sc enarios such as planning a marketing campaign, optimizing \nsupply chain logistics, or conducting sophisticated data analysis, positions agents as essential tools for transforming enterprise productivity.\n\n\nChapter 6. Artificial intelligence agents 95The true poten",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 251,
            "total_chunks": 313
          }
        },
        {
          "id": "e8dac5fe-e32a-4a20-965f-e4fe49410e04",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "r transforming enterprise productivity.\n\n\nChapter 6. Artificial intelligence agents 95The true potential of agents lies in their ca pacity to integrate seam lessly with existing \nsystems, tools, and data sources, which enable th em to act as intelligent  intermediaries that \nconnect disparate workflows. For enterprises, this approach means automating entire business processes rather than isolated tasks. For example, an agent can retrieve customer data, analyze purchasing patterns, generate  personalized recommendations, and trigger \nactions like sending tailored offers or updating customer relationship management systems. By eliminating manual handoffs and streamlining processes, agents enable employees to focus on strategic decision-making rather than repetitive or time-consuming tasks.\nFrom a business perspective, agents are the key to unlocking the next wave of productivity \ngains. They enhance operational efficiency, reduce costs, and drive faster time-to-market for critical initia",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 252,
            "total_chunks": 313
          }
        },
        {
          "id": "6e3184f3-f078-4f56-8b3b-c316fe168d20",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ey enhance operational efficiency, reduce costs, and drive faster time-to-market for critical initiatives. Moreover, they enable businesses to scale their efforts without proportionally increasing resources, which are vital in today’s competitive and resource-constrained environments. Imagine an agent that can plan, run, and monitor an entire ad campaign or a product launch task in hours, which typically require weeks of human effort. This scalability im proves efficiency, and it  creates opport unities for innovation because \nteams can redirect their focus to higher-value, creative, and strategic activities. In this context, agents are a technological enhancement and a strategic imperative for the modern enterprise. They represent a shift from reactive to proactive operations, which enable businesses to anticipate needs, respond faster to  market dynamics, and drive growth in ways \npreviously unimaginable. Enterp rises that adopt agent s will lead the charge in this new era of \nproduc",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 253,
            "total_chunks": 313
          }
        },
        {
          "id": "078a63f9-a453-4a56-8074-b7c7c2b10a00",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "iously unimaginable. Enterp rises that adopt agent s will lead the charge in this new era of \nproductivity, and set the benchmark for operational excellence and innovation in their industries.\n6.3  Multiple AI agents\nWhen you extend the concept of individual agents in software systems, you get multi-agent systems (MASs), which consist of multiple AI en tities working collaboratively or competitively \nto solve complex problems within shared digital environments. These systems are transformative when augmented by gen AI capabilities because th ey enable agents to \nengage in more sophisticated tasks, such as content generation, dynamic interaction with users, or collaborative reasoning. In a software-based MAS, each agent operates autonomously while adhering to predefined pr otocols for communication and collaboration. \nThis autonomy enables agents to handle tasks like distributed resource management, \nadaptive problem-solving, and even creative endea vors, such as generating ideas, plans",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 254,
            "total_chunks": 313
          }
        },
        {
          "id": "39fc5bf3-0c2e-49ff-9086-21e0fc3f1f44",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "management, \nadaptive problem-solving, and even creative endea vors, such as generating ideas, plans, or \npersonalized user experiences. Gen AI further amplifies their function by enabling natural language generation, image synthesis, and decision-making support, which enhances the agents' ability to interpre t, reason, and produce outputs in real time. \nA key challenge in software-based MASs, particular ly ones that leverage gen AI, is achieving \neffective inter-agent communication and collaboration. Protocols that are based on message-passing, such as JSON over RESTful APIs or advanced graph-based communication models, help ensure that agents can share knowledge, negotiate responsibilities, and resolve conf licts. Generative AI enhances these interactions by enabling \ncontext-aware dialog and summa rization capabilities, which make inter-ag ent exchanges \nmore natural and efficient. More over, coordination strategies within such systems are pivotal. \nIn leader-followe r setups, gener",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 255,
            "total_chunks": 313
          }
        },
        {
          "id": "188bffa4-19b1-4094-aba2-48c72b1e53ad",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "re over, coordination strategies within such systems are pivotal. \nIn leader-followe r setups, generative agent s with advanced reasonin g capabilities may take \non supervisory roles to craft high-level plans or  synthesize insights for the collective. Fully \ndistributed MASs enable agents to collaborate dynamically by relying on peer-to-peer negotiations or RL-based decision policies to achieve shared objectives. For example, in a \ncollaborative creative task such as automated marketing content generation, different agents in the system might specialize in headline creat ion, visual asset gener ation, and audience \nsentiment analysis. Together, these agents leverage gen AI to deliver cohesive, high-quality outputs that surpass the capab ilities of indivi dual components.\n\n96 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiThe integration of a MAS with gen AI promises to unlock solutions for some of the most \nintricate software challenges, such as distri buted k",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 256,
            "total_chunks": 313
          }
        },
        {
          "id": "a4ab5dcf-09b9-4c80-ba6a-de71270e5f2a",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ises to unlock solutions for some of the most \nintricate software challenges, such as distri buted knowledge synthesis, large-scale content \npersonalization, and dynamic adaptation to user behaviors. As these systems continue to evolve, they are poised to redefine how software systems interact, collaborate, and solve problems, which pave the way for a new era of  intelligent, cooperativ e, and gen AI-driven \napplications.\nThe decentralized nature of a MAS is critical in these addressed applications, which help \nscalability, robust ness, and fault tolerance. For example, a generative MAS that is deployed in \ncustomer support can assign tasks dynamically across agents, with some agents generating empathetic responses, other agents crafting visually appealing solutions, and yet other \nagents analyzing sentiment data in real time. This division of labor enables the system to manage workloads efficiently, respond quickly to changes, and help ensure seamless service continuity even if indivi",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 257,
            "total_chunks": 313
          }
        },
        {
          "id": "4da4d222-0c07-4966-8810-2d7ce2104144",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " efficiently, respond quickly to changes, and help ensure seamless service continuity even if individual agents face disruptions. \nBeing decentralized is one of the key advantag es of a MAS. Unlike centralized systems where \na single entity controls decision-making and system-wide operations, MASs are designed to \noperate without a single point of failure. If one agent fails or is compromised, the rest of the system can continue to function, which makes it highly resilie nt. This decent ralized approach \nalso allows MASs to scale efficiently because more agents can be added to the system without disrupting its overall performance. Furthermore, because the agents are distributed, they can operate in diverse environments, which include real-time scenarios, where traditional centralized systems might strugg le. The power of a MAS lies in  this collective intelligence, \nwhere the sum of  the parts is greater than th e individual agents' abilities.\nAs MASs continue to evolve, they are poise",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 258,
            "total_chunks": 313
          }
        },
        {
          "id": "910e8497-b287-4930-8ce9-b36ecff95eb0",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " parts is greater than th e individual agents' abilities.\nAs MASs continue to evolve, they are poised to unlock solutions for some of the most intricate \nchallenges in fields such as distributed computing, environmental monitoring, and autonomous exploration. For example, in distributed computing, a MAS can optimize resource usage across a network of machines , which enable more efficient computation and \ndata processing. In environmental monitoring,  you can use a MAS to deploy a network of \nsensors that autonomously gather and process data, which provides real-time insights into environmental conditions. In autonomous exploration, a MAS can enable a fleet of robotic vehicles or drones to work together to explor e unknown terrains, share information, and adapt \nto changes in the environment. \nA notable development in the realm of MASs is the advent of multi-agent orchestration \nframeworks. These frameworks provide a unified platform for coordinating conversations among multiple agents",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 259,
            "total_chunks": 313
          }
        },
        {
          "id": "f26f538f-cc57-4d89-bb6c-70bdadc4bef2",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ks. These frameworks provide a unified platform for coordinating conversations among multiple agents, serving as a high-level abstraction for using FMs. By integrating LLMs, tools, and human inputs, various framew orks enable the seamless orchestration of \nagent interactions in a way that enhances the capabilities of individual agents. These \nframeworks typically feature highly capable, customizable, and conversational agents, which \ncan collaborate through automa ted agent chats, which improv e their collective ability to \nhandle complex tasks. This integration enables MASs to function more efficiently by leveraging the power of language models, tools, and human expertise in a coordinated manner. As a result, MAS framew orks offer an exciting potenti al for creating more intelligent \nand adaptive systems that can address a wide range of challenges in both industrial and research contexts.\nThe ongoing evolution of MASs promises to push the boundaries of what is possible in \nautonomous ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 260,
            "total_chunks": 313
          }
        },
        {
          "id": "806266e5-0817-469d-a754-684baf4155d9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "s.\nThe ongoing evolution of MASs promises to push the boundaries of what is possible in \nautonomous systems by enabling new capabilit ies and driving innovation across various \nindustries. As researchers continue to explore the potential of these systems, new frameworks, coordination strategies, and interaction protocols continue to emerge, further enhancing the power and flexibility of MASs in tackling the most co mplex and large-scale \nproblems.\n\nChapter 6. Artificial intelligence agents 97The architectural diagram in Figure 6-5 outlines a sophisticated MAS framework that is \ndesigned to integrate diverse functions such as planning, memory management, \ncommunication, and task orchestr ation. The system employs a mo dular design that facilitates \nscalability, adaptability, and in teroperability, which caters to complex problem-solving \nscenarios across various domains. Let us delve into each component and their interplay within the architecture.\nFigure 6-5   High-level view of a multi",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 261,
            "total_chunks": 313
          }
        },
        {
          "id": "f5b009c2-4192-470a-893e-28703651a4c2",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " each component and their interplay within the architecture.\nFigure 6-5   High-level view of a multi-agent syst em architecture\nA MAS that is enhanced by gen AI represents a sophisticated and transformative paradigm in \nAI, where multiple autonomous agents collaborate or interact within a shared environment to achieve complex goals. These systems are built to harness the synergy of diverse agent capabilities by leveraging the powe rful reasoning, creativity, and  contextual un derstanding of \nLLMs to tackle problems beyond the scope of any single agent. Figure 6-5 lays out a comprehensive blueprint for such a system. Now, we will see a detailed, expert-level \nexplanation of its various interconnected components and their interplay.\nAt the heart of this MAS architecture lies the concept of the agent, which operates as an \nautonomous unit that is equipped with distinct roles, behaviors, and tools. Each agent is imbued with a profile or persona that defines its domain expertise, operation",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 262,
            "total_chunks": 313
          }
        },
        {
          "id": "23bdee04-7f0a-4ce9-8001-4e56284a17eb",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "d tools. Each agent is imbued with a profile or persona that defines its domain expertise, operational boundaries, and response style. For example, an agent might  function as a scientific researcher that is \nskilled in computational chemistry or as a cust omer service representative with expertise in \nnatural language understanding and resolution st rategies. This persona is not static because \nit evolves in response to feedback and enviro nmental changes, which help ensure that the \nagent remains relevant and effective in dynamic settings.\nCentral to an agent’s operation is its belief state, which is an internal model that encapsulates \nits understanding of the world, its knowledge of tasks at hand, and its memory of past interactions. This belief state is dynamic and constantly updated through observations, actions, and communications with other agents or external systems. The belief state also integrates inputs from memory. Memory is bifurcated into long-term and short-term storage",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 263,
            "total_chunks": 313
          }
        },
        {
          "id": "7d9e8271-15c5-4954-88cd-ee75b1b85875",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "state also integrates inputs from memory. Memory is bifurcated into long-term and short-term storage, each serving distinct roles. Short-term memory  retains ephemeral inform ation that is crucial \nfor immediate task execution and contextual reasoning. For example, an agent might use short-term memory to temporarily store user input or intermediate results of calculations. In contrast, long-term memory preserves knowledge that is accumulated over time, such as procedural expertise, domain-specific facts, or  historical interactions. The integration of \nmemory with the agent’s belief state helps ensure that decisions are informed by both the immediate context and historical knowledge. This dual-layered memory structure enhances the system’s ability to handle co mplex, multi-step tasks that r equire contextual continuity and \nlong-term planning.\n\n\n98 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiDriving the agent’s reasonin g and interaction is its policy , a fr",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 264,
            "total_chunks": 313
          }
        },
        {
          "id": "a8d51847-dc05-4031-8751-91235c60576a",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "e Power of AI with IBM watsonx.aiDriving the agent’s reasonin g and interaction is its policy , a framework of decision-making \nrules and algorithms that govern how the agent interprets inputs, prioritizes tasks, and generates outputs. Policies can be simple, predefined heuristics or sophisticated, dynamically updated models that are informed by machine learning (ML) techniques, such as RL. These policies are operationalized through the agent’s LLM core, which is the generative engine that processes natural language inputs, synthesizes insights, generates context-aware responses, and even creates novel solutions to problems. The LLM also serves as the agent’s interface to interpret ambiguous or unstructured information, which effectively bridges the gap between human language and computational logic.\nAgents in this system are equipped with advanc ed communication c apabilities that are \ndefined through a structured I/O schema that enables them to interact with other agents, external to",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 265,
            "total_chunks": 313
          }
        },
        {
          "id": "bf1653d9-4f83-47f3-a00f-6b57efb3aaf8",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "defined through a structured I/O schema that enables them to interact with other agents, external tools, and the environment. Communication is about exchanging messages, negotiating shared understandings, aligning goals, and ensuring coordinated action. To accomplish these tasks, agents can rely on tools that are integrated within the system. These tools extend the agent’s functio nal repertoire and in clude utilities like API access for external \ndata retrieval, calculators for mathematical computations, code interpreters for debugging and running programs, and multimodal models for handling complex data types, such as images or video. \nFor domain-specific applications, tools like chemistry simulators or Robotic Process \nAutomation (RPA) frameworks can be deployed, which enable agents to specialize in tasks that range from molecular modeling to workflow optimization. These tools are seamlessly integrated into the agent’s workflow, which enables it to run specialized tasks without requ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 266,
            "total_chunks": 313
          }
        },
        {
          "id": "89210382-172e-430b-b642-2ee1480a4524",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "mlessly integrated into the agent’s workflow, which enables it to run specialized tasks without requiring extra human intervention. \nThere is no defined limit or minimum requirement regarding the number of tools that are \navailable to a set of agents: it is something dependent on the specific use cases that are addressed. What is proposed here is just a pot ential set of tools that might be useful for \nvarious potential use cases in a specific set of industries. In a MAS, a set of tools might contain hundreds of tools, depending on what is the goal of the system.\nAt a higher level, the MAS orchestrates the activi ties of these agents to help ensure that they \nwork collaboratively toward shar ed objectives. This orchestration is managed by several \ncritical components. \nThe first is the goal and task decomposition me chanism, which takes high-level goals and \nbreaks them into smaller, ma nageable tasks and subtasks. Fo r example, if the system’s goal \nis to generate a comprehensive mark",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 267,
            "total_chunks": 313
          }
        },
        {
          "id": "0e71e7b7-3321-455f-baaa-c38072f68bd4",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "nageable tasks and subtasks. Fo r example, if the system’s goal \nis to generate a comprehensive market analys is, this task might be decomposed into tasks \nlike gathering data, analyzing trends, and generating a summary report, with each task that is further divided into specific steps like querying databases or visualizing data. \nTo enable task execution, the system relies on a planner that assigns tasks to the most \nsuitable agents based on th eir profiles, curr ent workloads, and skillsets. The \nplanner  plays a \npivotal role in the MAS by orchestrating the decomposition of high-level goals into granular tasks and subtasks. The planning  process leverages the agent’s  policy and LLM to identify \nthe optimal sequence of actions that are required to achieve the outcome. The planner also monitors progress, adapting task sequences or agent roles as needed to accommodate changing conditions or unexpected challenges. For example, if one agent encounters a bottleneck in data retrieval, the",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 268,
            "total_chunks": 313
          }
        },
        {
          "id": "8d6e66d3-e294-491a-a068-91c87aa3cca2",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "s or unexpected challenges. For example, if one agent encounters a bottleneck in data retrieval, the planner can reassign related tasks to another agent with overlapping capabilities. \n\nChapter 6. Artificial intelligence agents 99For critical use cases, th e architecture in corporates the ability to receive human feedback  \nduring the execution of the plan to help ensure that automation in high-stakes scenarios remains governable and aligned with human oversight. By enabling human intervention, the system can address unforeseen complexities, validate decisions, and maintain control over critical operations.\nTask decomposition ensures that even complex ob jectives are approached methodically, with \nsubtasks delegated to the appropriate agents or tools.\nCollaboration within the MAS is further enhanced through sophisticated communication \npatterns, which define how agents interact and share information. The architecture supports multiple \ncommunication patterns :\n/SM590000Layered communic",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 269,
            "total_chunks": 313
          }
        },
        {
          "id": "6fc71b80-1f89-484d-88e1-1e668f794efd",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " information. The architecture supports multiple \ncommunication patterns :\n/SM590000Layered communication: Hierarchical interactions where agents operate at different levels \nof abstraction by passing information up or down the chain.\n/SM590000Decentralized communication: Peer-to-peer exchanges among agents to help ensure \nflexibility and redu ce bottlenecks.\n/SM590000Centralized communication: A hub-and-spoke model where a central coordinator \nmanages all interactions.\n/SM590000Shared message pool: A collaborative mechanism where agents exchange messages \nthrough a shared repository.\nHybrid patterns, such as shared message pools or global workspaces, enable agents to post \nintermediate results or disco veries to a common repository , which enables asynchronous \ncollaboration and emergent problem-solving. The blackboard pattern  or global workspace \nserves as a central knowledge-sharing hub within the MAS. Agents use this shared repository to post updates, intermediate calculations, or",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 270,
            "total_chunks": 313
          }
        },
        {
          "id": "8da58440-68a1-4044-a05f-64edf0a3a197",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "hub within the MAS. Agents use this shared repository to post updates, intermediate calculations, or unresolved queries, which create a collaborative environment where other agents ca n contribute insights or take over pending \ntasks. For example, an agent working on a data analysis task might upload partial results to the blackboard, which another agent can use to generate visualizations or summaries. \nThe MAS is supported by  an agents and skills repository, wh ich is a centralized directory that \ncatalogs the capabilities, expert ise, and tools that are associ ated with each agent. This \nrepository enables dynamic discovery and allocati on of agents for specific tasks, which helps \nensure that the system can scale and adapt to  diverse challenges. It also facilitates the \nincorporation of new skills or agents so th at the system can evolve as new tools or \nrequirements emerge.\nThe architecture helps ensure that the output that is generated by the system is coherent, \ncontextually re",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 271,
            "total_chunks": 313
          }
        },
        {
          "id": "9115e6d8-adf0-4622-8490-74a3454b8101",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "hitecture helps ensure that the output that is generated by the system is coherent, \ncontextually relevant, and correctly formatted. The \nI/O schema  governs the structure of inputs \nand outputs by standardizing interactions across agents, tools, and external systems. This schema helps ensure compatibilit y and consistency regardless of the task or domain. The \nsystem’s communication module also plays a role in tailoring outputs to the intended audience. For example, technical results might be presented in a concise, data-rich format for domain experts, but layperson-oriented outputs would emphasize clarity and simplicity. \n\n100 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiDespite its sophisticated design, the MAS must handle potential errors that can arise during \nexecution:\n/SM590000Failed API calls to tools: A tool might be te mporarily unavailable or malfunction, which can \nlead to incomplete or erroneous task execution. Human feedback can help decide whe",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 272,
            "total_chunks": 313
          }
        },
        {
          "id": "87efd326-afb7-4986-82c1-2f7b3dc891dd",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ction, which can \nlead to incomplete or erroneous task execution. Human feedback can help decide whether to retry the tool, use an alternative tool, or modify the task parameters.\n/SM590000Infinite loops: Erroneous task decomposition or planning might result in a loop where the \nsystem repeatedly runs the same actions without progress. Human intervention can identify and correct the root cause to prevent resource wastage.\n/SM590000Rogue paths or wrong tool selection or input: The system might choose an inappropriate \ntool or misinterpret input data, which can lead to incorrect outputs. Human feedback can help realign the system’s actions to help ensure that the task remains on track.\nBy integrating human feedback into the loop, particularly for infinite loops or rogue paths, the \nMAS can maintain a high degree  of reliability and robustness by implementin g a fail-safe \ntechnique on human-in-the-loop interaction after deviant agents’ behavior is detected by the overall orchestration sy",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 273,
            "total_chunks": 313
          }
        },
        {
          "id": "48479713-9ba7-4e11-8560-60e1e1eeb313",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "n-in-the-loop interaction after deviant agents’ behavior is detected by the overall orchestration system. This hybrid approach of automation with human oversight is especially critical in high-stakes domains wh ere errors can have significant consequences.\nThe MAS operates within an environment, such as the external world or a digital context \nwhere tasks are performed and results are applied. Agents interact with the environment by observing its state, acting to modify it, and processing feedback to refine their belief states and policies. The environment is also where the whole MAS runs, and it is possible to have environments that are composed of multiple locations, such as cloud environments, virtual machines (VMs) and containers, or further proprietary software.\n6.4  AI agents on watsonx.ai \nIBM watsonx.ai enhances the development of an enterprise-grade agentic technology stack (Figure 6-6) by providing powerful  tools, models, and middleware  capabilities th at are tailored \nfor ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 274,
            "total_chunks": 313
          }
        },
        {
          "id": "eec0f73b-0d6f-469a-8afb-ddf2900cfd3b",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ure 6-6) by providing powerful  tools, models, and middleware  capabilities th at are tailored \nfor scalable, intelligent, and  adaptive operations. \nFigure 6-6   Enterprise agentic tech stack\n\n\nChapter 6. Artificial intelligence agents 101At the foundation level, watsonx.ai delivers robust LLMs optimized for specific enterprise use \ncases to enable agents to interpret and act on complex queries with high accuracy and relevance. These LLMs seamlessly integrate in to the Agentic Framework, which serves as \nthe operational backbone for orchestrating tasks, planning workflows, and retaining memory for context-driven decision-making. \nIBM watsonx.ai also supports the Agentic Service Deployment & Operation Platform, which \noffers a streamlined way to de ploy and manage agentic services while  ensuring reliability, \nscalability, and security at an enterprise level. To ensure en terprise-grade observability and \nmonitoring, watsonx.ai incorporat es observability mechanisms that  provide real-",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 275,
            "total_chunks": 313
          }
        },
        {
          "id": "a2084fd6-6c89-4a13-87bd-b32f4f175815",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "observability and \nmonitoring, watsonx.ai incorporat es observability mechanisms that  provide real-time insights \ninto agent performance, operational health , and user interactions. These mechanisms \nfacilitate continuous optimizati on and help ensure t hat agentic services  align with business \ngoals. By bridging models, middleware, and applications, watsonx.ai creates a cohesive and modular enterprise tech stack that can address the evolving demands of modern businesses with precision, adaptab ility, and innovation.\nwatsonx.ai agents are a transformative innovation in the domain of AI. These agents are \ndesigned to provid e businesses with unparalle led capabilities in automa ting tasks, processes, \nand decision-making. Through a blend of inte rfaces, cutting-edge technologies, and robust \nintegration options, watsonx.ai enables the dev elopment, deployment, and optimization of \nintelligent agents that cater to a diverse range of enterprise needs.\nAt the core of watsonx.ai's agent i",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 276,
            "total_chunks": 313
          }
        },
        {
          "id": "a929b4e5-ba9a-46dc-a8f4-a80ca8a5add5",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ligent agents that cater to a diverse range of enterprise needs.\nAt the core of watsonx.ai's agent ic capabilities is the Agent Build er (Figure 6-7), which is an \nintuitive and powerful tool that accelerates the development lifecycle. The visual interface of the Agent Builder enables developers to construct agents with ease, which reduces the complexity that is typically associated with designing and managing such systems. Agents \nwithin watsonx.ai are defined through natural language instructions, and they can be equipped with various tools to expand their fu nctions. These tools act as modular building \nblocks to enable developers to create sophisticated workflows that are tailored to specific requirements.\nFigure 6-7   Agent Builder view on the watsonx.ai UI\n\n\n102 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiOne of the standout features of the Agent Builder is its seamless integration with multiple \nagent frameworks. In addition to IBM proprietary technol",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 277,
            "total_chunks": 313
          }
        },
        {
          "id": "864fc4bf-901c-49a2-8b3c-24b33121827d",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " is its seamless integration with multiple \nagent frameworks. In addition to IBM proprietary technologies, developers can also leverage popular open-source fr ameworks like LangChain and LangGr aph. This flexib ility helps ensure \nthat businesses can use the best tools and methodologies that are available in the ecosystem, and adapt them to their unique operational need s. The ability to integrate \nopen-source solutions with IBM advanced technologies provides a level of customization and extensibility that is critical for modern  enterprises, as shown in Figure 6-8.\nFigure 6-8   Overview of customiz ations on watsonx.ai Agent Builder\nTesting and debugging are essential components of the agent development process, and \nwatsonx.ai Agent Builder excels in this area. Real-time te sting capabilities enable developers \nto identify and resolve issues as they arise,  which minimizes downtime  and iteration cycles. \nThis feature is complemented by the “one-clic k” deployment mechanism,  which s",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 278,
            "total_chunks": 313
          }
        },
        {
          "id": "47e18b8b-47fc-4007-ad0e-d663c3c8bb55",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "d iteration cycles. \nThis feature is complemented by the “one-clic k” deployment mechanism,  which simplifies the \nprocess of making agents operational. Once developed, agents can be deployed as watsonx.ai AI services, which effectively turn them into API endpoints that can be accessed by various applications and systems. This streamlined workflow reduces time-to-market and helps ensure that agents can be quickly integrated into enterprise operations.\nTo further enhance the functions of agents, wats onx.ai offers an extensive Tool Library of \nenterprise-ready tools that are designed to a ugment the capabilities of agents. The tools are \ndivided into the following categories:\n/SM590000Web Search\n/SM590000Document Search (RAG)\n/SM590000Code Execution\n/SM590000Data Connectors\n/SM590000Custom Tool Builder\nFor example, the Web Search tool empowers age nts to perform real-time internet searches, \nwhich provide them with up-to-date information to enhance their decision-making and responses. T",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 279,
            "total_chunks": 313
          }
        },
        {
          "id": "46a33ab8-24e8-4699-a441-a17156858491",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "s, \nwhich provide them with up-to-date information to enhance their decision-making and responses. The ability to access  fast and relevant search results helps ensure that agents \nremain informed and capable of handling dynamic queries.\n\n\nChapter 6. Artificial intelligence agents 103Another critical component of the Tool Library is the Document Search function, which uses \nRAG. This tool enables agents to efficiently index and retrieve documents from an organization’s knowledge base, which helps ensure that they can deliver accurate and context-aware responses. By leveraging RAG, agents can access vast amounts of information and distill it into  actionable insights, which makes them invaluable for \nknowledge-intensive tasks. \nThe Tool Library also includes a Code Execution feature, which enables agents to run Python \ncode in real time. This capability opens up a wide range of possibilit ies, from performing \ncomplex calculations to automati ng repetitive tasks. By integrat ing this fe",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 280,
            "total_chunks": 313
          }
        },
        {
          "id": "a34f2fa3-e7f6-456a-b225-84fe9218d28f",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " ies, from performing \ncomplex calculations to automati ng repetitive tasks. By integrat ing this feature, agents can \noperate as dynamic problem solvers that can adapt to various scenarios. \nData accessibility is another co rnerstone of the watsonx.ai a gentic framework. With Data \nConnectors, agents can seamlessly interact with enterprise databases and data warehouses, which grant them access to critical organizatio nal data. This tool helps ensure that agents \noperate with a comprehensive understanding of the business context, which enables more informed and effective decision-making. \nThe Tool Library supports the creation of custom tools so that organizations can extend the \nfunctions of their agents by integrating them with external services and unique enterprise systems. This level of customizat ion helps ensure that agents can meet the specific demands \nof any organization.\nDeployment is a critical phase in the lifecycle of any AI system, and watsonx.ai provides a \nrobust, fram",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 281,
            "total_chunks": 313
          }
        },
        {
          "id": "df80b85e-384e-4b79-bc94-009d7e058bb7",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "yment is a critical phase in the lifecycle of any AI system, and watsonx.ai provides a \nrobust, framework-neutral solution for deploying agents. The deployment process is scalable, secure, and highly available, which helps ensure that agents can meet the demands of enterprise-scale operations. Whether an organiza tion requires a single agent for a specific \ntask or a fleet of agents to ha ndle complex workflows, the wa tsonx.ai deployment capabilities \ncan handle the challenge. Once deployed, th e performance and reliabilit y of agents must be \nmonitored to ensure that they operate as intended. watsonx.ai includes comprehensive \nmonitoring tools that track key performance in dicators (KPIs) and analyze logs. These tools \nprovide valuable insights into agent behavior, wh ich enable developers and administrators to \nidentify areas for improvement and help ensure that agents deliver optimal results. \nwatsonx.ai emphasizes transparency  and explainability, which are crucial for building tr",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 282,
            "total_chunks": 313
          }
        },
        {
          "id": "7a51e8f0-bc16-4a4d-b794-c7524bd1eeb0",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " results. \nwatsonx.ai emphasizes transparency  and explainability, which are crucial for building trust in \nAI systems. By offering detailed explanations of agent decisions and actions, the platform \nhelps organizations maintain compliance with regulatory requirements and ethical standards.\nLooking to the future, watsonx.ai is poised to introduce the Flows Engine, which is a \nlightweight agentic framework and tool-building pl atform that will fu rther enhance the \ncapabilities of the platform. The Flows Engine will enable developers to rapidly build custom \ntools and integrate them with en terprise IT systems, which will provide unparalleled flexibility. \nThis framework is designed to facilitate reason ing and complex decision-making, which will \nmake agents more effect ive in handling intricat e tasks. Also, the Flows Engine will include a \nchat UI widget that can be easily inte grated into thir d-party applications, which will enable \nseamless AI-mediated interactions.\n\n104 Simplify ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 283,
            "total_chunks": 313
          }
        },
        {
          "id": "540436c2-24aa-4a8a-9b53-504f76bd6cb6",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "into thir d-party applications, which will enable \nseamless AI-mediated interactions.\n\n104 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiComplementing watsonx.ai is IBM watsonx Orchestrate, which is a platform that focuses on \nstarting AI assistants and agents for business processes and task automation (Figure 6-9). By combining the capab ilities of watsonx.ai and watsonx Orchestrate, organizations can \nachieve end-to-end automation to streamline operations and drive efficiency across their \nworkflows. This synergy highlights IBM’s commitment to providing comprehensive AI solutions that address the divers e needs of modern enterprises.\nFigure 6-9   Overview of watsonx  Orchestrate for Agents capabilities\nThe integration of watsonx Orchestrate and watsonx.ai offers an advanced, enterprise-grade \nframework that enhances agentic support by uniting robust autom ation, intelligent workflows, \nand conversational AI capabilitie s. watsonx Orchestrate function s as a ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 284,
            "total_chunks": 313
          }
        },
        {
          "id": "44311bf4-dc6f-465f-929a-58f3b2f9d053",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "n, intelligent workflows, \nand conversational AI capabilitie s. watsonx Orchestrate function s as a supervisory agent that \nleverages LLMs to coordinate interactions ac ross customers, employees, subject matter \nexperts (SMEs), and applications. By employin g Skills Studio, watsonx Orchestrate enables \nthe discovery, creation, and management of gen AI and digital automations by combining tasks, workflows, and skills to drive seamless operational efficiency. Skills and automations \ncan be trained and published in to a comprehensive skills catalo g, which includes prebuilt \nintegrations with enterprise so lutions such as SAP, Salesfor ce, and ServiceNow, to help \nensure compatibility wit h diverse enterprise systems. Si multaneously, watsonx.ai powers \nintelligent AI assistants, enabling conversational experiences that feature advanced functions like slot filling, LLM rout ing, disambiguation, and digression s. This combinat ion helps ensure \npersonalized and context-aware interactions. ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 285,
            "total_chunks": 313
          }
        },
        {
          "id": "cad4391b-73fb-4766-9049-2afebc46d257",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ion, and digression s. This combinat ion helps ensure \npersonalized and context-aware interactions. Together, watsonx Orchestrate and watsonx.ai establish a scalable ecosystem to enable enterprises to simplify complex processes, reduce operational bottlenecks, and deliver intuitive, guided experiences that align AI-driven solutions with business objectives in a dyna mic, flexible, and secure manner.\nBecause of these agentic frameworks on watsonx.ai that are combined with watsonx \nOrchestrate, you can use advanced assistants that can perform many types of tasks.\nFigure 6-10 on page 105 provides a comprehens ive depiction of Assistants with Agents on \nwatsonx, which shows the integration of wats onx.ai and watsonx Orchestrate to enable \nsophisticated AI-driven solutions for enterprise workflows.\n\n\nChapter 6. Artificial intelligence agents 105Figure 6-10   Assistant with Agents that use wats onx.ai and watsonx Orchestr ate for agentic use cases\nAt the core of this design is the concept of",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 286,
            "total_chunks": 313
          }
        },
        {
          "id": "0ad19d97-54ba-409e-907d-72a15e12018c",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "s onx.ai and watsonx Orchestr ate for agentic use cases\nAt the core of this design is the concept of a unified, intelligent a ssistant framework that \ncombines modularity, scalabilit y, and precision to deliver seamless experiences across \ndiverse use cases and tasks. Each component in Figure 6-10 plays a critical role in orchestrating complex interactions between users, agents, tools, and workflows, which create a robust and adaptable ecosystem for AI-powered operations. \nAt the top of the architecture is the Unified As sistant, which serves as the single, user-facing \ninterface. This layer is designed to provide a consistent and coherent interaction experience \nby simplifying user engagement and abstracting the complexities of the underlying system. The Unified Assistant is supported by the Supervisory AI Meta-Agent, which is a central coordinator that facilitates all workflows and he lps ensure that user requests ar e processed \naccurately and efficiently. The meta-agent acts as the",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 287,
            "total_chunks": 313
          }
        },
        {
          "id": "5120bcb5-1158-40d4-8cab-30bb4e9a6228",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "lps ensure that user requests ar e processed \naccurately and efficiently. The meta-agent acts as the brain of the system by orchestrating the activities of multiple subordi nate agents and tools to  fulfill user intents.  Its responsibilities \ninclude routing tasks to the appropriate agents, resolving ambiguities in user input, starting the necessary tools, reasoning through multi-step problems, and planning actions to achieve outcomes. These capabilities are made possible by the integratio n of LLMs that  are powered \nby watsonx.ai, which provides the advanced natural language understanding, contextual awareness, and reasoning ab ilities that are needed for high-quality interactions.\n\n\n106 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiBeneath the Supervisory AI Meta-Agent lies a network of specialized assistants that are \nlabeled as Assistant 1, Assistant 2, and so on, which represents a modular and scalable approach to task execution. Each assistant is tailo",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 288,
            "total_chunks": 313
          }
        },
        {
          "id": "cc8aabbb-d449-4e55-bf83-22d26bab6462",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "d so on, which represents a modular and scalable approach to task execution. Each assistant is tailored to handle specific domains or functions, \nand they operate autonomously while contributing to the overall system. These assistants are designed to run actions independently, and they leverage the power of LLMs for NLP, multi-turn conversations, and decision-making.  This autonomy enables them to reduce \nmanual intervention so that organizations can achieve higher levels of efficiency and \nproductivity. The assistants also facilitate user  interactions by maintaining context, \nunderstanding intent, and responding dy namically to evolving requirements. \nA defining feature of this architecture is its reliance on tool calling, which forms the backbone \nof the system’s operational fl exibility and extensibility. Figu re 6-10 on page 105 emphasizes \nthe integration of various categories of tools that the agents can call to complete tasks. These tools include fixed flows for handling repeti",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 289,
            "total_chunks": 313
          }
        },
        {
          "id": "238ecfa5-7985-4aa6-94c6-2375cea919c9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ools that the agents can call to complete tasks. These tools include fixed flows for handling repetitive and standardized operations; knowledge repositories for answering questions and providing insights; APIs for interacting with external systems; business automation modules for streamlining enterprise processes; and multi-agent frameworks for coordinating complex tasks that require collaboration among several AI agents. The usage of LLM-based tool calling helps ensure that the system can adapt to various workflows, which enables interoperability with existing IT infrastructures and third-party applicat ions. watsonx.ai enhances th is capability through its extensive Tool Library, \nwhich includes features such as RAG for knowledge discovery; real-time Python code execution for computational tasks; and seamless data connectors for integrating with enterprise databases and services. Custom tools can also be developed and incorporated, which enable organizations to tailor the system to t",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 290,
            "total_chunks": 313
          }
        },
        {
          "id": "4e6f545d-da1a-46ca-a9b7-ee32c2588dfe",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "m tools can also be developed and incorporated, which enable organizations to tailor the system to their unique needs and challenges.\nThe bottom part of Figure 6-10 on page 105 im plicitly connects to watsonx Orchestrate, \nwhich is the IBM platform for deploying and managing AI-driven workflows. By leveraging \nwatsonx Orchestrate, this archit ecture gains enterprise-grade ca pabilities for task automation, \ngovernance, and monitoring. This integration enables organizations to deploy AI assistants quickly and securely, with the ab ility to scale the system as the number of tasks , agents, and \nintegrations grows. Security and compliance ar e also ensured, which addresses the stringent \nrequirements of enterprise environments. The orchestration layer further optimizes performance by streamlining the deployment and management of agents, which minimizes operational overhead while maximizing system reliability.\nThe overall interaction flow in Figure 6-10 on page 105 begins with the user eng",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 291,
            "total_chunks": 313
          }
        },
        {
          "id": "dab47e51-8915-4e06-b1de-5101057d60fe",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "system reliability.\nThe overall interaction flow in Figure 6-10 on page 105 begins with the user engaging with the \nUnified Assistant. The user’s input is processed by the Supervisory AI Meta-Agent, which applies its routing, reasoning, and tool-calling capabilities to determine the be st course of \naction. Then, tasks are delegated to the appropriate assistants, which run them autonomously by using the integrated tools and workflows. The outputs and actions from these assistants are aggregated by the meta-agent and presented to the user, which helps ensure a cohesive and intuitive experience. This flow highlight s the system’s ability to handle \ndiverse and complex tasks while maintaining a si mple interface.\nIn conclusion, watsonx.ai agents represent a revolutionary approach to enterprise AI by \noffering a powerful combination of flexibility, function, and scala bility. From the intuitive Agent \nBuilder to the expansive Tool Library and fo rthcoming innovations like the Flows Engine,",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 292,
            "total_chunks": 313
          }
        },
        {
          "id": "c0d1a7a1-f771-4e1a-84d8-ab4408516d39",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ive Agent \nBuilder to the expansive Tool Library and fo rthcoming innovations like the Flows Engine, \nwatsonx.ai empowers businesses to create intelligent agents that drive productivity and \ninnovation. By leveraging th ese advanced capabilit ies, organizations can harness the full \npotential of AI to achieve their strategic goals and maintain a competitive edge in an increasingly digital world.\n\nChapter 6. Artificial intelligence agents 1076.5  AI agents use case examples\nAI agents are redefining the landscape of business operations and service delivery by \nunlocking unprecedented efficiencies and enabling more personalized, data-driven decision-making. Across  industries, these inte lligent systems ha ve found applications in \nareas such as customer service and support, sales and marketing automation, operational efficiency, financial advisory, healthcare, and supply chain management. \nThe following sections provide a detailed exploration of select use cases, and highlight their \nimp",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 293,
            "total_chunks": 313
          }
        },
        {
          "id": "3d3a36ac-a519-40de-9885-ad8339220132",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "\nThe following sections provide a detailed exploration of select use cases, and highlight their \nimpact and capabilities.\nCustomer service and support agents AI agents\nAI agents are revolutionizing customer service by offering continuous assistance. By using \ntools like watsonx.ai Web Search and Document Search (powered by RAG), these agents provide timely and accurate responses to customer inquiries. By leveraging NLP and ML, they can understand context, resolve issues, and e scalate complex cases to human agents when \nnecessary. For example, chatbots that are built by using watsonx.ai Agent Builder can be deployed as API endpoints to handle FAQs, help with troubleshooting, and manage order tracking. These agents help ensure instant support while reducing operational costs, improving customer satisfaction, and fostering loyalty.\nSales and marketing automation\nAI agents are indispensable tools in sales and marketing, where personalization and timely interactions drive success. Powered ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 294,
            "total_chunks": 313
          }
        },
        {
          "id": "33385ef9-374f-40f8-838a-280d5637b3b6",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " tools in sales and marketing, where personalization and timely interactions drive success. Powered by watsonx.ai Data Connectors, agents can analyze customer behavior, qualify leads, and deliver tailored recommendations. By automating follow-ups and dynamically adjusting strategies  by using real-time insights, these agents \nenhance engagement and drive conversions. For example, e-commerce platforms can use watsonx.ai agents to suggest personalized items, send promotional offers, and re-engage customers who abandon carts. Such integrations help businesses maximize revenue potential while delivering highly customized customer experiences.\nOperational efficiency and process automation\nAutomating repetitive and rout ine tasks is a hallmark of AI a gents, and watsonx.ai provides \nthe tools to optimize these processes. By using the Code Execution capa bility, agents can run \nPython scripts in real time to process data, verify documents, or automate workflows. Beyond task automation, these ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 295,
            "total_chunks": 313
          }
        },
        {
          "id": "5128c8c2-8d7e-47bf-8b78-a168cf3ee8d9",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "n real time to process data, verify documents, or automate workflows. Beyond task automation, these agents can monitor work flows, identify bottle necks, and recommend \nimprovements. For example, administrative tasks like employee onboarding can be fully automated with watsonx.ai by processing documentation, setting up accounts, and scheduling orientation sessions, which free employees to focus on more strategic responsibilities.\nHealthcare assistants and patient care\nIn healthcare, AI agents improve patient care an d operational efficiency by acting as virtual \nassistants. By integrating with watsonx.a Tool Library and leveraging custom tools, these agents can manage schedules, send medication reminders, and provide basic medical guidance. For example, telemedicine platforms can deploy agents to conduct preliminary symptom checks, monitor health metrics such as heart rate or blood pressure, and alert providers to abnormalities. These capabilities reduce the workload on healthcare staf",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 296,
            "total_chunks": 313
          }
        },
        {
          "id": "90224844-aecb-48be-829a-b693ec59b5fe",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ure, and alert providers to abnormalities. These capabilities reduce the workload on healthcare staff while ensuring proactive and timely patient care, which enhances outcomes and satisfaction.\n\n108 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiSupply chain and logistics optimization\nAI agents are transforming supply chain management by combining real-time data with \npredictive analytics. Tools like watsonx.ai Data Connectors enable agents to forecast demand, manage inventory, and plan delivery routes effectively. Logistics companies can leverage these capabilities to analyze historical  patterns, predict future needs, and maintain \noptimal stock levels. Also, AI-driven route optimization, when informed by traffic and weather data, helps ensure timely deliveries and reduces costs. With watsonx.ai, organizations can build scalable, secure agents that streamline supply chain operations, which enhance both \nefficiency and customer satisfaction.\n\n© Copyright IBM C",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 297,
            "total_chunks": 313
          }
        },
        {
          "id": "2c506519-028a-4845-8c11-55c57a0f17a1",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "upply chain operations, which enhance both \nefficiency and customer satisfaction.\n\n© Copyright IBM Corp. 2025. 109Chapter 7. Use cases\nThis chapter describes two separate use cases and shows what problems IBM watsox.ai \ntools can solve. It also describes a framework that outlines how companies who are trying to \nprepare for the future are thinking about use cases of the future to keep ahead of the curve.\nThe following topics are described in this chapter:\n/SM5900007.1, “Using RAG to aid a medical school admissions office” on page 110\n/SM5900007.2, “Embedding workflow automation to streamline recommendations” on page 1117\n\n110 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai7.1  Using RAG to aid a medi cal school admissions office\nAs a quick refresher, Retrieval-Augmented Generation (RAG) is a technique that combines \ninformation retrieval and language model generation to provide precise and contextually relevant responses to user queries. It works by first retri",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 298,
            "total_chunks": 313
          }
        },
        {
          "id": "419baefc-de0e-4a53-a6dc-a6ea628afbe5",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "tion to provide precise and contextually relevant responses to user queries. It works by first retrieving relevant documents or \npassages from a large corpus of documents by using a retrieval step, and then feeds these passages along with the original query into a large language model (LLM) to generate a response.\nRAG is one of the most commonly used techniques  that are used in production AI workloads. \nIt is used in various ways, such as in question-answering systems, chatbots, and digital workers. One powerful tool that can be used in these systems is a summary of ingested documents. This section describes how a lead ing medical school in th e US turned to the \nwatsonx.ai platform to enable it to accomplish its goals.\n7.1.1  The challenge\nA leading medical school in the US decides to offer tuition-free education to its admitted students. They anticipated a surge in applications. To help manage the expected increase in applications, the institution turned to wats onx.ai. They hoped t",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 299,
            "total_chunks": 313
          }
        },
        {
          "id": "ff8066e1-7790-4c11-a249-c6478609844c",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "lp manage the expected increase in applications, the institution turned to wats onx.ai. They hoped that IBM could provide a \ntechnology solution to help the admissions committee efficiently process and review the incoming applications. \n7.1.2  The solution\nWorking with the medical school, the IBM team developed an innovative solution by using watsonx.ai to generate one-page abstracts that summarized the incoming 50 - 70-page applications. The incoming applications included essays, with each application containing 5 - 8 essays that varied from several paragraphs to several pages. The IBM granite-13b-chat-v2 model within the watsonx.ai platform was used to generate a 1 - 2 paragraph summary of each of the essays, which was included with the application abstracts.\nFigure 7-1 shows the watsonx.ai workflow that accomplished this task.\nFigure 7-1   watsonx.ai workflow example\n\n\nChapter 7. Use cases 1117.1.3  Special considerations\nFrom the start of this project, both IBM and the medical scho",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 300,
            "total_chunks": 313
          }
        },
        {
          "id": "c5e079ed-a7d1-440c-8e9c-e95c02000b4f",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "cases 1117.1.3  Special considerations\nFrom the start of this project, both IBM and the medical school recognized the importance of \ndeveloping a solution that met the institution's needs and aligned with IBM AI Ethics. The usage of AI in the admissions process raised important ethical considerations, such as helping ensure fairness , transparency, and acc ountability. The goal was to create a system \nthat would augment human decision-making rather than replace it, and provide the admissions committee with the tools that they  needed to make informed decisions.\nThe project incorporated a range of both technical and non-technical guardrails to address AI \nethics considerations throughout the project lifecycle:\n/SM590000Augmenting human decision making: The solution was designed to support human \ndecision-makers, and not replace them. The AI-powered pipeline generated summaries and identified key information, but all decisions remained in the hands of humans.\n/SM590000Education and train",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 301,
            "total_chunks": 313
          }
        },
        {
          "id": "191d1f83-20b1-4e3e-a111-92ea0f4ef5f3",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ied key information, but all decisions remained in the hands of humans.\n/SM590000Education and training: The IBM team provided ongoing education and training on AI to \nboth technical and business users to help ensure that everyone that was involved in the project understood the ca pabilities and limitations  of the technology.\n/SM590000Thresholds and AI notices: The team im plemented technical guardrails by using \nwatsonx.governance to detect and prevent potential biases or errors in the system.\n/SM590000Feedback mechanism: The IBM team establishe d a continuous feedback loop with the \nclient to refine and improve the solution over time.\nBy considering these guardrails from the beginning, the project was able to meet the \ninstitutions needs while aligning to their governance frameworks regarding fairness, transparency, and accountability. It also limited the school’s risk exposure and reduced the \nlikelihood of having to redesign the system to incorporate new safeguards because decisio",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 302,
            "total_chunks": 313
          }
        },
        {
          "id": "73ccdbe0-f658-42be-bf7c-0425860272f4",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "duced the \nlikelihood of having to redesign the system to incorporate new safeguards because decision points were integrated from the start.\n7.2  Embedding workflow automation to streamline \nrecommendations\nThis use case describes at how workflow automation can lead to directly addressing the \nwants and needs of a financial institution, and act as a potential opportunity pipeline for the banks serving their customers.\n7.2.1  The challenge\nSmall local and regional banks, especially ones serving customers in more rural settings, \noften face different challenges than the large banks that service most of the total addressable market. Some of these challenges are self-e vident, such as having comparatively limited \naccess to capital, but other challenges are less apparent, such as an increased need to rely on low- and no-code solutions to maintain technological parity with the custom-built \napplications that are designed and managed by large centralized IT teams that are staffed by larger f",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 303,
            "total_chunks": 313
          }
        },
        {
          "id": "f1438585-51d1-4be8-aea3-ae1e4d155674",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "pplications that are designed and managed by large centralized IT teams that are staffed by larger finance institutions.\nFor one of these banks servicing rural clients in  the Midwest region of the US, they wanted to \naddress some of the market trends they read about in 5 banking customer experience trends \nto consider for 2024 , with a specific focus on providing customers with immediate service and \npersonalized recommendations. \n\n112 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiIn these rural settings, it is often difficult fo r a bank’s customer base to travel to the nearest \nbranch, and at specific times of the year, the journey takes time away from their responsibilities at farms, ranche s, and processing centers, whic h directly impacts their annual \nincome. \n7.2.2  The solution\nBy leveraging a collection of tools, this regional bank was able to build a solution that incorporated three IBM tools to reduce friction with its users while leveraging largel",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 304,
            "total_chunks": 313
          }
        },
        {
          "id": "17724e8f-9a1e-408d-a9e7-6fb11a97446d",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "solution that incorporated three IBM tools to reduce friction with its users while leveraging largely pre-built skills and solutions. The soluti on centered on a watsonx Orc hestrate business automation \napplication that takes in loan applications and responds to the loan applicant with near-real-time approval or rejection notices based on thresholds that are set by the bank.\nFigure 7-2 shows the watsonx Orchestrate workflow that was used in this use case.\nFigure 7-2   watsonx Orchestrate workflow example\nCustomers connected to the bank through IBM watsonx Assistant, which leveraged an IBM \nLLM to initiate a natural language dialog with the client and collect various loan application documents, which included custom forms that are specific to this bank. These documents were passed from watsonx Assistant to watsonx Orchestrate, which called on several of its prebuilt skills, and custom skills  that were developed in the Skills Studio feature to access \nadditional services outside of th ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 305,
            "total_chunks": 313
          }
        },
        {
          "id": "d44049bc-8f3c-4175-9acf-6e504791d3d8",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "ills  that were developed in the Skills Studio feature to access \nadditional services outside of th e bank (such as credit reports). \nAfter running through a decision tree based on the inputs, the client received an approval or \nrejection notification. In either case, the notice was sent back to the applicant through a custom-generated response that was tailored by watsonx.ai and specific to that customer, with an explanation of the decision based on that customer’s specific application. Furthermore, the bank leveraged its own client knowledge to augment the loan decision with additional offers or suggestions to the customer based on their specific customer profile. For those customers that applied for an automotive loan and had a mortgage and business account with the bank, the bank suggested that they sign up for a wealth management account that was serviced by the bank. For th ose customers who applied for a small business \nloan who did not have a checking or savings account that wa",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 306,
            "total_chunks": 313
          }
        },
        {
          "id": "cc5d05c4-416b-4150-b8c6-6a20001bf79a",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "tomers who applied for a small business \nloan who did not have a checking or savings account that was associated with the EIN on the application, the bank suggested that they open a full suite of business accounts with the loan. These examples pu lled on watsonx.ai gene rative capabilities to create personalized \nrecommendations based on individual customers rather than “one-size-fits-all” templates that are applied as a blanket policy to all applicatio ns. Holistically, this entire solution can be \nsummarized as an AI agent. \n\n\nChapter 7. Use cases 1137.2.3  Special considerations\nWith a smaller workforce to dedicate to this solution and limited previous AI experience, the \nbank leveraged existing tools to reduce the workload and expertise requirements on the bank’s workforce. This approach was one of the key reasons IBM and the client focused on \nleveraging watsonx Orchestrate over designing a custom application that integrated AI tools \nthrough API calls. Instead of building a solut",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 307,
            "total_chunks": 313
          }
        },
        {
          "id": "1904b6e1-867b-4eed-85b3-0a12f3013435",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "igning a custom application that integrated AI tools \nthrough API calls. Instead of building a solution, the bank relied on built-in AI functions that automatically combined pre- packaged skills dynamically  and in-context based on \norganizational knowledge and prior interactions to help workers design the workflow of their application. Users prov ided natural language in puts to select and sequ ence the required skills \nfor a task, and watsonx Orchestrate connected them with the associated applications, tools, data, and historical details. This approach enabled the team to automate processes without needing highly specialized IT skills or ex pert knowledge of business processes and \napplications.\n\n114 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai\n\n© Copyright IBM Corp. 2025.  115AI artificial intelligence\nAIaaS AI as a Service\nBYOM Bring Your Own Model\nCLI command-line interface\nCNN convolutional neural network\nDL deep learning\nDQN Deep Q-Network\nESG environ",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 308,
            "total_chunks": 313
          }
        },
        {
          "id": "2cfe1812-4dd0-4c03-bb98-b49e67ffa87e",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "mand-line interface\nCNN convolutional neural network\nDL deep learning\nDQN Deep Q-Network\nESG environmental, social, and \ngovernance\nFM foundation model\ngen AI Generative AI\nIBM International Business Machines \nCorporation\nKNN k-nearest neighbor\nKPI key performance indicator\nLLM large language model\nLLMOps large language model operations \nLoRA low-rank adaptation\nMAS multi-agent system\nMDP Markov decision processes\nML machine learning\nMLOps machine learning operations\nMMLU Massive Multitask Language \nUnderstanding\nNLP natural language processing\nPCA Principal Component Analysis\nQLoRA quantized low-rank adaptation\nRAG Retrieval-Augmented Generation\nRL reinforcement learning\nRNN recurrent neural network\nRPA Robotic Process Automation\nSaaS Software-as-a-Service\nSDG synthetic data generation\nSME subject matter expert\nUI user interfaceAbbreviations and acronyms\n\n116 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai\n\n© Copyright IBM Corp. 2025. 117Related publications\nT",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 309,
            "total_chunks": 313
          }
        },
        {
          "id": "d4091206-b3c1-4289-afe4-34d938725120",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "nleashing the Power of AI with IBM watsonx.ai\n\n© Copyright IBM Corp. 2025. 117Related publications\nThe publications that are listed in this section are considered suitable for a more detailed \ndescription of the topics that are covered in this book.\nIBM Redbooks\nThe following IBM Redbooks publications provid e additional information about the topics in \nthis document. Some publications that are refere nced in this list might be available in softcopy \nonly. \n/SM590000Simplify Your AI Journey: Ensuring Trustworthy AI with IBM watsonx.governance , \nSG24-8573\n/SM590000Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.data , SG24-8570\nYou can search for, view, download, or order these documents and other Redbooks, \nRedpapers, web docs, drafts, and addition al materials, at the following website: \nibm.com/redbooks\nOnline resources\nThese websites are also relevant as further information sources:\n/SM590000Code samples of common machine learning scenarios:\nhttps://github.com",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 310,
            "total_chunks": 313
          }
        },
        {
          "id": "32024700-bea4-43fa-8d4f-a485165fc71f",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": " information sources:\n/SM590000Code samples of common machine learning scenarios:\nhttps://github.com/IBM/watson-machine-learning-samples\n/SM590000Examples of using Instructlab and AI agents:\nhttps://github.com/IBM/watsonx-ai-platform-demos\n/SM590000IBM AI risk atlas\nhttps://www.ibm.com/docs/en/watsonx/w-and-w/2.1.x?topic=ai-risk-atlas\n/SM590000IBM watsonx documentation (Includes links to all watsonx products)\nhttps://www.ibm.com/docs/en/watsonx\n/SM590000IBM watsonx.governance product\nhttps://www.ibm.com/products/watsonx-governance\n/SM590000IBM watsonx product portfolio\nhttps://www.ibm.com/watsonx\n\n118 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiHelp from IBM\nIBM Support and downloads\nibm.com/support\nIBM Global Services\nibm.com/services\n\n(0.2”spine)\n0.17”<->0.473”\n90<->249 pagesSimplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai\n\n\n\n\n\nibm.com /redbooksPrinted in U.S.A .Back cover\nISBN 0738461989SG24-8574-00\n®\n\n",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 311,
            "total_chunks": 313
          }
        },
        {
          "id": "da10b9dd-c6f8-4a6f-a336-ffa63347d645",
          "doc_id": "7f20661b-ec09-4910-97bf-326b20e2acd9",
          "content": "m /redbooksPrinted in U.S.A .Back cover\nISBN 0738461989SG24-8574-00\n®\n\n",
          "metadata": {
            "source": "sg248574.pdf",
            "chunk_index": 312,
            "total_chunks": 313
          }
        }
      ]
    }
  ]
}
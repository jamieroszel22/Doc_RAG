[
  {
    "text": "RedbooksArtificial Intelligence\nData and AIFront cover\nSimplify Your AI Journey:\nEnsuring Trustworthy AI with \nIBM watsonx.governance\nDeepak Rangarao\nUpasana BhattacharyaSavitha Chinnappareddy PhDLarry CoyneDavid CruzShuvanker GhoshPrem Piyush GoyalVasfi GucerAmna Jamal PhDWarren LucasKaren MedhatBob RenoMohit Sharma\nMark SimmondsJasmeet SinghMartijn Wiertz\n\n\n\nIBM Redbooks\nEnsuring Trustworthy AI wi th IBM watsonx.governance\nJanuary 2025\nSG24-8573-00\n\nii Ensuring Trustworthy AI with IBM watsonx.governance\n\n\u00a9 Copyright IBM Corp. 2025. iiiContents\nNotices  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\nTrademarks  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\nForeword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\nPreface. . . . . . . . . ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 0,
      "total_chunks": 245
    }
  },
  {
    "text": " . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .x\nAuthors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .x\nNow you can become a published author, too!  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiiiComments welcome. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . x iii\nStay connected to IBM Redbooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiv\nChapter 1.  Challenges and opportunities in AI governance for responsible AI  . . . . . . 1\n1.1  What is AI governance?  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2  Governance as a key enabler for realizing A",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 1,
      "total_chunks": 245
    }
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2  Governance as a key enabler for realizing AI value . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.2.1  Concern 1: Governance is a brake on AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.2.2  Concern 2: Governance does not scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.2.3  Concern 3: Governance does not contribute to value generation. . . . . . . . . . . . . . 6\n1.3  Challenges with governance of enterprise AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.3.1  Generative AI has changed the governance game. . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.3.2  Bring together diverse stakeholder perspectives  . . . . . . . . . . . . . . . . . . . . . . . . . . 71.3.3  Technical complexity is increasing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1.3.4  Regulatory and risk complexity is increasing  . . . . . .",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 2,
      "total_chunks": 245
    }
  },
  {
    "text": ". . . . . . . . . . . . . . . . . 9\n1.3.4  Regulatory and risk complexity is increasing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.4  An example of legislation and standards related to AI  . . . . . . . . . . . . . . . . . . . . . . . . . 11\n1.4.1  AI-specific legislation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n1.4.2  General regulations that apply to AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n1.4.3  Technical standards for AI governance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nChapter 2.  Introduction to IBM watsonx.governance  . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.1  Introduction to the IBM watsonx platform and its core components . . . . . . . . . . . . . . . 16\n2.2  Introduction to IBM watsonx.ai  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.3  Introduction to IBM watsonx.data . . . . ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 3,
      "total_chunks": 245
    }
  },
  {
    "text": " . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.3  Introduction to IBM watsonx.data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.4  Introduction to IBM watsonx.governance  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.4.1  Key capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.4.2  Use cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1\n2.4.3  Benefits of watsonx.governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.5  Reference architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2\n2.5.1  Data Onboarding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222.5.2  Data Preparation. . . . . . . . . . . . . . . . . . . . . . . . . .",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 4,
      "total_chunks": 245
    }
  },
  {
    "text": ". . . . . . . . . . . . 222.5.2  Data Preparation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n2.5.3  AI Building and Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n2.5.4  AI Lifecycle Management and Governance  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nChapter 3.  Implementing AI governance strategy  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.1  Understanding the end-to-end AI lifecycle governance process. . . . . . . . . . . . . . . . . . 28\n3.2  Elements of model risk governance  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.2.1  Personas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.2.2  Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n3.2.3  Workflows . . . .",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 5,
      "total_chunks": 245
    }
  },
  {
    "text": " . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n3.2.3  Workflows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 6\n3.3  Considerations to implement AI governance strategy. . . . . . . . . . . . . . . . . . . . . . . . . . 37\n3.3.1  Understanding organizational characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383.3.2  Configuring AI governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n\niv Ensuring Trustworthy AI with IBM watsonx.governance3.3.3  Leveraging out-of-the-box product content . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.3.4  Example use case. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\nChapter 4.  Onboarding a new foundation model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n4.1  Key considerations to onboard a foundation model",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 6,
      "total_chunks": 245
    }
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . . 41\n4.1  Key considerations to onboard a foundation model  . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n4.1.1  Data transparency. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n4.1.2  Model evaluation and validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424.1.3  Model security and robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n4.1.4  Ensuring model health and performance  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n4.2  Considerations for legal team for approving a new foundation model  . . . . . . . . . . . . . 44\n4.2.1  Model licensing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n4.2.2  Legal obligations on the part of the vendor. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.2.3  A final note on lega",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 7,
      "total_chunks": 245
    }
  },
  {
    "text": "the vendor. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.2.3  A final note on legal considerations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n4.3  Ethical considerations for approving a new foundation model  . . . . . . . . . . . . . . . . . . . 47\n4.3.1  Fairness  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  47\n4.3.2  Transparency  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484.3.3  Privacy  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.3.4  Explainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n4.3.5  Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494.3.6  Third-party help. . . . . . . . . . . . . .",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 8,
      "total_chunks": 245
    }
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . . . . 494.3.6  Third-party help. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n4.4  Considerations for financial stakeholders  for approving a new foundation model  . . . . 49\n4.4.1  Total cost of ownership . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504.4.2  Return on investment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.4.3  Build or buy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.4.4  Exit strategy  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.4.5  Other factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1\nChapter 5.  Assessing a new use case  . . . . . . . . . . . . . . . . . . . . . . . . ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 9,
      "total_chunks": 245
    }
  },
  {
    "text": ". . . . . 5 1\nChapter 5.  Assessing a new use case  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n5.1  Business process workflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n5.2  Approval workflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  55\n5.3  Risk identification assessment  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 565.4  Applicability asse ssment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\nChapter 6.  Governing the end-to-end lifecycle of an AI asset  . . . . . . . . . . . . . . . . . . . 61\n6.1  What is the AI lifecycle?  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n6.2  Metrics in watsonx.governance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 10,
      "total_chunks": 245
    }
  },
  {
    "text": "onx.governance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6.2.1  Drift detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6.2.2  Explainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6.2.3  Model health . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 656.2.4  Generative AI quality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n6.2.5  RAG quality metrics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n6.3  How to implement Lifecycle Governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n6.3.1  Getting started: Setting up your AI use cases. . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n6.4  Lifecycle implementation a",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 11,
      "total_chunks": 245
    }
  },
  {
    "text": " use cases. . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n6.4  Lifecycle implementation and considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n6.4.1  UI-driven implementation of lifecycle governance. . . . . . . . . . . . . . . . . . . . . . . . . 686.4.2  Considerations for lifecycle governance for traditional ML hosted on watsonx.ai. 70\n6.4.3  Considerations for prompt templates from another platform. . . . . . . . . . . . . . . . . 72\n6.4.4  Considerations for traditional ML from another platform. . . . . . . . . . . . . . . . . . . . 746.4.5  Governing AI embedded in a business application. . . . . . . . . . . . . . . . . . . . . . . . 74\nChapter 7.  Use cases  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n7.1  Overview of use case 1- Banking credit risk management . . . . . . . . . . . . . . . . . . . . . . 78\n7.1.1  Banking credit risk management use case . . . . . . . . . . . .",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 12,
      "total_chunks": 245
    }
  },
  {
    "text": " . . . . . . . . . . . . . 78\n7.1.1  Banking credit risk management use case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n7.1.2  Business context. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n\n Contents v7.1.3  Client need . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n7.1.4  Client challenges. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n7.1.5  Business benefits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n7.1.6  Pilot solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  80\n7.2  Overview of use case 2 - Automated governance for universal bank's AI chatbot . . . . 80\n7.2.1  Business context. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 13,
      "total_chunks": 245
    }
  },
  {
    "text": "ss context. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n7.2.2  Client need . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n7.2.3  Client challenges. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n7.2.4  Business benefits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n7.2.5  Pilot solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  81\n7.3  Overview of use case 3 - Belgian biopharmaceutical company . . . . . . . . . . . . . . . . . . 81\n7.3.1  Business context. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n7.3.2  Client need . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n7.3.3  Client",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 14,
      "total_chunks": 245
    }
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n7.3.3  Client challenges. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n7.3.4  Business benefits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n7.3.5  Pilot solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  82\nRelated publications  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\nIBM Redbooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\nOnline resources  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\nHelp from IBM  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 15,
      "total_chunks": 245
    }
  },
  {
    "text": " . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . 85\n\nvi Ensuring Trustworthy AI with IBM watsonx.governance\n\n\u00a9 Copyright IBM Corp. 2025. viiNotices\nThis information was developed for prod ucts and services offered in the US . This material might be available \nfrom IBM in other languages. However, you may be required  to own a copy of the product or product version in \nthat language in order to access it. \nIBM may not offer the products, services, or features di scussed in this document in other countries. Consult \nyour local IBM representative for information on the produc ts and services currently available in your area. Any \nreference to an IBM product, program, or service is not intended to state or imply that only that IBM product, \nprogram, or service may be used. Any functionally equi valent product, program, or service that does not \ninfringe any IBM intellectual property right may be used instead. However, it is t he user",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 16,
      "total_chunks": 245
    }
  },
  {
    "text": "does not \ninfringe any IBM intellectual property right may be used instead. However, it is t he user\u2019s responsibility to \nevaluate and verify the operation of any non-IBM product, program, or service. \nIBM may have patents or pending patent applications covering subject matter described in this document. The \nfurnishing of this document does not grant you any license to these patents. You can send license inquiries, in \nwriting, to:\nIBM Director of Licensing, IBM Corporation, North Castle Drive, MD-NC119, Armonk, NY 10504-1785, US \nINTERNATIONAL BUSINESS MACHINES CORPORATIO N PROVIDES THIS PUBLICATION \u201cAS IS\u201d \nWITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED \nTO, THE IMPLIED WARRANTIES OF NON-INFR INGEMENT, MERCHANTABILITY OR FITNESS FOR A \nPARTICULAR PURPOSE. Some jurisdictions do not allow disclaimer of express or implied warranties in \ncertain transactions, therefore, this statement may not apply to you. \nThis information could include technical in",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 17,
      "total_chunks": 245
    }
  },
  {
    "text": "ctions, therefore, this statement may not apply to you. \nThis information could include technical inaccuracies or  typographical errors. Changes are periodically made \nto the information herein; th ese changes will be incorporated  in new editions of the publication. IBM may make \nimprovements and/or changes in the product(s) and/or the program(s) described in this publication at any time \nwithout notice. \nAny references in this information to non-IBM websites are provided for convenience only and do not in any \nmanner serve as an endorsement of those websites. The materials at those websites are not part of the \nmaterials for this IBM product and use of those websites is at your own risk. \nIBM may use or distribute any of the information you provide in any way it believes appropriate without \nincurring any obligation to you. \nThe performance data and c lient examples cited are presented fo r illustrative purposes only. Actual \nperformance results may vary depending on specific configu",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 18,
      "total_chunks": 245
    }
  },
  {
    "text": " fo r illustrative purposes only. Actual \nperformance results may vary depending on specific configurations and operating conditions. \nInformation concerning non-IBM products was obtained from the suppliers of those products, their published \nannouncements or other publicly available sources. IBM has not tested those products and cannot confirm the \naccuracy of performance, co mpatibility or any other clai ms related to non-IBM pr oducts. Questions on the \ncapabilities of non-IBM products should be addr essed to the suppliers of those products. \nStatements regarding IBM\u2019s future direction or intent are subject to change or withdrawal without notice, and \nrepresent goals and objectives only. \nThis information contains exam ples of data and reports used in daily business operations. To illustrate them \nas completely as possible, the exam ples include the names of individual s, companies, brands, and products. \nAll of these names are fictitious and any similarity to  actual people or busi",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 19,
      "total_chunks": 245
    }
  },
  {
    "text": "rands, and products. \nAll of these names are fictitious and any similarity to  actual people or business enterprises is entirely \ncoincidental. \nCOPYRIGHT LICENSE:\nThis information contai ns sample application prog rams in source language, which illustrate programming \ntechniques on various operating platforms. You may co py, modify, and distribute these sample programs in \nany form without payment to IBM, for the purposes of developing, using, marketing or distributing application programs conforming to the application programming interface for the operating platform for which the sample \nprograms are written. These examples have not been th oroughly tested under all conditions. IBM, therefore, \ncannot guarantee or im ply reliability, serviceability, or function of  these programs. The sample programs are \nprovided \u201cAS IS\u201d, without warranty of any kind. IBM sha ll not be liable for any damages arising out of your use \nof the sample programs. \n\nviii Ensuring Trustworthy AI with IBM wat",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 20,
      "total_chunks": 245
    }
  },
  {
    "text": "damages arising out of your use \nof the sample programs. \n\nviii Ensuring Trustworthy AI with IBM watsonx.governanceTrademarks\nIBM, the IBM logo, and ibm.com are trademarks or regi stered trademarks of International Business Machines \nCorporation, registered in many jurisdictions worldwide. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on the web at \u201cCopyright \nand trademark information\u201d at https://www.ibm.com/legal/copytrade.shtml  \nThe following terms are trademarks or registered trademarks of International Business Machines Corporation, \nand might also be trademarks or registered trademarks in other countries. \nIBM\u00ae\nIBM Cloud\u00ae\nIBM Research\u00aeIBM Watson\u00ae\nOpenPages\u00ae\nRedbooks\u00aeRedbooks (logo) \u00ae\nThe following terms are trademarks of other companies:\nMicrosoft, and the Windows logo are trademarks of Microsoft Corporation in the United States, other \ncountries, or both.\nOpenShift, Red Hat, are trademarks or registe",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 21,
      "total_chunks": 245
    }
  },
  {
    "text": "ation in the United States, other \ncountries, or both.\nOpenShift, Red Hat, are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in the United \nStates and other countries.\nRStudio, and the RStudio logo are registered trademarks of RStudio, Inc.Other company, product, or service names may be trademarks or service marks of others. \n\n\n\u00a9 Copyright IBM Corp. 2025. ixForeword\nThis trilogy of IBM\u00ae Redbooks\u00ae publications positions and explains IBM watsonx, the \nIBM strategic AI and Data platform. Each book focuses on one of the three main components of the watsonx platform:\n/SM590000IBM watsonx.ai: A next-generation enterprise studio for AI developers to train, validate, \ntune, and deploy both tradit ional ML and new generative AI capabilities powered by \nfoundation models.\n/SM590000IBM watsonx.data:  A fit-for-purpose data store built on an open-lakehouse architecture, \noptimized for different and governed data and AI workloads.\n/SM590000IBM watsonx.governance:  A set o",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 22,
      "total_chunks": 245
    }
  },
  {
    "text": "ptimized for different and governed data and AI workloads.\n/SM590000IBM watsonx.governance:  A set of AI governance capabilities enabling trusted AI \nworkflows, helping organizations implement and comply with ever-changing industry and government regulations.\nOrganizations have long recognized the value that IBM Redbooks provide in guiding them \nwith best practices, frameworks, clear explanations, and use cases as part of their solution evaluations and implementations.\nThis trilogy of books was only possible due to the close coll aboration involving many skilled \nand talented authors that were selected from our IBM global technical sales, development, Expert Labs, Client Success Mana gement, and consulting servic es organizations, using their \ndiverse skills, experiences, and technical knowledge across the watsonx platform.\nI would like to thank the authors, contributors, reviewers, and the IBM Redbooks team for their \ndedication, time, and effort in making these pu blications a valuab",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 23,
      "total_chunks": 245
    }
  },
  {
    "text": "the IBM Redbooks team for their \ndedication, time, and effort in making these pu blications a valuable asset that organizations \ncan use as part of their journey to AI.\nI also want to thank Mark Simmonds and Deepak Rangarao for taking the lead in shaping this \nrequest into yet another successful IBM Redbooks project.\nIt is my sincere hope that you enjoy this wats onx trilogy as much as the team who wrote and \ncontributed to them.\nSteve Astorino, IBM General Manager - De velopment, Data, AI and Sustainability.\n\nx Ensuring Trustworthy AI with IBM watsonx.governancePreface\nIBM\u00ae watsonx\u2122  is the IBM strategic AI and Data platform. This book focuses on \nwatsonx.governance , a key component of the platform.\nIBM watsonx.governance offers a comprehensive solution for governing data and AI \nworkloads within a secure and scalable environment. Built on an open architecture, it empowers organizations to manage data access, compliance, and security across hybrid multi-cloud deployments. IBM watsonx",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 24,
      "total_chunks": 245
    }
  },
  {
    "text": "s to manage data access, compliance, and security across hybrid multi-cloud deployments. IBM watsonx.governance simplifies data governance with built-in automation tools and integrates seamlessly with existing databases and tools, streamlining workflows and enhancing user experience\nThis IBM Redbooks publication provides a broad understanding of watsonx.governance \nconcepts and architecture, and the services that are available in the product. In addition, several common use cases and scenarios are included that should help you better understand the capabilities of this product.\nThis publication is for watsonx customers who se ek best practices and real-world examples of \nhow to best implement their solu tions while optimizing the value of their existing and future \ntechnology, AI, data, and skills investments.\nAuthors\nThis book was produced by a team of specia lists from around the world working with the\nIBM Redbooks, Tucson Center. \nDeepak Rangarao  is an IBM Distinguished Engineer an",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 25,
      "total_chunks": 245
    }
  },
  {
    "text": " working with the\nIBM Redbooks, Tucson Center. \nDeepak Rangarao  is an IBM Distinguished Engineer and CTO responsible for Technical \nSales-Cloud Paks. Currently, he leads the technical sales team to help organizations modernize their technology landscape with IBM Cloud\u00ae Paks. He has broad cross-industry experience in the data warehousing and analytic s space, building analytic applications at \nlarge organizations and technical pre-sales with start-ups and large enterprise software vendors. Deepak has co-authored several books on topics, such as OLAP analytics, change data capture, data warehousing, and object storage and is a regular speaker at technical conferences. He is a certified technical sp ecialist in Red Hat OpenShift, Apache Spark, \nMicrosoft SQL Server, and web development technologies.\nUpasana Bhattacharya  is a Senior Product Manager for watsonx.governance, based in \nMarkham, Canada. In this role she defines the product vision, guides its development, collaborating with cr",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 26,
      "total_chunks": 245
    }
  },
  {
    "text": ", Canada. In this role she defines the product vision, guides its development, collaborating with cross-functional teams. In her previous role she was a Product Manager for Data and AI. Upasana holds a Bachelor of Arts in Economics and Foreign Affairs from the University of Virginia and an MBA from the McCombs School of Business at the University of Texas.Note:  Other books in this series are:\n/SM590000Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai , SG24-8574\n/SM590000Simplify Your AI Journey: Hybrid, Open Data Lakehouse with IBM watsonx.data,  \nSG24-8570\n\n Foreword xiSavitha Chinnappreddy, PhD  is a Senior AI Engineering Manager at IBM with over 17 years \nof experience in AI and Data Analytics. She holds a PhD in AI and Data Analytics and is currently pursuing a post-doctorate focused on Human & AI Collaboration: Governance strategies for trustworthy AI & Safe AI systems. She has extensive Experience in managing and scaling large AI and Data Science teams, s",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 27,
      "total_chunks": 245
    }
  },
  {
    "text": " AI systems. She has extensive Experience in managing and scaling large AI and Data Science teams, she has worked closely with architecture and infrastructure teams to establish compliant pipe lines for AI and analytics, delivering impactful \nsolutions to global customers. With 11 public ations in esteemed journals and conferences, as \nwell as holding a patent, she is also an active guest speaker and participant in faculty development programs, committed to sharing her knowledge and inspiring the next generation of AI professionals.\nLarry Coyne is a Project Leader at the IBM International Technical Support Organization, \nTucson, Arizona, center. He has over 35 years of IBM experience, with 23 years in IBM storage software management. He holds degrees in Software Engineering from the University of Texas at El Paso and Project Management from George Washington University. His areas of expertise include client relationship management, quality assurance, development management, and support",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 28,
      "total_chunks": 245
    }
  },
  {
    "text": "rtise include client relationship management, quality assurance, development management, and support management for IBM storage management software.\nDavid Cruz is a Data Scientist and AI Engineer working under IBM\u2019s Client Engineering \nteam. In this role, David has been dedicated to the Federal Market where he works to implement a wide range of AI solutions for federal clients. In his prior role, he worked under the Data Science Elite team where he gained skills with IB M platforms fo r Governance, \nnamely IBM OpenScale, and this has transl ated into a growing skill set with watsonx \ngovernance. He is constantly working to implement the cutting edge of AI and AI Governance technology, and has written various blog posts on topics ranging from Unsupervised Learning techniques, to RAG how-to guides for beginners.\nShuvanker Ghosh  is a certified Executive Architect and Worldwide Platform Leader for Data \nand AI in Worldwide Solution Architecture in IBM Technology Expert Labs. With 18 years",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 29,
      "total_chunks": 245
    }
  },
  {
    "text": "der for Data \nand AI in Worldwide Solution Architecture in IBM Technology Expert Labs. With 18 years of experience at IBM, he serves as a trusted adviso r to clients, offering thought leadership on \nIBM's Data and AI portfolio. He guides organizations in their responsible AI journey, helping them adopt best practices. His current focus is on defining solution blueprints and architectural patterns that assist clients in addressing their business challenges through responsible and trustworthy AI solutions. He po ssesses extensive expertise in the IBM Data \nand AI portfolio, including the watsonx platform and Cloud Pak for Data. Shuvanker has successfully led and delivered co mplex programs that involve multiple teams, providing \ntechnical management, architecture, technology thought leadership, and software development methodologies and processes. His experience spans various industries, including retail, finance, insurance, healthcare, telecommunications, and government\nPrem Piyush Goya",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 30,
      "total_chunks": 245
    }
  },
  {
    "text": "ncluding retail, finance, insurance, healthcare, telecommunications, and government\nPrem Piyush Goyal  is a problem solver with extensive experience in developing cutting-edge \ntechnologies at IBM. Specializing in full-stack development, cloud-based microservices, and AI solutions, he has worked on high-impact projects like IBM Watson\u00ae Data Platform and IBM Watson OpenScale. His expertise spans Python, JavaScript, React, Kubernetes, and \nAI-driven solutions like Explainable AI and Concept Drift Detection. Passionate about building transparent and scalable AI, he continually enhances user experience and optimizes performance for enterprise applications. His in novative mindset and pr oblem-solving abilities \nhelp drive trust and tran sparency in AI systems.\nVasfi Gucer leads projects for the IBM Redbooks team, leveraging his 20+ years of \nexperience in systems management, networking, and software. A prolific writer and global IBM instructor, his focus has shifted to storage and cloud co",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 31,
      "total_chunks": 245
    }
  },
  {
    "text": "software. A prolific writer and global IBM instructor, his focus has shifted to storage and cloud computing in the past eight years. Vasfi holds multiple certificatio ns, including IBM Certified Senior IT Specialist, PMP, ITIL V2 \nManager, and ITIL V3 Expert. \n\nxii Ensuring Trustworthy AI with IBM watsonx.governanceAmna Jamal PhD  is a seasoned Data and AI Subject Matter Expert (SME) at IBM, boasting \nover 8 years of expertise in data management and data science. With a Ph.D. in Engineering \nfrom the National University of Singapore, she brings a wealth of knowledge and experience to the field, driving innovation and excellence in the intersection of data and artificial intelligence.\nWarren Lucas is a member of IBM Expert Labs. Prior to his time at IBM, Warren has spent \nnearly a decade working in Regulatory Compliance, Operational Risk, and Model Risk Governance supporting a number of Fortune 50 companies in their efforts to redesign and implement internal governance processes. As a S",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 32,
      "total_chunks": 245
    }
  },
  {
    "text": "ortune 50 companies in their efforts to redesign and implement internal governance processes. As a Solution Architect, Warr en has specialized in \nGovernance Console (IBM OpenPages\u00ae) for over seven years, where he has personally performed development, design, advisory, and configuration within the platform. Warren has a \ncurrent patent submission for a novel approach in governance and confidence assessments in large language models (LLMs); he holds a degree in Quantitative Economics.\nKaren Medhat  is a Customer Success Manager Architect in the UK and the youngest \nIBM Certified Thought Leader Level 3 Technica l Specialist. She is the Chair of the IBM \nTechnical Consultancy Group and an IBM Academy of technology member. She holds an MSc degree with honors in Engineering in AI and Wireless Sensor Networks from the Faculty of Engineering, Cairo University, and a BSc degree with honors in Engineering from the same faculty. She co-creates curriculum and exams fo r different IBM professional",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 33,
      "total_chunks": 245
    }
  },
  {
    "text": "gineering from the same faculty. She co-creates curriculum and exams fo r different IBM professional certificates. She \nalso created and co-created courses for IBM Skills Acad emy in various areas of \nIBM technologies. She serves on the review board of international conferences and journals in AI and wireless communication. She also is an IBM Inventor and experienced in creating applications architecture and leading teams of different scales to deliver customers' projects successfully. She frequently mentors IT professionals to help them define their career goals, learn new technical skills, or acquire professional certifications. She has authored publications on Cloud, IoT, AI, wireless networ ks, microservices architecture, and Blockchain.\nBob Reno  is a Principal Technical Sale s Specialist with over 30 years of experience in Data \nWarehousing, Analytics, and AI. As a member of the IBM World Wide Data and AI Technical Sales team, Bob is a watsonx.governance leader working with custo",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 34,
      "total_chunks": 245
    }
  },
  {
    "text": "M World Wide Data and AI Technical Sales team, Bob is a watsonx.governance leader working with customers to enable their organizations to embrace responsible AI. Bob has contributed to the creation of several IBM Certification Tests and writ ten several workshops in the wa tsonx, Cloud Pak for Data and \nData Warehousing space to enable customers and the IBM Technical Community. Prior to joining IBM, Bob has held role s as a Developer, Technical Ar chitect, and Director of Data \nWarehousing and Analytics.\nMohit Sharma  is an AI engineering lead on the Client Engineering watsonx team in \nBangalore, India. Prior to this, Mohit was associated with IBM consulting, and worked on client production projects involving classical ML and deep learning. Mohit has around 14 years of experience in AI, and worked at Hewlett Packard, Wipro (where he conceptualized the Holmes AI platform) and Accenture before joining IBM in 2018. An AI practitioner having experience in design and development of AI-based",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 35,
      "total_chunks": 245
    }
  },
  {
    "text": "fore joining IBM in 2018. An AI practitioner having experience in design and development of AI-based solutions using both open-source and commercial technologies, Mohit is interested in both data and the science behind it. He has 4 published patents to his credit, and has filed his first patent at IBM. \nMark Simmonds  is a Program Director in IBM Data and AI. He writes extensively on AI, data \nscience, and data fabric, and holds multiple author recognition awards. He previously worked as an IT architect leading complex infrastructure design and corporate technical architecture projects. He is a member of the British Computer Society, holds a Bachelor\u2019s Degree in Computer Science, is a published author, and a prolific public speaker.\nJasmeet Singh  is a watsonx Client Success Manager with 17 years of experience in IT and 8 \nyears of experience in watsonx Technologies with IBM Technology Expert Labs team with 4 years focused as watsonx.governance and AI Governance SME. \n\n Foreword xiiiHe",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 36,
      "total_chunks": 245
    }
  },
  {
    "text": "xpert Labs team with 4 years focused as watsonx.governance and AI Governance SME. \n\n Foreword xiiiHe has delivered high-quality implementations of AI Governance with big named IBM clients. \nJasmeet holds 5 patents in AI field and holds an MS degree in Cybersecurity from NC A&T State University.\nMartijn Wiertz  is the Technical Sales Leader for IBM watsonx.governance in the EMEA \nregion. In this role, he combines his technica l, analytical and industry knowledge to help \nclients understand and validate the unique value that the solution can bring to help them enact responsible AI. He has more than 25 ye ars of experience in the field of advanced \nanalytics, experiencing all major developments in the evolution of the industry first hand. Prior to his current role, Martijn was the global techni cal sales lead for IBM\u2019 s solution to combat \nfinancial crimes and he was Dir ector, Enterprise Solutions at SPSS Inc.  when that company \nwas acquired by IBM. \nThanks to the following people for t",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 37,
      "total_chunks": 245
    }
  },
  {
    "text": "olutions at SPSS Inc.  when that company \nwas acquired by IBM. \nThanks to the following people for their contributions to this project:\n/SM590000Steve Astorino , IBM General Manager - Developm ent, Data, AI and Sustainability\nNow you can become a published author, too!\nHere\u2019s an opportunity to  spotlight your skills , grow your career, and become a published \nauthor\u2014all at the same time! Join an IBM Redbooks residency project and help write a book in your area of expertise, while honing your experience using leading-edge technologies. Your efforts will help to increase pr oduct acceptance and customer satisfaction, as you expand \nyour network of technical contacts and relationships. Residencies run from two to six weeks in length, and you can participate either in person or as a remote resident working from your home base.\nFind out more about the residency program, browse the residency index, and apply online at:\nibm.com/redbooks/residencies.html\nComments welcome\nYour comments are impo",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 38,
      "total_chunks": 245
    }
  },
  {
    "text": "ndex, and apply online at:\nibm.com/redbooks/residencies.html\nComments welcome\nYour comments are important to us!\nWe want our books to be as helpful as possible . Send us your comments about this book or \nother IBM Redbooks publications in one of the following ways:\n/SM590000Use the online Contact us  review Redbooks form found at:\nibm.com/redbooks\n/SM590000Send your comments in an email to:\nredbooks@us.ibm.com\n/SM590000Mail your comments to:\nIBM Corporation, IBM Redbooks\nDept. HYTD Mail Station P0992455 South RoadPoughkeepsie, NY 12601-5400\n\nxiv Ensuring Trustworthy AI with IBM watsonx.governanceStay connected to IBM Redbooks\n/SM590000Find us on LinkedIn:\nhttps://www.linkedin.com/groups/2130806\n/SM590000Explore new Redbooks publications, residencies, and workshops with the IBM Redbooks \nweekly newsletter:\nhttps://www.redbooks.ibm.com/subscribe\n/SM590000Stay current on recent Redbooks publications with RSS Feeds:\nhttps://www.redbooks.ibm.com/rss.html\n\n\u00a9 Copyright IBM Corp. 2025. 1Chapte",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 39,
      "total_chunks": 245
    }
  },
  {
    "text": "lications with RSS Feeds:\nhttps://www.redbooks.ibm.com/rss.html\n\n\u00a9 Copyright IBM Corp. 2025. 1Chapter 1. Challenges and opportunities in \nAI governance\nIn 2024, the topic of governance of Artificial Intelligence (AI) has grown enormously in both \nattention and in adoption. This chapter provides a definition of AI governance and describes the key challenges and opportunities it presents.\nThis chapter has th e following sections:\n/SM590000\u201cWhat is AI governance?\u201d on page 2\n/SM590000\u201cGovernance as a key enabler for realizing AI value\u201d on page 4\n/SM590000\u201cChallenges with governance of enterprise AI\u201d on page 6\n/SM590000\u201cAn example of legislation and standards related to AI\u201d on page 121\n\n2 Ensuring Trustworthy AI with IBM watsonx.governance1.1  What is AI governance?\nThere is not one formal definition of AI go vernance that is generally accepted. Many \norganizations have created a definition that focuses on specific elements of the overall AI governance picture. The one that, in the opinion ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 40,
      "total_chunks": 245
    }
  },
  {
    "text": "hat focuses on specific elements of the overall AI governance picture. The one that, in the opinion of the author, captures a broad spectrum succinctly best is this one fr om the Data & AI Alliance: \n___________________________________________________________________\n\u201cA system of rules, practices, processe s and tools that he lp an organization \nuse AI in alignment with its values and strategies, address compliance \nrequirements and drive trustworthy performance.\n1\u201d \n___________________________________________________________________\nIn other words, it is about two things. First, it is about the rules that an organization sets for \nthemselves to make sure their use of AI is profitable, compliant, secure and fair. And second, it is about the methods that an organization applies to ensure and document that those rules are followed using tools as the enabler.\nAI governance comprises a set of activities that run in parallel to the operational process of \ncreating, deploying and maintaining",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 41,
      "total_chunks": 245
    }
  },
  {
    "text": "f activities that run in parallel to the operational process of \ncreating, deploying and maintaining AI assets. Ta ble 1-1 contrasts the typical activities related \nto different aspects of an AI solution. Many of the activities in the AI governance  column will \nbe discussed in more detail in the following chapters of this Redbooks publication.\nTable 1-1   Contrast between typical Governance and Operations activities\n1  Data & Trust Alliance, IBM - T he urgency of AI governance, 2023Note:  Some clients also refer to AI govern ance as responsible governance. While AI \ngovernance focuses on the broader framework, responsible governance specifically highlights the ethical and societal implications.\nAspect of an AI \nsolutionTypical AI governance activities Typ ical AI operat ions activities\nFoundation models Review and approve new foundation models before \nthey're applied in your use cases.Acquire and host foundation models \nso they can be used when \ndeveloping AI solutions.\nUse cases Asse",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 42,
      "total_chunks": 245
    }
  },
  {
    "text": "cquire and host foundation models \nso they can be used when \ndeveloping AI solutions.\nUse cases Assess and review new use cases before starting \ndevelopment. \nList the expected risk mitigation and compliance \nmeasures that the technical and other stakeholders \nneed to apply.\nDetermine the level and ext ent of governance based on \nthe risk profile of the use case.Create an initial solution design and \narchitecture to support assessment \nand review.\nAI asset development Document the technical characteristics and \ndevelopment process of the AI assets.\nReview the developed AI assets for adequate risk \nmitigation and regulatory compliance.\nApprove for deployment.Create and evaluate the AI assets \nthat are needed to deliver a use case.\n\nChapter 1. Challenges and opportunities in AI governance 3Like any business practice, AI governance can benefit from applying the people, process and \ntechnology (PPT) framework. When applied to AI governance, this framework helps organizations ensure that th",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 43,
      "total_chunks": 245
    }
  },
  {
    "text": "gy (PPT) framework. When applied to AI governance, this framework helps organizations ensure that their AI systems are aligned with their strategic goals, are \ndeveloped and deployed responsibly, and deliver  value to stakeholders. Here is a breakdown \nof how the PPT framework might apply to AI governance\n2:\n/SM590000People: The first element of the PPT framework focuses on the people involved in the AI \ngovernance lifecycle, this includes technical experts, legal advisors, compliance officers, AI asset deployment Document the technical characteristics and \ndevelopment process of the AI deployments.\nReview the deployed assets for adequate risk mitigation \nand compliance.Deploy the AI assets to enable their \nday-to-day use.\nAI asset \npost-deploymentManage issues and incidents according to compliance \nand risk mitigation plans.\nReview and approve requests for changes to AI assets, \nincluding any changes to the required risk mitigation \nand compliance measures.\nReview and approve the deco",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 44,
      "total_chunks": 245
    }
  },
  {
    "text": "ng any changes to the required risk mitigation \nand compliance measures.\nReview and approve the decommissioning of AI assets.\nManage periodic attestations about use cases and AI \nassets.Setup automated monitoring of \ndeployed AI assets.\nManage the AI assets from a \ntechnical perspective.\nCreate and evaluate new versions \nof AI assets as needed.\nAI embedded in \nenterprise applicationsReview and approve new embedded AI capabilities \nbefore they're applied in your use cases.<not typically involved since these \ncapabilities co me to the \norganization prebuilt>\nCompliance with \nlegal obligations \nacross use casesGather evidence to demonstrate compliance with \nobligations that go across individual use cases (for \nexample: \u201cAI literacy\u201d in the EU AI Act)<not typically involved>\nRegulator \ninteractionsManage inbound (for example: a request for \ninformation) interactions with regulators.\nManage outbound (for example: registering high-risk AI \nuse cases) interactions with regulators.<not typical",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 45,
      "total_chunks": 245
    }
  },
  {
    "text": "utbound (for example: registering high-risk AI \nuse cases) interactions with regulators.<not typically involved unless \nadditional technical information is \nrequired for a specific AI solution.>\nRegulatory change Proactively assess how regulatory proposals related to \nAI might impact the organization.\nReview regulatory changes and determine with use \ncases are affected.\nManage the activities to bring use cases and AI assets \ninto compliance with the changed regulation.<not typically involved unless the \nregulatory change requires \nadditional technical work.>\nRisk control \nevaluationsAssess effectiveness of AI risk controls. <not typically involved>\nImplement AI \ngovernance policiesCreate and maintain compliance libraries, plans and \nassessment templates.\nCreate and maintain risk/control libraries, plans and \nassessment templates.<not typically involved>Aspect of an AI \nsolutionTypical AI governance activities Typ ical AI operat ions activities\n2  Based on IBM watsonx.ai chat interface,",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 46,
      "total_chunks": 245
    }
  },
  {
    "text": " governance activities Typ ical AI operat ions activities\n2  Based on IBM watsonx.ai chat interface, using the IBM Granite 13B Chat v2 model \n\n4 Ensuring Trustworthy AI with IBM watsonx.governancebusiness leaders, ethicists, and other stakeholders. In the context of AI governance, \npeople play a crucial role in ensuring that  AI systems are designed and used ethically, \ntransparently, and resp onsibly. This involves:\n\u2013 Building diverse and inclusive teams\n\u2013 Providing adequate training and support\n\u2013 Fostering a culture of ethical AI practices\n/SM590000Process: The second element of the PPT framework emphasizes the importance of \nwell-defined and repeatable processes for the governance of AI systems. In the context of AI governance, processes help organizations manage risks associated with AI, ensure compliance with regulations, and maintain the quality and reliab ility of AI solutions. This \nincludes:\n\u2013 Defining AI principles and policies\n\u2013 Establishing clear governance structures\u2013 Defi",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 47,
      "total_chunks": 245
    }
  },
  {
    "text": "is \nincludes:\n\u2013 Defining AI principles and policies\n\u2013 Establishing clear governance structures\u2013 Defining roles and responsibilities\n\u2013 Defining decision-making frameworks\n\u2013 Implementing robust change management processes\n/SM590000Technology: The third element of the PPT framework focuses on the technology \ninfrastructure and tools used to enact the principles, structures and processes in \nday-to-day work. In the context of AI governance, AI governance platforms provide a centralized and integrated view of AI systems, enabling organizations to govern the lifecycle of AI solutions, manage AI risks and ensure compliance. These platforms typically include features such as:\n\u2013 An inventory of use cases and AI assets\n\u2013 Automated workflows\u2013 Risk and legal assessments\n\u2013 Compliance plans\n\u2013 Issue management\u2013 Reporting and dashboarding \n1.2  Governance as a key enab ler for realizing AI value\nAI governance serves as a critical enabler for organizations to unlock the full value of AI in a \nresponsib",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 48,
      "total_chunks": 245
    }
  },
  {
    "text": "rnance serves as a critical enabler for organizations to unlock the full value of AI in a \nresponsible and trustworthy manner. As organizations adopt AI to achieve ambitious goals, governance provides the structure needed to scale these efforts responsibly and effectively. Executives are pushing for the adoption of AI in enterprise contexts, whic h differs significantly \nfrom consumer-focused applications.\nScaling AI to meet these objectives is only possible when governance frameworks are firmly \nin place. Without consistent governance, or ganizations risk operational inefficiencies, \ncompliance failures, brand reputation, and ethical concerns that can hinder AI adoption.\nThis section addresses three common concerns that people have about AI governance as an \nenabler of AI value.\n\nChapter 1. Challenges and opportunities in AI governance 51.2.1  Concern 1: Gover nance is a brake on AI\nIn some ways this is a true statement: governance does set boundaries in the application of \nAI and the",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 49,
      "total_chunks": 245
    }
  },
  {
    "text": "some ways this is a true statement: governance does set boundaries in the application of \nAI and then holds people to that.\nBut compare it to a car: what would happen if your car doesn't have brakes? How would you \neven make it out of your street without the precise control to stop at the first intersection? How would you drive on a highway if you could not make an emergency stop or reliably take an exit? How would you park your car? Without brakes you would actually never be able to \ndrive fast.\nWe have developed a governance system to make  sure millions of people can get in cars, \ntrucks and buses each day and get to their destination safely. This includes traffic laws, driver's licenses, speed limits, road si gns, technical safety devices and more.\nExtending that traffic metaphor to AI, AI governance helps you:\n/SM590000Educate your employees about the rules of the road so they don't need a huge level of \noversight every time they get in their car.\n/SM590000Train your employees how",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 50,
      "total_chunks": 245
    }
  },
  {
    "text": " need a huge level of \noversight every time they get in their car.\n/SM590000Train your employees how to drive their AI safely (for example through an Ethics by Design \nmethod)\n/SM590000Determine when it is safe to go fast (typically large numbers of AI use cases that an \norganization agrees are low- to no-risk).\n/SM590000Identify those use cases where extra care is required, and clearly lay out what the \nobligations are.\n/SM590000Implement guardrails in the extra dangerous ha irpin turns to catch AI mishaps even if the \ndriver is momentarily distracted.\n/SM590000Maintain your brakes by adjusting your policie s and procedures as AI  continues to evolve.\nLike brakes, AI governance, when applied properly, allows an organization to go fast most of \nthe time.\n1.2.2  Concern 2: Governance does not scale\nThat is absolutely true in the sense that manual and ad-hoc  governance doesn't scale.\nThrough the people, process, and technology ap proach referred to earlier, organizations can \nmake their",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 51,
      "total_chunks": 245
    }
  },
  {
    "text": "ugh the people, process, and technology ap proach referred to earlier, organizations can \nmake their AI governance more systematic and use technology to automate and scale.\nIBM takes this approach in our own AI governance, and we now create 100s of new and \ngoverned AI use cases per quarter. Governan ce can absolutely scale  to the level of a \ncomplex global company with large AI ambitions.\nSoftware can help scale AI governance through:\n/SM590000A single enterprise inventory  of AI use cases and assets that all parties can use for their \nspecific purposes. Note that this does not mean that all AI development needs to be done on a single platform.\n/SM590000Discovery  of any AI that is already in use but not yet registered (shadow AI).\n/SM590000Integration with systems of record  - depending on the type of business you are in, you will \nalready have an inventory of your IT systems, a product catalog and other systems of record. Use technology to leverage the information you already captu",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 52,
      "total_chunks": 245
    }
  },
  {
    "text": "ct catalog and other systems of record. Use technology to leverage the information you already capture there so you do not have to redo that for governance purposes.\n\n6 Ensuring Trustworthy AI with IBM watsonx.governance/SM590000Automated reports and dashboards  for specific roles and users.\n/SM590000Self-service methods  to business owners to register the relevant information about their \nuse cases. Such a data gathering can be maintained by and combine the needs of specialists (legal, ethics, security, etc.).\n/SM590000Automated assessment  of the risk mitigation and compliance requirements for a use case.\n/SM590000Automated workflows  to involve specialists, facilitate re views, resolve issues and manage \nescalation points.\n/SM590000A standardized method for technical teams to  capture metadata  about the AI that they are \ndeveloping.\n/SM590000Standardized monitoring  capabilities that are mapped to  the legal obligations and \ncorporate risk.\n/SM590000Integration with  AI platforms a",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 53,
      "total_chunks": 245
    }
  },
  {
    "text": " are mapped to  the legal obligations and \ncorporate risk.\n/SM590000Integration with  AI platforms and processes  to avoid duplicate work for the technical \nteams.\n/SM590000Integration with any adjacent tooling  where governance requirements might be fulfilled (for \nexample: security monito ring of AI systems).\n1.2.3  Concern 3: Governance does no t contribute to value generation\nGovernance is often approached, and justified, as a way to avoid losses such as fines, \ndamage to brand reputation or negative operational impact.\nWhile this  loss avoidance  can be of substantial value to an organization, authors from the \nNotre Dame - IBM Tech Ethics Lab make the case for a holistic ROI framework that also incorporates a value generation  perspective\n3 that includes amongst others:\n/SM590000Building unique organizational capabilities to take advantage of market opportunities more \nquickly.\n/SM590000Ability to attract and reta in talent through the or ganization's reputation.\n/SM590000A since",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 54,
      "total_chunks": 245
    }
  },
  {
    "text": "590000Ability to attract and reta in talent through the or ganization's reputation.\n/SM590000A sincere commitment to values-based leadership.\n1.3  Challenges with governance of enterprise AI\nWith the rise in popularity of ChatGPT4 and other consumer AI tools, organizations have been \nlooking for ways to apply thes e capabilities also to their bus iness environment. Like other \ntechnologies, transitioning from customer to business usage comes with specific challenges. This chapter describes the main challenges as they relate to the governance of an organization's AI initiatives.\n1.3.1  Generative AI has changed the governance game\nWhile business teams are smitten by the potential for business improvement, and the \ntechnical specialists are enamored by the latest technological breakthroughs, generative AI is \ndifferent (enough) from earlier forms of AI to approach them thoughtfully.\nThese differences impact the governance of the AI in the following ways:\n3  On the ROI of AI Ethics and Go",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 55,
      "total_chunks": 245
    }
  },
  {
    "text": "differences impact the governance of the AI in the following ways:\n3  On the ROI of AI Ethics and Governance Investme nts: From Loss Aversi on to Value Generation  \n4  OpenAIs consumer chatbot  \n\nChapter 1. Challenges and opportunities in AI governance 7/SM590000Generative AI enables use cases in new areas  of an organization increasing the \ncomplexity of governance - more use cases to review, more (types of) users to educate, \nmore room for accidental errors.\n/SM590000Generative AI carries amplified and new risks  such as bias in generated content, \nintellectual property concerns, hallucination, energy consumption and misinformation. \n/SM590000Generative AI has a different supply chain  - most organizations do not train their own large \nlanguage models, but leverage pre-trained models from commercial vendors and the open-source community. These external models need to be vetted before being applied in use cases. See Chapter 4, \u201cOnboarding a new foundation model\u201d on page 41 for more on",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 56,
      "total_chunks": 245
    }
  },
  {
    "text": "eing applied in use cases. See Chapter 4, \u201cOnboarding a new foundation model\u201d on page 41 for more on this topic.\n/SM590000Generative AI introduces different assets to govern  - organizations now routinely work with \nprompt templates, foundation models (FMs), and other AI assets. These assets come with different metadata and different ways to measure their performance and robustness. Chapter 6, \u201cGoverning the end-to-end lifecycle of an AI asset\u201d on page 61 for more on this topic.\n/SM590000Generative AI is evolving rapidly . ChatGPT  broke through to the mainstream in 2023, in \n2024 many organizations turned to retrieval-augmented generation (RAG) solutions and AI assistants in various forms, and 2025 looks to be the year of agentic AI . Each of these \ndevelopments also impacts how AI is governed. To help keep up, look for AI governance partners with first-hand experience in delivering these types of cutting-edge AI solutions.\nHaving said all that, generative AI is not repl acing machine",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 57,
      "total_chunks": 245
    }
  },
  {
    "text": "se types of cutting-edge AI solutions.\nHaving said all that, generative AI is not repl acing machine learning (ML) - traditional machine \nlearning remains a core technology in businesse s and continues to deliver value. Consistent \ngovernance is essential for both ML and generative AI to address overlapping and unique challenges effectively. \n1.3.2  Bring together dive rse stakeholder perspectives\nAI governance is not just a matter for the technical teams. It is an organizational team sport \nthat must be informed by a wide range of perspectives to address the diverse challenges associated with AI systems.\nFigure 1-1 shows different perspectives typically in volved in the AI-related \u201crules\u201d referred to \nin 1.1, \u201cWhat is AI governance?\u201d on page 2. \n\n8 Ensuring Trustworthy AI with IBM watsonx.governanceFigure 1-1   Comprehensive AI governance involves different stakeholder perspectives\nEven if an organization does not have a dedicated department for each of these, the more \nthese perspect",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 58,
      "total_chunks": 245
    }
  },
  {
    "text": " if an organization does not have a dedicated department for each of these, the more \nthese perspectives are represen ted in an AI governance framew ork, the more effective it will \nbe. Table 1-2 gives examples of which of the AI governance activities from in paragraph 1.1 are commonly informed by which perspectives. Organizations w ill need to create their own \nversion of such a mapping and define the roles and responsib ilities of all involved parties.\nTable 1-2   Mapping stakeholder perspectives to AI governance activities \nGovernance activity\nData Science / AI\nAI Ethics\nBusiness unit\nLegal / compliance\nRisk management\n(Cyber) Security\nPrivacyProcurementOther\nAssess and approve foundation models X X X X X X X\nAssess and approve use cases X X X X X X X\nGovern the development of AI assets X X X X X X X\nGovern the deployment of AI assets X X X X XGovern the post-deployme nt of AI assets X X X X X\nCompliance with legal obligations across \nuse cases (such as AI literacyXX X\nAssess and ap",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 59,
      "total_chunks": 245
    }
  },
  {
    "text": "X X X X X\nCompliance with legal obligations across \nuse cases (such as AI literacyXX X\nAssess and approve AI embedded in \nenterprise applicationsXXXXXXX\nRegulator interactions X\nRegulatory change XRisk control assessment X X X\n\n\nChapter 1. Challenges and opportunities in AI governance 9Given this multi-dimensional nature of AI governance, organizations should be aware of \ncommon gaps:\n/SM590000Communication gaps: AI engineers are not legal experts and vice versa, and the same \ngoes for the other stakeholders. To overcome this gap, consider:\n\u2013 Education programs to create a common base of understanding, without everyone \nhaving to know the full extent of the other's roles.\n\u2013 Automated workflows to integrate everyone's contributions into a unified framework.\n/SM590000Technology gaps: Indivi dual teams will be tempted to create  or deploy tooling for their \nspecific piece of the AI governance puzzle. While their efforts are of course with the best of intentions, it does create an increasi",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 60,
      "total_chunks": 245
    }
  },
  {
    "text": "ce puzzle. While their efforts are of course with the best of intentions, it does create an increasing in tegration problem as the company matures to \na more interconnected approach.\nTo overcome this gap, look for tooling that provides integrated support all of the activities and \nroles listed in the table above. Limit the use of bespoke integration to only those situations \nwhere generally available AI governance tooli ng does not have the integrated capabilities.\nIn summary, the challenge lies no t in providing isolated capab ilities to each group but in \nensuring that all these tools and workflows are interconnected effectively. For example:\n/SM590000Do the technical teams have a clear view of the compliance re quirements they will \nspecifically need to fulfill as they develop an AI solution?\n/SM590000Do risk teams have a clear view of the expos ure created by using the same AI models \nacross AI solutions that are created in-house and those that are purchased as part of an applicati",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 61,
      "total_chunks": 245
    }
  },
  {
    "text": " \nacross AI solutions that are created in-house and those that are purchased as part of an application?\n/SM590000Does the governance solution actively support technical teams in gathering technical \ndocumentation details in an easy way?\n/SM590000When the technical teams update an AI solu tion, will the legal teams see those changes \nand be able to re-assess them for any new compliance obligations?\nA holistic approach enables s eamless governance, fosterin g trust, accountability, and \nalignment across the organization.\n1.3.3  Technical comp lexity is increasing\nAs AI evolved from machine/deep learning to  generative AI, the technical complexity has \nincreased in the following ways:\nMore complex relationship bet ween use cases and AI assets\nIn machine learning and deep learning projects , typically there was a very tight one-to-one \nrelationship between the use case and the AI assets (models). Models were trained to perform a specific task, and if you had a different task, you would tra",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 62,
      "total_chunks": 245
    }
  },
  {
    "text": "els). Models were trained to perform a specific task, and if you had a different task, you would train another model. The use case comes first, and then the model(s) follow.\nWith generative AI and foundation models, this is typically no longer the case as these \nmodels are now pre-trained to handle a large variety of use cases. Not only does this mean a one-to-many relationship, but the relationship is also reversed: the model is already there before any specific use cases are considered. One result of this is that the use case has become more important as a governed item in its own right, separate from the (foundation) models applied to deliver a use case. Chapter 5 describes the governance considerations around use cases\n\n10 Ensuring Trustworthy AI with IBM watsonx.governanceMore AI assets\nIn machine learning and deep learning projects, typically the asset created was a trained \nmodel. Depending on the use case and the dat a, these can take many forms but they're \ngenerally all refer",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 63,
      "total_chunks": 245
    }
  },
  {
    "text": " Depending on the use case and the dat a, these can take many forms but they're \ngenerally all referred to as models .\nWith generative AI and foundation models, there are other types of assets used to make a \nspecific use case come to life. The primary way to interact with a foundation models is \nthrough a prompt, which is typically text-based instructions on what you want the model to do. For many enterprise use cases, these are not one-off but repeated interactions that take the form of a parametrized prompt template. For example, for a retrieval-augmented generation use case, an LLM is prompted each time a user query comes in. The prompt always has the same structure and core instru ctions, with parameters for t he user's query and the context \ndata that should be used to answer the query. See Figure 1-2 on page 10.\nFigure 1-2   Example of a parametriz ed prompt template for a RAG use case\nSo instead of training models , a lot of projects now involve  creating prompts .\nAdditionally",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 64,
      "total_chunks": 245
    }
  },
  {
    "text": " case\nSo instead of training models , a lot of projects now involve  creating prompts .\nAdditionally, organizations might decide to fine-tune a pre-trained model to make it fit their \nspecific needs better. There are different methods to do this that result in what is effectively a new model. It is derived from the base model but is a new object in its own right that needs the appropriate amount of oversight.\nLastly, other AI techniques such as prompt chaining (where different prompts are executed in \na defined sequence) might result in other assets to be governed.\nChapter 6, \u201cGoverning the end-to-end lifecycle of an AI asset\u201d on page 61 describes the \ngovernance considerations for different AI assets\nMore sources of AI\nIn machine learning and deep learning projects , organizations usually train their own models, \npotentially with assistance from external service providers, using either their data or trusted \nthird-party data.\nWith the advent of Generative AI and founda tion models, th",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 65,
      "total_chunks": 245
    }
  },
  {
    "text": "their data or trusted \nthird-party data.\nWith the advent of Generative AI and founda tion models, these models are typically sourced \nfrom third party as pre-trained models, such as IBM's Granite  family of models. However, \norganizations often lack adequate insight into the data used to train these external foundation models. Therefore, it is crucial for organizations to conduct a thorough assessment and review of the foundation model before adopting it for their own use. Chapter 4, \u201cOnboarding a new foundation model\u201d on page 41 describes the governance considerations when bringing in these externally developed models.\n\n\nChapter 1. Challenges and opportunities in AI governance 11These multi-purpose foundation models also spur on your technology providers to embed \nmore and more AI in any enterprise applications that an organization might use. Chapter 4, \u201cOnboarding a new foundation model\u201d on page 41 describes the governance considerations for AI embedded in business applicat ions. Ano",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 66,
      "total_chunks": 245
    }
  },
  {
    "text": "l\u201d on page 41 describes the governance considerations for AI embedded in business applicat ions. Another trend to watch is  the prebuilt AI capabilities \nin devices such as la ptops and smartphones.\n1.3.4  Regulatory and risk complexity is increasing \nIn this section we review regulatory compliance management and operational risk \nmanagement.\nRegulatory compliance management\nThe AI regulatory space is very dynamic: There are many AI-specific legislations in force or in \nprogress across the world, there are general regulations that also apply to AI and there are several options for volu ntary commitment schemes. \nEspecially for organizations that operate across multiple regions, the regulatory challenge is \nbecoming more complex quickly and one cannot expect everybody in an organization to be a regulatory expert for A I. As a result, the role of complianc e will become more prominent in the \n\u201cteam sport\u201d described in 1.3.2, \u201cBring togeth er diverse stakeholder perspectives\u201d on page 7.\nL",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 67,
      "total_chunks": 245
    }
  },
  {
    "text": "he \n\u201cteam sport\u201d described in 1.3.2, \u201cBring togeth er diverse stakeholder perspectives\u201d on page 7.\nLuckily, regulatory compliance management is a well-establis hed discipline, which can now \nalso be applied to AI. This includes actions such as:\n/SM590000Defining compliance requirements and obligations\n/SM590000Implementing legal assessment tools\n/SM590000Connecting AI use cases with mandates\n/SM590000Defining compliance plans\n/SM590000Managing legal obligations across use cases (for example, AI literacy)\n/SM590000Managing regulator interactions\n/SM590000Managing regulatory change\n/SM590000Regulatory reporting\n/SM590000Documenting and reporting on compliance status\nAs AI is set to impact more and more busin ess processes, AI governance becomes more \nthan just a legal check box. Organizations will need to de fine/update their risk frameworks \nand enterprise policies; especially as generative AI and agentic AI bring amplified and new risks to an organization.\nOperational risk management\nO",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 68,
      "total_chunks": 245
    }
  },
  {
    "text": "ve AI and agentic AI bring amplified and new risks to an organization.\nOperational risk management\nOperational risk management is also a well-established discipline, which can now also be applied to AI. This includes actions such as:\n/SM590000Defining AI risks and mitigation strategies\n/SM590000Implementing libraries of risks and controls\n/SM590000Implementing AI risk identification tools\n/SM590000Connecting AI use cases with business processes\n/SM590000Defining test plans\n/SM590000Managing loss events, loss impacts and loss recoveries\n/SM590000Documenting and reporting on risk assessments and mitigation strategies\n\n12 Ensuring Trustworthy AI with IBM watsonx.governance1.4  An example of legislati on and standards related to AI\nLike AI in general, legislation and standards around AI are evolving quickly. In this chapter we \ncover some headlines and refer the reader to th e respective sources of these legislations and \nstandards for their current state and planned further enhancements.\n",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 69,
      "total_chunks": 245
    }
  },
  {
    "text": "rces of these legislations and \nstandards for their current state and planned further enhancements.\n1.4.1  AI-specific legislation \nAcross the world, AI-specific legislation is being developed or has been enacted. This \nparagraph describes the key characteristics of one example.\nRegulation (EU) 2024/ 1689 - The EU AI Act\nThe European AI Act  went into force on August 1st, 2024, and is applicable to any \norganization placing AI on the market in the European Union (EU), regardless of their home base. That means that organizations headquartered elsewhere in the world do have to comply with the Act for AI systems that impact EU citizens, as consumers, employees or other \nroles.\nThe AI Act regulates two things:\n1. \u201cGeneral purpose AI models\u201d (the legal terminology for what we call foundation models in \nthis publication).\n\u2013 A special category is models with \u201c systemic risk \u201d - the really large models that can be \nused so widely that their potential impact is very extensive.\n2. \u201cAI systems\u201d\n\u2013",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 70,
      "total_chunks": 245
    }
  },
  {
    "text": " models that can be \nused so widely that their potential impact is very extensive.\n2. \u201cAI systems\u201d\n\u2013 Some uses of AI are classified as prohibited , they are not allowed to be placed on the \nmarket in the EU. Examples are AI systems that use subliminal techniques, enact forms of social scoring or employ untargeted scraping of facial images from the internet or CCTV footage.\n\u2013 Some uses of AI are classified as high risk , they are allowed but come with a set of \nrequirements to ensure they don't violate the rights of EU citizens. Some of these \ndefined uses are industr y-specific (for exampl e in utilities, financial services and public \nsector) and some are horizontal (for example certain use cases in HR or education).\n\u2013 Some uses of AI are classified as having transparency  risk, they are also allowed and \ncome with obligations to disclose that a person  is interacting with an  AI system, or that \ncontent is created by an AI system.\nAn organization\u2019s legal obligatio ns depend on the cl",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 71,
      "total_chunks": 245
    }
  },
  {
    "text": ", or that \ncontent is created by an AI system.\nAn organization\u2019s legal obligatio ns depend on the classification  of the system or the model, \nand the role that an organization plays with respect to that system or model (such as provider, deployer, others).\nThe Act defines penalties for:\n/SM590000Bringing prohibited AI systems onto the market.\n/SM590000Not meeting the obligations for high-risk use cases and/or general purpose AI models.\n/SM590000Providing incorrect, incomplete or misleading information to notified bodies or national \ncompetent authorities in reply to a request. \nFor large enterprises, the fines can be as high as  35 million euro or 7% of its total worldwide \nannual turnover for the preceding fi nancial year, whichever is higher.\nWhile the Act is in force sinc e August 2024, subsets of obligat ions will apply fr om different \ndates:\n\nChapter 1. Challenges and opportunities in AI governance 13/SM590000February 2025: prohibited AI systems, AI literacy requirements\n/SM5900",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 72,
      "total_chunks": 245
    }
  },
  {
    "text": "s in AI governance 13/SM590000February 2025: prohibited AI systems, AI literacy requirements\n/SM590000August 2025: obligations for providers of general purpose AI models\n/SM590000August 2026: obligations for high-risk AI systems\n/SM590000August 2027: obligations for high-risk AI systems \u201c intended to be used as a safety \ncomponent of a product, or the AI system is  itself a product, covered by the Union \nharmonisation legislation5\u201d (for example toys or medical devices)\nOther AI-specific legislation includes, for example:\n/SM590000Act on the Development of Artificial Intelligen ce and Establishment of Trust (AI Basic Act) \n(South Korea)  \n/SM590000Local Law 144 regarding automated employment decision tools (US, New York City)  \n/SM590000Executive Order on the Safe, Secure, and Trustworthy Development and Use of AI (US)  \n/SM590000Artificial Intelligence and Data Act (Canada)  \n/SM590000AI regulation: a pro-innovation approach (UK)  \n1.4.2  General regulations that apply to AI\nBesides th",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 73,
      "total_chunks": 245
    }
  },
  {
    "text": " regulation: a pro-innovation approach (UK)  \n1.4.2  General regulations that apply to AI\nBesides the evolving AI-specific regulations there are also many general regulations that \napply to AI use cases and systems in areas such as:\n/SM590000Non-discrimination\n/SM590000Privacy protection \n/SM590000Data protection\n/SM590000Product liability\n/SM590000Fair advertising\n/SM590000Industry-specific regula tions, such as rules around financial advice\n1.4.3  Technical standa rds for AI governance\nIn addition to regulations, there are various initiatives underway to define technical standards \nfor AI systems. The standards define the state-of-the-art tools and methods that can be \napplied when creating and using AI systems.\nFrom a governance perspective, these stan dards will provide a bas eline of \u201cgenerally \naccepted\u201d practices that organizations are encouraged to adopt.\nTechnical standards suppor ting the EU AI Act \nThe European Commission has requested the development of a set of technical s",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 74,
      "total_chunks": 245
    }
  },
  {
    "text": "or ting the EU AI Act \nThe European Commission has requested the development of a set of technical standards \ncovering the following requirements:\n/SM590000Risk management system for AI systems.\n/SM590000Governance and quality of datasets used to build AI systems.\n/SM590000Record keeping thro ugh logging capabilities by AI systems.\n/SM590000Transparency and information provisions for users of AI systems.\n/SM590000Human oversight of AI systems.\n/SM590000Accuracy specifications for AI systems.\n/SM590000Robustness specificatio ns for AI systems.\n5  European Union, Article 6: Classification Ru les for High-Risk AI Systems  (Section 1, Paragraph (a))\n\n14 Ensuring Trustworthy AI with IBM watsonx.governance/SM590000Cybersecurity specifications for AI systems.\n/SM590000Quality management systems for providers of AI systems, including post-market \nmonitoring processes.\n/SM590000Conformity assessment for AI systems.\nTwo European standardization bodies (CEN/CENELEC) have accepted that request and",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 75,
      "total_chunks": 245
    }
  },
  {
    "text": "ent for AI systems.\nTwo European standardization bodies (CEN/CENELEC) have accepted that request and \nhave formed a joint technical committee (\u201cJTC21\u201d) for the development and adoption of standards for AI and related data. \nThese technical stand ards will become important co mpliance tools since they \u201c will grant  a \nlegal presumption of conformity to AI systems develo ped in accordan ce with them.\u201d \nIn other words: if you build your AI systems to the specifications in these technical standards, the EU will assume your system is  in conformity with the AI Act.\nThe following list of organizations provides useful information related to AI and AI governance \nstandards.\n/SM590000NIST RFM \nNational Institute of Standards and Technology - AI Risk Management Framework\n/SM590000ISO\nInternational Organizati on for Standard ization - Artificial Intelligence\n/SM590000OWASP Top 10 for Large La nguage Model Applications\nOpen Web Application Se curity Project (OWASP)\nNote:  Given the evolving nature",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 76,
      "total_chunks": 245
    }
  },
  {
    "text": "e Model Applications\nOpen Web Application Se curity Project (OWASP)\nNote:  Given the evolving nature of AI governance standards, a flexible approach is crucial. \nRegularly review and adopt new or updated standards as they emerge to ensure your AI systems align with the latest best practices.\n\n\u00a9 Copyright IBM Corp. 2025. 15Chapter 2. Introduction to \nIBM watsonx.governance\nThis chapter provides an overview of the watsonx platform, its core components, and \nfeatures.\nThis chapter includes the following topics:\n/SM590000\u201cIntroduction to the IBM watsonx platform and its core components\u201d on page 16\n/SM590000\u201cIntroduction to IBM watsonx.ai\u201d on page 17\n/SM590000\u201cIntroduction to IBM watsonx.data\u201d on page 19\n/SM590000\u201cIntroduction to IBM watsonx.governance\u201d on page 202\n\n16 Ensuring Trustworthy AI with IBM watsonx.governance2.1  Introduction to the IBM watsonx platform and its core \ncomponents\nOrganizations today face a common challenge: ac cessing data that is trapped in silos across \nvarious s",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 77,
      "total_chunks": 245
    }
  },
  {
    "text": "anizations today face a common challenge: ac cessing data that is trapped in silos across \nvarious systems. Business applications typically store their data in application-specific sources, which often leads to duplication across multiple systems to meet  different application \nneeds. This fragmentation results in disconnected datasets that are not easily accessible to the broader enterprise ecosyst em while still maintaining appr opriate access rights. As a \nresult, this limitation hinders opportunities for gaining insights and fostering innovation.\nIBM AI and data platforms enable organizations to access and integrate isolated datasets for \nanalytics purposes, en suring that the data is of high quality and trus tworthy. This capability \nallows companies to develop responsible and ethical solutions, fostering the creation of new products, offerings, and opportunities for business growth.\nThe IBM watsonx platform, along with IBM Cloud Pak for Data, is built on the robust \nfoundation of",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 78,
      "total_chunks": 245
    }
  },
  {
    "text": ".\nThe IBM watsonx platform, along with IBM Cloud Pak for Data, is built on the robust \nfoundation of Red Hat OpenShift. This next-generation AI and data platform enables enterprises to develop and deploy AI applications, access data stored in legacy systems, catalog data while implementing governance pr actices, and improve their understanding of \ndata quality. It also allows organizations to establish policies and rules related to data privacy, ensuring that end-to-end AI solutions are ethical, responsible, and trustworthy.\nAccessing high-quality datasets can be challenging in organizations when the data assets are \nlocked in silos. The key for organizations to gain insights and bring innovation to their business is leveraging these data assets but requires a platform that enables access to data assets residing in silos, integrate the data assets, and build and deploy AI assets in an ethical and responsible way. The IBM watsonx platform consists of three core components\nFigure 2-1 on ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 79,
      "total_chunks": 245
    }
  },
  {
    "text": "hical and responsible way. The IBM watsonx platform consists of three core components\nFigure 2-1 on page 17 illustrates the components of the watsonx platform.\n\nChapter 2. Introduction to IBM watsonx.governance 17Figure 2-1   IBM watsonx platform and its core components -.ai, .data, .governance\nThis platform has three components:\n/SM590000IBM watsonx.ai: An enterprise-grade AI studio that helps operationalize and scale the \ndevelopment of AI applications by bringing together traditional machine learning and generative AI capabilitie s with high-quality data across the AI lifecycle. With watsonx.ai, AI \ndevelopers can build, train, adapt, and tune models with your enterprise data and operationalize the models to generate insights, support tasks, and automate business workflows.\n/SM590000IBM watsonx.data: A fit-for-purpose data lakehouse service, that makes it possible for \nenterprises to scale AI workloads using all th eir data optimized for governed data and AI \nworkloads. It serves as",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 80,
      "total_chunks": 245
    }
  },
  {
    "text": "scale AI workloads using all th eir data optimized for governed data and AI \nworkloads. It serves as a data source for AI, enabling the enterprise ecosystem to access trusted and quality data while enforcing policies and rules for data privacy and security. watsonx.data supports querying, governance, and open data formats to access and share data for different AI use cases such as Retrieval Augmented Generation (RAG). It is based on open-source technologies, including Presto, Iceberg, and Milvus.\n/SM590000IBM watsonx.governance: An end-to-end solution for AI governance to enable \nresponsible, transparent, and explainable AI  workflows in addition to monitoring and \nevaluation capabilities that allo w you to keep track of yo ur entire AI landscape. \nIBM watsonx.governance helps business analysts understand the trustworthiness of their AI solutions.\n2.2  Introduction to IBM watsonx.ai\nIBM watsonx.ai is a cutting-edge AI platform that empowers organizations to scale and accelerate their A",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 81,
      "total_chunks": 245
    }
  },
  {
    "text": "watsonx.ai is a cutting-edge AI platform that empowers organizations to scale and accelerate their AI initiatives. \n\n\n18 Ensuring Trustworthy AI with IBM watsonx.governanceOffering a comprehensive suite of tools including a foundation model library, enterprise grade \nstudio, machine learning frameworks, and a runtime serving environment. watsonx.ai enables enterprises to build, train, and deploy models with ease. As a core component of the larger IBM watsonx platform, it drives AI-driven transformation across multiple industries.\nKey components of watsonx.ai\nThe following list describes the key components of watsonx.ai:\n/SM590000Foundation models: IBM watsonx.ai provides access to large-scale, pre-trained foundation \nmodels designed for various AI tasks. These models, such as generative AI and large language models (LLMs), can understand and generate human-like text, making them suitable for applications like customer service automation, content generation, data analysis and more.\n/SM5",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 82,
      "total_chunks": 245
    }
  },
  {
    "text": " for applications like customer service automation, content generation, data analysis and more.\n/SM590000Generative AI: Generative AI in watsonx.ai enables businesses to create new content, \ngenerate insights, automate workflows and et c. With the capability to  train custom models, \nwatsonx.ai empowers organizations to build personalized AI-driven solutions.\n/SM590000Machine Learning and ModelOps: watsonx.ai supports the entire AI model development \nlifecycle, from data preparation and training to deployment and monitoring. Through ModelOps, watsonx.ai ensures that models are optimized, accountable, and compliant with industry standards.\n/SM590000Data Science and Analytics: With advanced data science tools, watsonx.ai allows \norganizations to analyze and us e data for training AI models. These ca pabilities provide a \nstrong foundation for data-driven decision-making.\nIBM watsonx.ai in enterprise workflows\nIBM watsonx.ai integrates seamlessly with existing enterprise workflows. By off",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 83,
      "total_chunks": 245
    }
  },
  {
    "text": "enterprise workflows\nIBM watsonx.ai integrates seamlessly with existing enterprise workflows. By offering pre-built \nconnectors and APIs, the platform allows businesses to embed AI models into their operational workflows quickly, tailoring solutions to meet specific requirements.\nIBM watsonx.ai as a develope r and data scientist toolkit\nIBM watsonx.ai provides a flexible toolkit for developers, AI engineers, and data scientists, \nsupporting coding and no/low-coding environments. Developers can leverage SDKs, APIs, and workflows in their preferred programming languages, while non-coders benefit from \nintuitive natural language processing tools. wa tsonx.ai also supports IDEs like RStudio and \nPython Notebooks, accelerating AI model production.\nDomains where watsonx.ai is useful\nIBM watsonx.ai has been widely adopted across industries, supporting use cases such as natural language processing, image recognition, fraud detection, and predictive analytics. Key industry applications include:",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 84,
      "total_chunks": 245
    }
  },
  {
    "text": "ng, image recognition, fraud detection, and predictive analytics. Key industry applications include:\n/SM590000Banking and Financial Services: Automating processes such as credit scoring, fraud \ndetection, and risk assessment, whic h improves accuracy and efficiency.\n/SM590000Healthcare: Enhancing diagnostics, person alizing treatment plans, and optimizing \nworkflows using large AI models.\n/SM590000Retail: Improving customer experience through personalized recommendations (next best \noffer), basket analysis, inventory management, and automated support systems.\n/SM590000Manufacturing: Streamlining production processes, predictive maintenance, and quality \ncontrol to improve efficiency and reduce downtime.\n/SM590000Transport: Enhancing route optimization, fleet management, and safety systems for \nimproved logistics and customer service.\n\nChapter 2. Introduction to IBM watsonx.governance 19/SM590000Leisure and Luxury: Personalizing customer experiences, optimizing inventory, and \nenhancing",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 85,
      "total_chunks": 245
    }
  },
  {
    "text": "SM590000Leisure and Luxury: Personalizing customer experiences, optimizing inventory, and \nenhancing marketing strategies to meet customer preferences.\nAdvantages of watsonx.ai\nThe advantages of watsonx.ai include the following items:\n/SM590000Dependability: All models offere d within the watsonx.ai pla tform are thoroughly tested, \nresulting in highly robust models  that deliver consistent results.\n/SM590000Transparent Accountability: All Granite models (IBM\u2019s propri etary family of models) are \ntrained exclusively on open -source data, ensuring no hi dden liabilities or legal \ncomplexities when used  in AI solutions.\n/SM590000Scalability: Supports large AI  models and integrates seam lessly with data platforms to \nscale AI operations.\n/SM590000Flexibility: Offers customization options, allo wing AI models to be tailored to specific \nbusiness needs. Bring your own foundation model and upload to watsonx.ai to accomplish a range of industry and domain-specific generative AI use cases.\n/",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 86,
      "total_chunks": 245
    }
  },
  {
    "text": "pload to watsonx.ai to accomplish a range of industry and domain-specific generative AI use cases.\n/SM590000Enhanced Governance: Ensures responsible development and deployment of AI \nsolutions.\n/SM590000Faster ROI realization: Accelerates AI solution development and deployment, enabling \nbusinesses to quickly realize benefits.\nConclusion\nIBM watsonx.ai represents a significant advancement in AI, combining the creativity of generative AI with the precision of traditional machine learning. Its comprehensive suite of tools and strong focus on governance enable businesses to accelerate AI initiatives responsibly and at scale. As AI continues to evolve, watsonx.ai stands at the forefront, helping organizations unlock new possib ilities and drive impactful inno vations across many sectors.\n2.3  Introduction to IBM watsonx.data\nIBM watsonx.data provides a modern, open data  lakehouse architecture that integrates \nseamlessly across on-premises and multi-cloud environments. Its design enables o",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 87,
      "total_chunks": 245
    }
  },
  {
    "text": "re that integrates \nseamlessly across on-premises and multi-cloud environments. Its design enables organizations to manage all types of data workloads, from traditional analytics to AI training.\nCore features of watsonx.data\nThe following list highlights the core features of watsonx.data:\n/SM590000High-performance Data Querying and Analytics: Provides rapid querying capabilities for \nlarge datasets, empowering businesse s to extract insights efficiently.\n/SM590000Built-in Governance and Security: Ensures compliance with data privacy and security \nregulations across multi-cloud environments.\n/SM590000Cost Optimization: Reduces data warehousing costs by up to 50%, offering a more \neconomical solution for data storage and processing.\n/SM590000Shared Metadata Layer:  Facilitates seamless data acce ss and operations, streamlining \nworkflows and improving data consistency.Note:  For more information on watsonx.ai refer to IBM Redbooks Simplify Your AI Journey: \nUnleashing the Power of AI wit",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 88,
      "total_chunks": 245
    }
  },
  {
    "text": "mation on watsonx.ai refer to IBM Redbooks Simplify Your AI Journey: \nUnleashing the Power of AI with IBM watsonx.ai , SG24-8574.\n\n20 Ensuring Trustworthy AI with IBM watsonx.governanceKey features\nIBM watsonx.data key features include the following items:\n/SM590000Scalable Data Lakehouse: wats onx.data blends the flexibilit y of data lakes with the high \nperformance of data warehouses. It is de signed for hybrid cloud environments and \nsupports open data formats like Parquet and ORC.\n/SM590000SQL-based Querying: Enables users to exec ute high-speed analytics with SQL, allowing \ndata engineers and analysts to hand le extensive datasets effectively.\n/SM590000Machine Learning Integration: Seamlessly integrates with machine learning workflows, \nenabling the development and deployment of AI models using real-time data streams. This integration enhances decision-making and automates processes within organizations.\n2.4  Introduction to IBM watsonx.governance\nIBM watsonx.governance is a dedic",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 89,
      "total_chunks": 245
    }
  },
  {
    "text": " within organizations.\n2.4  Introduction to IBM watsonx.governance\nIBM watsonx.governance is a dedicated application for governance of AI to help \norganizations ensure that their use of AI is profitable, compliant, secure and fair.\n2.4.1  Key capabilities\nThe key capabilities of watsonx.gov ernance include th e following items:\n/SM590000Maintaining an inventory of all AI inside an organization, including self-built solutions, AI \nembedded in your enterprise application and AI embedded in your products and services.\n/SM590000Automated AI discovery - through integration with IBM Guardium AI Security, you can \nidentify AI deployments that are not yet registered and apply the appropriate level of governance.\n/SM590000Role-based dashboards.\n/SM590000Identify legal obligations for your use cases.\n/SM590000Identify AI risks for your use cases, leveraging the IBM AI Risk Atlas  created by \nIBM Research and the IBM AI Ethics Board .\n/SM590000Automate your use case review processes.\n/SM590000Aut",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 90,
      "total_chunks": 245
    }
  },
  {
    "text": "esearch and the IBM AI Ethics Board .\n/SM590000Automate your use case review processes.\n/SM590000Automate your third-party foundation model onboarding processes.\n/SM590000Automate your lifecycle governance processes such as model reviews and model change \nrequests.\n/SM590000Automate your regulatory compliance management processes such as managing \nregulatory chance and managing regulator interactions.\n/SM590000Manage legal obligations across use cases (for example, AI literacy).\n/SM590000Automate your operational risk management processes such as managing loss events, \nloss impacts and loss recoveries.\n/SM590000AI documentation through capture of metadata about the (versions of) AI assets you build.\n/SM590000Quantitative evaluations of your AI assets - dozens of pre-built metrics to measure model \nhealth, drift, quality, toxic language, PII, fairness, adversarial robustness of both machine learning and generative AI use cases.\n/SM590000Global and local explainability of machine learnin",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 91,
      "total_chunks": 245
    }
  },
  {
    "text": "ne learning and generative AI use cases.\n/SM590000Global and local explainability of machine learning models.Note:  For more information on watsonx.data refer to IBM Redbooks Simplify Your AI \nJourney: Hybrid, Open Data Lakehouse with IBM watsonx.data,  SG24-8570.\n\nChapter 2. Introduction to IBM watsonx.governance 21/SM590000Standard and ad-hoc reporting.\n/SM590000Integration with AI platforms such as IBM watsonx.ai, Amazon SageMaker and Bedrock, \nGoogle Vertex AI, and Microsoft Azure.\n/SM590000Configure the solution to fit your specific requirements.\n2.4.2  Use cases\nIBM watsonx.governance is a comprehensive governance solution. Depending on a \ncustomer's situation, they could use the solution for one or more of the following use cases: \n/SM590000Comply with the EU AI Ac t or other legislation.\n/SM590000Mitigate reputational and operational risks from their use of AI.\n/SM590000Govern the onboarding of AI-enabled enterprise applications.\n/SM590000Govern the development of AI-enabled pr",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 92,
      "total_chunks": 245
    }
  },
  {
    "text": "e onboarding of AI-enabled enterprise applications.\n/SM590000Govern the development of AI-enabled products and services.\n/SM590000Integrate AI risks into the op erational risk management of AI -enabled business processes.\n/SM590000Set up regular monitoring of an already deployed AI solution.\n2.4.3  Benefits of watsonx.governance\nBy centralizing and automating governance of AI, customers achieve benefits such as:\n/SM590000Bring together all technical and non-technical stakeholders into a common governance \nframework.\n/SM590000Decrease the risk of being fined or damaging your brand reputation.\n/SM590000Reduce the cost of compliance.\n/SM590000Free up time to deploy more models.\n/SM590000Capture model benefits earlier by reducing the time to deployment.\n/SM590000Recapture model benefits lost due to model drift.\nAs mentioned in 1.2.3, \u201cConcern 3: Governance does not contribute to value generation\u201d on \npage 6, using AI governance software also contributes to setting up an organization for th",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 93,
      "total_chunks": 245
    }
  },
  {
    "text": "tion\u201d on \npage 6, using AI governance software also contributes to setting up an organization for the value generation benefits mentioned in that paragraph.\n2.4.4  Synergy between watsonx .data and watsonx.governance \nIBM watsonx.data and IBM watsonx.governance provide a holistic solution for organizations. \nWhile watsonx.data focuses on data analysis and AI workloads, watsonx.governance ensures that all data is managed responsibly, securely , and in compliance with  relevant regulations.\nBy combining the strengths of both components, organizations can develop a unified data \nstrategy that balances innovation with accountability.\n2.4.5  Synergy between wats onx.ai and watsonx.governance\nThe synergy between IBM watsonx.ai and IBM watsonx.governance makes it easy for AI \ndevelopers to contribute to AI governance processes. The integration between the two components automates even more of the work, for example:\n/SM590000Model metadata is automatically captured.\n/SM590000Model evaluations ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 94,
      "total_chunks": 245
    }
  },
  {
    "text": "he work, for example:\n/SM590000Model metadata is automatically captured.\n/SM590000Model evaluations can be easily set up from their development workspaces (UI or \nprogrammatic).\n\n22 Ensuring Trustworthy AI with IBM watsonx.governance/SM590000Inferencing payload data can be auto-logged for fully automated runtime model \nmonitoring.\nThis comprehensive approach allows organizations to balance rapid innovation with the \nresponsibility of managing AI ri sks effectively. Th e combined capabilit ies help businesses \nstreamline their workflows and maintain an audit trail, enabling a seamless interplay between AI development and AI governance.\n2.5  Reference architecture\nFigure 2-2 illustrates a reference architecture for an end-to-end AI and da ta solution that \nleverages the IBM watsonx platform and IBM Cloud Pak for Data. This architecture is designed to build, test, deploy, manage, govern, and consume AI solutions across the enterprise.\nFigure 2-2   Reference architecture for integrated AI ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 95,
      "total_chunks": 245
    }
  },
  {
    "text": "d consume AI solutions across the enterprise.\nFigure 2-2   Reference architecture for integrated AI  and data platform for an enterprise using IBM watsonx\n2.5.1  Data Onboarding\nThe left side of Figure 2-2 illu strates the existing and legacy data sources that drive the \norganization's day-to-day operations. These data sources are often designed and developed for specific purposes and are accessible only by a select group of users or applications. \nTypically, these existing and legacy data so urces are built in silos, lacking a clear \nunderstanding of the relationships between the data assets they contain. To resolve this, \norganizations can use watsonx.data to store structured, semi-structured, and unstructured data and make it directly accessible  for AI and business intelligence (BI).\nTo integrate these disjointed data sources, we can onboard them onto IBM's open \narchitecture lakehouse, watsonx.data, which combines elements of a data warehouse and data lakes. It offers a unified pl",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 96,
      "total_chunks": 245
    }
  },
  {
    "text": "se, watsonx.data, which combines elements of a data warehouse and data lakes. It offers a unified platform where users can store data or connect data sources to manage and analyze enterprise data.\nIBM watsonx.data allows two approaches for onboarding data into the platform:\n/SM590000Accessing the data in place.\n\n\nChapter 2. Introduction to IBM watsonx.governance 23/SM590000Replicating the data onto the platform in Iceberg open data format using various extract, \ntransform, and load (ETL) options.\nIBM watsonx.data allows users to access data in existing data warehouses and data lakes \nthrough predefined platform connectors, including Teradata, Snowflake, SingleStore, SQL Server, PostgreSQL, MySQL, MongoDB, IBM Db2, and IBM Netezza. This solution reduces data duplication, and the costs associated with storing data in multiple locations.\nSuppose existing data is stored in an external storage system such as IBM Cloud Object \nStore, Amazon S3, IBM Storage Ceph, MinIO, HDFS (in Hadoop/Cloude",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 97,
      "total_chunks": 245
    }
  },
  {
    "text": "e system such as IBM Cloud Object \nStore, Amazon S3, IBM Storage Ceph, MinIO, HDFS (in Hadoop/Cloudera), Google Cloud Storage, or Azure Data Lake Storage. In that case, it can be accessed directly if the data is in Iceberg or Delta Format. If the data is stored in other common formats like Parquet or CSV, it can be accessed in its native format, followed by running ETL jobs to convert it to the open format like Iceberg and brought into the platform.\nCirata is an IBM partner that developed a cloud migration solution that automates the \nseamless transfer of continuous HDFS data and Hive metadata to watsonx.data.\nIBM watsonx.data supports loading data from existing on-premises data lakes using ETL jobs \ndeveloped with DataStage or Spark. Additionally, data loading can be done through a web console or command line interface.\nExisting data sources like Db2 for z/OS, IMS, and VSAM on the mainframe can be replicated \nusing IBM Data Gate in the Iceberg open data format within watsonx.data. Dat",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 98,
      "total_chunks": 245
    }
  },
  {
    "text": "rame can be replicated \nusing IBM Data Gate in the Iceberg open data format within watsonx.data. Data Gate is a replication technology that sy nchronizes data from IBM Z to various hybrid-cloud targets.\nThe architecture of watsonx.data enforces schema and da ta integrity, facilitating the \nimplementation of robust data security and governance mechanisms. Integrating watsonx.data with IBM Knowledge Catalog on Cloud Pak for Data provides knowledge \nworkers with self-ser vice access to data as sets, allowing them to utilize these assets to gain \ninsights. Once the data from these sources is onboarded into the platform in its raw format, the data assets from watsonx.data are imported into a governed catalog in IBM Knowledge Catalog, where they can be enriched with business semantics by mapping business terms to technical data assets (such as database tables and columns). Data quality can be assessed through profiling and runn ing data quality analyse s. Data protection ru les are establish",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 99,
      "total_chunks": 245
    }
  },
  {
    "text": "assessed through profiling and runn ing data quality analyse s. Data protection ru les are established to \ncontrol access to sensitive data in governed catalogs, which can include denying access, redacting columns, obfuscating columns, substituting columns, or filtering rows.\nIn addition, semi-structured and unstructured data and documents can be processed, split, \nand stored in the watsonx.data vector database, Milvus, along with their metadata and vector embeddings in the same datastore. Milvus is designed to store, index, and manage embedding vectors used for similarity search and retrieval-augmented generation, empowering embedding similarity search and AI applications.\n2.5.2  Data Preparation\nBusinesses can derive a greater value by integrating data from various sources and domains into higher layers, such as t he silver and gold layers of a medallion data architecture. \nAdditionally, watsonx.data provides multiple query engines, including Presto and Spark, \nallowing users to sele",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 100,
      "total_chunks": 245
    }
  },
  {
    "text": "y, watsonx.data provides multiple query engines, including Presto and Spark, \nallowing users to select the most suitable engine based on the characteristics of their workload. IBM watsonx.data seamlessly integrates with Db2 Warehouse and Netezza Performance Service, facilitating data sharin g across these products and enabling users to \nleverage the most appropriate engine for each specific task.\n\n24 Ensuring Trustworthy AI with IBM watsonx.governanceOnce structured and unstructured data is onboarded to watsonx.data in the watsonx platform, \nusers can access the data assets as long as they have the necessary access rights. They can build, train, tune, and deploy tr aditional ML models or utilize ge nerative AI mode ls to enhance \ntheir business use cases with AI.\n2.5.3  AI Buildin g and Deployment\nIBM watsonx.ai provides low-code and no-code tools, including Auto AI for creating traditional \nmachine learning models, Prompt Lab for prompt engineering, and Prompt Studio for adapting and ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 101,
      "total_chunks": 245
    }
  },
  {
    "text": "nal \nmachine learning models, Prompt Lab for prompt engineering, and Prompt Studio for adapting and fine-tuning generative AI models. Additionally, watsonx.ai offers a Studio and a Python Software Development Kit (SDK) designe d for data scientists and AI engineers to \nbuild, train, adapt, fine-tune, and deploy both traditional machine learning and generative AI assets.\n2.5.4  AI Lifecycle Management and Governance\nThe IBM watsonx platform enables end-to-end AI lifecycle management and governance. IBM recommends following an end-to-end process to build AI solutions that are ethical and responsible - secure, safe, transparent, and trustworthy, as shown in Figure 2-3. \nFigure 2-3   End-to-end flow for AI life cycle and governance powered by IBM watsonx\nIn this approach, an AI Use Case Owner or Requestor starts by creating an AI use case in \nwatsonx.governance. This involves outlining the business purpose of the use case and \ndetailing how AI will be utilized to achieve the intended outco",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 102,
      "total_chunks": 245
    }
  },
  {
    "text": "usiness purpose of the use case and \ndetailing how AI will be utilized to achieve the intended outcomes.\nNext, the AI Use Case Owner identifies potential risks associated with the AI use case by \nanswering a set of questions and conducts an initial risk assessment within watsonx.governance. Once this risk identification and assessment are completed, input from legal, HR, the AI Ethics Council, operations, security, and finance is gathered. Based on the risk analysis and risk profiles associated with the use case, a decision is made to approve or \nreject the development of the AI use case and its related assets.\nIf the use case is approved for development, the AI Developer, which can be an AI Engineer \nor Data Scientist, collaborates with a Data Engineer to shape and prepare the dataset in watsonx.data for either training traditional ML models or tuning large language models.\n\n\nChapter 2. Introduction to IBM watsonx.governance 25Following the data preparation, the AI Developer can build",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 103,
      "total_chunks": 245
    }
  },
  {
    "text": " Introduction to IBM watsonx.governance 25Following the data preparation, the AI Developer can build and train traditional ML models or \nadapt or tune for generative AI models specific to the use case. The AI Developer then validates these traditional ML models and AI assets leveraging generative AI models and test data to evaluate performance metrics such as F1 score and ROUGE, as well as fairness, explainability, and model health (including latency, number of  transactions, and token count).\nSubsequently, a Model Validator-an independent Data Scientist or AI Engineer- evaluates the \ntraditional ML model or Generative AI assets in a pre-production environment using production-like data within watsonx.governance.\nAn AI Risk Reviewer then examines the evalua tion performance metrics, risk scorecard, and \nmodel health metrics. Based on this final risk assessment, the reviewer either approves or rejects the AI asset for deployment in production within watsonx.governance.\nFinally, the Mode",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 104,
      "total_chunks": 245
    }
  },
  {
    "text": "es or rejects the AI asset for deployment in production within watsonx.governance.\nFinally, the ModelOps Engineer deploys the approved model into production using CI/CD \npipelines and activates ongoing AI monitoring to track key metrics, such as runtime quality, drift, fairness, explaina bility, and other relevant indicato rs specific to the use case. The \nongoing monitoring metrics are published in the Governance Console in watsonx.governance. Each metric has associated thresholds, and alerts can be configured in the Governance Console to notify relevant stakeholders, such as the AI Use Case Owner or AI Risk Reviewer, when these thresholds are br eached. Once the appropriate parties receive \nnotifications about threshold breaches, they can initiate the investigation process or follow the procedures for issue and change management, which may involve redeveloping a new version or completely different AI asset.\nFacts, documentation, and evidence regarding AI assets are collected and stor",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 105,
      "total_chunks": 245
    }
  },
  {
    "text": "ly different AI asset.\nFacts, documentation, and evidence regarding AI assets are collected and stored in \nwatsonx.governance for future reviews and audits by regulatory agencies, internal stakeholders, and external partie s. Additionally, specif ic reports related to  the AI lifecycle and \nits stages can be generated in the Governance Console of watsonx.governance.\n\n26 Ensuring Trustworthy AI with IBM watsonx.governance\n\n\u00a9 Copyright IBM Corp. 2025. 27Chapter 3. Implementing AI governance \nstrategy\nArtificial Intelligence (AI) governance is a mu ltifaceted process that  ensures AI systems are \ndeveloped and deployed responsibly and ethica lly. This chapter provides a comprehensive \noverview of the end-to-end AI lifecycle gove rnance process, exploring the various steps \ninvolved in achieving this goal.\nThis chapter contains the following sections:\n/SM590000\u201cUnderstanding the end-to-end AI lifecycle governance process\u201d on page 28\n/SM590000\u201cElements of model risk governance\u201d on page 29\n/",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 106,
      "total_chunks": 245
    }
  },
  {
    "text": "I lifecycle governance process\u201d on page 28\n/SM590000\u201cElements of model risk governance\u201d on page 29\n/SM590000\u201cConsiderations to implement AI governance strategy\u201d on page 373\n\n28 Ensuring Trustworthy AI with IBM watsonx.governance3.1  Understanding the end- to-end AI lifecycle governance \nprocess\nThis section describes the governance process which is divided into two primary levels: \nmacro and micro as shown in Figure 3-1. \nFigure 3-1   End-to-end AI lifecycle governance process\nMacro level\nThe macro level encompasses high-level strategic steps such as defining legal obligations, \narticulating AI principles, and extending enterprise risk frameworks. These steps ensure that the overarching governance structure aligns with organizational values and regulatory requirements.\nMicro level\nThe micro level involves more granular tas ks, including assigning business owners, \nperforming risk assessments, and documenting go/no-go decisions. These steps ensure that day-to-day operations are conducte",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 107,
      "total_chunks": 245
    }
  },
  {
    "text": "ents, and documenting go/no-go decisions. These steps ensure that day-to-day operations are conducted in a manne r that supports the macro-level objectives.\nThe intersection of macro and micro-level specific s dictates actions related to AI models, such \nas model approval, deployment, and monitoring . This level ensures that each AI model is \nrigorously evaluated and managed throughout its lifecycle.\nThe process begins with model proposal approval, where a model entry is created in the \nModel Inventory and continuously updated with new information. The data scientist then uses a tool of their choice to develop the model, with training data and metrics from popular open-source frameworks automatically captured and saved to the model entry. Custom information can also be saved.\nNext, the pre-production model is evaluated for accuracy, drift, and bias, with performance \nmetadata captured and synced. The model is t hen reviewed and approved for production, and \ndeployed in the preferred pl",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 108,
      "total_chunks": 245
    }
  },
  {
    "text": "d synced. The model is t hen reviewed and approved for production, and \ndeployed in the preferred platform, with relevant metadata captured and synced. Finally, the production model is continuously monitored, with performance data captured and synced, \n\n\nChapter 3. Implementing AI governance strategy 29and a dashboard provides a comprehensive view  of the performance metrics for all models, \nallowing stakeholders to proactively identify and react to any issues.\nThis chapter will delve into the end-to-end AI lifecycle  governance proces s, highlighting the \nkey personas, and components of each level and how they interconnect to ensure that AI systems are developed and deployed in a manner consistent with organizational values and goals. We will also discuss the challenges an d opportunities associ ated with implementing \nthis process, emphasizing the benefits of improved transparency, accoun tability, and trust.\nThroughout this ch apter, we will examine the key components of each  gover",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 109,
      "total_chunks": 245
    }
  },
  {
    "text": "oun tability, and trust.\nThroughout this ch apter, we will examine the key components of each  governance level and \ndiscuss how they work together to ensure responsible and ethical AI development and \ndeployment. By understanding these components, readers will ga in a comprehensive view of \nthe governance process and its importance.\nBy the end of this chapter, readers will have a thorough underst anding of the end-to-end AI \nlifecycle governance proc ess. They will be able to identify  the key componen ts of each level \nand understand how they work together to ensure that AI systems are developed and deployed responsibly and ethically. This kn owledge will be invaluable  for applying AI \ngovernance strategies within their own organizations.\n3.2  Elements of model risk governance \nTo convert macro-level requirements into micro-level ones, a framework is needed to map these requirements. This is achieved through the use of personas that define the high-level requirements, while watsonx.",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 110,
      "total_chunks": 245
    }
  },
  {
    "text": "This is achieved through the use of personas that define the high-level requirements, while watsonx.governance objects are essential for effective management and governance of AI models thro ughout their lifecycle. This chapter will explore the elements \nrequired to implement the AI governance strategy\n3.2.1  Personas\nFigure 3-2 on page 30 shows a typical governance flow which begins with defining an AI use case to solve a business problem, followed by requesting an AI asset, such as a model or prompt template, to address the issue. The process involves various roles, starting with the model owner, who defines the problem and identifies the need for an AI solution. The developer then builds the AI asset, which is subsequently tested by the validator to ensure it meets the required standards. Next, the risk officer reviews and approves the solution, taking into account organizational risk management policies. Once approved, the ModelOps engineer deploys the AI asset into production, and",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 111,
      "total_chunks": 245
    }
  },
  {
    "text": " management policies. Once approved, the ModelOps engineer deploys the AI asset into production, and finally, the application developer monitors its performance, identifying areas for improvement. Throughout this process, some organizations may choose to combine certai n roles or responsib ilities, tailoring the \ngovernance flow to their specific needs.\n\n30 Ensuring Trustworthy AI with IBM watsonx.governanceFigure 3-2   Typical personas involved in a governance process ( source )\nConsider the expertise required for your governance team. A typical governance plan may \ninclude the following roles, wh ich can sometimes be filled by the same person or, in other \ncases, represent a team of people.\n/SM590000Model Owner: This individual creates an AI use case to address a business need, \nrequests the model or prompt template, manages the approval process, and tracks the solution through the AI lifecycle.\n/SM590000Risk and Compliance Manager/ Legal Team: This person determines the policies and",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 112,
      "total_chunks": 245
    }
  },
  {
    "text": "lifecycle.\n/SM590000Risk and Compliance Manager/ Legal Team: This person determines the policies and \ncompliance thresholds for the AI use case, such  as rules for testing fairness or screening \noutput for hateful and abusive speech.\n/SM590000Model Developer or Data Scientis t: This role involves working with the data in a dataset or \na large language model (LLM) to create the machine learning model or LLM prompt template.\n\n\nChapter 3. Implementing AI governance strategy 31/SM590000Model Validator: The validator tests the solution to ensure it meets the goals outlined in \nthe AI use case.\n/SM590000Model Evaluator: After deployment, the app developer evaluates the deployment to \nmonitor performance against the metric thresholds set by the risk and compliance manager. If performance falls below specified thresholds, the app developer collaborates with other stakeholders to address issues and update the model or prompt template.\n3.2.2  Objects\nFigure 3-3 illustrates the key comp onents of",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 113,
      "total_chunks": 245
    }
  },
  {
    "text": "nd update the model or prompt template.\n3.2.2  Objects\nFigure 3-3 illustrates the key comp onents of AI governance, whic h is a critical aspect of \nensuring responsible and ethical AI development and deployment. The three main elements of AI governance are model risk governance, model inventory, and lifecycle tracking, and evaluation and monitoring. Model risk governance involves identifying and mitigating potential \nrisks associated with AI models, such as bias, accuracy, and security. Model inventory and lifecycle tracking involves maintaining a comprehensive record of all AI models in use, including their development, deployment, and maintenance. Evaluation and monitoring involves continuously assessing the performance and impact of AI models, identifying areas for improvement, and making necessary adjustments. By implementing these components, organizations can ensure that their AI systems are transparent, accountable, and aligned with their values and goals.\nFigure 3-3   A governe",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 114,
      "total_chunks": 245
    }
  },
  {
    "text": "ystems are transparent, accountable, and aligned with their values and goals.\nFigure 3-3   A governed, trusted AI lifecycle\nTo implement the framework, several objects play a crucial role in ensuring the effective management and governance of AI models throughout the lifecycle. These objects interact with different personas, including Model Developers, Model Owners, Model Validators, and Model Risk Reviewers. \nFigure 3-4 on page 32 illustrates a hierarchical  framework for implem enting AI governance \nstrategies, comprising three key components: Evaluation and Monitoring, Model Validation and Review, and Risk and Compliance. Such a framework enables the creation of multiple hierarchical structures, each with its own unique architecture, which can be used to organize and capture Models. One possible organizational structure could include divisions into groups such as geography, business unit, line of business etc.\n\n\n32 Ensuring Trustworthy AI with IBM watsonx.governanceFigure 3-4   Hier",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 115,
      "total_chunks": 245
    }
  },
  {
    "text": "nit, line of business etc.\n\n\n32 Ensuring Trustworthy AI with IBM watsonx.governanceFigure 3-4   Hierarchical framework for implementing AI governance\nBusiness Entity\nBusiness Entities are abstract representations of your business structure. A business entity \ncan contain sub-entities (such as departments, business units, or geographic locations). This \nstructure is used within the system to organize access rights and simplify corporate reporting needs.\nFor example: A bank's retail lending department is a Business Entity.\nPersona interactions:\n/SM590000Model Developers: Create models that meet the specific needs of the Business Entity. For \nexample, a Model Developer may create a credit risk model for the retail lending department.\n/SM590000Model Owners: Ensure that models are aligned with the Business Entity's goals and \nobjectives. For example, the Model Owner for the retail lending department may ensure that the credit risk model is aligned with the department's risk appetite.\n/SM590",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 116,
      "total_chunks": 245
    }
  },
  {
    "text": "artment may ensure that the credit risk model is aligned with the department's risk appetite.\n/SM590000Model Validators: Validate models to ensure they meet the Business Entity's requirements. \nFor example, a Model Validator may validate the credit risk model to ensure it meets the retail lending department's requirements for accuracy and fairness.\nInventory\nA centralized repository that organizes, documents, and maintains an enterprise-wide collection of models or AI assets, including their usage, issues, and governance activities.\nPersona interaction:\n/SM590000Model Owner: Interacts with the inventory to document model changes and updates, track \nissues and resolve model-related problems\n\n\nChapter 3. Implementing AI governance strategy 33/SM590000Risk and Compliance Manager: Interacts with the inventory to schedule and track model \nreviews, assign and track model risk assessments, monitor model performance and identify potential compliance risks associated with models\nInventory is pr",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 117,
      "total_chunks": 245
    }
  },
  {
    "text": "tor model performance and identify potential compliance risks associated with models\nInventory is primarily comprised of the following objects:\n1.Use Case  - The Use Case object is a subclass of Entity and a superclass of the Model \nobject. Its main function is to serve as a repository for models during development. The \nuse case encapsulates both the qualitative and quantitative requirements of the AI application to be deployed. It helps define and demonstrate how a specific model can solve a particular business problem or achi eve an objective. It describes practical \nscenarios and contexts in wh ich the model will be implement ed, offering comprehensive \ninsight into its intended uses and expected results. Additionally, use cases synchronize the watsonx.governance Evaluation and Monitoring Component (performance metrics and quality control) with the Model Inventory and Lifecycle Tracking Component (model reports). Changes in one are automatically reflected in the other. This ensures",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 118,
      "total_chunks": 245
    }
  },
  {
    "text": "ing Component (model reports). Changes in one are automatically reflected in the other. This ensures that users always have up-to-date information about AI models.\n2.Model - The Model object represents a quantitative method, system, or approach used \nwithin an organization to transform input data in to quantitative estimates. This is achieved \nthrough the application of statistical, economic, financial, or mathematical theories, techniques, and assumptions.\nThe Model object captures essential information, including:\n\u2013Model Description : A detailed description of the model\n\u2013Model Ownership : Information about the model's owner\n\u2013Model Status : The current status of the model\n\u2013Development Lifecycle Dates : Important dates related to the model's development\n\u2013Model Type and Category:  Classification of the model\n\u2013Model Risk Assessment data : Data related to the model's risk assessment\n3.Model Deployment  - The Model Deployment object is a child entity of the Model object, \nserving as a cruc",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 119,
      "total_chunks": 245
    }
  },
  {
    "text": " Deployment  - The Model Deployment object is a child entity of the Model object, \nserving as a crucial component for tracking the deployment of one or more models. The Model Deployment object is designed to:\n\u2013 Govern individual usages of a Model in a production ecosystem\n\u2013 Record-specific Model Versions and their deployment locations\u2013 Inform risk tiering of Models by highlighting the number of areas or functions supported \nby each Model\n4.Model Attestation  - Model Attestation is a process that enables organizations to request \nregular sign-offs or attestations for their models. The MRG administrator initiates this process by creating a set of blank model attestations, which are then assigned to the respective model owners. These owners are re quired to answer a series of questions \nabout their models and submit their completed attestations. Typically, model attestations are conducted on an annual or quarterly basis, serving as a way to verify the completeness and accuracy of a model'",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 120,
      "total_chunks": 245
    }
  },
  {
    "text": "n an annual or quarterly basis, serving as a way to verify the completeness and accuracy of a model's information, as well as the overall model inventory.\n5.Model Output  - For organizations seeking a more detailed approach to model \ndocumentation, the model output object offers a solution. This object enables the recording of a model's outputs, with a focus on capturing the description and overview of each output from a governanc e perspective. By utilizin g the model output object, \norganizations can maintain a more granular and comprehensive record of their models' outputs.\n\n34 Ensuring Trustworthy AI with IBM watsonx.governance6.Model Input  - For organizations seeking a more detailed approach to model \ndocumentation, the Model Input object provides a means to capture and record the inputs of a model. The object includes key fields such as:\n\u2013Input Owner : The individual or team responsible for the input\n\u2013Type : The classification of the input\n\u2013Status : The current state of the inpu",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 121,
      "total_chunks": 245
    }
  },
  {
    "text": "nsible for the input\n\u2013Type : The classification of the input\n\u2013Status : The current state of the input\n\u2013Description : A detailed explanation of the input\nAdditionally, a model input object can be linke d to a model output object, allowing for the \ncreation of model chains at a granular level. This provides an alternative to the model link approach, offering a more detailed and nuanced understanding of model relationships.\n7.Model Risk Scorecard  - Model risk assessments are a critical component of the model \ndevelopment and documentation process, as well as an ongoing requirement for models in production. To facilitate th is process, the Model Risk Sc orecard object is utilized to \nconduct thorough risk assessments. This process involves answering a series of questions about the model, which triggers a calc ulation of a risk score . This score, in turn, \ndetermines the model's tier, providing a clear indication of the model's risk level.\nModel validation and review\nDuring this phase, th",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 122,
      "total_chunks": 245
    }
  },
  {
    "text": "ding a clear indication of the model's risk level.\nModel validation and review\nDuring this phase, the model use case and facts collected during the development phase are utilized to validate the mo del. The goal is to ensure the mo del is functionin g as intended and \nmeets the required standards.\nPersona interaction:\n/SM590000Model Validator reviews the model documentation and use-case. In case of any \nshortcomings, they challenge the inconsistence and clarify any unclear or missing information\nTo implement a model validation strategy, the following  objects are primarily utilized:\n1.Review  - The review object is a critical component of model governance, serving as a \nrecord of all model review activities. As a child of both the model deployment and model objects, it provides a comprehensive view of review outcomes. The review object is \ndesigned to capture the results of various types of reviews (for example, pre- and \npost-implementation ), and reviews consumed by independent teams",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 123,
      "total_chunks": 245
    }
  },
  {
    "text": " of reviews (for example, pre- and \npost-implementation ), and reviews consumed by independent teams to ensure model integrity and effectiveness.\nBy utilizing the review object, organizations can maintain a ce ntralized record of all model \nreview activities, facilitating informed deci sion-making and effective model governance.\n2.Challenge  - The challenge object serves as a re pository for documenting and evidencing \nconcerns or issues related to any part of the Model Inventory. When a challenge is raised, the response is recorded, providing a clear audit trail. As a child of both the model and model deployment objects, the challenge object ensures that all relevant information is linked and easily accessible.\nThere could be many factors prompting di fferent personas to challenge existing \ndeployments, such as:\n\u2013 Regulatory non-compliance: Models that fail to meet new or updated regulatory \nrequirements\n\u2013 Data relevancy issues: Models that rely on outdated, incomplete, or inaccurate ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 124,
      "total_chunks": 245
    }
  },
  {
    "text": "tory \nrequirements\n\u2013 Data relevancy issues: Models that rely on outdated, incomplete, or inaccurate data\n\u2013 Performance deterioration: Models that experience a decline in performance over time\n\nChapter 3. Implementing AI governance strategy 35When a challenge is identified, stakeholders will review it in accordance with established \nprocesses to address the concerns. This may involve remediation efforts to bring the model into compliance, update the data, or improve the model's performance.\nEvaluation an d monitoring\nTo ensure the model operates within acceptable parameters, performance metrics are \ncontinuously monitored to circumvent reputational and organizational risks. During the evaluation and monitoring phase, developers, model validators and ModelOps engineers interact with the platform to:\n/SM590000Monitor performance metrics and thresholds.\n/SM590000Analyze model outputs and detect potential issues.\n/SM590000Receive alerts and notifications for threshold breaches.\n/SM590000Inv",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 125,
      "total_chunks": 245
    }
  },
  {
    "text": "ect potential issues.\n/SM590000Receive alerts and notifications for threshold breaches.\n/SM590000Investigate and respond to potential issues or anomalies.\n/SM590000Collaborate to identify root causes and develop solutions.\nPersona interaction:\n/SM590000Model Validator interacts with the platform to: Monitor performance metrics and \nthresholds, analyze model outputs and detect potential issues, and identify data quality problems or biases\n/SM590000ModelOps Engineer interacts with the platform to Interacts with the platform to receive \nalerts and notifications for threshold breaches , investigate and respond to potential issues \nor anomalies and collaborate with Model Evaluators to identify root causes\nFor evaluation and monito ring, the following object s are primarily utilized:\n1.Metric  - The Metric object is used to record the definition of a performance measurement \nthat an organization wants to track. This  involves setting key parameters, including:\n\u2013Metric Type:  The type of perf",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 126,
      "total_chunks": 245
    }
  },
  {
    "text": "on wants to track. This  involves setting key parameters, including:\n\u2013Metric Type:  The type of performance metric being tracked\n\u2013Threshold:  The threshold at which the metric is  considered cautionary, critical, etc.\n\u2013Collection information: Additional details about the metric's collection and calculation\n2.Metric Value - The Metric Value object records the result of the metric performance \nmeasurement. It is designed to behave in a way to allow the organization to store time \nseries results of measurement.\n3.Change Request - The change request object is a critical component of model \ngovernance, providing a structured process for requesting, justifying, and approving changes to models after they have gone live. This object's workflows enable organizations \nto:\n\u2013Request and justify changes: Clearly articulate the need for changes and provide \nsupporting rationale\n\u2013Route changes for approval:  Direct changes to the relevant stakeholders and \napprovers, based on the type and impact of t",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 127,
      "total_chunks": 245
    }
  },
  {
    "text": "oval:  Direct changes to the relevant stakeholders and \napprovers, based on the type and impact of the change\n\u2013Obtain auditable approvals: Ensure that all changes are properly approved and \ndocumented, with a clear audit trail\nThe Change Request object allows for various  approval paths and levels of approval, \ndepending on the nature and scope of the change. This flexibility e nables organizations to \nbalance the need for control and oversight wi th the need for agilit y and responsiveness.\n\n36 Ensuring Trustworthy AI with IBM watsonx.governanceRisk and compliance\nThe increasing use of AI models in various industries has raised concerns about risk and \ncompliance. AI models can pose significant risks if not properly designed, trained, and deployed, including biases, errors, and uninten ded consequences. Moreover, AI models must \ncomply with various regulations and standards, such as the European Union AI Act, United States AI Executive Order, CCPA, and HIPAA, to ensure the protection ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 128,
      "total_chunks": 245
    }
  },
  {
    "text": " European Union AI Act, United States AI Executive Order, CCPA, and HIPAA, to ensure the protection of sensitive data and \nprevent harm to individuals. To mitigate these risks, organizations must implement robust risk \nmanagement and compliance frameworks that include model risk assessments, data quality checks, and ongoing monitoring and evaluation. \nSince risk and compliance management involves  identifying, assessing, and mitigating \npotential risks associated with non-compliance with external mandates and internal policies. The following components are essential to a robust risk and compliance framework:\n1.Mandate  - Mandates represent external requirements that organizations must comply \nwith, such as laws, regulations, and standards. When necessary, mandates can be broken down into Sub-Mandate objects, which provide a more detailed understanding of the mandate's sub-sections. This hierarchical structure enables organizations to effectively manage and comply with complex regulator",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 129,
      "total_chunks": 245
    }
  },
  {
    "text": "hierarchical structure enables organizations to effectively manage and comply with complex regulatory requirements.\n2.Requirement  - Requirements represent specific ob ligations that an organization must \nfulfill to comply with related mandates or sub-mandates. Th ese requirements are detailed \nin the requirement object, which provides a clear understanding of what the organization needs to do to meet regulatory obligations. By detailing these requir ements, organizations \ncan ensure they are meeting their regulatory obligations and maintaining compliance.\n3.Policy  - Policies represent internal guidelines adopted by an organization's Board of \nDirectors or senior governance body. These guidelines are designed to provide direction and oversight for the organization's operatio ns and decision-making processes. The Policy \nobject is used to store and manage policy information, including the policy text, which can be stored in standardized fields or as an attachment to the object.\nPolicie",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 130,
      "total_chunks": 245
    }
  },
  {
    "text": "e policy text, which can be stored in standardized fields or as an attachment to the object.\nPolicies are managed through a review and approval process, which ensures that they are \nproperly vetted and authorized before being implemented. \n4.Questionnaire Assessment - Questionnaires are a powerful tool for assessing risk and \ncompliance, as well as collect ing information for specific processes and asset risks. \nwatsonx.governance streamlines, standardizes, and centralizes the collection of \nquestionnaire-based assessment information, making it easier to gather insights from \nacross the organization.\nWith Questionnaire Assessment object, information from business users within the \norganization can be gathered. Respondents complete the questions and submit the finished questionnaire assessment, providing valuable insights into risk and compliance.\n3.2.3  Workflows\nThe implementation of AI governance requires structured workflows to manage the lifecycle, \nrisk, and compliance of AI model",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 131,
      "total_chunks": 245
    }
  },
  {
    "text": " governance requires structured workflows to manage the lifecycle, \nrisk, and compliance of AI models and their associated use cases. This section outlines key workflows provided by watsonx.governance Model Risk Governance (MRG) , integrating them \ninto the end-to-end go vernance process discussed in this chapter. These workflows facilitate \nresponsible AI development, deployment, and monitoring, ensuring adherence to organizational processes and regulatory requirements. These workflows can be automated and involve multiple user interactions and feedback/approval capturing mechanisms effectively reducing the time and effort to manage the entire process of AI governance. All actions taken by users with workflows are auditable ensuring conf idence in the system \nresults. Predefined workflows are provided as part of watsonx.governance, but more can be built or existing workflows can be customized through the UI designer.\n\nChapter 3. Implementing AI governance strategy 37These available wo",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 132,
      "total_chunks": 245
    }
  },
  {
    "text": "omized through the UI designer.\n\nChapter 3. Implementing AI governance strategy 37These available workflows can be categorized into:\n/SM590000Models : Model Candidate, Model Validation, Model Deployment, Model Risk Assessment, \nModel Attestation, Model Decommission, and Model Change Request\n/SM590000Use Cases : Use Case Request, Use Case Stakeh older Review, Use Case Development \nand Validation, and Use Case Deployment Approval\n/SM590000Metrics : Metric Value and Metric Value Creation\n/SM590000Questionnaires : AI Assessment and Questionnaire Assessment\n/SM590000Other : Challenges and Model Risk Assessment\nFigure 3-5 illustrates a matrix of  various objects and personas involved in AI governance \nworkflows. Each row and column represent different components and stakeholders discussed in 3.2, \u201cElements of model risk governance\u201d on page 29, showcasing their interconnected roles in executing standardized processes. Th is visualization unders cores the necessity of \ncollaboration and coordi",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 133,
      "total_chunks": 245
    }
  },
  {
    "text": " standardized processes. Th is visualization unders cores the necessity of \ncollaboration and coordination among diverse stakeholders to ensure the effective implementation of AI governance workflows.\nFigure 3-5   Workflows bridging governan ce and personas for standardized processes\nBenefits of str uctured workflows\nBy integrating these workflows into the AI governance process, organizations can:\n/SM590000Ensure transparency, accounta bility, and ethical compliance throughout the AI lifecycle.\n/SM590000Mitigate risks effectively while adhering to regulatory requirements.\n/SM590000Foster trust in AI systems through robust and scalable governance practices.\n/SM590000Minimize efforts to govern the AI use cases in their entirety.\n/SM590000Standardize the approval and data capture process for all the use cases.\nThese workflows are critical in enabling organizations to manage their AI models and use \ncases with confidence and precision, aligned with the macro and micro-level governance stra",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 134,
      "total_chunks": 245
    }
  },
  {
    "text": "and use \ncases with confidence and precision, aligned with the macro and micro-level governance strategies discussed  in this chapter.\n3.3  Considerations to implement AI governance strategy\nImplementing an effective AI governance strategy requires careful consideration of an \norganization's unique characteristics and needs. This section outlines the key factors to consider when configuring an AI governance solution to meet the specific requirements of an individual organization.\n\n\n38 Ensuring Trustworthy AI with IBM watsonx.governance3.3.1  Understanding orga nizational characteristics\nWhen implementing an AI governance strategy, it is essential to consider the following factors \nthat are specific to each organization:\n/SM590000Geographies : Different regions have distinct regulatory requirements, cultural nuances, \nand market conditions that impact AI governance.\n/SM590000Market sectors : Various industries, such as banking, telecommunications, and public \nsector, have unique challen",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 135,
      "total_chunks": 245
    }
  },
  {
    "text": "s : Various industries, such as banking, telecommunications, and public \nsector, have unique challenges and requirements for AI governance.\n/SM590000AI use cases : Organizations may employ different types of AI, such as machine learning \nor generative AI or general purpose AI, for in ternal or external purposes, which affects \ngovernance needs.\n/SM590000Organizational structure : Centralized or decentralized structures influence how AI \ngovernance is implemented and managed.\n/SM590000Tech stack : The use of specific platforms, AI in enterprise apps, or open-source \ntechnologies impacts AI governance requirements.\n3.3.2  Configuring AI governance\nBased on these organizational characteristics, the following steps can be taken to configure \nan AI governance solution:\n/SM590000Implementing legal obligations : Develop a library of mandates and assessment \ntemplates that reflect the organizat ion's specific legal requirements.\n/SM590000Establishing a risk framework : Create a library of risk",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 136,
      "total_chunks": 245
    }
  },
  {
    "text": "ion's specific legal requirements.\n/SM590000Establishing a risk framework : Create a library of risks and controls, and assessment \ntemplates that align with the organization's risk management policies.\n/SM590000Defining organizational structure : Configure business entities, roles, and \nresponsibilities to match th e organization 's structure.\n/SM590000Manage collaboration and access control : Use roles and access control features to \nensure that team members have appropriate access to meet governance goals\n/SM590000Developing policies and procedures : Create workflows for lifecycle governance, risk \nmanagement, and compliance management that reflect the organization's policies and procedures.\n/SM590000Develop a communication plan : Establish a plan for communication and \ndecision-making, including the use of email, messaging tools, or other collaboration platforms\n/SM590000Implement a simple governance solution : Start with a basic implementation and build \nincrementally to a more  c",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 137,
      "total_chunks": 245
    }
  },
  {
    "text": "simple governance solution : Start with a basic implementation and build \nincrementally to a more  comprehensive solution\n/SM590000Plan for more complex solutions:  Consider extending the AI governance \nimplementation to include external models, custom properties, and tailored reports\n3.3.3  Leveraging out-of-th e-box product content\nWhile configuration is necessary, it is essential to note that out-of-the-box \nwatsonx.governance capabilities and product content can provid e a solid foundation for AI \ngovernance. This content can be used to illustrate  best practices and provide a starting point \nfor customization, rather than requiring organizations to start from scratch.\n\nChapter 3. Implementing AI governance strategy 39By considering these factors and configuring an AI governance solution accordingly, \norganizations can establish a strong foundation for effective AI governance and set the stage \nfor successful implementation of subsequent chapters' topics.\n3.3.4  Example use case\nWh",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 138,
      "total_chunks": 245
    }
  },
  {
    "text": " the stage \nfor successful implementation of subsequent chapters' topics.\n3.3.4  Example use case\nWhen implementing an AI governance strategy , an organization should start by analyzing \nvarious factors including: regi ons where it operates and utilizes AI models,  the industry it \nserves, the specific AI tools and applications it relies on, its organizational structure, and its existing technology stack (for example, IBM Watson Studio ). This evaluation will help the \norganization understand its operational landscape and tailor its AI governance strategy to address its specific needs. A well-designed governance strategy enables the organization to manage cross-team collaboration efforts, control access to sensitive data, and ensure compliance with regional regulations. Additionally, it helps support the implementation of a clear communication, aligning stakeholders and providing a strong foundation for continuous \nmonitoring, auditing, transparency, and acco untability over time.\n\n40 ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 139,
      "total_chunks": 245
    }
  },
  {
    "text": "g foundation for continuous \nmonitoring, auditing, transparency, and acco untability over time.\n\n40 Ensuring Trustworthy AI with IBM watsonx.governance\n\n\u00a9 Copyright IBM Corp. 2025. 41Chapter 4. Onboarding a new foundation \nmodel\nThe pace of innovation in artifi cial intelligence (AI) is high, es pecially in the development of \nnewer and better foundation models. New versions of major models are regularly released, and numerous smaller models are created for specific languages, such as Danish; data types like geospatial; or business domains, such as  IT programming. Additionally, a vibrant \nopen-source community ex ists, with over one million models available on Hugging Face, an \nonline marketplace for open-source AI models.\nThat increasing choice in models means there is an increasing need for organizations to \ngovern those choices.\n/SM590000Organizations should define the minimal standards a foundation model needs to meet \nbefore they will allow it to be  applied in their use cases. T",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 140,
      "total_chunks": 245
    }
  },
  {
    "text": "rds a foundation model needs to meet \nbefore they will allow it to be  applied in their use cases. Th is chapter describes the key \nconsiderations from the point of view of different stakeholders.\n/SM590000These considerations might require a trade-off decision when the minimal standards are in \nconflict (for example, would you accept a new model that provides better performance on a certain business task, but it creates a copyright infringement risk if it's not clear where the \ntraining data was sourced?). Organizations should define who gets to make the decision \nand how it gets documented.\n/SM590000Lastly, organizations should define the worfklow to automate the evaluation of a new \nmodel candidate by the various stakeholders.\nThis chapter has th e following sections:\n/SM590000\u201cKey considerations to onboard a foundation model\u201d on page 42\n/SM590000\u201cConsiderations for legal team for approving a new foundation model\u201d on page 44\n/SM590000\u201cEthical considerations for approving a new found",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 141,
      "total_chunks": 245
    }
  },
  {
    "text": "roving a new foundation model\u201d on page 44\n/SM590000\u201cEthical considerations for approving a new foundation model\u201d on page 47\n/SM590000\u201cConsiderations for financial stakeholders for approving a new foundation model\u201d on \npage 494\n\n42 Ensuring Trustworthy AI with IBM watsonx.governance4.1  Key considerations to  onboard a foundation model\nWhen onboarding a foundation model, several key aspects must be considered to ensure a \nsmooth and effective process. The following subsections outline the crucial considerations for data acquisition and preparation, data processing and filtering, model evaluation and validation, model security and robustness, and model performance monitoring.\n4.1.1  Data transparency\nTransparency is highly desirable as it makes information available, shareable, legible, and verifiable. In the context of training a foundation model, which involves multiple stages, transparency efforts are often targeted at different parts of the pipeline. Documentation is particularly cru",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 142,
      "total_chunks": 245
    }
  },
  {
    "text": "ncy efforts are often targeted at different parts of the pipeline. Documentation is particularly crucial for the data pile used for training the foundation model, where gathering information on data acquisition and preparation methodologies for foundation model training is essential. \nThere are frameworks, such as The Foundation Model Transparency Index , which evaluate \nthe transparency of foundation models across several composite indexes, including data, data labor, data access, and others. These indexes collectively aim to assess the transparency of various aspects of model development, such as data usage, labor practices, computational resources, methodologies, and strategies to  mitigate privacy an d copyright concerns \n(Bommasani, 2024\n1). \nData scientists or AI Center of Excellence or Enterprise AI team onboarding a foundation \nmodel should leverage such assessments to ensure compliance with internal and external regulations and policies. \n4.1.2  Model evaluation and validation",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 143,
      "total_chunks": 245
    }
  },
  {
    "text": "pliance with internal and external regulations and policies. \n4.1.2  Model evaluation and validation\nTo ensure the reliability and sa fety of large langu age models, a comprehensive evaluation \nand validation strategy is required, incorporating the following key elements\n/SM590000FMEval framework: Leverage the Foundation Model Evaluation Framework (FMEval) for \nsystematic, reproducible, and consistent validation and evaluation of new large language models.\n/SM590000Evaluation modes: Support both fine-tuning and prompting (in-context learning) \nevaluation, with readily available academic and business benchmarks.\n/SM590000Content filtering: Implement robust content filtering mechanisms to detect and mitigate the \ngeneration of hate speech, abuse, profanity (HAP), pirated content, malware, and other undesirable outputs.\nTo evaluate foundation models, a data scientist should compare the foundation model to be \nonboarded with current state-of-the-art models using a wide range of standard be",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 144,
      "total_chunks": 245
    }
  },
  {
    "text": "dation model to be \nonboarded with current state-of-the-art models using a wide range of standard benchmark evaluations across top-level categories, such as:\n/SM590000human exams ( MMLU , MMLU-Pro (Wang, 2024) )\n/SM590000common sense ( OBQA , SIQA ), \n/SM590000reading comprehension ( BoolQ , SQuAD 2.0 ), \n/SM590000reasoning ( ARC-C , GPQA ), \n/SM590000code ( HumanEval) , \n/SM590000math ( GSM8K ), \n1  Bommasani, R. K. (2024). The Foundation Model Transparency Index v1.1 May2024 . arXiv \npreprint.\n\nChapter 4. Onboarding a new foundation model 43/SM590000Hugging Face's Open LLM leaderboards\nAdditionally, it is important to  evaluate for different functi ons such as tool calling, RAG \npatterns, and other target domains specific to organizational use-cases, as discovered and discussed by data scientists or AI Center of Excellence or Enterprise AI team.\n4.1.3  Model security and robustness\nFoundation models can be vulnerable to  various security risks, including:\n/SM590000Data poisoning: Att",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 145,
      "total_chunks": 245
    }
  },
  {
    "text": "ndation models can be vulnerable to  various security risks, including:\n/SM590000Data poisoning: Attackers can manipulate training data to compromise model \nperformance or inject malicious behavior.\n/SM590000Model stealing (or extraction): Attackers can steal the model itself or its weights, allowing \nthem to use it for malicious purposes or create competing products.\n/SM590000Adversarial attacks: Attackers cr aft specific input data (adversarial examples) designed to \nmislead the model, causing incorrect or malici ous outputs. This includes techniques like \nprompt injection, where carefully crafted prompts can manipulate the model's behavior.\nRed teaming (IBM, n.d.)  is crucial for data scientists to i dentify model vulnerabilities. This \ninvolves ethical hackers attempting to elicit unintended and potentially harmful behavior from the model, such as generating undesirable content through adversarial attacks and prompt injection. Data scientists responsible for onboarding foundation m",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 146,
      "total_chunks": 245
    }
  },
  {
    "text": "gh adversarial attacks and prompt injection. Data scientists responsible for onboarding foundation models should employ a comprehensive approach, using a combination of internal and external, automated and manual red teaming techniques to thoroughly i dentify and mitigate weaknesses. Traditional \nsecurity measures, such as data encryption (at rest and in transit), user access controls, and firewalls, are also essential.\n4.1.4  Ensuring model health and performance\nTo guarantee that a model meets the desired output and performance expectations of \nend-users, data scientists should evaluate model  health metrics relative to the use cases \nexpected to be delivered with this foundation model. Two key metrics to track are latency and throughput.\nLatency\nLatency measures the time it takes for a model to process a scoring request. It is calculated by tracking the time elapsed between rece iving a request and generating a response, \ntypically measured in millisecon ds (ms). This metric helps i",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 147,
      "total_chunks": 245
    }
  },
  {
    "text": " a request and generating a response, \ntypically measured in millisecon ds (ms). This metric helps i dentify any delays or bottlenecks \nin the model's processing pipeline.\nThroughput\nThroughput measures the number of scoring requests and transaction records that a model can process per second. This metric indicates the model's ability to han dle a high volume of \nrequests efficiently.\nThe following are additional references:\n/SM590000Bommasani, R. K. (2024). The Foundation Model Transparency Index v1.1 May2024 . \narXiv preprint.\n/SM590000IBM. (n.d.). Responsible Use Guide . Note:  AIR-Bench  is a benchmark for evaluating robustness against adversarial attacks. It \nprovides a standardized way to measure how well a model resists these attacks.\n\n44 Ensuring Trustworthy AI with IBM watsonx.governance/SM590000Wang, Y. X. (2024). Mmlu-pro: A more robust and challenging multi-task language \nunderstanding benchmark. arXiv preprint arXiv:2406.01574 .\n4.2  Considerations for legal team for appro",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 148,
      "total_chunks": 245
    }
  },
  {
    "text": "erstanding benchmark. arXiv preprint arXiv:2406.01574 .\n4.2  Considerations for legal team for approving a new \nfoundation model\nThe Legal Team must have special considerations when approving a new Foundation Model. \nWhat kind of license is attached to the foundation model? Who owns the model? Does \nindemnification apply to the new Foundation Model in review? This  section will cover how \nwatsonx.governance can be used to answer these questions and more.\n4.2.1  Model licensing\nStandard software licensing applies to all foundation models and is a great place for the legal team to begin. Once foundation models are published, they are packaged with a license which provides a great depth of detail as to how the model can and should be used. These licenses can be found at multiple places. In  watsonx.governance, the model license can be \nfound attached to Model cards. You can access the model card by using the hamburger menu and clicking through Inventory \u2192 Models \u2192 Choose a specific model ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 149,
      "total_chunks": 245
    }
  },
  {
    "text": " card by using the hamburger menu and clicking through Inventory \u2192 Models \u2192 Choose a specific model . The license will be \nlisted under Development/Training. The model license can also be found on the model card in watsonx.ai, under terms. Finally, before the acceptance of a foundation model onto the watsonx.governance platform, a legal team may obtain the model License through the official publication page. As of this wr iting, any model which is public ly available can have its license \nchecked through the huggingface.co  hub. Table 4-1 highlights several common model \nlicenses.\nTable 4-1   Some common Model Licenses\nTerms and conditions at tached to AI assets\nNot all AI assets come with a simple model license. For example, much of the popular OpenAI \nmodels are not covered under these licenses. In  cases like these, additional questions must \nbe answered. A legal team may want to sit directly with model providers and other teammates such as the data science team to gain a thorough u",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 150,
      "total_chunks": 245
    }
  },
  {
    "text": "directly with model providers and other teammates such as the data science team to gain a thorough understanding of the foundation model. Some of the questions to ask may be:\n/SM590000What sources of data are used to train the model?\n/SM590000What are the terms and co nditions under which this  model will be made available?\n/SM590000Are there contractual limitations on the use cases that the model can be used for (facial \nrecognition, military, any non-research use)? License Name Summarization\nApache 2.0 License A permissive free software license that allows users to use, modify, \nand distribute the licensed software, including in commercial \nproducts, without paying royalties to the original authors.\nLlama Community License Similar to Apache 2.0 but in addition, if the monthly active users of \nthe products or services made avai lable by or for the licensee is \ngreater than 700 million monthly active users in the preceding \ncalendar month, the licensee must  request a license from Meta",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 151,
      "total_chunks": 245
    }
  },
  {
    "text": "onthly active users in the preceding \ncalendar month, the licensee must  request a license from Meta, \nwhich Meta may grant to the licensee in its sole discretion. \nMIT License Similar to the Apache 2.0 license. This license is succinct and \npermissive for ensuring open ownership of software. \n\nChapter 4. Onboarding a new foundation model 45/SM590000Are there indemnification limitations (such as, no indemnification for customer-facing \nuse)?\n/SM590000What is the carbon footprint of the training for this model?\nBy answering these and other questions, a legal team may make the educated decision as to \nwhether or not to onboard the foundation model.\nIntellectual property ownershi p in generative AI deployments\nBeyond the ownership of the foundation models themselves, several critical intellectual \nproperty considerations arise when deploying generative AI. These concerns extend to the \nsoftware used to package and deploy these models, as well as the input data provided and the output gene",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 152,
      "total_chunks": 245
    }
  },
  {
    "text": "ware used to package and deploy these models, as well as the input data provided and the output generated. Therefore, thorough discussions with legal counsel and the model/software provider are essential. Key questions to address include:\n/SM590000Training Data Ownership and Licensing: Does the model provider possess clear legal \nrights (including ownership or valid licenses) to use all data incorporated into the model's training dataset? This is crucial for avoidi ng potential copyright infringement claims.\n/SM590000Copyrighted Material Exclusion: What sp ecific measures has the model provider \nimplemented to identify and exclude copyrighted material from its training data? Understanding their methodology is vital for assessing the risk of IP issues.\n/SM590000Customer Data Usage for Model Training: Does the model provider use customer data to \nfurther train or refine its models? If so, what mechanisms are in place to allow customers to opt out of such usage? Transparency and control o",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 153,
      "total_chunks": 245
    }
  },
  {
    "text": "what mechanisms are in place to allow customers to opt out of such usage? Transparency and control over data usage are paramount.\n/SM590000Fine-Tuned Model Ownership: If the model pr ovider offers fine-tuning services, who \nretains ownership of the intellectual property rights in the resulting fine-tuned model: the provider or the customer? This needs to be clearly defined in contractual agreements.\nThese points should be carefully reviewed to ensure compliance with in tellectual property law \nand protect the interests of all parties involved. More on this will be covered in section \u201cLegal \nindemnification\u201d on page 46.\nLocal and national law implications\nAI legislation has advanced at different rates throughout the world. The only thing that is \ncommon across all government bodies is that they are all putting forth some effort to regulate AI usage. It is not in the scope of this book (and would be challenging) to document all of the world's efforts to regulate AI. This section will pro",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 154,
      "total_chunks": 245
    }
  },
  {
    "text": "d would be challenging) to document all of the world's efforts to regulate AI. This section will provide a brief introduction to  two different \nmethods that have been used to implement AI Regulation - State level, and National level. At the state level, we will quickly introduce how the United Stat es has been implementing AI \nregulation. At a national level, we mention the EU AI act in Western Europe. \nThis section has two following subsections - Implications in the United States, and \nImplications in Western Europe. These two examples are meant to serve as a comparison to how a government may enforce its AI policies. Legal teams should consult their respective Local and National laws for specifics.\nImplications in the United States\nBefore approving any foundation model, it is critical to review local and national regulations as it applies to the industry. In the US, many state laws are beginning to form which work to govern the usage of AI. The legal team should be aware of any rele",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 155,
      "total_chunks": 245
    }
  },
  {
    "text": "e beginning to form which work to govern the usage of AI. The legal team should be aware of any relevant law and perform the appropriate questioning to cover these. Some  examples of passed legislation in the US \ninclude:\n\n46 Ensuring Trustworthy AI with IBM watsonx.governance/SM590000NYC Local Law 144 - Implemented in 2021, the first law in the United States requiring bias \nauditing against AI tools. New York has propo sed more than 30 additional AI laws since \nthen, with almost all still in Propo sal or Failed as of this writing.\n/SM590000Tennessee ELVIS Act - Signed into law on March 21, 2024. This law protects the voices of \nArtists from all disciplines against AI-generated deepfakes created without their permission.\n/SM590000Colorado Consumer Protections for Artificial Intelligence Ac t - Signed into  law May 17, \n2024. The act requires high-risk AI systems to use reasonable safeguards to protect consumers from \nalgorithmic discrimination .\nIn October of 2022, The United States Fe",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 156,
      "total_chunks": 245
    }
  },
  {
    "text": "rds to protect consumers from \nalgorithmic discrimination .\nIn October of 2022, The United States Federal Government also published guidance which \ncould serve for legal conversation s under the name of \u201cBlueprint for an AI Bill of Rights.\u201d This \ndocument outlines important topics which should be considered by the larger team when onboarding a Foundation Model. As of this writing, the blueprint serves only as guidance and does not enforce any official Federal law.\nImplications in Western Europe (The European Union)\nFor completeness of consideration, the legal team must review any candidate Foundation Model and how it relates to the EU AI Act. Th e Act defines a specific set of obligations for \nproviders of general purpose AI models (as foundation models are called in its legal terminology). In addition, the European Union will provide an as sociated Code of Practice \nwhich is planned to be  approved in or before May 2025. The ob ligations and the Code will \ngive buyers some concrete ex",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 157,
      "total_chunks": 245
    }
  },
  {
    "text": "be  approved in or before May 2025. The ob ligations and the Code will \ngive buyers some concrete expectation towards a provider of a model that's brought onto the market after the Act has gone into force (August 2024).\nNote that foundation models already on the market before entry into force, need to be \ncompliant by August 2nd, 2027. Vendors might decide to withdraw their model from the market before that date to avoid the compliance implications. Don't wait too long to engage your existing model providers to understand th eir intent and plan for compliance, and make \nsure you have a plan yourself to switch models when needed.\nUsers of foundation models have no specific obligations, beyond what might result from the AI \nsystem(s) that use that foundation model.\n4.2.2  Legal obligations on the part of the vendor\nAny vendor of AI is tied to the same local and national laws as mentioned earlier. In addition, \nmost of the earlier section around model licensing also applies to the vendor.",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 158,
      "total_chunks": 245
    }
  },
  {
    "text": "arlier. In addition, \nmost of the earlier section around model licensing also applies to the vendor. One more important topic left to cover is the topic of Legal Indemnification. In law,  indemnification (or an \nindemnity) is any undertaking by one party to protect another party of some financial burden.\n In AI, the usage of foundation models can expose parties to novel financial burdens. As such, \na major effort has been put forth around the idea of legal indemnification from AI models. This next subsection will cover this point.\nLegal indemnification\nAI has been controversial in its effects and re percussions. Countless lawsuits have risen \nagainst AI companies, because of this, a growing need of consumers of AI is to be indemnified against any legal repercussions. Every AI producer has a different position on indemnification and therefore, specifics must be explored on a per-foundation model basis. By approaching the legal review of Foundation Models in a systemic way, the legal tea",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 159,
      "total_chunks": 245
    }
  },
  {
    "text": "n model basis. By approaching the legal review of Foundation Models in a systemic way, the legal team can ensure that an AI program will be implemented with proper co ntrol and in a safe and legal \nway.\n\nChapter 4. Onboarding a new foundation model 474.2.3  A final note on  legal considerations\nAs was covered in the earlier sect ion of the US Blueprint for AI Bill of Rights, no t all measures \nof AI safety are required by law. Legal consider ations typically follow the letter of the law, but \nthis is not the end of the story for what dangers an un-controlled AI system could pose. Even with all relevant legal details be ing covered, th e ethics behind any AI  effort must still be \nquestioned. \nIt is also important to note that many laws a nd regulations are vague in their definitions and \nguidance. Similar to other types of technology-focused requirements, those for AI require broad statements as to not grow stale shortly after going into effect given the current speed of evolution in t",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 160,
      "total_chunks": 245
    }
  },
  {
    "text": "ments as to not grow stale shortly after going into effect given the current speed of evolution in the industry. For these reasons it is important to approach any requirement that is new or without established precedent with a conservative and ethical mindset.\nThe next section 4.3, \u201cEthical considerations for approving a new foundation model\u201d on \npage 47 will focus on this topic of AI ethics.\n4.3  Ethical considerations for approving a new foundation \nmodel \nWith AI usage gaining momentum, there is an increased focus on the ethical aspect of \nunderstanding how a foundation model is trained and any ethical risk it can expose if it is in used as part of an enterprise use case. When considering a new foundation model, an ethical stakeholder will want to consider at least the following dimensions.\n4.3.1  Fairness\nAn AI system or a model should be fair and free of any direct or indirect bias in its prediction. \nWith the AI system using a foundation model, the foundation model itself needs t",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 161,
      "total_chunks": 245
    }
  },
  {
    "text": "in its prediction. \nWith the AI system using a foundation model, the foundation model itself needs to be evaluated for fairness.\nBias can be detected by evaluating the found ation model for differences in accuracy and \nperformance by using social de mographic-protected attributes.\nA model can exhibit fairness issues in various wa ys. If the model is looked at in isolation and \nwithout sufficient context, it might not appear to exhibit a favorable or unfavorable outcome. In other cases, the model output can decide the prioritization level and can introduce bias in the entire end-to-end process where the source of bias is tied to the model behavior. \nIt is critical that the business owners, stakeho lders, and designers look at the model in the \ncontext of the overall ecosystem where the training, testing, or the production of data is generated, how the model is developed and evaluated, and where the model is being used to decide different ways in which the model might lead  to a biased o",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 162,
      "total_chunks": 245
    }
  },
  {
    "text": " where the model is being used to decide different ways in which the model might lead  to a biased outcome. This is a key step in \nthe overall methodology relating to bias detection.\nIt is important to note that bias in some attributes of a model might be reasonable. For \nexample, a foundation model that is fine-tuned to pre-screen loan applications might be biased against people with poor credit. This is deemed as reasonable. Since most model bias \nis injected unwittingly through skewed data or constrained training methods, it cannot be effectively detected and resolved exclusively through manual testing or checklist validations. \n\n48 Ensuring Trustworthy AI with IBM watsonx.governance4.3.2  Transparency\nWith AI usage gaining momentum, there is an increased focus on the transparency aspect of \nthe foundation models for audit purposes. There are various governance, risk, compliance, or regulatory needs for information covering the nature and intended uses of the foundation model. Trans",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 163,
      "total_chunks": 245
    }
  },
  {
    "text": "egulatory needs for information covering the nature and intended uses of the foundation model. Transparency about the model's overall accuracy, its ability to explain particular \ndecisions, its fairness regarding protected cl asses, and information about the provenance of \ntraining data and assurances that suitable privacy protections have been maintained, all should be properly documented and available for audit purposes.\nTransparency builds the trust in the AI system by increasing the unde rstanding of how the \nmodel was created and deployed and enabling th e ability to control how AI is created and \ndeployed. This can prevent undesirable situations, such as a model training with unapproved data sets, models having biases, or mode ls having unexpected performance variations. \nThis documentation of facts about the foundation model (for example model cards) can have \nthe following properties:\n/SM590000It can vary in content and are tailored to the particular foundation model being \ndoc",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 164,
      "total_chunks": 245
    }
  },
  {
    "text": "ties:\n/SM590000It can vary in content and are tailored to the particular foundation model being \ndocumented.\n/SM590000It is tailored to the needs of their target audience or consumer documenting the tasks the \nfoundation model can be used for.\n/SM590000It captures the details about the data the foundation model has been trained on.\n/SM590000It includes the information about training algorithms, parameters, fairness constraints or \nother applied approaches, and features.\n/SM590000It shows the benchmark accuracy documenting the performance of the model.\n/SM590000It contains information about the model owner (organization), model version, as well as the \nlicense model can be used with. \n4.3.3  Privacy\nAn important aspect to build trust in an AI system or model is to take measures to manage and safeguard foundation models and its data that is trained on Personal Information (PI). If a model is trained on PI without applying any sp ecific privacy techniques, and that model is \nmade publicly",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 165,
      "total_chunks": 245
    }
  },
  {
    "text": "is trained on PI without applying any sp ecific privacy techniques, and that model is \nmade publicly available or shared with a nontrusted third party, the model might reach the wrong hands, and potentially violate the privacy of the people whose information was used in \ntraining it. PI must be properly handled and safeguarded wherever it is stored or used in the organization.\nThe same safeguards must be applied for models trained on proprietary, intellectual, \ncopyrighted and confidential information.\nModel providers must implement measures to prevent data leakage during inference. This \nincludes:  \n/SM590000Data minimization: Process only the absolute minimum amount of user data necessary for \nthe model to function.\n/SM590000Differential privacy: Apply noise to user data or model outputs to enhance privacy and \nmake it difficult to identify or isolate individual contributions.\n/SM590000Federated learning: Train models collaborati vely across multiple  decentralized data \nsources usin",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 166,
      "total_chunks": 245
    }
  },
  {
    "text": "0Federated learning: Train models collaborati vely across multiple  decentralized data \nsources using techniques like federated learning, thereby avoiding the risks associated with centralizing sensitive user data\n\nChapter 4. Onboarding a new foundation model 49/SM590000Model monitoring: Continuously monitor model behavior for signs of unexpected data \nleakage or privacy violations.\n/SM590000Transparency and user control: Provide users with clear information about how their data \nis used by the model and offer options for controlling data access and sharing.\n4.3.4  Explainability\nAI systems are increasingly us ed to inform high stakes de cisions. Explainability and \ninterpretability of these systems and the models within the system are beco ming essential. \nThere are many ways to explain these models and systems, the appropriate choice depends \non the usage context and type of explanatio n that is needed by the consumer of the \nexplanation.\n4.3.5  Robustness\nAn AI system is considered ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 167,
      "total_chunks": 245
    }
  },
  {
    "text": " n that is needed by the consumer of the \nexplanation.\n4.3.5  Robustness\nAn AI system is considered robust if it can continue to perform well and reliably even when \nfaced with difficult or unexpected situations. These situations can include anything from slight changes in the data it receives  to deliberate attempts to tr ick or manipulate the system. A \nrobust AI system is designed to handle these challenges effectively, minimizing mistakes and providing consistent and trustworthy results.\n4.3.6  Third-party help\nThere are third-party evaluations to help you und erstand various ethical characteristics of a \nfoundation model you are considering using.  Use these tools to speed up your model \nonboarding process.\nTwo examples from the Standard Center for Research on Foundation Models:\n/SM590000Foundation Model Transparency Index  - This index scores foundation models on 100 \ntransparency indicators ac ross several dimensions.\n/SM590000AIR-Bench  (short for \u201cAI risk benchmark\u201d - This ben",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 168,
      "total_chunks": 245
    }
  },
  {
    "text": "indicators ac ross several dimensions.\n/SM590000AIR-Bench  (short for \u201cAI risk benchmark\u201d - This benchmarks scores foundation models on \nan extensive safety taxonomy that covers co ntent safety risks, societal risks, legal and \nrights-related risks, and syst em and operational risks.\nThere are many ot her options available. A quick web sear ch will give you plenty of options to \nconsider.\nThe IBM AI Ethics Board has published an extensive overview of AI risks, called the AI Risk \nAtlas . It lists several specific risks in the ethics dimensions listed in this paragraph. Each risk \ncomes with a description, categorizations, and links to examples of that risk in third-party publications whenever possible.\n4.4  Considerations for financial stakeholders for approving a \nnew foundation model \nA stakeholder from a Finance function will want to consider at least the following dimensions \nwhen considering a new foundation model.\n\n50 Ensuring Trustworthy AI with IBM watsonx.governance4.4.1  Tota",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 169,
      "total_chunks": 245
    }
  },
  {
    "text": "nsidering a new foundation model.\n\n50 Ensuring Trustworthy AI with IBM watsonx.governance4.4.1  Total cost of ownership\nFirst, what is the sum total of the costs associated with acquiring and using another \nfoundation model? Many models are available un der a \u201cfree\u201d license, but there's more to the \ntotal cost of ownership than that.\nWhat is the init ial investment?\n/SM590000What are the initial license costs to acquire the foundation model? This might take the \nform of a license for a model specifically, but could (also) include licenses for a platform that the model is hosted on. Based on existi ng vendor relationships, discounts might be \navailable. As the market for generative AI evolves, new pricing models might appear.\n/SM590000What are the costs to onboard this new model ? Consider procurement, IT, legal, security \nand other cost components. The assessment that is the topic of this chapter is part of such an onboarding process and comes with a cost.\n/SM590000What are the costs o",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 170,
      "total_chunks": 245
    }
  },
  {
    "text": "s chapter is part of such an onboarding process and comes with a cost.\n/SM590000What are the costs of skills development to use this new m odel effectively? Consider \ntraining for administrators, data scientists, risk management and other relevant roles.\nWhat are the run costs?\n/SM590000If the model is deployed as SaaS, what are the charges for inferencing? Foundation model \ninferencing is typically charge d by the number of calls to the model and/or the number of \ntokens processed/generated by the model. Pricing models might differ, as does the amount charged for comparable numbers of calls/tokens.\n/SM590000If the model is deployed in-house, what are the total costs over the foreseeable lifetime of \nthis investment? Consider hard ware, electricity, cooling, personnel and all other relevant \ncost components. These costs will depend on the technical specifications of the model, \nsuch as the number of parameters or the model architecture.\n4.4.2  Return on investment\nSecond, how will the ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 171,
      "total_chunks": 245
    }
  },
  {
    "text": "he number of parameters or the model architecture.\n4.4.2  Return on investment\nSecond, how will the investm ent in another foundation model pay back for itself?\nExisting use cases\nWill this foundation model allow us to execute existing use ca ses better/cheaper/faster? How? \nHow much? How well does it align with our st rategic imperatives? Consider the following \nfactors, and the interplay between them:\n/SM590000Improve the accuracy of an existing application.\n/SM590000Improve legal indemnification for an existing application.\n/SM590000Improve the ease of governing one or more existing applications.\n/SM590000Reduce AI ethics risk for an existing application.\n/SM590000Reduce inferencing costs fo r an existing application.\n/SM590000Reduce energy consumption for an existing application.\nNew use cases\nWill this foundation model allow us to enabl e new use cases? How? How much? How well \ndoes it align with our strategic imperatives? For example:\n/SM590000The new model has been tuned to hand",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 172,
      "total_chunks": 245
    }
  },
  {
    "text": " it align with our strategic imperatives? For example:\n/SM590000The new model has been tuned to handle IT optimization use cases better than our \nexisting models, so we can now enable our IT developers to use generative AI and enhance their productivity by X%.\n\nChapter 4. Onboarding a new foundation model 51/SM590000The new model has been tuned to handle the Dutch language better than our existing \nmodels, so we can now enable a conversational assistant for our Dutch customers and improve first-time resolution by Y%.\n4.4.3  Build or buy\nInstead of creating a solution ourselves with a new foundation model, can we buy tooling that already does that? As more applications become AI-enabled, the benefits of buy rather than build could be:\n/SM590000Achieve the projected benefits faster with a turn-key application.\n/SM590000Enhanced functionality from a specialist vendor compared to what we could deliver in a \nfirst stage ourselves.\n/SM590000Focus scarce internal AI resources on  the most str",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 173,
      "total_chunks": 245
    }
  },
  {
    "text": "ld deliver in a \nfirst stage ourselves.\n/SM590000Focus scarce internal AI resources on  the most strategic AI projects.\n4.4.4  Exit strategy\nThe speed of innovation in AI is very high, how easily can we change course if something \nbetter comes along? Consider aspects such as:\n/SM590000Contractual cancellation periods.\n/SM590000Non-recoverable license, support or services costs already committed.\n/SM590000Early termination fees.\n/SM590000Charges to extract our data/IP/solutions from a vendor's platform.\n/SM590000Portability of assets and skills to a new solution.\n4.4.5  Other factors\nLastly, there might be factors not yet considered  by other stakeholders that would fall to the \nfinance team in an organization. For example:\n/SM590000Risks - many risks will already have been addressed by other stakeholder s as described in \nthe previous paragraphs in this chapter, but there might be specific ones to be addressed by the Finance function.\n/SM590000Sustainability - how does a new foundation",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 174,
      "total_chunks": 245
    }
  },
  {
    "text": "ic ones to be addressed by the Finance function.\n/SM590000Sustainability - how does a new foundation model impa ct the organization's ESG \n(environmental, social, and governance) posture? How does it impact our direct and indirect emissions or freshwater usage?\n/SM590000Security - How does the model vendor protect the organization against adversarial \nattacks, data leaks and other security risks?\n\n52 Ensuring Trustworthy AI with IBM watsonx.governance\n\n\u00a9 Copyright IBM Corp. 2025. 53Chapter 5. Assessing a new use case\nA use case in watsonx. governance is the starting point to solve a business problem using an \nArtificial Intelligence (AI) asset, such as a mode l or prompt template as  part of the solution. \nThe process of assessing a new use case for an organization involves following a business process facilitated by a workflow engine. This  process identifies risks, assesses applicable \nregulations and policies, and decides whether to approve or reject the use case from development th",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 175,
      "total_chunks": 245
    }
  },
  {
    "text": "\nregulations and policies, and decides whether to approve or reject the use case from development through production to decommissioning.\nThis chapter covers the following topics:\n/SM590000\u201cBusiness process workflow\u201d on page 54\n/SM590000\u201cApproval workflow\u201d on page 55\n/SM590000\u201cRisk identification assessment\u201d on page 56\n/SM590000\u201cApplicability assessment\u201d on page 585\n\n54 Ensuring Trustworthy AI with IBM watsonx.governance5.1  Business process workflow \nThe typical business process to assess a new use case, shown in Figure 5-1, combines \nautomated workflows in watsonx.governance with manual checks and balances to ultimately \napprove or reject a use case and record the transparent process and findings along the way. This process can always be customized to fit an organization's specific needs.\nFigure 5-1   Typical use case assessment process workflow leveraging watsonx.governance\nThe typical process flows as follows:\n/SM590000Propose the use case: The process starts with a member of the bu",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 176,
      "total_chunks": 245
    }
  },
  {
    "text": " process flows as follows:\n/SM590000Propose the use case: The process starts with a member of the business proposing a use \ncase within the watsonx.governance console. The creator could be any member of the business who originates the problem to be solved. Typically, this would be a member of a business line, but could also be technical depending on the problem to be solved.\n/SM590000Questionnaire Assessments: Once the use case is created, watsonx.governance creates \nassessment questionnaires to be completed by the business line and necessary technical teams. The answers to these questions allo w watsonx.governance to auto-populate the \nuse case with applicable risks from the IBM AI  Risk Atlas that will ne ed to be addressed \nduring the life of the use case. \n/SM590000Applicability Assessments: It is important to ensure your us e case will rema in compliant \nwith external regulations, such as the EU AI Act in Europe. Once a use case is created, \nwatsonx.governance will create an EU AI",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 177,
      "total_chunks": 245
    }
  },
  {
    "text": "uch as the EU AI Act in Europe. Once a use case is created, \nwatsonx.governance will create an EU AI Act applicability assessment to determine if the \nuse case is complaint or at risk of violating the mandates of this regulation. While the EU AI Act Assessment is installed with watsonx.governance, additional assessments are available from IBM partners for regulations that may be applicable in other regions.\n/SM590000SME Review: Once the use case and the appropriate assessments are completed, a \nsubject matter expert (SME) will review the auto -populated risks and further enrich the use \ncase with mandates, processes, and policies that could be affected by the identified risks. The SME can also setup controls to address issues that arise after the use case is approved for the next steps in the development lifecycle. watsonx.governance can help to automate the assignments of the mandates, processes, policies and controls through custom workflow created to accommodate the specific needs o",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 178,
      "total_chunks": 245
    }
  },
  {
    "text": "processes, policies and controls through custom workflow created to accommodate the specific needs of the organization.\n\n\nChapter 5. Assessing a new use case 55/SM590000Stakeholder Review & Approval: Once SME review is complete, required business \nstakeholder will be automatically notified by wa tsonx.governance  to review the use case \nand approve or reject the case . They will also be asked to provide comments to explain the \nreview decision. The number of stakehol ders notified can be automated via a \nwatsonx.governance workflow or pre-defined in a use case template depending on organization needs.\n/SM590000Use Case Approved for Development: If all stakeholders approve of the use case, \nwatsonx.governance marks it as \u201cApproved for Development\u201d. This provides audited \nauthorization for development of the solution to begin and marks the end of the initial use case assessment.\n5.2  Approval workflow\nOnce a use case is created in the watsonx. governance console, a use case approval \nwor",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 179,
      "total_chunks": 245
    }
  },
  {
    "text": "val workflow\nOnce a use case is created in the watsonx. governance console, a use case approval \nworkflow is triggered by the watsonx.gove rnance console, as shown in Figure 5-2.\nFigure 5-2   The default use case approval workfl ow executed in the watsonx.governance console\nThe use case approval workflow and a revi ew and approval process that collects key \ninformation about the use case and captures the approval or rejection of the use case from the identified stakeholders, guiding those users through all required actions along the way.\nWhile any workflow can be customized to the needs of any organization, the steps in the \ndefault use case approval workflow are:\n/SM590000Start: This step in the flow begins the moment a new use case is created. The creator is \nrequired to provide a name for the use case, and use case owner, a description of the use case, and a primary Business Entity. Optionally, the creator may also provide the purpose of the use case and its type. There are two defa",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 180,
      "total_chunks": 245
    }
  },
  {
    "text": "ptionally, the creator may also provide the purpose of the use case and its type. There are two default use case types: AI or Non-AI.\n/SM590000Use Case Data Gathering: Once the use case is created, the approval workflow moves to \ngather more detailed information about the use case to be governed. At this stage, the use case owner or risk assessor can provide the initial assumed risk level, identify the stakeholders who will provide fi nal approval, and identify the technical owner of the use \ncase. It must also be determi ned if the use case will lever age foundation mo dels during \nthis stage in the workflow. Once all the required information is complete, the user can submit the case for Initial Approval.\n\n\n56 Ensuring Trustworthy AI with IBM watsonx.governance/SM590000Initial Approval: In this stage, the use case owner completes a Risk Identification \nassessment questionnaire and validates a series of risks automatically assigned based on answers provided. In addition, the use case o",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 181,
      "total_chunks": 245
    }
  },
  {
    "text": "ates a series of risks automatically assigned based on answers provided. In addition, the use case owner can optionally complete and applicability assessmen t questionnaire for the EU AI Act. Completion of the Risk \nIdentification assessment is required to move to the Stakeholder Review stage.\n/SM590000Stakeholder Review: At this stage, the stakeholders identified in the Use Case Data \nGathering stage are notified to review the use case details, provide comments and approve or reject the use case for development. The use case cannot be approved unless all stakeholders have provided approval. On ce all stakeholders have approved the use \ncase, it is placed in \u201cApproved for development\u201d status. The owner and technical owner of the use case are automatically notified when the use case has been approved for development.\n5.3  Risk identification assessment\nAs part of the Initial Approval stage of the us e case assessment workflow, the use case owner \nor risk assessor completes a risk identi",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 182,
      "total_chunks": 245
    }
  },
  {
    "text": "e of the us e case assessment workflow, the use case owner \nor risk assessor completes a risk identifi cation assessment created by an assessment \nworkflow in the watsonx governance console, as seen in Figure 5-3.\nFigure 5-3   Risk Identification Workflow as  seen in the watsonx.governance console\nA risk identification assessment is a dynamic q uestionnaire that identifies applicable risks \nfrom the IBM Risk Atlas based upon answers provided as seen in Figure 5-4 on page 57. \n\n\nChapter 5. Assessing a new use case 57Figure 5-4   Dynamic assignment of use case risks based on risk assessment answers\nRisks assigned are determined based on a \u201cif this , then that\u201d selection structure as seen in \nFigure 5-5.\nFigure 5-5   Dynamic risk applicability  assignment selection structure rules\nOnce assigned, each risk must be evaluated and appr ove or rejected for applicability for the \nuse case. These risks are the primary bases for assessing and managing risk throughout the lifecycle of your use cas",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 183,
      "total_chunks": 245
    }
  },
  {
    "text": "risks are the primary bases for assessing and managing risk throughout the lifecycle of your use case. Wh ile risks are assigned from th e IBM Risk Atlas by default, \nadditional risks or risk sources can be added as required by an organization.\nOnce all assigned risks have been approved or re jected, the use case can be approved for \nstakeholder review and defined in the primary use case approval workflow.\n\n\n58 Ensuring Trustworthy AI with IBM watsonx.governance5.4  Applicability assessment\nApplicability assessment, as seen  in Figure 5-6, should also be completed by the use case \nowner as part of the Initial Approval stage of the use case assessment workflow.\nFigure 5-6   Applicability Assessment Workflow  as seen in the watsonx.governance console\nThis assessment enables the use case owner to assess their AI use cases using a simple \ndynamic questionnaire that aids in determining whether a use case is in scope for government regulations (such as the EU AI Act ) and which risk category",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 184,
      "total_chunks": 245
    }
  },
  {
    "text": "r a use case is in scope for government regulations (such as the EU AI Act ) and which risk category the use case aligns \nto (Prohibited, High, Limited, Minimal,  Out of Scope) as seen in Figure 5-7. \nFigure 5-7   Dynamic assignment of risk category based on applicability assessment \n\n\nChapter 5. Assessing a new use case 59The questionnaire to determine the risk category follows a \u201cif this then that\u201d selection \nstructure as seen in Figure 5-8.\nFigure 5-8   The questionnaire to determine the risk category\n\n\n60 Ensuring Trustworthy AI with IBM watsonx.governance\n\n\u00a9 Copyright IBM Corp. 2025. 61Chapter 6. Governing the end-to-end \nlifecycle of an AI asset\nLifecycle governance is a majo r pillar of the IBM AI Govern ance framework. This pillar \nrepresents the idea that any AI asset should be tracked throughout the lifetime of its usage, without gaps in lineage  or traceability. \nRegardless of where the AI asset originates from, watsonx.governance comes with tooling to \nmonitor performance a",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 185,
      "total_chunks": 245
    }
  },
  {
    "text": " where the AI asset originates from, watsonx.governance comes with tooling to \nmonitor performance and also to  monitor the lifecycle stage that  the asset is in. Lifecycle \ngovernance applies to traditional ML/AI use cases as well as generative AI.\nEnd-to-end lifecycle govern ance results in increased efficiency through automated \nevaluations of models as well as the publishing of evaluation results and metrics. This also introduces traceab ility, auditability, and en hanced project management  capabilities through \ntransparent ML/AI asset lifecycle management functionality. \nThis chapter has th e following sections:\n/SM590000\u201cWhat is the AI lifecycle?\u201d on page 62\n/SM590000\u201cMetrics in watsonx.governance\u201d on page 64\n/SM590000\u201cHow to implement Lifecycle Governance\u201d on page 66\n/SM590000\u201cLifecycle implementation and considerations\u201d on page 676\n\n62 Ensuring Trustworthy AI with IBM watsonx.governance6.1  What is the AI lifecycle?\nBefore reading through how watsonx.governance defines the lif",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 186,
      "total_chunks": 245
    }
  },
  {
    "text": "vernance6.1  What is the AI lifecycle?\nBefore reading through how watsonx.governance defines the lifecycle of an AI asset, it is \nuseful to review the general lifecycle for data projects. \nFigure 6-1 represents the CRISP-DM methodolog y which encapsulates an industry-standard \nblueprint for completing data mining or data science projects.\nFigure 6-1   The CRISP-DM Methodology (Cross- industry Standard Process for Data Mining)\nBecause AI is a form of machine learning (albeit a very large and complex version), every \nstep of the CRISP-DM applies to the AI lifecyc le during initial found ation model development \n(commonly referred to as model pretraining.) Differences begin to appear when we consider an AI asset after it has been developed. It is unreasonable to expect every enterprise to train their own model from scratch. For this reason, in virtually every case, citizen engineers and scientists will engage with AI asse ts which have already been pr e-trained, and therefore, a \nmodified",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 187,
      "total_chunks": 245
    }
  },
  {
    "text": "entists will engage with AI asse ts which have already been pr e-trained, and therefore, a \nmodified version of the CRISP-DM methodology should be used when considering the lifecycle of an AI asset.\nThe pre-trained nature of foundation models allows us to create a more focused view on the \nlifecycle of AI assets being used in use case s. This lifecycle can be generally depicted with \nthree chronological stages:  Develop stage, Validate stage, and Operate stage .\nFigure 6-2 on page 63 highlights the general lifecycle stages in a red box.\n\n\nChapter 6. Governing the end-to-end lifecycle of an AI asset 63Figure 6-2   A Sample Lifecycle Diagram\nWithin each stage of this AI lifecycle, action s should be taken by users to progress the AI \nasset through its lifecycle. The following section cove rs the high-level activities that take place \nwithin each of the three stages.\nDevelopment stage\nIn the development stage, users must work to stand up the initial AI use case solution. As \ndescribed in ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 188,
      "total_chunks": 245
    }
  },
  {
    "text": "e development stage, users must work to stand up the initial AI use case solution. As \ndescribed in Chapter 4, \u201cOnboarding a new foundation model\u201d on page 41 and Chapter 5, \u201cAssessing a new use case\u201d on page 53, the AI use case will go through an approval process \nbefore being worked on by the appropriate pr actitioners. Once the approval process is \ncomplete, an engineer may begin to develop the technical assets for the AI use case. The engineer can achieve many things in this st age. Prompt engineering, parameter tuning, and \nsolution experimentation all fall within the deve lopment stage. For trad itional ML, this stage \nalso includes activities such as feature en gineering, exploratory data analysis, and other \npre-processing tasks which go into the model development process.\nValidation stage\nAfter development efforts are co mplete, the validation stage of the life cycle is started. This \nstage represents the process of putting development efforts through testing. An independent pr",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 189,
      "total_chunks": 245
    }
  },
  {
    "text": "This \nstage represents the process of putting development efforts through testing. An independent practitioner such as an AI engineer or data scientist can use IBM's watsonx.governance to \nrun automated Prompt Evaluations, which will re turn evaluation metrics for the given AI use \ncase. A programmer can also use custom code to run evaluations. Note:  In traditional ML use cases, we also split use Testing stages in addition to validation \nstages. This will be covered in t he considerations section for ML.\n\n\n64 Ensuring Trustworthy AI with IBM watsonx.governanceSimilarly, for traditional ML, validation is achieved through the traditional data science \napproach of utilizing validat e/test datasets to unde rstand model performance.\nUsing watsonx.governance, the validation stage represents our first opportunity at automating \notherwise lengthy evaluations of ML and AI models. It is the first glance of how our models may perform against performance metrics such  as readability for Q&A LLM u",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 190,
      "total_chunks": 245
    }
  },
  {
    "text": " glance of how our models may perform against performance metrics such  as readability for Q&A LLM use cases, or \naccuracy for classi fication use cases.\nOperation stage\nThe operation stage represents any AI or ML asset which reaches production-level efforts. In \nthis stage of the AI lifecycle, the model asset is live and consumed by users. Model \nmonitoring and asset lineage become central at this stage. The IBM watsonx.governance platform provides tools for monitoring critical asset metrics such as drift, fairness, and bias.\nBefore diving into implementation, the next section will briefly review the different kinds of \nmetrics that can be utilized during lif ecycle governance with watsonx.go vernance.\n6.2  Metrics in watsonx.governance\nThe metrics in this section are by no means exhaustive and watsonx.governance is capable \nof implementing custom monitors  and metrics based on user ne eds. The ability to add custom \nmonitors and metrics sets watsonx.governance apart from its competit",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 191,
      "total_chunks": 245
    }
  },
  {
    "text": "eds. The ability to add custom \nmonitors and metrics sets watsonx.governance apart from its competitors in the market. This section gives an overview to some of the most common and important kinds of metrics that can be applied to assets for effective lifecycle monitoring.\n6.2.1  Drift detection\nUsers can configure Drift v2 evaluations  in watsonx.governance to measure changes in their \ndata over time to ensure consistent outcomes  for the model. These evaluations can be used \nto identify changes in the model output, the accu racy of your predictions, the distribution of \nthe input features, the metadata and more.\nThe drift in the user deployments is always detected in comparison to a baseline data. This \nbaseline data needs to be a good representation of the ideal dataset that the user is expecting in their deployment. It can be the training data used to train the predictive model, the test data used to validate the model, or even the past production data. As part of the monitor confi",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 192,
      "total_chunks": 245
    }
  },
  {
    "text": "test data used to validate the model, or even the past production data. As part of the monitor configuration process, certain computations are learned on the baseline data to learn the data patterns. These can vary from dividing  data in frequency bins to learn the density \nfunctions of your input features ( feature drift ), to training auto-encoders to learn the context \nrepresented by the embeddings ( embedding drift ), training proxy/meta models to learn user \nmodel behavior ( model quality drift ) and to look at how the meta data like character counts and \nword counts is changing ( input and output metadata drift ). Any change in the data is reported \nas a metric on different dimensions. \n6.2.2  Explainability\nFor the predictive AI models, watsonx.governance gives users a sneak peek into the black box by giving localized explanations to understand how the different feature values are impacting the outcomes of the specific transact ions. By aggregating these local explanations \nfor ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 193,
      "total_chunks": 245
    }
  },
  {
    "text": " impacting the outcomes of the specific transact ions. By aggregating these local explanations \nfor a sample of such transactions, a global explanation is presented so that the user can understand the general factors that are influencing the model decisions. \n\nChapter 6. Governing the end-to-end lifecycle of an AI asset 65To this end, wats onx.governance utilizes both  open-source algorithms ( LIME and SHAP ) and \nIBM Research\u00ae built contrastive explanations. By generating and analyzing data points, in \nthe vicinity or the local neighborhood of a given transaction, Local Interpretable Model-agnostic Explanations or LIME can tell which features of a structured record, words \nand phrases from a text paragraph, and which areas of the image are responsible for the model outcome. SHAP (SHapley Additive exPlanati ons) is rooted in game theory as it uses \nthe classic Shapley values, to determine how much of each of the features has contributed to the model prediction. IBM Research built contr",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 194,
      "total_chunks": 245
    }
  },
  {
    "text": "e how much of each of the features has contributed to the model prediction. IBM Research built contrastive explanations that look at the neighborhood of a given data point, to determine how much of a delta change is required in the input features to flip the model outcome or to maintain the same outcome. A highly important feature in this case is a feature to wh ich the model is least se nsitive as they require \na large change in the value for model to flip the outcome.\n6.2.3  Model health\nTo understand the model health and performance of a given model deployment (for both predictive AI and generative AI AI-based models), it is imperative to know the how the said deployment is being used. To aid with that, watsonx.governance helps in calculating and visualizing the total number of scoring requests  received by the deployment in a given time \nperiod. Across these requests, common statistical attributes like minimum, maximum, mean, and median of the number of records  are also calculated",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 195,
      "total_chunks": 245
    }
  },
  {
    "text": "cal attributes like minimum, maximum, mean, and median of the number of records  are also calculated. It is also important to know how \nmuch time is taken by the system to process a record and/or a request. Similarly, watsonx.governance can also measure throughput of the system by looking at the number of \nrecords and requests processed in a second. For generative AI-based deployments, the input and output token counts processed across the scoring requests can be visualized as well. If \nthere are multiple users registered on the system and using the deployments, watsonx.governance can also present the real-tim e view to see the total number of users and \nthe aggregated views to see the average number of users.\n6.2.4  Generative AI quality\nTo assess the quality of the content generated by a prompt, watsonx.governance has many Generative AI Quality metrics. Some of these metrics work in the presence of a reference input and hence work off the feedback data. However, there are reference f",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 196,
      "total_chunks": 245
    }
  },
  {
    "text": "e presence of a reference input and hence work off the feedback data. However, there are reference free metrics as well, that do not require any reference input and can be calculated on the production data.\nIBM watsonx.governance also allows the use of widely available LLM models to evaluate the \nperformance of the user prompts through the LLM-as-a-judge feature.\n6.2.5  RAG quality metrics\nBy adding external sources of context to the prompt of an LLM, Retrieval Augmented Generation (RAG) systems enhance the quality of the content generated by the model. With this perspective in place, watsonx.governance can monitor both the phases of a RAG-based \nsystem. \nThe retrieval phase can be assessed by looking at the context pulled and seeing how relevant \nit is to the question asked by the user (context  relevance). By looking at retrieval precision, \none can tell if the retrieved information is directly addressing the user query. The system also looks at how the different retrieved contexts a",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 197,
      "total_chunks": 245
    }
  },
  {
    "text": " directly addressing the user query. The system also looks at how the different retrieved contexts are ranked. If the most relevant context has the top rank, the metric reciprocal rank will be 1 el se it will be much lower. The ranking quality of \nthe retrieved information can also be measured  by Normalized Discoun ted Cumulative Gain \n(NDCG) as a higher score on this metric, indicates the retrieved contexts are ranked in the correct order. \n\n66 Ensuring Trustworthy AI with IBM watsonx.governanceThe content generation phase can be assessed not only by looking at the overall quality of the \nanswers generated by the system, but also by analyzing the content watsonx.governance can tell how much of the context is used in the answers generated. By measuring how well the answer aligns with the context (faithfulness), or  by measuring if the answer is relevant to the \nprompt (answer relevance) or by looking at the number of questions that were unanswered by the model (unsuccessful requests),",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 198,
      "total_chunks": 245
    }
  },
  {
    "text": " or by looking at the number of questions that were unanswered by the model (unsuccessful requests), watsonx.governance gives the quality of the answers. The prominent content analysis metrics measure the percentage of keywords in the answer that are derived from the context (coverage) and the overall sequence of words in the answer are direct extractions from the context (density). \nIn addition to the above metrics, along with th e faithfulness, watsonx.governance also gives \nout the top source attributions for the generated by answer by highlighting the relevant sentences in the context. This capability tries to o pen the black box, and is a step in the \ndirection of providing explainability for the foundation model- generated answers. \nWith a review of the common stages, we can now move onto implementation details for how \nto enable and complete lifecycle monitoring.\n6.3  How to implement Lifecycle Governance\nLifecycle Governance begins at the use case level. In watsonx.governance, ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 199,
      "total_chunks": 245
    }
  },
  {
    "text": "ment Lifecycle Governance\nLifecycle Governance begins at the use case level. In watsonx.governance, an AI use case is created and used to maintain a hierarchical organization of AI assets. This AI use case structure allows users to view their ML/AI assets  as they relate to the business problem of the \nuse case. \nOne use case can hold multiple assets with each asset being represented by a factsheet. \nThese factsheets hold all details of a given asset. Consider an AI factsheet to be a sort of \u201cnutrition label\u201d to the underlyin g AI asset. This factsheet is w hat will be used to  represent the \nAI asset as it moves through each stage of its lifecycle.\nThe following sections will show us how to se t up AI use cases and how to create AI \nfactsheets for AI assets.\n6.3.1  Getting started: Setting up your AI use cases\nAfter installation and adminis tration of watsonx.governance,  we can implement lifecycle \ngovernance beginning at the use case level. The purpose of lifecycle governance is mon",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 200,
      "total_chunks": 245
    }
  },
  {
    "text": "nt lifecycle \ngovernance beginning at the use case level. The purpose of lifecycle governance is monitoring the progress of AI assets individually. These AI assets progress through what is called an AI use case. These AI use cases are central to lifecycle monitoring because it allows us to group AI assets together which are working towards similar goals.\nBefore setting up lifecycle moni toring for the AI assets, we will set up our AI use case in \nwatsonx.governance.\nAI use case and AI  factsheet setup\nFollow these steps to set up the AI use case in watsonx.govenance:\n1. Use the Options menu from the watsonx home page  to access the AI use cases  page \n(under  AI governance \u2192 AI use cases ). See Figure 6-3 on page 67.\n\nChapter 6. Governing the end-to-end lifecycle of an AI asset 67Figure 6-3   AI use cases\n2. Create a new inventory using the options menu from the AI use cases  page. \nBe sure to activate Governance Console integration  to ensure that your AI Inventory is \nsynced with you",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 201,
      "total_chunks": 245
    }
  },
  {
    "text": "ure to activate Governance Console integration  to ensure that your AI Inventory is \nsynced with your Governance console. This synchronization effect persists throughout the work done in this chapter.\n3. Once the new inventory is created, create an AI use case  and fill out all de tails for that AI \nuse case. Any details posted to this AI use case  will also be reflected in the Governance \nConsole. \nWith an AI use case successfully  created, we will move  on to tracking the lifecycle of an AI \nasset for that AI use case. This  will be achieved by attaching AI  factsheets to the AI assets \nrealizing and implementing the AI use case and using that AI factsheet to accomplish lifecycle governance. As mentioned 6.3, \u201cHow to implement Lifecycle Governance\u201d on page 66, an AI factsheet is the organizational representation of  an AI asset which gives us all asset details. \nThe factsheet also gives us an effective vehicle for traversing between lifecycle stages.\nCreating an AI factsheet for a gi",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 202,
      "total_chunks": 245
    }
  },
  {
    "text": "s us an effective vehicle for traversing between lifecycle stages.\nCreating an AI factsheet for a given as set will be covered in de tail in subsequent \nsub-sections. Once the AI factsheet is crea ted, we can move on to lifecycle governance.\nAs described in 6.1, \u201cWhat is the AI lifecycle?\u201d on page 62, there are general activities in each \nlifecycle stage which should be completed before moving onto the next lifecycle stage. Using \nwatsonx.governance, we can achieve full lifecycle monitoring for the AI asset. The following subsections will cover the basic steps for accomplishing lifecycl e governance using \nwatsonx.governance.\nThe following subsections assume that the read er is experienced with LLM experimentation \nand that they are ready to begin governing and monitoring their AI solutions. Special considerations exist based on implementation det ails such as what platform the model is \nhosted on, and what is the use case. Thes e considerations will be  found under each \nappropriate s",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 203,
      "total_chunks": 245
    }
  },
  {
    "text": "\nhosted on, and what is the use case. Thes e considerations will be  found under each \nappropriate subsection.\n6.4  Lifecycle implementation and considerations\nThis section of the chapter will assume that there is a valid AI use case configured in \nwatsonx.governance. It will also  make the assumption that a solution exists (either on \nwatsonx.ai or another third party)  which realizes that AI use case. One final major assumption \nis the existence of ground-truth data. \n\n\n68 Ensuring Trustworthy AI with IBM watsonx.governanceAs mentioned in the explanation of the validation stage, evaluations provide first-hand \nperformance reviews to assets in a qualitative and quantitative way. To achieve this, ground-truth data is critical and must be acquired in order to effectively implement lifecycle governance.\nThe goal of this section is to explain the connection between the assets (prompt template \nassets or models) that realize the use case, which lifecycle stage those assets belong in, and h",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 204,
      "total_chunks": 245
    }
  },
  {
    "text": "te \nassets or models) that realize the use case, which lifecycle stage those assets belong in, and how we automate the lifecycle of those assets as they  progress from devel opment through to \noperation.\nBefore diving into specific considerations, here are the higher-level activities which must be \ncompleted to effectively govern the lifecycle of an AI asset. A Prompt Template Asset or a Model must exist that contributes to the implementation of an established use case. A Prompt Template Asset (PTA) can be either a Prompt Te mplate from IBM's watsonx.ai, or a detached \nprompt template representing a prompt on a 3rd party platform. Similarly, a Model is simply any ML model which solves a traditional data  science / ML use case and can be either \ndeployed in IBM watsonx.ai runtime or a 3rd party serving platform. With the asset ready, the steps are as follows:\n/SM590000Track the solution in an AI use case to place in the development stage.\n/SM590000Perform an evaluation to move the asset",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 205,
      "total_chunks": 245
    }
  },
  {
    "text": "n an AI use case to place in the development stage.\n/SM590000Perform an evaluation to move the asset to the validation stage.\n/SM590000Promote the asset to a production space to move the asset into the operation stage.\nThe above three steps are repeated in some variation based on platform and use case \nspecifics. As a reminder, everyt hing accomplished in this chapter will be reflected across the \nGovernance Console for administrative and business users to be automatically updated of \nprogress.\n6.4.1  UI-driven implementa tion of lifecycle governance\nAfter installation and adm inistration, watsonx.ai s eamlessly integrates with \nwatsonx.governance. Once an AI use case is established, users can run through a typical \nwatsonx.ai workflow to begin experimenting with prompts in the prompt lab, or in a jupyter notebook environment. For instructions on how to perform this experimentation, refer to the watsonx.ai documentation  or the Redbooks publication: Simplify Your AI Journey: Unleashing",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 206,
      "total_chunks": 245
    }
  },
  {
    "text": "r to the watsonx.ai documentation  or the Redbooks publication: Simplify Your AI Journey: Unleashing \nthe Power of AI with IBM watsonx.ai , SG24-8574.\nDevelopment\nAfter saving your prompt lab experiment, from  the project home screen of watsonx.ai, view \nyour assets tab. Find your saved prompt lab template, and using the hamburger menu of that prompt lab template, choose Track in AI use case . After following the gu ided click-through \nsetup, your Prompt Template Asset (PTA) will be automatically placed under the development stage, tracked in the AI use case of your choice.\nValidation\nWith a PTA being tracked against an AI use ca se in the development stage, users can utilize \nthe power of watsonx.governance to perform fast and effective evaluations of their assets. To perform this step, validation data is required. Note:  This subsection covers a UI-driven implementation of lifecycle governance using \nwatsonx.gov and watsonx.ai. For code-driven approaches, review 6.2.3, \u201cModel health\u201d",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 207,
      "total_chunks": 245
    }
  },
  {
    "text": "vernance using \nwatsonx.gov and watsonx.ai. For code-driven approaches, review 6.2.3, \u201cModel health\u201d on page 57 and 6.2.4, \u201cG enerative AI quality\u201d on page 57 . The watsonx.ai documentation will \nalso be helpful.\n\nChapter 6. Governing the end-to-end lifecycle of an AI asset 69Work with use case SMEs and other teammates in order to build up a quality dataset of at \nleast 10 records in CSV format which you can use to validate the performance of your PTA. Based on your use case, the fo rmat of your data will vary. Expl ore the evaluation page for \nyour specific use case in order to identify the correct format for your validation data. For example, a summarization use case will have the following format for th e test data:\n-Original_Text1, Ground_Truth_Summary1-\n-Original_Text1, Ground_Truth_Summary2-\nOnce data is acquired, use the hamburger menu to ac cess the options of the given PTA. Click \non Evaluate to open the evaluation page. Using the Actions drop-down, click on \u201cEvaluate Now\u201d and",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 208,
      "total_chunks": 245
    }
  },
  {
    "text": "k \non Evaluate to open the evaluation page. Using the Actions drop-down, click on \u201cEvaluate Now\u201d and follow through the options on-screen to set up the evaluation experiment. To review the available evaluation metrics, check the latest documentation for your version of watsonx.governance.\nAfter completing the evaluation, go back to your AI use case and scroll down. You should now \nsee this asset as being tracked in the validation stage as a result of running the evaluation experiment. Before moving onto the operation stage, it is most common for assets to sit in the validation stage while iterations happen to improve performance. Evaluations can be constantly run against assets in the validation stage and the records of the latest evaluation can be viewed from the AI factsheet for that asset.\nOperation\nOnce the validation stage is completed and the asset is ready to move into operation stage, head to your watsonx platform home page. Using the hamburger menu from the left, click on Depl",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 209,
      "total_chunks": 245
    }
  },
  {
    "text": "tage, head to your watsonx platform home page. Using the hamburger menu from the left, click on Deployments  to access the watsonx.ai runtime platform (traditionally known as Watson \nMachine Learning or WML), and create a new deployment space with \u201cProduction\u201d type selected. Go back to the watsonx.ai project which holds the PTA, and using the options menu attached to the PTA, choose the Promote to Space  option. Follow the on-screen guidance to \npromote the PTA to the newly created production space.\nGo to the home page of the deployment space on watsonx.ai runtimes. From the assets tab, \nthe recently promoted PTA should be listed. Use the options menu attached to the PTA from this screen and choose the Deploy  option. Follow the guided scree n to fill out the relevant \ndetails, and click OK.\nAfter a few moments, your new PTA will be deployed (and  consumable) on the production \ndeployment space. You can now check again from your AI use case page to observe that the asset has moved from",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 210,
      "total_chunks": 245
    }
  },
  {
    "text": "t space. You can now check again from your AI use case page to observe that the asset has moved from development, to validation, and finally into the operation stage. Congratulations, you have successfully performed a lif ecycle on this AI asset!Note:  There are multiple ways to access PT A evaluations. You could access this page \nthrough your AI Factsheet, on the second tab. You could also run evaluations directly from the Prompt lab. Feel free to explore the a ccessibility of this wats onx.governance feature.\nNote:  Configuring monitors is an additional ta sk beyond the concept of simply moving \nthrough a lifecycle stage. This is specifically related to the operation stage of any AI asset; \nonce the asset is in production (operation stage), the monitors for that deployment can be configured to track specific metrics. To achieve this through the UI for a watsonx.ai prompt, use the Actions  drop down menu on the top right section of the production deployment \nand choose Activate Monito",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 211,
      "total_chunks": 245
    }
  },
  {
    "text": "ns  drop down menu on the top right section of the production deployment \nand choose Activate Monitors . Finally, follow the guided instru ctions to complete monitor \nconfiguration.\n\n70 Ensuring Trustworthy AI with IBM watsonx.governanceFrom this point on, considerat ions for each of th e following sub-sections will avoid repeating \ninformation and more ad vanced topics will be introduced throughout each se ction. At the \nhighest level, all integrations of watsonx.governance with ML/AI providers work with the same \nconcept of having an AI use ca se set up at watsonx.gover nance. The following sections will \nbecome more technical with various links to resources in the form of SDKs, notebooks, and code commands to help programmers accelerate implementation.\n6.4.2  Considerations for lifecycle go vernance for traditional ML hosted on \nwatsonx.ai\nThe same seamless integration applies for traditional ML when considering the watsonx.ai \nand watsonx.governance platforms. Assuming the initial ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 212,
      "total_chunks": 245
    }
  },
  {
    "text": "ditional ML when considering the watsonx.ai \nand watsonx.governance platforms. Assuming the initial ML model has been saved as an asset to the watsonx.ai project space, it can be associated to a use case through the UI in the same manner as explained in section 6.4.1. In addition to the UI-based approach, lifecycle governance can be implemented via code. \nThis section will focus on useful APIs that can be used to achieve full lifecycle governance this \nas well as code examples. It will make an assumption of intermediate Python programming skills on behalf of the implementer.\nAPI and SDK fo r watsonx.governance\nThis section assumes an API or Python-driven SDK approach to programming is being \nundertaken. Two main API and SDK can be used to work with watsonx.governance. Other APIs also exist which can help, and they w ill be mentioned as ne cessary throughout the \nchapter.\n/SM590000AI Factsheets API and SDK - The watsonx.governance AI Factsheet API  and SDK is used \nto control the factsh",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 213,
      "total_chunks": 245
    }
  },
  {
    "text": "sheets API and SDK - The watsonx.governance AI Factsheet API  and SDK is used \nto control the factsheets co mponent of watsonx. governance. It will be used to set up \nfactsheet objects and associate them with given assets.\n/SM590000OpenScale API and SDK - The watsonx.governance OpenScale API  and SDK controls \nthe computational layer of watsonx.governance. The brunt of activity comes from this API. \nUsers can accomplish a variet y of activities including:\n\u2013 Creating subscriptions to monitor deployments.\n\u2013 Performing model evaluations.\u2013 Associating existing factsheets to AI use cases.\nAdditionally, Governance Console APIs  are also available for interacting with Governance \nConsole within watsonx.governance.\nMetrics\nThe software comes out-of-box with a plethora of traditional metrics to monitor against. For more information, see Quality evaluations . Additionally, users can implement their own \ncustom evaluation metrics.\nFor custom metrics implementation, users can use this sample noteb",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 214,
      "total_chunks": 245
    }
  },
  {
    "text": "r own \ncustom evaluation metrics.\nFor custom metrics implementation, users can use this sample notebook  from the \nIBM GitHub.Note:  This subsection will utilize code-drive n approaches. For more information, see \nTracking a machine learning model .\n\nChapter 6. Governing the end-to-end lifecycle of an AI asset 71Inventory and AI use case setup\nAI Inventories are used to hold AI use cases. These Inventories are agnostic to asset type - a \ngenerative AI use case can be held in the same inventory as a traditional ML/AI use case.\nInventory setup\nThis notebook from IBM's github showcases how to create an AI Inventory. This sample \nnotebook  demonstrates a variety of functionality including:\n/SM590000Open a new AI inventory\n/SM590000Add collaborators\n/SM590000Delete inventory\n/SM590000Modify existing inventories\nBecause the AI Inventory concept is native to watsonx.governance, it is always implemented \nregardless of where the AI assets live which are being tracked.\nAI use case setup\nThis not",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 215,
      "total_chunks": 245
    }
  },
  {
    "text": "lemented \nregardless of where the AI assets live which are being tracked.\nAI use case setup\nThis notebook from the IBM GitHubshowcases how to create an AI use case. The sample \nnotebook  demonstrates a variety of functionality including:\n/SM590000Storing a model as an asset into watsonx.ai (this creates the AI factsheet)\n/SM590000Create a new AI use case\n/SM590000Associate the AI use case with a watsonx.ai project\n/SM590000Track/Untrack a model under an AI use case\nDevelopment\nIn \u201cInventory and AI use case setup\u201d on page 71, the linked notebook holds all relevant code \nfor placing a model into the development stage of its lifecycle. Here it is again in condensed form.\n# create an AI use case\nai_usecase = \nfacts_client.assets.create_ai_usecase(catalog_id=ai_usecase_inventory_id,name=ai_usecase_name,description=ai_usecase_desc)\n# track the model in the use casewatsonx_ai_model.track(usecase=ai_usecase,approach=decisiontree_approach,\nversion_number=\"major\",version_comment=\"major update to",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 216,
      "total_chunks": 245
    }
  },
  {
    "text": "e=ai_usecase,approach=decisiontree_approach,\nversion_number=\"major\",version_comment=\"major update to previous version\")Note:  The notebook linked above also covers how to code the configuration of quality \nmonitors, fairness, drift, and explanations. It is highly recommended to consult sample \nnotebooks for thorough and up-to-date instructions. For users who are looking to create data configurations to specific data science problems, see the IBM directory  for different \noptions on creating data configurations.\nNote: The following pseudo-code requires additional code to function properly. Users \nshould consult the notebook and the API documentation linked in the previous sections for a full walkthrough.\n\n72 Ensuring Trustworthy AI with IBM watsonx.governanceValidation\nOnce the model is successfully tracked against an  AI use case, it is placed into development \nstage. If the model is already deployed into a production space,  it will instead show in the \noperation stage. This section w",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 217,
      "total_chunks": 245
    }
  },
  {
    "text": "eady deployed into a production space,  it will instead show in the \noperation stage. This section will assume we have a model in the development stage of the \nlifecycle. \nThere are multiple ways to move a model from the development stage into the validation \nstage. One way is to use the direct API call:\nmodel.set_environment_type(from_container=\"develop\", to_container=\"validate\"). \nFor more information, see Managing the Lifecycle Phases of a Model .\nAdditionally, you can deploy the model to a development deployment space or a validation \ndeployment space and this will trigger the lifecycle move . For more information, see Tracking \nprompt templates .\nThis functionality applies to all kinds of implementation, not just traditional ML.Users can set the environment directly if the as set is in a preceding stage, or users can \npromote models to a validation space in watsonx.ai runtimes.\nOperation\nThe operation stage is a uniquely important stage of the AI/ML lifecycle. Section 6.4.1, \u201cUI-d",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 218,
      "total_chunks": 245
    }
  },
  {
    "text": "ation\nThe operation stage is a uniquely important stage of the AI/ML lifecycle. Section 6.4.1, \u201cUI-driven implementation of lifecycle governance\u201d on page 68 covered basic steps to move an asset to the operation, but many more ac tivities can be accomplished for a thorough \nimplementation of a production monitor. As mentioned in section 6.2, \u201cMetrics in watsonx.governance\u201d on page 64 and again in 6.4.2, \u201cConsiderations for lifecycle governance for traditional ML hosted on watsonx.ai\u201d on page 70, a variety of metrics are available for configuration through watsonx.governance. When considering a model monitor in the operation stage, these metrics can be configured using the watsonx.governance UI or using notebooks. Review and follow the notebooks and directions provided in section 6.4.2, \u201cConsiderations for lifecycle governance for tradi tional ML hosted on watsonx.ai\u201d on page 70 \nto establish baseline data configurations and respective monitors.\nAdditionally, code commands also exist to ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 219,
      "total_chunks": 245
    }
  },
  {
    "text": "ish baseline data configurations and respective monitors.\nAdditionally, code commands also exist to directly move assets into the production (operation) \nstage. Here is a snippet on how to move a model into a production deployment space to see the lifecycle stage progre ss into operation stage:\nmodel.set_environment_type(from_container=\"validate\", to_container=\"production\")\nwatsonx.governance comes with an expansive toolkit for handling the lifecycle of traditional \nML/AI assets. Considerations for third parties do exist, but the tools that we have covered up to this point will be adapted to cove r those additional considerations.\n6.4.3  Considerations for prompt  templates from another platform\nIBM watsonx.governance empowers organization s to evaluate and monitor prompt templates \nfor a variety of externally-hosted LLMs without the need to conduct inference on those models. This flexibility allows AI and Data Sc ience practitioners to work with models hosted \non platforms such as Goo",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 220,
      "total_chunks": 245
    }
  },
  {
    "text": "ility allows AI and Data Sc ience practitioners to work with models hosted \non platforms such as Google Vertex AI, Azure OpenAI, or AWS Bedrock, where all inference is performed remotely.\nThe platform offers a method known as the \ndetached prompt template  for evaluating and \nmonitoring prompt templates for externally-hosted LLMs without requiring model inference. \n\nChapter 6. Governing the end-to-end lifecycle of an AI asset 73This approach involves programmatically creating a detached prompt template asset, which \nprovides a high level of control. Evaluations are then conducted on the generated prompt output, with the results logged into watsonx.governance against the detached prompt template.\nAdditionally, one can evaluate a detached prompt template within a deployment space by \ncreating a detached deployme nt. This setup offers severa l benefits and capabilities:\n/SM590000Evaluating a prompt template within a project or space enhances the experience of \nreviewing evaluation results",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 221,
      "total_chunks": 245
    }
  },
  {
    "text": "a prompt template within a project or space enhances the experience of \nreviewing evaluation results.\n/SM590000One can utilize access control for projects and sp aces to invite collabo rators or restrict \naccess as needed.\n/SM590000The results of the evaluations can be tracked in factsheets related to AI use cases as part \nof the governance solution.\nThe sample code to create a detached prompt template is shown in Example 6-1.\nExample 6-1   Sample code to create a detached prompt template\nfrom ibm_aigov_facts_client import DetachedPromptTemplate, PromptTemplate\ndetached_information = DetachedPromptTemplate(\nprompt_id=prompt_id,model_id=model_id,    model_provider=model_proivder,    model_name=model_name,    model_url=model_url,    prompt_url=prompt_url,    prompt_additional_info=prompt_additional_info)\nprompt_template = PromptTemplate(\n    input=input_text,    prompt_variables=prompt_variables,    input_prefix=input_prefix,    output_prefix=output_prefix,    model_parameters = model_pa",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 222,
      "total_chunks": 245
    }
  },
  {
    "text": "iables,    input_prefix=input_prefix,    output_prefix=output_prefix,    model_parameters = model_parameter)\nexternal_prompt_template_details = facts_client.assets.create_detached_prompt(\n    name=prompt_name,    description=prompt_description,    model_id=model_id,    task_id=task_id,    prompt_details=prompt_template,    detached_information=detached_prompt_template)project_pta_id = external_prompt_template_details.to_dict()[\"asset_id\"]\nOnce the detached prompt template is created, it follows the same lifecycle as described in \n6.4.2, \u201cConsiderations for lifecycle governance for traditional ML hosted on watsonx.ai\u201d on page 70.Note:  The following pseudo-code requires additional code to function properly. For more \ninformation, see the notebook  and the API documentation .\n\n74 Ensuring Trustworthy AI with IBM watsonx.governance6.4.4  Considerations for tradit ional ML from another platform\nJust like prompt templates from other platforms, ML assets and deployments can exist \noutside of",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 223,
      "total_chunks": 245
    }
  },
  {
    "text": "orm\nJust like prompt templates from other platforms, ML assets and deployments can exist \noutside of watsonx.governance  and still be monitored and tr acked by watsonx.governance. \nLifecycle governance of third-party ML mo dels deployed on other platforms can be \nimplemented through python code. IBM watsonx.governance provides all of the tools and methods necessary to achieve lifecycle governance regardless of where the asset exists. Section 6.4.2, \u201cConsiderations for lifecycle governance for traditional ML hosted on watsonx.ai\u201d on page 70 introduces the tools which will be used to achieve these goals.\nMany of the same steps apply when implementing lifecycle governance for ML assets \ndeployed on other platforms. Review the setup steps covered in section 6.4.2, \u201cConsiderations for lifecycle governance for tradi tional ML hosted on watsonx.ai\u201d on page 70 \nto configure the AI Inventory and AI use case and review the introductions to the packages along with the data statistics configuratio",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 224,
      "total_chunks": 245
    }
  },
  {
    "text": "AI use case and review the introductions to the packages along with the data statistics configurations note books in that section. Once those activities \nare complete, new assets can be tracked against use cases in their appropriate lifecycle stages.\nIt is worth noting that many ML processes wh ich exist on other platforms are typically \nconsidered to be in a production state. Because of this, model tracking for third-party assets is most typically an administrative and organizati onal task while that asset is being developed \nand validated in its respective environment. Once the asset arrives into the production or operation stage, monitors can be configured for thorough and effective ML governance. For full instructions on how to configure headless subscriptions for ML models which are not hosted through watsonx.governance, see this notebook .\nBatch processing can also be achieved with watsonx.governance. For step-by-step \ninstructions on achieving batch processing, see this notebook",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 225,
      "total_chunks": 245
    }
  },
  {
    "text": " watsonx.governance. For step-by-step \ninstructions on achieving batch processing, see this notebook , the documentation .\n6.4.5  Governing AI embedde d in a business application\nIf an AI/ML asset which is consumed in a busine ss application is directly owned and operated \nby the governance team (or a department from the same organization as the governance team), any of the above techniques can be used to apply AI governance. Depending on the \nkind of AI/ML being used in the business application, a user may have to configure a detached prompt template for LLM (as described in section 6.4.3, \u201cConsiderations for prompt templates from another platform\u201d on page 72) or they may have to configure a headless subscription for ML (as described in section 6.4.4, \u201cConsiderations for traditional ML from another platform\u201d on page 74) If the ML/AI process is being provided by an outside vendor, various considerations should be made when look ing to apply governance to that process. \nThis section wil",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 226,
      "total_chunks": 245
    }
  },
  {
    "text": "s considerations should be made when look ing to apply governance to that process. \nThis section will shed light on some of the mo st common and im portant things to consider \nwhen looking to govern an existing business  application where the AI/ML process is being \nprovided through a vendor / service from outside the organization.\nConsider accessibility\nIf the governance team  is not the direct owner of an AI process, the accessi bility of that AI \nprocess must be explored and evaluated. Contact the AI vendor and learn about the accessibility to the model. Can a headless subscription or a de tached prompt template be \nconfigured to create a live monitor of that model or process? If not, what are the alternatives that the vendor can provide? Do those alternatives meet your organization's requirements for AI/ML governance?\n\nChapter 6. Governing the end-to-end lifecycle of an AI asset 75Considering requirements\nThink about what the requirements are for governing the business application ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 227,
      "total_chunks": 245
    }
  },
  {
    "text": "nsidering requirements\nThink about what the requirements are for governing the business application or process. Is \nlive monitoring the only way to achieve those requirements? Can the requirements be achieved through some other means such as a report card of the AI process, or some other status update?\nUltimately, implementing end-to-end lifecycle monitoring of AI embedded in a business \napplication is unique. Because of the fact that the AI already exists in a business application, some assumption may be made about the stage of that AI asset (it would be considered to be in production if it is live and being consumed by users.) This would make the process shorter than tracking a model from init ial development; A user may in itiate lifecycle governance on a \nbusiness application's AI model beginning with the production or operation stage. \nGoverning an AI/ML model from a business application must be accomplished with the same \nattention to detail regardless of who is the ve ndor / pro",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 228,
      "total_chunks": 245
    }
  },
  {
    "text": "ation must be accomplished with the same \nattention to detail regardless of who is the ve ndor / provider. As the process continues to \nevolve for AI governance, rules and requirements should be established with regarding which AI/ML providers and vend ors are to be sbe utilized by the organization. Befo re configuring \nlifecycle monitoring for those AI /ML processes, verify ing the vendor's ability to meet any legal \nrequirements is an important starting point. Review Chapter 4, \u201cOnboarding a new foundation model\u201d on page 41 and Chapter 5, \u201cAssessing a new use case\u201d on page 53 for organizational processes and considerations to make prior to setting up lifecycle governance.\n\n76 Ensuring Trustworthy AI with IBM watsonx.governance\n\n\u00a9 Copyright IBM Corp. 2025. 77Chapter 7. Use cases\nThis chapter highlights various implementations of watsonx.governance, focusing on fairness, \ndrift, regulatory compliance, and accountability. These implemen tations span multiple sectors \nsuch as healthcare,",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 229,
      "total_chunks": 245
    }
  },
  {
    "text": "ry compliance, and accountability. These implemen tations span multiple sectors \nsuch as healthcare, banking, and finance, levera ging robust data tracking and model auditing \nmechanisms.\nThis chapter has th e following sections:\n/SM590000\u201cOverview of use case 1- Banking credit risk management\u201d on page 78\n/SM590000\u201cOverview of use case 2 - Automated governance for universal bank's AI chatbot\u201d on \npage 80\n/SM590000\u201cOverview of use case 3 - Belgian biopharmaceutical company\u201d on page 817\n\n78 Ensuring Trustworthy AI with IBM watsonx.governance7.1  Overview of use case 1- Banking credit risk management\nThis section explains the concept of credit risk management, emphasizing fair, transparent \napproval processes, proactive monitoring with alerts, and automation of model metadata documentation to support credit decisions.\nIt highlights of the importance of a successf ul credit risk management system include the \nfollowing:\n/SM590000Fair and transparent approval processes.\n/SM590000Proactive r",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 230,
      "total_chunks": 245
    }
  },
  {
    "text": "ystem include the \nfollowing:\n/SM590000Fair and transparent approval processes.\n/SM590000Proactive risk detection with al erts to avoid biased decisions.\n/SM590000Automating the capture and documentation of model metadata with fact sheets to support \ncredit decisions.\nFigure 7-1 shows how watsonx.governance can improve businesses credit risk management \nsystem. Improvements are shown based on bu siness function to help illustrate the \ncross-departmental value that proper governance has in any organization.\nFigure 7-1   Credit risk management\n7.1.1  Banking credit ri sk management use case\nCredit risk management is the practice of mitigating losses by assessing borrowers' credit \nrisk - including payment behavi or and affordability. This proc ess has been a longstanding \nchallenge for financial institutions requiring co ntinuous adaptations by businesses to better \ntrack borrower behavior.\n7.1.2  Business context\nA French cooperative bank provides banking products and services, focusing",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 231,
      "total_chunks": 245
    }
  },
  {
    "text": ".\n7.1.2  Business context\nA French cooperative bank provides banking products and services, focusing on risk management and regulatory compliance. However, regulatory monitoring was done manually, leading to inefficiencies and challenges in identifying emerging regulatory risks resulting from regulatory changes and new requirements impacting customer services. The French \n\n\nChapter 7. Use cases 79cooperative bank caters to individuals and busines ses by offering a range of banking products \nand services.\nHowever, the bank faced challenges due to:\n/SM590000A lack of centralized tools for regulatory monitoring.\n/SM590000Manual processes using Excel, resulting in inefficiencies in identifying applicable \nregulatory risks and regulatory changes.\n7.1.3  Client need\nThe client required a centralized tool to aggregate legal texts from multiple data sources, enabling lawyers to track regulatory changes and link them to specific business units and banking product offerings, such as banking card",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 232,
      "total_chunks": 245
    }
  },
  {
    "text": "changes and link them to specific business units and banking product offerings, such as banking cards and insurance.\nThe client required a governance tool to:\n/SM590000Aggregate legal texts from multiple data sources.\n/SM590000Maintain a history of regulatory changes and link this data to impacted products and \nbusiness units.\n7.1.4  Client challenges\nThe client wants to address the challenges in the following areas:\n/SM590000The client faced significant hurdles in stre amlining their governance, risk, and compliance \n(GRC) processes:\n\u2013Fragmented data systems : GRC data was scattered across multiple disparate \nsystems, creating silos and making it difficult to obtain a unified view of risks.\n\u2013Manual dependency : Reliance on manual processes for managing GRC was \ninefficient and error-prone, especially in th e context of the monitoring and management \nof the rapid number of regulatory changes.\n/SM590000The handling of large volumes of data, which overwhelmed manual workflows and \nreduce",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 233,
      "total_chunks": 245
    }
  },
  {
    "text": "nges.\n/SM590000The handling of large volumes of data, which overwhelmed manual workflows and \nreduced accuracy.\n7.1.5  Business benefits\nHere are several business benefits and improved risk management benefits:\n/SM590000By addressing these challenges, the solution delivered substantial business benefits:\n\u2013Unified Governance Framework : Consolidating GRC processes into a single, \nintegrated platform provided a comprehensive view of risks, enabling better decision-making and streamlined operations.\n\u2013AI-driven automation : Leveraging AI technologies accelerated GRC processes, \nreducing manual effort and improving efficien cy. This automation not only minimized \nhuman errors but also ensured faster compliance with evolving regulatory requirements.\n/SM590000Improved risk management : Enhanced visibility and co ntrol over GRC processes \nempowered the organization to proactively identify and address potential risks, fostering resilience and complia nce across operations.\n\n80 Ensuring Trustwor",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 234,
      "total_chunks": 245
    }
  },
  {
    "text": "dress potential risks, fostering resilience and complia nce across operations.\n\n80 Ensuring Trustworthy AI with IBM watsonx.governance7.1.6  Pilot solution \nThis pilot solution uses watsonx.gover nance Governance Console (OpenPages).\nIn a prior Minimum Viable Product (MVP), IBM OpenPages demonstrated value by importing \nconsumer code and GDPR laws via standard R EST API such as L\u00e9gifrance REST API in \nFrance. Initially, Excel was used  for linking with the eventual goal being direct integration \nwithin IBM OpenPages.\nList of key steps:\n/SM590000Develop a Connector : Connect the L\u00e9gifrance REST API and IBM OpenPages REST \nAPI to fetch and properly format regulatory data.\n/SM590000Automate Hyperlinks : Enable referencing between articles for better navigation and \nusability.\nBy streamlining legal monitoring, the solution enhances risk identification and regulatory \ncompliance, reduces prior manual steps, and improves the quality of formatting legal data.\n7.2  Overview of use case 2 - Aut",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 235,
      "total_chunks": 245
    }
  },
  {
    "text": "r manual steps, and improves the quality of formatting legal data.\n7.2  Overview of use case 2 - Automated governance for \nuniversal bank's AI chatbot \nA prominent British universal bank and financia l services group, provides a broad range of \nofferings such as savings accounts, loans, insurance, and investment options. A key focus of the bank is continuous improvements on ef ficiencies around managing risk and ensuring \nregulatory compliance.\nTheir AI-powered \nchatbot  is designed to provide AI-driven solutions and must adhere to strict \nstandards of ethics, explainabilit y, and expected perfo rmance. To support this, the bank seeks \nto govern its AI systems with robust, automated, and integrated platforms as data and AI technologies evolve.\n7.2.1  Business context\nThey require a governance framework that ensures:\n/SM590000Ethical and explainable AI behavior.\n/SM590000Reliable results in alignmen t with business objectives.\n/SM590000Adequate control, testing, and audit mechanisms to ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 236,
      "total_chunks": 245
    }
  },
  {
    "text": "in alignmen t with business objectives.\n/SM590000Adequate control, testing, and audit mechanisms to manage evolving data and AI models.\nThe solution will leverage IBM watsonx.gove rnance, an automated and integrated AI \ngovernance platform, to manage the lifecycle and compliance of the AI application.\n7.2.2  Client need\nThe client aims to automate the governance lifecycle of a chatbot  across various metrics, \nlifecycle stages, and compliance requirements.\n7.2.3  Client challenges\nThe client focused on plans to address two main challenges:\n\nChapter 7. Use cases 81/SM590000The current AI implementation did not have an adequate monitoring system to maintain a \nstable solution. \n/SM590000Need to find a way to improve mechanisms to  address the diverse aspects such as bias \ndetection, ethical compliance, handling of highly autonomous processes (HAP), and protection of personally identifiable information (PII).\n7.2.4  Business benefits\nThe following benefits support addressing the client\u2019s ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 237,
      "total_chunks": 245
    }
  },
  {
    "text": " information (PII).\n7.2.4  Business benefits\nThe following benefits support addressing the client\u2019s challenges:\n/SM590000Consolidated governance : Provide a unified platform for an aggregated view of risks.\n/SM590000Automation of GRC processes:  Leverage AI to streamline governance, risk, and \ncompliance (GRC) processes, signific antly accelerating time to value.\n7.2.5  Pilot solution\nThe pilot addressed these requirements with the following key features:\n/SM590000Quantified Quality Metrics : Faithfulness, answer relevance, handling of unsuccessful \nrequests, keyword inclusion, answer coverage , and spelling robustness were all measured \nto ensure high performance.\n/SM590000Governance Dashboard : Developed a comprehensive dashboard to simplify the \noversight of their AI governance lifecycle.\n/SM590000External Model Governance : Implemented governance for external models using \ndetached prompt templates.\nThis structured solution ensures bank's chatbot  AI product operates within ethical",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 238,
      "total_chunks": 245
    }
  },
  {
    "text": "rompt templates.\nThis structured solution ensures bank's chatbot  AI product operates within ethical, regulatory, \nand performance parameters while automating the lifecycle governance for enhanced efficiency.\n7.3  Overview of use case 3 - Belgian biopharmaceutical \ncompany \nA Belgian biopharmaceutical company, is leveraging watsonx solutions to address challenges \nin website development,  including technical documentation and module reusability, for their \nDrupal-based global web ecosystem.\n7.3.1  Business context\nIBM is developing and managing over 150 global websites for the pharma company using Drupal based technology. Client  faces challenges in mainta ining high-quality technical \ndocumentation, which limits clarit y on the functionality of existing Drupal modules and creates \nbarriers for local developers seeking to reuse available modules.\n7.3.2  Client need\nClient seeks a solution th at improves the website development process by:\n/SM590000Enhancing technical documentation.\n/SM",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 239,
      "total_chunks": 245
    }
  },
  {
    "text": "n th at improves the website development process by:\n/SM590000Enhancing technical documentation.\n/SM590000Simplifying module discovery and reuse.\n\n82 Ensuring Trustworthy AI with IBM watsonx.governance/SM590000Supporting local developers with efficient tools and governance mechanisms.\n7.3.3  Client challenges\nThe following challenges need to be addressed:\n/SM590000Lack of clear, comprehensive technical documentation for existing Drupal modules.\n/SM590000Inefficiencies in reusing mo dules across local teams due to limited information.\n/SM590000Fragmented governance, making it difficult to maintain consistency and compliance \nacross websites.\n7.3.4  Business benefits\nSeveral key benefits are:\n\u2013 Streamlined documentation creation and maintenance with AI-driven tools.\n\u2013 Improved developer productivity throug h better access to reusable modules.\n\u2013 A unified governance framework for cons istent and efficient website development.\n7.3.5  Pilot solution\nThe following steps highlight the impleme",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 240,
      "total_chunks": 245
    }
  },
  {
    "text": "t and efficient website development.\n7.3.5  Pilot solution\nThe following steps highlight the implementation of the pilot solution:\n1.Objective:  Demonstrate how watsonx solutions can enhance website development by \naddressing documentat ion challenges, impr oving module reusability, and supporting \ngovernance.\n2.Steps implemented :\nThe following steps show how the client addressed the challenges:\na.Understanding requirements :\nConducted workshops with pharma company stakeholders to identify pain points and \ngather insights into their Drupal-based ecosystem.\nb.Developing solutions :\nThe client used the following products to develop the solution:\n\u0081IBM watsonx.ai:  Used for generating and maintaining AI-driven technical \ndocumentation.\n\u0081IBM watsonx.governance : Implemented to centralize and streamline governance \nfor Drupal modules.\nc.Knowledge base creation :\nBuilt a searchable repository with detailed descriptions and usage guidelines for Drupal \nmodules.\nd.Prototype and demonstration :",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 241,
      "total_chunks": 245
    }
  },
  {
    "text": "with detailed descriptions and usage guidelines for Drupal \nmodules.\nd.Prototype and demonstration :\nThe client developed the prototype to demonstrate improved productivity:\n\u0081 Created a prototype showing how watsonx tools improve documentation and \nmodule discovery.\n\u0081 Demonstrated how developers can leverage  these tools to enhance productivity.\ne.Feedback loop :\nThe client implemented a feedback loop to improve the solution:\n\nChapter 7. Use cases 83\u0081 Collected feedback during the pilot from client's technical teams.\n\u0081 Iteratively refined the solution to a lign with client's specific workflows and \nrequirements.\n3.Outcome :\nThe outcome of the following demonstrated the solution improvement:\n\u2013 Enhanced module documentat ion quality and accessibility.\n\u2013 Increased developer efficiency by enabling effective module reuse.\n\u2013 Established a robust governance fram ework for Drupal website development.\n4.Future scope :\nThe following items were identified for future additional improvement to the ",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 242,
      "total_chunks": 245
    }
  },
  {
    "text": "ment.\n4.Future scope :\nThe following items were identified for future additional improvement to the solution;\n\u2013 Scale the pilot soluti on across global websites.\n\u2013 Expand the use of watsonx tools to other areas of the pharmaceutical company's \ndigital ecosystem for broader impact.\n\n84 Ensuring Trustworthy AI with IBM watsonx.governance\n\n\u00a9 Copyright IBM Corp. 2025. 85Related publications\nThe publications listed in this section are considered particularly suitable for a more detailed \ndiscussion of the topics covered in this book.\nIBM Redbooks\nThe following IBM Redbooks publications provide additional information about the topic in this document. Note that some publications referenced  in this list might be available in softcopy \nonly. \n/SM590000Simplify Your AI Journey: Hybrid, Open Data Lakehouse with IBM watsonx.data,  \nSG24-8570\n/SM590000Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai , SG24-8574\nYou can search for, view, download or order these documents and",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 243,
      "total_chunks": 245
    }
  },
  {
    "text": "f AI with IBM watsonx.ai , SG24-8574\nYou can search for, view, download or order these documents and other Redbooks, \nRedpapers, Web Docs, draft and additional materials, at the following website: \nibm.com/redbooks\nOnline resources\nThese websites are also relevant as further information sources:\n/SM590000IBM watsonx documentation  \n/SM590000IBM watsonx.governance  \n/SM590000IBM watsonx product portfolio  \n/SM590000IBM AI risk atlas  \nHelp from IBM\nIBM Support and downloads\nibm.com/support\nIBM Global Services\nibm.com/services\n\n86 Ensuring Trustworthy AI with IBM watsonx.governance\n\n\n\nibm.com /redbooksPrinted in U.S.A .Back cover\nISBN 0738461970SG24-8573-00\n\u00ae\n\n",
    "metadata": {
      "source": "sg248573.pdf",
      "chunk_index": 244,
      "total_chunks": 245
    }
  }
]
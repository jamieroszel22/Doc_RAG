RedbooksData and AIFront cover
Simplify Your AI Journey: 
Unleashing the Power of AI with IBM watsonx.ai
Deepak Rangarao
Phillip GerrardCharley BellerCarl BrokerDaniele ComiLakshmana EkambaramShuvanker GhoshKaren MedhatPayal PatelMatthew Price
Shirley ShumMark Simmonds

IBM Redbooks
Simplify Your AI Journey: Un leashing the Power of AI 
with IBM watsonx.ai
January 2025
SG24-8574-00

© Copyright International Bu siness Machines Corp oration 2025. All rights reserved.
Note to U.S. Government Users Restricted Rights -- Use, duplication or disclosure re stricted by GSA ADP Schedule
Contract with IBM Corp.First Edition (January 2025)
This edition applies to Version 2, Release 1, Modification x of IBM watsonx.ai.Note: Before using this information and the product it supports, read the information in “Notices” on 
page vii.

© Copyright IBM Corp. 2025. iiiContents
Notices  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii
Trademarks  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii
Foreword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix
Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix
Authors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .x
Now you can become a published author, too!  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiiComments welcome. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xii
Stay connected to IBM Redbooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii
Chapter 1.  Competing with artificial intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1  Competing with AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . 2
1.2  Challenges in building and deploying AI models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2.1  Technical considerations for building and deploying AI models . . . . . . . . . . . . . . . 5
1.3  Opportunities around using AI on trusted data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.3.1  Enhancing decision-making with accurate insights. . . . . . . . . . . . . . . . . . . . . . . . . 61.3.2  Driving operational efficiency  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.3.3  Accelerating innovation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.3.4  Enhancing governance and compliance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71.3.5  Unlocking new revenue streams. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.3.6  Transforming industries with AI and trusted data . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.4  Improving AI model reliability  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.4.1  Enabling cross-enterprise collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.4.2  Enhancing real-time decision-making . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.4.3  Scaling AI-driven ecosystems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101.4.4  Driving sustainability and environmental, social, and governance goals  . . . . . . . 10
1.4.5  Personalizing customer experiences  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.5  Creating new AI-enabled products and services. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Chapter 2.  Introducing IBM watsonx.ai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.1  Overview of watsonx.ai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.1.1  Key capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.1.2  The watsonx.ai architecture  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142.1.3  watsonx.ai empowering IBM Software offerings . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.1.4  Benefits of using watsonx.ai for businesses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2  Synergy between watsonx.ai and other components in the watsonx platform . . . . . . . 15
2.2.1  Synergy between watsonx.ai and watsonx.data . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2.2  Synergy between watsonx.ai and watsonx.governance . . . . . . . . . . . . . . . . . . . . 16
2.3  Business impact of these synergies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
Chapter 3.  Tools for diverse data science teams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.1  Key personas for watsonx.ai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.1.1  Data scientists. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 8
3.1.2  Machine learning engineers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.1.3  AI engineers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 9
3.2  Low-code, no-code, and full-code tools  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.2.1  No-code, low-code, and full-code tools on IBM watsonx.ai. . . . . . . . . . . . . . . . . . 20
Chapter 4.  Building and using artificial intelligence models  . . . . . . . . . . . . . . . . . . . . 27

iv Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai4.1  Prerequisites and assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
4.2  How to use this chapter. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 8
4.3  Building and using AI models in watsonx.ai  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
4.3.1  Overview of the watsonx.ai platform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284.3.2  Key features and capa bilities  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
4.4  Getting started with watsonx.ai: Setting up the environment  . . . . . . . . . . . . . . . . . . . . 29
4.5  Data preparation and ingestion for AI model building . . . . . . . . . . . . . . . . . . . . . . . . . . 34
4.5.1  Understanding the importance of data in AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
4.5.2  Preparing and cleaning data: data quality considerations. . . . . . . . . . . . . . . . . . . 35
4.5.3  Handling missing data, outliers, and bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354.5.4  Ingesting data into watsonx.ai Studio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.5.5  Connecting to data repositories and cloud services . . . . . . . . . . . . . . . . . . . . . . . 36
4.6  Building AI models in watsonx.ai. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.6.1  Choosing the right model for your use case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.6.2  Model creation workflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.7  Deploying AI models in watsonx.ai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
4.7.1  watsonx.ai Studio deployments  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
4.8  watsonx.ai LLM deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
4.8.1  Model packaging and exporting  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
4.9  Operationalizing machine learning and LLM models  . . . . . . . . . . . . . . . . . . . . . . . . . . 50
4.9.1  Calling ML models by using API calls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
4.9.2  Calling Prompt Lab LLM models by using API ca lls . . . . . . . . . . . . . . . . . . . . . . . 52
4.9.3  IBM watsonx Assistant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.10  Additional information and where to go next . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
4.10.1  Additional support and documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554.10.2  watsonx.ai API reference  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.10.3  watsonx.ai data pipeline and orchestration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
Chapter 5.  Advanced capabilities of watsonx.ai  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
5.1  Prompt engineering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
5.1.1  Prompting techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585.1.2  Importance of system tokens  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.1.3  Model-specific peculiarities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.1.4  How watsonx.ai supports prompt engineering  . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
5.2  Multitask prompt tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1
5.2.1  Prompt tuning parameters  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 625.2.2  Interdependencies and holistic tuning strategies  . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.3  Fine-tuning  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
5.3.1  Challenges with fine-tuning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645.3.2  How watsonx.ai addresses fine-tuning challenges . . . . . . . . . . . . . . . . . . . . . . . . 65
5.4  InstructLab  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
5.4.1  Advantages of InstructLab  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 705.4.2  How to use InstructLab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
5.4.3  InstructLab on watsonx.ai Software-as-a-Service. . . . . . . . . . . . . . . . . . . . . . . . . 79
5.4.4  InstructLab use case examples  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
Chapter 6.  Artificial intelligence agents  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
6.1  What makes an AI agent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
6.2  Why AI agents are needed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
6.3  Multiple AI agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . 95
6.4  AI agents on watsonx.ai  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1006.5  AI agents use case examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
Chapter 7.  Use cases  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109

 Contents v7.1  Using RAG to aid a medical school admissions office  . . . . . . . . . . . . . . . . . . . . . . . . 110
7.1.1  The challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
7.1.2  The solution  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
7.1.3  Special considerations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
7.2  Embedding workflow automation to streamline recommendations . . . . . . . . . . . . . . . 111
7.2.1  The challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
7.2.2  The solution  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1127.2.3  Special considerations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Abbreviations and acronyms  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
Related publications  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
IBM Redbooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
Online resources  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
Help from IBM  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . 118

vi Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai

© Copyright IBM Corp. 2025. viiNotices
This information was developed for prod ucts and services offered in the US . This material might be available 
from IBM in other languages. However, you may be required  to own a copy of the product or product version in 
that language in order to access it. 
IBM may not offer the products, services, or features di scussed in this document in other countries. Consult 
your local IBM representative for information on the produc ts and services currently available in your area. Any 
reference to an IBM product, program, or service is not intended to state or imply that only that IBM product, 
program, or service may be used. Any functionally equi valent product, program, or service that does not 
infringe any IBM intellectual property right may be used instead. However, it is t he user’s responsibility to 
evaluate and verify the operation of any non-IBM product, program, or service. 
IBM may have patents or pending patent applications covering subject matter described in this document. The 
furnishing of this document does not grant you any license to these patents. You can send license inquiries, in 
writing, to:
IBM Director of Licensing, IBM Corporation, North Castle Drive, MD-NC119, Armonk, NY 10504-1785, US 
INTERNATIONAL BUSINESS MACHINES CORPORATIO N PROVIDES THIS PUBLICATION “AS IS” 
WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED 
TO, THE IMPLIED WARRANTIES OF NON-INFR INGEMENT, MERCHANTABILITY OR FITNESS FOR A 
PARTICULAR PURPOSE. Some jurisdictions do not allow disclaimer of express or implied warranties in 
certain transactions, therefore, this statement may not apply to you. 
This information could include technical inaccuracies or  typographical errors. Changes are periodically made 
to the information herein; th ese changes will be incorporated  in new editions of the publication. IBM may make 
improvements and/or changes in the product(s) and/or the program(s) described in this publication at any time 
without notice. 
Any references in this information to non-IBM websites are provided for convenience only and do not in any 
manner serve as an endorsement of those websites. The materials at those websites are not part of the 
materials for this IBM product and use of those websites is at your own risk. 
IBM may use or distribute any of the information you provide in any way it believes appropriate without 
incurring any obligation to you. 
The performance data and c lient examples cited are presented fo r illustrative purposes only. Actual 
performance results may vary depending on specific configurations and operating conditions. 
Information concerning non-IBM products was obtained from the suppliers of those products, their published 
announcements or other publicly available sources. IBM has not tested those products and cannot confirm the 
accuracy of performance, co mpatibility or any other clai ms related to non-IBM pr oducts. Questions on the 
capabilities of non-IBM products should be addr essed to the suppliers of those products. 
Statements regarding IBM’s future direction or intent are subject to change or withdrawal without notice, and 
represent goals and objectives only. 
This information contains exam ples of data and reports used in daily business operations. To illustrate them 
as completely as possible, the exam ples include the names of individual s, companies, brands, and products. 
All of these names are fictitious and any similarity to  actual people or business enterprises is entirely 
coincidental. 
COPYRIGHT LICENSE:
This information contai ns sample application prog rams in source language, which illustrate programming 
techniques on various operating platforms. You may co py, modify, and distribute these sample programs in 
any form without payment to IBM, for the purposes of developing, using, marketing or distributing application programs conforming to the application programming interface for the operating platform for which the sample 
programs are written. These examples have not been th oroughly tested under all conditions. IBM, therefore, 
cannot guarantee or im ply reliability, serviceability, or function of  these programs. The sample programs are 
provided “AS IS”, without warranty of any kind. IBM sha ll not be liable for any damages arising out of your use 
of the sample programs. 

viii Simplify Your AI Journey: Unleashi ng the Power of AI with IBM watsonx.aiTrademarks
IBM, the IBM logo, and ibm.com are trademarks or regi stered trademarks of International Business Machines 
Corporation, registered in many jurisdictions worldwide. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on the web at “Copyright 
and trademark information” at https://www.ibm.com/legal/copytrade.shtml  
The following terms are trademarks or registered trademarks of International Business Machines Corporation, 
and might also be trademarks or registered trademarks in other countries. 
Cloudant®
Cognos®
DataStage®
Db2®IBM®
IBM API Connect®
IBM Cloud®IBM Instana™
IBM Spectrum®
IBM Watson®
Informix®InfoSphere®
Instana®
Netezza®Orchestrate®
Redbooks®
Redbooks (logo) ®
SPSS®Turbonomic®
z/OS®
The following terms are trademarks of other companies:
The registered trademark Linux® is used pursuant to a sublicense from the Linux Foundation, the exclusive 
licensee of Linus Torvalds, owner of  the mark on a worldwide basis.
Microsoft, and the Windows logo are trademarks of Microsoft Corporation in the United States, other 
countries, or both.
Java, and all Java-based trademarks and logos are trademarks or registered trademarks of Oracle and/or its 
affiliates.
Red Hat, Fedora, OpenShift, are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in 
the United States and other countries.
RStudio, and the RStudio logo are registered trademarks of RStudio, Inc.
Other company, product, or service names may be trademarks or service marks of others. 


© Copyright IBM Corp. 2025. ixForeword
This IBM Redbooks® publication is part of a trilogy that positions and explains IBM watsonx, 
which is IBM’s strategic artificial intelligence (AI) and data platform. Each book focuses on 
one of the three main components of the watsonx platform:
/SM590000IBM watsonx.ai: A next-generation enterprise studio for AI builders to train, validate, tune, 
and deploy both traditional machine learning (ML) and new generative AI (gen AI) capabilities that are powered  by foundation models (FMs).
/SM590000IBM watsonx.data: A fit-for-purpose data store that is built on an open lakehouse 
architecture, and is optimized for different and governed data and AI workloads.
/SM590000IBM watsonx.governance: A set of AI Governance capabilities that e nables trusted AI 
workflows, which help organizations implement and comply with ever-changing industry and government regulations.
Organizations have long recognized the value that IBM Redbooks publications provide in 
guiding them with best practices, frameworks, clear explanations, and use cases as part of their solution evaluations and implementations.
This trilogy of books was poss ible because of close collabo ration among many skilled and 
talented authors that were selected from IBM Technical Sales, IBM Development, IBM Expert Labs, IBM Client Success Management, and consul ting services organizations to use their 
diverse skills, experiences, and technical knowledge across the watsonx platform.
Thanks to the authors, contributors, reviewers, and the IBM Redbooks team for their 
dedication, time, and effort in making this publication a valuable asset that organizations can use as part of their journey to AI.
Thanks to Mark Simmonds and Deepak Rangarao for taking the lead in shaping this request 
into yet another successful IBM Redbooks project.
Steve Astorino, IBM General Manager - Development, Data, AI, and Sustainability
Preface
IBM watsonx is IBM’s strategic AI and data platform. This book focuses on watsonx.ai , one of 
the three main components of the platform. IBM watsonx.ai is a next-generation enterprise studio that you can use to train, validate (test), tune, and deploy both traditional ML and new gen AI capabilities, which are powered by FMs through an open and intuitive user interface (UI). This AI studio provides a range of FMs, training and tuning tools, and a cost-effective infrastructure that facilitates the entire data and AI lifecycle, from data preparation through model development, deployment, and monitoring. Th e studio also includes an FM library that 
provides IBM® curated and trained FMs. FMs use a large, curated set of enterprise data that is backed by a robust filtering and cleansing process, and with an auditable data lineage. These models are trained on language and other modalities, such as code, time-series data, tabular data, geospatial data, and IT events data. 

x Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiHere are some examples of the model categories:
/SM590000fm.code: Models that automatically generate code for developers through a 
natural-language interface to boost developer productivity and enable the automation of many IT tasks.
/SM590000fm.NLP: A collection of large language models (LLMs) for specific or industry-specific 
domains that use curated data to help mitigate bias and quickly make domains customizable by using client data.
/SM590000fm.geospatial: Models that are built on climate and remote sensing data to help 
organizations understand and plan for changes in natural disaster patterns, biodiversity, 
land use, and other geophysical processe s that might impact their businesses
The watsonx.ai studio builds on  Hugging Face open-source libraries, which offer thousands of 
Hugging Face open models and datasets. Users can leverage the power of IBM Granite LLMs, along with the latest Mistral, Llama, and other third-party LLMs. It is part of IBM's commitment to deliver an open ecosystem approach that enables users to leverage the best models and architecture for their unique business needs.
This IBM Redbooks publication provides a broad understanding of watsonx.ai concepts, its 
architecture, and the services that are available with the product. Also, several common use cases and scenarios are include d that should help you better u nderstand the capabilities of 
this product. Code samples of common scenarios are available at this GitHub repository . For 
more examples, which include using Instructlab and AI agents, see this GitHub repository .
This publication is for watsonx customers who se ek best practices and real-world examples of 
how to best implement their solu tions while optimizing the value of their existing and future 
technology, AI, data, and skills investments.
Authors
This book was produced by a team of specialists from around the world:
Deepak Rangarao  is an IBM Distinguished Engineer and CTO who is responsible for 
Technical Sales-Cloud Paks. He leads the technical sales team that helps organizations modernize their technology landscape with IBM Cloud Paks. He has broad, cross-industry experience in the data warehousing and analytics space from building analytic applications at large organizations and performing technical pre-sales for start-ups and large enterprise software vendors. Deepak has co-authored several books on many topics, such as OLAP analytics, change data capture, data warehousing, and object storage. He is a regular speaker at technical conferences. He is a certif ied technical specialist in Red Hat OpenShift, 
Apache Spark, Microsoft SQL Server, and web development technologies.Note:  Here are the other books in the trilogy:
/SM590000Simplify Your AI Journey: Ensuring Trustworthy AI with IBM watsonx.governance , 
SG24-8573
/SM590000Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.data,  
SG24-8570

 Foreword xiPhillip Gerrard  is a Project Leader for the International Technical Support Organization 
working out of Beaverton, Oregon. As part of IBM for over 15 years, he has authored and contributed to hundreds of technical documents that were published to IBM.com, and worked directly with IBM's largest customers to resolve cr itical situations. As a team lead and subject 
matter expert (SME) for the IBM Spectrum® Protect support team, he is experienced in leading and growing international teams of talented IBM employees by developing and implementing team processes,  and creating and de livering education. Ph illip holds a degree 
in computer science and business admini stration from Oregon State University.
Charley Beller  is a Principal Data Scientist and IBM Master Inventor. He  works with clients 
as the Worldwide Solution Engineering Lead for watsonx.ai and AI Assistants within Technology Expert Labs. Charley has been working with IBM language technologies since 
joining the Watson group in 2014. He is an inventor with over 100 patents, and holds a PhD in Cognitive Science.
Carl Broker  is an AI Architect at IBM who specializes in enterprise gen AI solutions. With a 
background in both gen AI and traditional data science, Carl leads design sessions and develops proof-of-concepts for clients. Before this role, he worked as an AI Engineer and Data Scientist at IBM, focusing on predictive modeling and AI-driven solutions. Carl holds a Master of Science degree from Johns Hopkins University. 
Daniele Comi  is a Data Scientist, AI Engineer, and Software Engineer at IBM Italy, with over 
3 years of experience in data analytics, ML, and deep learning (DL). His expertise spans the entire spectrum of AI, from architectural design to scientific research, with a focus on ML, reinforcement learning (RL), and DL. Daniele holds a master's degree in Computer Science 
Engineering, with a specialty in AI frameworks and models. At IBM, Daniele has been a key member of the AI and gen AI team in Italy, where he has designed and implemented complex AI and gen AI architectures for many industr y applications. His technical expertise also 
includes Fully Homomorphic Encrypted AI, which enables secure AI solutions that help ensure data privacy. 
Lakshmana Ekambaram  is an IBM Senior Technical Leader with over 30 years of 
experience in database development, advanc ed analytics, and building hybrid cloud 
solutions. He is part of the IBM Expert La bs SWAT organization where he leads the data 
fabric and trusted AI journey for customers worldwide. He has developed many IBM certification courses and co-authored books about data science, AI, and data fabric. 
Karen Medhat is a Customer Success Manager Architect in the UK and the youngest 
IBM Certified Thought Leader Level 3 Techni cal Specialist. She is the Chair of the 
IBM Technical Consultancy Group and an IBM Academy of technology member. She holds a MSc degree with honors in Engineering in AI and Wireless Sensor Networks from the Faculty of Engineering, Cairo University, and a BSc de gree with honors in Engineering from the 
Faculty of Engineering, Cairo University. She co-creates curriculum and exams for different 
IBM professional certificates. She also crea ted and co-created cour ses for the IBM Skills 
Academy in various areas of IBM technologies. She serves on the review board of international conferences and journals in AI and wireless communication. She also is an IBM Inventor who is experienced in creating applications architecture and leading teams of different scales to deliver customers' projects successfully. She frequently mentors IT professionals to help them def ine their career goals, learn ne w technical skills, or acquire 
professional certifications. She has authored publications on cloud, IoT, AI, wireless networks, microservices architecture, and blockchain.

xii Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiPayal Patel works in Data and AI Technical Content Development at IBM, where she creates 
technical learning materials for sellers, IBM Busi ness Partners, and clients to enable them to 
get the most value out of IBM Data and AI products and solutions. She has worked in various roles at IBM, which include marketing analytics and as a Solutions Architect in IBM Technology Expert Labs, with a focus on Data and AI. She has worked in various technical roles across the financial services, insura nce, and technology industries. She holds a 
Bachelor of Science degree in  Information Science from UNC Chapel Hill, and a Masters in 
Analytics degree from North Carolina State University.
Matthew Price  is a Senior watsonx Client Success Man ager with 20 years of experience in IT 
and 10 years of experience focusing on Wats on technologies. His previous experience 
includes writing the base code that went on to become the IBM Watson® Assistant for Citizens application, which is IBM’s no-charge offering that was released during 2020 to help business and government agencies navigate the pandemic. His previous publications 
centered on application migration and the cloud.
Shirley Shum is a Senior Software Engineer for the IBM Fusion team. She has worked as a 
technical lead on IBM Storage products, such as IBM Storage Insights and Fusion. Her areas of expertise include Kafka, complex event proc essing, backup and restore, and AI solutions, 
such as watsonx.ai and InstructLab on the Red Hat OpenShift platform. 
Mark Simmonds  is a Program Director with IBM Data and AI. He writes extensively on AI, 
data science, and data fabrics, and holds multiple author recognition awards. He previously worked as an IT architect leading complex infrastructure design and corporate technical architecture projects. He is a member of the British Computer Society, holds a bachelor's degree in Computer Science, is a published author, and a prolific public speaker.
Now you can become a published author, too!
Here’s an opportunity to  spotlight your skills , grow your career, and become a published 
author—all at the same time! Join an IBM Redbooks residency project and help write a book in your area of expertise, while honing your experience using leading-edge technologies. Your efforts will help to increase pr oduct acceptance and customer satisfaction, as you expand 
your network of technical contacts and relationships. Residencies run from two to six weeks in length, and you can participate either in person or as a remote resident working from your home base.
Find out more about the residency program, browse the residency index, and apply online at:
ibm.com/redbooks/residencies.html
Comments welcome
Your comments are important to us!
We want our books to be as helpful as possible . Send us your comments about this book or 
other IBM Redbooks publications in one of the following ways:
/SM590000Use the online Contact us  review Redbooks form found at:
ibm.com/redbooks
/SM590000Send your comments in an email to:
redbooks@us.ibm.com

 Foreword xiii/SM590000Mail your comments to:
IBM Corporation, IBM Redbooks
Dept. HYTD Mail Station P0992455 South RoadPoughkeepsie, NY 12601-5400
Stay connected to IBM Redbooks
/SM590000Find us on LinkedIn:
https://www.linkedin.com/groups/2130806
/SM590000Explore new Redbooks publications, residencies, and workshops with the IBM Redbooks 
weekly newsletter:
https://www.redbooks.ibm.com/subscribe
/SM590000Stay current on recent Redbooks publications with RSS Feeds:
https://www.redbooks.ibm.com/rss.html

xiv Simplify Your AI Journey: Unleashi ng the Power of AI with IBM watsonx.ai

© Copyright IBM Corp. 2025. 1Chapter 1. Competing with artificial 
intelligence
In today's fast-paced digital landscape, artificial intellig ence (AI) has emerged as a 
game-changer that is revolutionizing the way bus inesses operate, innovate, and compete. As 
AI technologies continue to advance and becom e increasingly ubiquitous, organizations are 
faced with the daunting task of competing with AI-driven rivals while also leveraging AI to stay ahead of the competition. This chapter delves into the world of competing with AI by exploring the challenges, opportunities, and strategies that organizations can employ to remain competitive in an AI-dominated market.
The following topics are described in this chapter:
/SM5900001.1, “Competing with AI” on page 2
/SM5900001.2, “Challenges in building and deploying AI models” on page 4
/SM5900001.3, “Opportunities around using AI on trusted data” on page 5
/SM5900001.4, “Improving AI mo del reliability” on page 8
/SM5900001.5, “Creating new AI-enabled pr oducts and services” on page 111

2 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai1.1  Competing with AI
To harness the competitive advantage of using AI, organizations must first understand the AI 
landscape and the various types of AI that exis t. Organizations must also be aware of the 
various AI technologies that are being used to drive innovation and competitiveness, which include the following ones:
/SM590000Machine learning (ML): ML represents a pi votal subset of AI where algorithms are 
developed and trained to recognize patterns and make data-driven predictions or decisions. Unlike traditiona l programming, ML systems learn iteratively from data, 
improving performance with ex perience. These systems rely on vast datasets to develop 
statistical models that enable predictions across diverse applications, such as anomaly detection, natural language processing (NLP), and image recognition. The training process involves feeding labeled (supervi sed learning) or unlabeled (unsupervised 
learning) data to the algorithm. Over time, the system refines its parameters to minimize errors and maximize predictive accuracy. ML algorithms are central to AI's practical applications, and they drive everything from recommendation systems to fraud detection in modern business ecosystems.
/SM590000Deep learning (DL): DL is an advanced branch of ML that employs artificial neural 
networks that are modeled after the human brain's structure and functioning. Unlike traditional ML, which often depends on manual feature engineering, DL automates the extraction of complex features from raw data through multiple layers of interconnected neurons. DL excels in handling unstructured data such as images, audio, and text, making it instrumental in solving tasks like computer vision, spee ch recognition, and language 
translation. For example, convolutional n eural networks (CNNs) specialize in image 
processing by identifying spatial hierarchies in pixels, and recurrent neural networks (RNNs) and transformers tackle sequential data with unparalleled efficiency. By leveraging high-performance computing and large datasets, DL approximates nonlinear functions to enable machines to solve intricate, high-dimensional problems.
/SM590000Unsupervised learning: Unsupervised learning focuses on deriving patterns and 
structures from unlabeled datasets. This method trains algorithms to identify inherent groupings, clusters, or associations in data without human-provided annotations. Common techniques include clustering algorithms, such  as k-means and hier archical clustering, 
and dimensionality reduction methods, such as Principal Component Analysis (PCA) and t-SNE. Applications of unsupervised learning are vast, ranging from customer segmentation in marketing to anomaly detection in cybersecurity. These systems are 
particularly valuable in exploratory data a nalysis, where insights emerge from raw data 
without prior assumptions. By uncovering hidden relationships, unsupervised learning enhances your understanding of data and informs decision-making processes.
/SM590000Reinforcement learning (RL): RL is a paradigm of ML where algorithms learn optimal 
behaviors by interacting with an environment and receiving feedback in the form of rewards or penalties. RL systems employ agents that act based on policies, and aim to maximize cumulative rewards over time. Core to RL are concepts such as the exploration-exploitation tradeoff, Markov deci sion processes (MDPs), and value functions. 
Techniques such as Q-learning  and Deep Q-Networks (DQNs) extend RL’s capabilities to 
enable applications in robotics, game play ing, and autonomous vehicles. RL's emphasis 
on learning through trial and error aligns it closely with real-world problem-solving, where dynamic environments require adaptive strategies.

Chapter 1. Competing wit h artificial intelligence 3/SM590000Foundation models: Foundation models (FMs) represent a transformative leap in AI and 
ML, characterized by their scala bility, versatility, and ability to genera lize across tasks. 
These models, such as Granite, are pre-trained on vast and diverse datasets, enabling them to adapt to specific applications with minimal fine-tuning. Unlike traditional ML 
models that are tailored to single tasks, FMs leverage their pre-trained knowledge to excel in multiple domains. This adaptability is achieved throug h transfer learning, where the 
model's pre-trained weights are fine-tuned on domain-specific data sets. FMs empower organizations to reduce the cost and complexity of training AI systems while achieving state-of-the-art performance in tasks like natural language understanding, summarization, and multimodal reasoning.
Leveraging AI for co mpetitive advantage
To thrive in an AI-driven landscape, organizati ons must strategically harness AI technologies 
to drive innovation, enhance operational efficiency, and improve decision-making. Key areas of focus include the following ones:
/SM590000AI-powered automation: Automation that is fueled by AI enables the running of repetitive 
and high-volume tasks with speed and precision,  such as applications in Robotic Process 
Automation (RPA), intelligent document processing, and automated workflows. By offloading routine operations, organizations can redirect human resources to strategic and creative endeavors, which foster innovation and competitive differentiation.
/SM590000AI-driven analytics: AI-powered analytics transform raw data into actionable insights, 
which equips businesses to make data-informed decisions. Predictive analytics, which is powered by ML, enables forecasting trends and identifying potential challenges, and prescriptive analytic s suggests optimal courses of action. These ca pabilities enable 
organizations to stay ahead in dynamic market s by responding proactively to opportunities 
and risks.
/SM590000AI-based innovation: AI acts as a catalyst for innovation to enable organizations to 
conceptualize and deliver groundbreaking products, services, and business FMs. From personalized healthcare solutions to autonomous logistics systems, AI's potential to redefine industries is immense. By embedding AI in their innovation processes, companies can create unique value propositions that resonate with customers and stakeholders alike.
By embracing these AI strategies, organizations can position themselves as leaders in the era 
of digital transformation. As AI  continues to evolve, its syner gy with trusted data will unlock 
unprecedented op portunities, which will re shape the competitive landscape and drive 
sustainable growth.

4 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai1.2  Challenges in building and deploying AI models
Building and deploying AI models is a complex and challenging endeavor that requires 
expertise, resources, and infrastructure. Despite the potential benefits of AI, many organizations struggle to overcome the numerous hurdles that are associated with AI model development and deployment. Here are some of the key challenges:
/SM590000Data quality and availability: AI  models require vast amounts of high-quality,  relevant, and 
diverse data to learn, train, and validate. However, many organizations face challenges in collecting, processing, and integrating data from disparate sources, which can lead to issues with data quality, consistency, an d availability. Furthermo re, data privacy and 
security concerns can limit access to sensitive data, which can hinder the development of accurate and reliable AI models.
/SM590000Model complexity and in terpretability: As AI models be come increasingly complex, they 
can be difficult to interpret and understand, which makes it challenging to identify biases, errors, or flaws in the decision-making process. The lack of transparency and 
explainability in AI models can lead to mistrust, regulat ory issues, and reputational 
damage. Moreover, the complexity of AI models can make it difficult to integrate them with existing systems, processes, and infrastructure.
/SM590000Talent acquisition and retention: The development and deployment of AI models require 
specialized skills and expertise, which in clude data science, ML, and software 
engineering. However, the demand for AI talent far exceeds the supply, which can lead to challenges in acquiring and retaining top talent. Furthermore, the constant evolution of AI technologies means that professionals must continually update their skills to remain relevant, which can add to the talent acquisition and retention challenges.
/SM590000Infrastructure and scalability:  AI models require significan t computational resources, 
memory, and storage to process and analyze large datasets. However, many organizations lack the necessary infrastructure to support the development and deployment of AI models, which can lead to  issues with scalability, performance, and 
reliability. Furthermore, the inte gration of AI models with existing systems and processes 
can be complex, requiring si gnificant investment in infrastructure and architecture.
/SM590000Bias and fairness: AI models can perpetuate and amplify existing biases and inequalities if 
they are trained on biased data or designed with a particular world view. The lack of diversity and inclusion in AI development t eams can exacerbate these issues, which can 
lead to unfair outcomes and reputational damage. Furthermore, the identification and mitigation of bias in AI models can be challe nging, which requires significant expertise and 
resources.
/SM590000Regulatory compliance: The development and deployment of AI models are subject to 
various regulations and laws, which include data protection, intellectual property, and anti-discrimination legislation. However, the rapidly evolving nature of AI technologies can make it challenging to ensure regulatory compliance, particular ly in industries with strict 
regulations, such as healthcare and finance.
/SM590000Model maintenance and updates: AI models require continuous maintenance and updates 
to help ensure that they remain accurate, reliable, and relevant. However, the process of updating AI models can be complex and require significant resources and expertise. Furthermore, the integration of updated AI models with existing systems and processes can be challenging, which can lead to issues with compatib ility and performance.

Chapter 1. Competing wit h artificial intelligence 5/SM590000Explainability and transpar ency: The lack of explainability a nd transparency in AI models 
can make it challenging to understand the decision-making process, which can lead to 
mistrust and reputational damage. Furthermore, the identification and mitigation of errors or biases in AI models can be difficult, which can require expertise and resources.
/SM590000Cybersecurity: AI models can be vulnerable to cyberthreats, such as data poisoning, 
model hijacking, and adversarial attacks. The identification and mitigation of these threats can be challenging and require expertise and resources. Furthermore, the integration of AI models with existing security systems and processes can be complex, which can lead to issues with compatib ility and performance.
1.2.1  Technical considerations fo r building and depl oying AI models
When building and deploying AI models, there are several technical considerations that must 
be accounted for:
/SM590000Data preprocessing: Ensuring that the data that is used to train and test the model is 
accurate, complete, and relevant.
/SM590000Model selection: Choosing the most suitable algorithm and model architecture for the 
problem being addressed.
/SM590000Hyper-parameter tuning: Optimizing the mode l's hyper-parameters to achieve the best 
possible performance.
/SM590000Model evaluation: Evaluating the model's perfor mance by using metric s such as accuracy, 
precision, and recall.
/SM590000Model deployment: Deploying the model in a production-ready environment, such as a 
cloud-based API or a containerized application.
/SM590000Model monitoring: Continuously monitoring the model's performance and updating it as 
necessary to help ensure that it  remains accurate  and relevant.
In addition to these technical considerations, organizations must also consider the following 
items:
/SM590000Data governance: Establishi ng policies and procedures for data management, which 
include data quality, security, and compliance.
/SM590000Model governance: Establishing policies and procedures for model development, 
deployment, and maintenance, which include model validation, testing, and updating.
/SM590000Infrastructure governance: Establishing po licies and procedures for infrastructure 
management, which include infrastructure provisioning, scaling, and security.
By being conscious of these technical consider ations and establishing effective governance 
policies and procedures, organizations can help ensure the successful development and deployment of AI models that drive business value and competitiveness.
1.3  Opportunities around using AI on trusted data
AI thrives on data. However, the effectiveness of AI systems is not solely dependent on the volume of data but also on its quality and trustworthiness. Trusted data (data that is accurate, consistent, secure, and compliant) serves as the foundation for reliable AI-driven insights. Organizations today are exploring opportunities to harness AI on trusted data to drive operational efficiency, enhance decision-maki ng, and unlock new revenue streams. This 
section explores the myriad possibilities that AI unlocks when it operate s on a foundation of 
high-quality, trusted data.

6 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai1.3.1  Enhancing de cision-making with accurate insights
The fusion of AI and trusted data is reshapin g decision-making processes across industries, 
and enabling organizations to derive precise and actionable insights. This transformation is 
critical in domains where de cisions significantly impact outcomes, such as healthcare, 
finance, and supply chain management. By leve raging high-quality, trusted data, AI systems 
can identify patterns, predict outcomes, and provide recommendations that drive superior decisions.
For example, in healthcare, AI systems that are powered by trusted clinical and patient data 
enhance diagnostic precision, predict pati ent outcomes with remarkable accuracy, and 
suggest personalized treatment plans that are tailored to individual needs. In the financial sector, AI models that are trained on trusted datasets excel in detecting fraudulent activities, assessing credit risks, and automating sophisticated trading strategies that are based on dynamic market trends. These examples illustrate how truste d data amplifies the reliability 
and impact of AI-driven decision-making,  minimizing risks and maximizing outcomes.
One of the most transformative applications of AI on trusted data is in improving 
decision-making. High-quality data enables AI algorithms to deliver precise and actionable 
insights, which are beneficial in industries like healthcare, finance, and supply chain management, where even minor errors in decision-making can have significant consequences.
/SM590000Healthcare: Trusted data enables AI systems  to accurately predict patient outcomes, 
suggest personalized treatment plans, and enhance diagnostic accuracy.
/SM590000Finance: In financial services, AI models that are trained on trusted data can detect fraud, 
assess credit risks, and automate trading stra tegies based on market predictions. 
1.3.2  Driving operational efficiency
AI's ability to automate  and optimize complex processes is  magnified when it is built on a 
foundation of trusted data. By eliminating inefficiencies and reducing the need for human 
intervention, organizations can achieve unpr ecedented levels of operational efficiency.
In the manufacturing and energy sectors, predictive maintenance systems leverage trusted 
sensor data to foresee equipment failures, which enable preemptive interventions that 
minimize downtime and reduce maintenance costs. Similarly, AI-powered customer service platforms, which are underpinned by reliable customer interaction data, provide accurate, context-aware responses that deliver personalized experiences while alleviating the workload of human agents. These advancements highlight the transformative potential of combining AI with trusted data to streamline operations across industries.
When AI is applied to trusted data, it automates complex processes, which reduce the need 
for human intervention, and improves efficiency:
/SM590000Predictive maintenance: In the manufacturing and energy sectors, AI systems use trusted 
sensor data to predict equipment failures, which minimize downtime and optimizes maintenance schedules.
/SM590000Customer service automation: AI-powered chatbots, which are fueled by reliable customer 
data, provide accurate responses and deliver personalized experiences, which reduce the burden of human agents.

Chapter 1. Competing wit h artificial intelligence 71.3.3  Accelerating innovation
The convergence of AI and trusted data is a catalyst for innovation, which unlocks hidden 
patterns and opportunities that were previously  inaccessible. By analyzing vast amounts of 
high-quality data, AI systems empower organizations to develop groundbreaking products and solutions.
For example, in product development, companies use AI to analyze customer feedback, 
market trends, and usage data th at is extracted from trusted sources to create offerings that 
resonate with consumer preferences. In the realm of scientific rese arch, AI accelerates 
discovery processes by interpreting extensive experimental datasets, which lead to advancements in fields such as drug develop ment and material science. Trusted data 
enhances the accu racy of these insights and helps ensure the repr oducibility of  outcomes, 
which drive sustained innovation.
The combination of AI and trusted data fosters innovation by uncovering hidden patterns and 
opportunities that were previously inaccessible:
/SM590000Product development: Companies leverage AI to analyze customer feedback and market 
trends from trusted datasets, which helps the companies to design products that align with consumer preferences.
/SM590000Research and development: In scientific rese arch, AI accelerates discovery by analyzing 
large volumes of trusted experimental data, which can lead t o breakthroughs in areas 
such as d rug discovery and material science.
1.3.4  Enhancing gover nance and compliance
In an era where regulatory landscapes are bec oming increasingly stringent, trusted data 
plays a pivotal role in helping ensure that AI systems operate within legal and ethical frameworks. Governance and co mpliance initiatives are fortified by AI systems that 
continuously monitor operations, detect anomalies, and flag potential risks.
For example, compliance monitoring AI tool s analyze operational data to help ensure 
adherence to industry regulations and standa rds, which reduce the risk of noncompliance 
penalties. Also, the usage of diverse and repres entative trusted datasets mitigates biases in 
AI model training, which foster fairness and ethi cal outcomes in critical applications such as 
hiring or loan approvals. Trusted data serves as a cornerstone for responsible AI development and deployment.
Trusted data helps ensure that AI systems operate within legal and ethical boundaries, which 
critical factors in maintain ing customer trust and avoiding regulatory penalties:
/SM590000Compliance monitoring: AI models can continuously analyze operational data to help 
ensure adherence to regulations by flagging any potential compliance risks.
/SM590000Bias mitigation: Trusted data, when diverse and representative, helps train AI models that 
are fair and unbiased, which helps ensure ethical outcomes in areas like hiring or loan approvals.
1.3.5  Unlocking new revenue streams
The monetization of trusted data through AI-driven services and products has emerged as a significant avenue for revenue generation. Or ganizations across sectors are capitalizing on 
this synergy to create innovative business models.

8 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiFor example, in telecommunications and retail, companies offer AI-powered insights or 
analytics as services to their partners and c lients, which transform data into a valuable asset. 
Moreover, trusted customer data enables hyper-targeted marketing campaigns, which enhance conversion rates and foster customer loyalty. By harnessing the power of trusted data, organizations can unlock untapped revenue opportunities while delivering value to stakeholders.
Organizations are monetizing their trusted data through AI-driven services and products:
/SM590000Data monetization: Companies in sectors such as telecommunications and retail generate 
new revenue by offering AI-powered insights or analytics as a service to their partners and 
clients.
/SM590000Personalized marketing: AI leverages trusted customer data to deliver hyper-targeted 
marketing campaigns that increase conversion rates and customer loyalty.
1.3.6  Transforming industrie s with AI and trusted data
The integration of AI with trusted data is revolutionizing industries in unique and profound 
ways. Retailers use transaction and customer data to fuel recommendation engines, which enhance sales and customer satisfaction. In agriculture, AI models analyze environmental and crop data to optimize farming practices an d maximize yields. Similarly, in the energy 
sector, AI systems leverage consumption and grid data to predict demand, optimize distribution, and enhance energy efficiency. These transformative applications underscore the versatility and impact of trusted data-driven AI across diverse domains.
Different industries are leveraging AI and trusted data in unique ways:
/SM590000Retail: Trusted transaction and customer data power recommendation engines that boost 
sales and improve customer experiences.
/SM590000Agriculture: AI models analyze trusted environmental and crop data to optimize farming 
practices and increase yield.
/SM590000Energy: AI systems use trusted consumption and grid data to predict demand and 
optimize energy distribution.
1.4  Improving AI model reliability
The reliability and interp retability of AI models  are enhanced when they are tr ained and 
validated on trusted data. High-quality data helps ensure that AI systems deliver consistent and accurate outputs, which foster stakeholder trust a nd facilitate br oader adoption.
For example, explainable AI (XAI) models rely  on trusted data to generate transparent and 
interpretable insights, which address concerns  about the “black-box” nature of AI. Also, 
trusted data simplifies model auditing and debugging by enabling the identification of inconsistencies and anomalies, which leads to continuous performance improvements. By prioritizing data quality, organizations can overcome one of the primary challenges of scaling AI systems.

Chapter 1. Competing wit h artificial intelligence 9Trusted data enhances t he reliability and interpre tability of AI models, addressing one of the 
major challenges in deploying AI at scale:
/SM590000Explainability and trust: AI m odels that are trained on hi gh-quality data provide more 
consistent and interpretable outputs, which enable stakeholders to trust and adopt AI-driven solutions.
/SM590000Model auditing and debugging: Trusted data helps identify inconsistencies and anomalies 
in AI predictions, which make it easier to debug models and improve their performance over time.
1.4.1  Enabling cross-enterprise collaboration
Trusted data serves as a bridge for collaboratio n across organizational silos and with external 
partners. This capability enhance s operational transparency and fosters innovation by 
enabling seamless data sharing and integration.
Enterprises can leverage trusted data infrastructures to break down silos so that 
cross-functional teams can work collaboratively on shared objectives. Secure data exchange mechanisms further facilitate partnerships  across geographies, which helps ensure 
compliance with data privacy regulations and fosters rust among stakeholders. Such collaborative ecosystems are critical for driving comprehensive digital transformation initiatives.
AI on trusted data enables organizations to collaborate more effectively across departments 
and even with external partners:
/SM590000Data sharing across silos: Enterprises can break down data silos and enable 
cross-functional collaboration to enhance operational transparency.
/SM590000Secure data exchange: Trusted data infrastructures help ensure that shared data between 
partners or across geographies remain secure and compliant.
1.4.2  Enhancing real -time decision-making
The ability to make real-time de cisions is a cornerstone of mo dern business strategies, and 
trusted data is a key enabler of  this capability. By processing and analyzing data  streams in 
real time, AI systems empower organizations to act swiftly and effectively.
In industries like finance, dynamic pricing models use real-time trusted data to optimize 
stock-pricing strategies based on demand and in ventory levels. Financia l institutions employ 
AI systems to perform instant risk assessments to mitigate fraud and help ensure secure 
transactions. These applicatio ns demonstrate how trusted da ta enhances the agility and 
responsiveness of AI-driven decision-making processes.
Real-time analytics that are powered by trusted data enable businesses to make faster, more 
informed decisions:
/SM590000Dynamic pricing: In e-commerce or travel industries, AI leverages real-time trusted data to 
optimize pricing strategies based on demand and inventory levels.
/SM590000Real-time risk assessment: Fi nancial institutions use AI to perform instant risk 
assessments for transactions to hel p mitigate fraud or credit risks.

10 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai1.4.3  Scaling AI-driven ecosystems
Large-scale AI implementations  depend on the robustness and reliability of trusted data 
ecosystems. By building scalable infrastructures that integrate trusted data with advanced AI capabilities, organizations can unlock the full potentia l of AI-driven solutions.
AI as a Service (AIaaS) platforms exemplify this integration by offering modular services such 
as ML models, NLP, and predictive analytics that are powered by trusted data. In parallel, the integration of IoT devices with AI systems generates vast volumes of real-time data to enable actionable insights in sectors like logistics, healthcare, and smart cities. These scalable 
ecosystems are the foundation for sustained growth and innovation.
Trusted data serves as the backbone for large- scale AI implementations, fostering robust AI 
ecosystems:
/SM590000AI as a Service (AIaaS): Companies are building scalable platforms where trusted data 
powers modular AI services like ML models, NLP, and predictive analytics.
/SM590000Integration with IoT: IoT devices generate vast amounts of data. Trusted IoT data enables 
AI systems to deliver real-time analytics for industries like logistics, healthcare, and smart cities.
1.4.4  Driving sustainability and envi ronmental, social, an d governance goals
AI-powered sustainabilit y initiatives are gain ing momentum, with trus ted data playing a 
central role in achieving environmental, social, and governance (ESG) objectives. By analyzing environmental and operational datasets, AI systems help organizations optimize resource usage, reduce carbon footprints, and enhance supply chain transparency.
For example, sustainabilit y analytics tools use trusted data to identify inefficiencies and 
recommend strategies for improv ing energy efficiency. Similarl y, AI systems provide visibility 
into supply chains to enable organizations to address waste and improve sustainability 
practices. By aligning AI initiatives with ESG goals, organizations can demonstrate their commitment to responsible and ethical operations.
AI, combined with trusted data, helps organizations meet ESG objectives:
/SM590000Sustainability analytics: AI mode ls analyze trusted environmen tal and operational data to 
optimize resource usage and reduce carbon footprints.
/SM590000Supply chain transparency: Truste d data provides visibility into the supply chain so that AI 
can identify inefficiencies, reduce waste, and improv e sustainability practices.
1.4.5  Personalizing customer experiences
Customer-centric in dustries are leveraging AI's abilit y to deliver highly personalized 
experiences, which is a capability that is ro oted in truste d data. By analyzing customer 
behavior, preferences, and interactions, AI systems create tailored experiences that enhance satisfaction and loyalty.
For example, adaptive AI systems use real-time trusted data to modify services dynamically 
to help ensure relevance and engagement. Behavioral analytics enable companies to predict customer needs, which reduce churn and fosters long-term relationships. Trusted data empowers organizations to elevate customer experiences to new heights.

Chapter 1. Competing with artificial intelligence 11Customer-centric in dustries are capitalizing on AI's ab ility to deliver highly personalized 
experiences through trusted data:
/SM590000Adaptive AI systems: Real-time, trusted cu stomer data enables AI systems to adapt and 
tailor services to create more engaging user experiences.
/SM590000Behavioral analytics: Trusted behavioral data enables companies to predict customer 
preferences, which reduce churn and increase satisfaction.
1.5  Creating new AI-enabled products and services
Trusted data serves as the foundation for dev eloping innovative AI-enabled products and 
services that redefine industries. By harnessin g historical and real-time data, organizations 
can anticipate needs and deliver solutions proactively.
For example, proactive support systems leverage AI to predict and address issues before 
they escalate, which enhances cu stomer satisfaction and operational efficiency. Custom AI 
solutions, which are tailo red to specific market niches, furt her demonstrate the transformative 
potential of trusted data. As organizations continue to invest in data governance and quality, the opportunities for creating AI-driven innovations will expand.
By aligning AI initiatives with business objectives and prioritizing trusted data infrastructures, 
organizations can unlock unparalleled levels of  efficiency, innovation, and growth. As the 
complexity of data landscapes continues to evolve, the role of trusted data in enabling AI to achieve its full potential becomes increasingly indispensable.
Trusted data opens doors to innovations that can redefine industries:
/SM590000Proactive support systems: AI systems, which are trained on historical  and real-time data, 
predict customer or machine needs before problems occur to offer preemptive solutions.
/SM590000Custom AI solutions: Organizations use their proprietary trusted data to build AI products 
that are tailored to niche market requirements.
By continuously investing in trusted data infr astructures and aligning AI initiatives with 
business goals, organizations can unlock new levels of efficiency, growth, and innovation.AI and trusted data offers a powerful opportunity to drive growth, innovation, and operational excellence. Organizations that prioritize data governance, ensure data quality, and build AI systems on trusted data are better positioned to harness these opportunities. As the volume 
and complexity of data continue to grow, the role of trusted data in enabling AI to deliver its full potential will become more critical.

12 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai

© Copyright IBM Corp. 2025. 13Chapter 2. Introducing IBM watsonx.ai
IBM watsonx is IBM’s next-generation platform that is designed to help businesses accelerate 
their journey into artificial in telligence (AI)-driven insights, de cision-making, and automation. 
The platform offers a comprehensive suite of t ools and services that are tailored to simplify 
and streamline the development and deployment of AI solutions. It is built on three foundational pillars:
/SM590000IBM watsonx.data: A scalable data lakehouse that is designed for efficient and secure 
data management to enable hybrid cloud deployments and optimize data for AI workloads.
/SM590000IBM watsonx.governance: Provides robust governance to help ensure that AI models 
remain ethical, transparent, and compliant with regulatory standards. It also helps businesses monitor and mitigate AI-related risks.
/SM590000IBM watsonx.ai: A cutting-edge AI development and deployment environment. It supports 
the full lifecycle of AI, from model training and fine-tuning to deployment and monitoring.
The seamless integration of these components enables enterprises to leverage trusted data 
(watsonx.data), enforce governance and ethical standards (watsonx.governance), and develop AI-powered solutions (watsonx.ai). Together, they form a comprehensive ecosystem for deploying enterprise-grade AI at scale.
The following topics are described in this chapter:
/SM5900002.1, “Overview of watsonx.ai” on page 14
/SM5900002.2, “Synergy between watsonx.ai and other components in the watsonx platform” on 
page 15
/SM5900002.3, “Business impact of these synergies” on page 162

14 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai2.1  Overview of watsonx.ai
watsonx.ai serves as the core engine of the watsonx platform, which is focused on the rapid 
development and deployment of AI models. Its architecture is designed to support various AI workloads, such as traditional machine learning  (ML), deep learning (DL), and generative AI 
(gen AI).
2.1.1  Key capabilities
Here are the key capab ilities of watsonx.ai:
/SM590000Foundation models (FMs): watsonx.ai provides access to pre-trained FMs, such as large 
language models (LLMs) and vision models, wh ich can be fine-tuned for specific business 
needs.
/SM590000Machine learning operations (MLOps): The platform integrates tools for version control, 
model monitoring, and deployment to streamline AI lifecycle management.
/SM590000Multi-cloud compatibility: Su pports hybrid and multi-cl oud environments so that 
businesses can run AI workloads wherever they see fit.
/SM590000Extensibility: Developers can br ing their own models or inte grate open-source frameworks 
like PyTorch and TensorFlow.
2.1.2  The watsonx.ai architecture
The watsonx.ai architecture components include the following items:
/SM590000Model Studio: An interface for training, fine-tuning, and deploying models.
/SM590000Inference Engine: Optimized for running AI models in production environments to help 
ensure low latency and high scalability.
/SM590000Integration Layer: Enables seamless integration with watsonx.data for real-time data 
access and watsonx.governance for compliance.
2.1.3  watsonx.ai empoweri ng IBM Software offerings
The watsonx.ai FMs are being infused throughout all of IBM's major software offerings. The 
FMs are as follows:
/SM590000IBM watsonx Code Assistant: Uses gen AI so  that developers can automatically generate 
code by using a natural-language prompt.
/SM590000IBM watsonx AIOps Insights: Includes FMs for code and natural language processing 
(NLP) to provide greater visibility in to performance acro ss IT environments.
/SM590000IBM watsonx Assistant and IBM watsonx Orchestrate®: Boosted by an NLP FM, IBM's 
digital labor products enhance employee prod uctivity and customer service experiences.
/SM590000Environmental Intelligence  Suite: powered by th e IBM geospatial FM, 
IBM EIS Builder Edition creates tailored solutions that address and mitigate environmental risks.

Chapter 2. Introducing IBM watsonx.ai 152.1.4  Benefits of using watsonx.ai for businesses
Adopting watsonx.ai offers several key advantages for enterprises looking to harness the 
power of AI:
/SM590000Accelerated AI development: watsonx.ai simplifies the development process with 
pre-trained FMs and built-in tools for traini ng and deployment. Businesses can achieve 
faster time-to-value by reducing the complexity of building AI from scratch.
/SM590000Enhanced productivity and efficiency: Through automation of repetitive tasks, watsonx.ai 
enables teams to focus on high er-value activities. gen AI capabilities can handle complex 
processes, which improve the mean time to resolution for IT incidents and streamlining customer service.
/SM590000Scalable and cost-efficient: watsonx.ai support for hybrid cloud and open architecture 
provides cost flexibility. Businesses can choose the most economical deployment 
environment and scale AI workloads as needed.
/SM590000Trust and governance: With watsonx.governance tightly integrated, watsonx.ai helps 
ensure that AI models operate transparently and ethically. Businesses can meet regulatory compliance standards, which mitigate risks that are associated with biased or unexplainable AI decisions.
/SM590000Business innovation: watsonx.ai enables companies to explore new AI-driven 
opportunities, such as personalizing customer experiences, optimizing supply chains, and driving data-driven decision-making.
watsonx.ai is a transformative tool that empowers businesses to unlock the full potential of AI. 
By integrating seamlessly with watsonx.data and watsonx.governance, it offers a unified platform that combines innovation, efficiency, and compliance. Organizations adopting watsonx.ai can expect to gain a competitive edge through smarter automation, better decision-making, and faster scaling of AI solutions.
2.2  Synergy between watsonx.a i and other components in the 
watsonx platform 
This section covers the following topics:
/SM590000Synergy between watsonx.ai and watsonx.data
/SM590000Synergy between watsonx.ai and watsonx.governance
2.2.1  Synergy between wa tsonx.ai and watsonx.data
watsonx.ai and watsonx.data streamline the development and deployment of AI models by 
ensuring that AI systems are powe red by high-quality, trusted data. Here is how their synergy 
creates value:
/SM590000Efficient AI model development: watsonx.data provides a robust and scalable data 
lakehouse that is optimized for AI workloads.  This lakehouse helps ensure that watsonx.ai 
has instant access to vast amounts of clean, organized, and queryable data, which accelerates training and fine-tuning of AI models.
/SM590000Real-time data for AI: watson x.data facilitates real-time da ta streaming, which enables 
watsonx.ai to build and run AI models on up-to-date information. This approach enables dynamic AI use cases, such as predictive maintenance and fraud detection.

16 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai/SM590000Hybrid and multi-cloud flexibility: Both wats onx.ai and watsonx.data  support deployment 
across hybrid and multi-clou d environments, which help en sure scalability and cost 
efficiency while keeping data sovereignty intact.
/SM590000Unified data governance: With  watsonx.data acting as th e backbone, organizations can 
ensure data integrity, enhance data sharing, and maintain a single source of truth for AI models that are developed in watsonx.ai.
2.2.2  Synergy between wats onx.ai and watsonx.governance
The relationship between watsonx.ai and watsonx.governance helps ensure that AI models 
are high-performing, compliant, ethical, and transparent. Here is how they complement each other:
/SM590000Ethical AI deployment: watsonx.governance provides guardrails for AI models that are 
developed and deployed through watsonx.ai. These guardrails include bias detection, 
explainability, and compliance trac king to help ensure  that AI decisions are fair and aligned 
with regulatory standards.
/SM590000Lifecycle management and monitoring: watsonx.governance tracks the entire lifecycle of 
AI models by monitoring performance, drift, and adherence to governance policies. This approach enables watsonx.ai users to maintain the integrity of deployed models over time.
/SM590000Transparency and a uditability: Models that are built  on watsonx.ai benefit from the 
watsonx.governance robust repo rting and audit capabilities, which provide stakeholders 
with clear insights into how AI models make decisions, which help ensure trustworthiness.
/SM590000Risk mitigation and remediation: If there are anomalies or breaches in governance 
policies, watsonx.governance en ables quick remediation. Th is capability is crucial for 
mission-critical applications where trust in AI outputs is paramount.
2.3  Business impact of these synergies
By leveraging the combined strengths of watsonx.ai, watsonx.data, and watsonx.governance, enterprises can achieve the following goals:
/SM590000Help ensure that their AI models are trained on trusted, compliant data.
/SM590000Maintain high performance and ethical standards across AI deployments.
/SM590000Accelerate innovation while minimizing the risks that are associated with AI adoption.
This holistic approach empowers organizations to maximize ROI on their AI investments and 
gain a competitive edge in their industries.

© Copyright IBM Corp. 2025. 17Chapter 3.Tools for diverse data science 
teams
Data science teams today are di verse in terms of skill sets, ba ckgrounds, and experiences. 
These teams are also diverse in terms of the types of solutions that are implemented. Common roles of data science teams are data scientists, machine learning (ML) engineers, and artificial intelligence (AI) engineers. Therefore, different types of tools and solutions are needed to support data science teams. 
This chapter describes a few of the key personas for IBM watsonx.ai and how the different 
types of tools that are available on watsonx.ai support these individuals in their day-to-day 
work. 
The following topics are described in this chapter:
/SM5900003.1, “Key personas for watsonx.ai” on page 18
/SM5900003.2, “Low-code, no-code, and full-code tools” on page 203

18 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai3.1  Key personas for watsonx.ai 
Many organizations are building data science te ams. Depending on the level of data science 
maturity within the organization, the types of ro les and the experience of individuals in these 
roles can vary. Common roles in data science te ams include data analysts, data scientists, 
ML engineers, and AI engineers. Other roles in organizations that can benefit from IBM watsonx.ai include, but are not limited to, data science leaders, directors of enterprise architecture, and line-of-business users. This section goes through a few of these key personas by providing an overview of each ro le’s responsibilities, common challenges that 
individuals in this role often fa ce, and how IBM watsonx.ai is designed to enable individuals in 
these roles. 
3.1.1  Data scientists
Data scientists use data to solve proble ms and improve decision making within an 
organization. Depending on th e team and organ ization, their respon sibilities include data 
collection, model development, data analysis, communication of findings, and providing recommendations. Data scientists often have a background in mathematics, specifically 
statistics and linear algebra, and programming.
Individuals in these roles often face challenges  that are related to a lack of self-service 
access to the correct, or accurate data, for cleaning, transforming, and generating insights. They also lack integrated tools across the model lifecycle, and depending on their level of experience and skill set, they might lack experi ence with creating models by using certain 
tools or programming languages.
IBM watsonx.ai addressees this situation with vari ous tools within the platform to provide data 
scientists with the flexibility that  they need to build  models. These tools include no-code, 
low-code, and full-co de solutions such as AutoAI, IBM SPSS® Modeler, and Jupyter 
Notebooks. watsonx.ai provides many tools to select from because the correct tool to use varies based on factors such as the indivi duals’ level of expertise and the project 
requirements.
Data scientists often collaborate with others in their team, such as business stakeholders, 
data science leaders, or fellow data scientists. watsonx.ai enables storing and sharing of assets among users within an organization through projects. In watsonx.ai, 
projects  are 
collaborative workspaces where individuals can work with and share data and other assets to 
accomplish a specific goal.
3.1.2  Machine learning engineers
ML engineers work with data scientists and other IT experts, such as software developers to automate and move ML models into production. Typically, ML engineers have a background in computer science, mathematics, statistics, or software engineering. ML engineers are responsible for the data science pipeline, which can include sourcing and preparing data, building and training models, deploying models to production, and maintaining and improving existing ML systems. 
Common challenges that are faced by ML engi neers include difficulty in defining short and 
long-term goals, difficulty in scaling ML models , general lack of support for the services that 
are used by various teams, an d incompatibility between tools.

Chapter 3. Tools for diverse data science teams 19IBM watsonx.ai helps overcome these challenges by providing various no-code, low-code, 
and full-code tools, which are compatible with each one. Also, with IBM Watson Machine Learning on IBM watsonx.ai, the model deployment process is simplified through capabilities 
such as deployment spaces. Regardless of the tool on watsonx.ai that is used to develop the model, ML engineers can deploy the model, which makes it simpler to deploy and manage ML assets. 
3.1.3  AI engineers
AI engineers are responsible for developing, maintaining, and implementing AI systems. Common tasks that are performed by AI engineers include building and maintaining AI systems, and tuning AI models. AI engineers generally have a background in computer science, mathematics, software engineering, or programming.
Because the AI engineer role is new to many organizations, a common challenge many 
teams and individuals in this ro le face involves having varying levels of technical skills and 
experience to implement AI solutions. Also, other challenges include selecting and fine-tuning models for a specific use case, and managing the cost to implement and maintain an AI solution. 
IBM watsonx.ai helps AI engineers overcome these challenges in a few ways, starting with 
the foundation model (FM) library that is provid ed by IBM. This library includes a diverse 
selection of AI models, such as the following ones:
/SM590000IBM trained models (the Granite and Slate model series) 
/SM590000IBM selected open-source models through Hugging Face
/SM590000Third-party models such  as llama and mixtral 
Teams can choose among many different FMs for their use case, and choose a model that is 
best suited for their use case. Teams are not locked into one specific vendor or model series. Also, for more advanced AI engineers, th ey can upload and deploy their own FMs. 
With watsonx.ai, there are different ways for AI engineers to work with models. The Prompt 
Lab tool in watsonx.ai enables AI engineers to write effective prompts (by using a GUI) to deploy to FMs for inferencing. For individu als who have more programming experience and 
technical expertise, there is also the programmatic alternative to the Prompt Lab, where users can prompt FMs by using the Python library or REST API. 
You can use the tuning studio in watsonx.ai to tune a smaller FM to improve its performance. 
AI engineers can tune a smaller FM to achieve comparable results to larger models in the same model family, which can lead to reduced inference costs in the long term. 

20 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai3.2  Low-code, no-code, and full-code tools
Because the demand for data scientists has grown over the years, finding experienced data 
scientists is difficult for many organizations. For organizations that are newer to data science, 
they find that experienced data scientists often request higher salaries or tend to seek opportunities with interesting and advanced problems to solve.
Therefore, it is important for organizations to find ML and AI platforms that support individuals 
with varying skill sets. There are many different types of tools for implementing data science, 
machine learning operations (MLOps), and generative AI (gen AI) solutions. At a high level, they can fall into one of three categories: no-code, low-code, or full-code. 
/SM590000With no-code tools, users can create soluti ons and applications without writing code. The 
tool provides a GUI and includes “drag-and-drop” features so that users can build models with little to no prog ramming knowledge. 
/SM590000Low-code tools provide a visual approach to  development so that users can generate 
solutions, such as models, with minimal ha nd-coding. Like no-code tools, these tools 
typically provide a GUI and include “drag-and-drop” features. Low-code tools enable users with minimal coding experience, such as citize n data scientists or business analysts to 
quickly build and implement a solution.
/SM590000With full-code tools, users can write their own code to develop solutions. These tools are 
typically leveraged by experien ced data scientists. Full-code to ols enable greater flexibility 
and more customization, but require deeper programming knowledge and expertise. 
3.2.1  No-code, low-code, and fu ll-code tools on  IBM watsonx.ai
When to use a no-code, lo w-code, or full-code tool varies by project requirements, team skill 
set, and the time and cost that a specific solution has for an organization. IBM watsonx.ai provides data scientists, ML e ngineers, and AI engin eers with the flexibility of choose the right 
tool for a use case by providing various no-code, low-code, and full-code tools as part of the overall platform. 
No-code solutions on IBM watsonx.ai
This section highlights a few of the no-code tools that are available on the IBM watsonx.ai 
platform. 
AutoAI
AutoAI is a no-code tool that data scientists can use to develop and prototype models quickly, 
without requiring the user to code or have programming knowledge. AutoAI helps data scientists and data science teams compare the resu lts of multiple models efficiently, thus 
providing teams with the opportunity to save time and money.
Figure 3-1 on page 21 shows an AutoAI experiment on watsonx.ai. 

Chapter 3. Tools for diverse data science teams 21Figure 3-1   AutoAI experiment on IBM watsonx.ai
Although AutoAI is a no-code tool, the ML pipelines that are generated by an AutoAI 
experiment can be exported as a notebook. Therefore, more experienced data scientists can view the code “under the hood” and make modifications and updates to the underlying code. 
Figure 3-2 shows an example notebook that was created from a pipeline that was generated 
from an AutoAI experiment. 
Figure 3-2   Pipeline notebook that wa s generated from an AutoAI experiment


22 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiAutoAI for Retrieval-Augmented Generation 
Retrieval-Augmented Generation (RAG) is an AI framework for improving the quality of large 
language model (LLM)-generated responses by grounding the model in external sources of knowledge to supplement the LLM’s internal representation of information. The key benefits of RAG solutions include reducing the chance that an LLM leaks sensitive data or ‘hallucinate’ incorrect or misleading information, and reducing the need for users to continuously train the model on new data, thus resulting in lower computational and financial costs. For more information about RAG, see What is Retrieval-Augmented Generation?
Many organizations are implementing RAG solutions  as part of their gen AI efforts. Although 
RAG has many benefits, there are challenges too, such as creating a robust and scalable pipeline, and the time to deliver and implement RAG solutions. 
AutoAI for RAG is a tool that was created by IBM. Similar to AutoAI, AutoAI for RAG is 
intended to help AI engineers quickly build RAG solutions. With AutoAI for RAG, AI engineers can quickly build and test multiple  RAG pipelines without writing code. From the pipelines that 
are generated, the AI engineer can assess the performance of each pipeline, select the best pipeline for their team and project, and deploy it into a production or non-production environment.
Figure 3-3 shows an AutoAI for RAG experiment.
Figure 3-3   AutoAI for RAG experiment on IBM watsonx.ai 
For more information about AutoAI for RAG, view the documentation and demo video at 
Creating a RAG experiment (fast path) (Beta) .
Synthetic Data Generator 
The Synthetic Data Generator is a no-code tool that you can use to generate tabular data for 
model training. Users have two options to genera te synthetic data by using the graphical flow 
editor in the Synthetic Data Generator tool:
/SM590000Generate synthetic tabular data based on production data, with the goal of masking and 
mimicking this data. 
/SM590000Generate synthetic data from a custom data sc hema that is defined by the user by using 
visual flows and modeling algorithms. 


Chapter 3. Tools for diverse data science teams 23Figure 3-4 shows a view of the Synthetic Data Generator interface on watsonx.ai. 
Figure 3-4   Synthetic Data Generator on IBM watsonx.ai
Data Refinery
Data Refinery is a no-code tool that you can use to prepare and visualize data without writing 
any code. With this tool, you can prepare the da ta for analysis by applying operations such as 
filters and aggregations, and you can generate visualizations such as pie charts and bar charts to extract insights and share findings with stakeholders. 
Figure 3-5 shows an example of a Data Refinery flow on watsonx.ai. 
Figure 3-5   Data Refinery flow on IBM watsonx.ai


24 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiPrompt Lab
Prompt Lab is a tool in IBM watsonx.ai that  you can use to experiment with prompting 
different FMs, and create and share effective prompts to submit to deployed FMs for inferencing. Prompt Lab is a no-code tool that provides AI engineers with three different edit 
modes for prompt editi ng: Chat, Structured, and  Freeform. This flexib ility enables novice and 
experienced AI engineers to get the most out of Prompt Lab and watsonx.ai. The Chat mode enables users to converse with a FM of their choice. The Structured mode is great for novice users because it helps them create effective pr ompts by providing defined fields, and also by 
providing sample templates to build on. The Freeform mode is great for more experienced AI engineers who know how to format a prompt; with this option, users submit prompts in plain text. 
Figure 3-6 shows the Chat mode in Prompt Lab on watsonx.ai.
Figure 3-6   Prompt Lab Chat on IBM watsonx.ai
Low-code solutions on IBM watsonx.ai
This section highlights a few of the low-code tools that are available on the IBM watsonx.ai 
platform.
SPSS Modeler
SPSS Modeler is a low-code tool that you can use to  transform data and build models with 
little to no-programming experience. You can drag-and-drop various nodes onto the canvas to create a flow to perform various tasks, such as importing data, merging data, and creating models. 
Figure 3-7 on page 25 shows an exampl e SPSS Modeler flow in watsonx.ai. 


Chapter 3. Tools for diverse data science teams 25Figure 3-7   SPSS Modeler flow on IBM watsonx.ai
Full-code solutions on IBM watsonx.ai
This section highlights a few of the full-code tools that are available on the IBM watsonx.ai 
platform. 
Jupyter Notebook editor
The Jupyter Notebook editor is a full-code tool  that is available on IBM watsonx.ai. Jupyter 
Notebooks enable more experienced data scientists and developers to write and run code directly on the IBM watsonx.ai platform. Teams can build more customized and flexible solutions. 
Figure 3-8 shows an example of a Jupyter Notebook on the watsonx.ai platform. 
Figure 3-8   Jupyter Notebook on IBM watsonx.ai


26 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiRStudio
RStudio, like the Jupyter Notebook editor, is  a full-code tool that is available on 
IBM watsonx.ai. RStudio enables individuals with  programming experience in R to visualize 
data, create models, and build solutions by using the R programming language on the IBM watsonx.ai platform. 
Figure 3-9 shows the RStudio interface on the watsonx.ai platform. 
Figure 3-9   RStudio on IBM watsonx.ai
Programmatic alternative to Prompt Lab
IBM watsonx.ai has a Python library and REST API that you can use to prompt FMs. This 
approach is an alternative to the GUI in the Prompt Lab that is used to prompt FMs. This option is great for users who have more programming or technical experience, and for projects and teams that might require an alternative to the Prompt Lab.
For more information about using the REST API or Python library to prompt FMs, see Coding 
generative AI solutions .


© Copyright IBM Corp. 2025. 27Chapter 4.Building and using artificial 
intelligence models
This chapter serves as a resource for setting up , building, fine-tuning, and deploying artificial 
intelligence (AI) models within  the IBM watsonx.ai ecosystem. By exploring key features, and 
best practices, it aims to empower users (begin ners or experienced practitioners) to harness 
the power of AI for developing effective business solutions and enhancing their AI initiatives.
The following topics are described in this chapter:
/SM5900004.1, “Prerequisites and assumptions” on page 28
/SM5900004.2, “How to use this chapter” on page 28
/SM5900004.3, “Building and using AI models in watsonx.ai” on page 28
/SM5900004.4, “Getting started with watsonx.ai: Setting up the environment” on page 29
/SM5900004.5, “Data preparation and ingestion for AI model building” on page 34
/SM5900004.6, “Building AI models in watsonx.ai” on page 38
/SM5900004.7, “Deploying AI models in watsonx.ai” on page 40
/SM5900004.8, “watsonx.ai LLM deployment” on page 45
/SM5900004.9, “Operationalizing machine learning and LLM models” on page 50
/SM5900004.10, “Additional information and where to go next” on page 544

28 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai4.1  Prerequisites and assumptions
In this chapter, it is assumed that you have met the following prerequisites:
/SM590000An active IBM Cloud® account: You have an ac tive IBM Cloud account. If you do not have 
one, see watsonx.ai .
/SM590000Familiarity with Jupyter Noteboo ks: You understand how to na vigate and work with Jupyter 
Notebooks for data exploration and model development.
/SM590000Proficiency in Python: You have basic to intermediate knowledge of Python programming 
because many examples, scripts, and workfl ows in this chapter use Python code.
/SM590000Cloud computing concepts: You have a basic und erstanding of cloud computing principles, 
including concepts such as APIs, data storage, computing resources, and cloud-based environments.
/SM590000Knowledge of large language models (LLMs): A general understanding of LLMs, their 
capabilities, and use cases is  beneficial for build ing and fine-tuning AI models within 
watsonx.ai.
/SM590000You read Chapters 1 - 3 of this book.
4.2  How to use this chapter
This chapter is structured to help users of varying expertise levels by providing a general 
approach to understanding, building, deploying, and optimizing AI models by using the watsonx.ai platform. 
For beginners, start with the overview sections to familiarize yourself with the platform's 
features and capabilities. Progress through the chapter sequentially, beginning with 
environment setup and basic concepts before delving into more comp lex topics, such as 
model building an d optimization. 
For experienced users, you can go directly to sp ecific chapters of in terest, such as model 
optimization techniques, deployment strategies, or advanced configurations. Each section is modular and provides targeted information and best practices that you can apply immediately to your projects. 
Throughout the chapter, you find practical examples, hands-on exercises, and links to more 
resources to help ensure a well-rounded learning experience. 
4.3  Building and using AI models in watsonx.ai
This section covers the following topics:
/SM590000Overview of the watsonx.ai platform
/SM590000Key features and capabilities
4.3.1  Overview of the watsonx.ai platform
watsonx.ai is an enterprise-grade AI studio that you use to streamline the development, 
training, tuning, and depl oyment of AI models. It  includes generative AI  (gen AI) capabilities 
that are powered by foundation models (FMs).

Chapter 4. Building and using artificial intelligence models 294.3.2  Key features and capabilities
Here are the key features a nd capabilities of watsonx.ai:
/SM590000Foundation models: Access various powerful, low-cost, and fit-for-purpose models, such 
as the IBM Granite series and other LLMs for tasks such as content generation, summarization, and classification.
/SM590000Data preparation: Use tools for refining and visualizing data to help ensure high-quality 
inputs for model training. 
/SM590000Model development: Build machine learning (ML) models by using open-source 
frameworks with options for code-based, automated, or visual data science approaches.
/SM590000Prompt Lab: Experiment with gen AI prompts to enable tasks like question answering, 
content generation, summarization, text classification, and data extraction.
/SM590000Tuning Studio: Fine-tune FMs to customize outputs for specific use cases to enhance 
model performance and accuracy.
/SM590000InstructLab: At the time of writing, this feature is planned to be integrated into watsonx.ai 
in the future.
4.4  Getting started with watsonx .ai: Setting up the environment
To set up the watsonx.ai environment, you must have an IBM Cloud account. To register for 
an account, see Create an IBM Cloud account .
At the time of writing, here are the high-level steps to provision watsonx.ai in a 
Software-as-a-Service (SaaS) environment:
1. Set up your IBM Cloud account.2. Create a Project in watsonx:
a. Log in to https://dataplatform.cloud.ibm.com/login
b. Expand the ‘hamburger’ navigation menu, as shown in Figure 4-1.
Figure 4-1   watsonx navigation menu icon
c. Select Projects → View all projects , as shown in Figure 4-2.
Figure 4-2   View all projects menu


30 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aid. Click New project + , as shown in Figure 4-3.
Figure 4-3   New project+ menu
e. Enter the project name and, if app licable, upload lo cal files. Click Create , as shown in 
Figure 4-4.
Figure 4-4   Create menu
3. Provision watsonx.ai Studio:
a. Log in to https://cloud.ibm.com/ .
b. To add services from the catalog, use the search box that is shown in Figure 4-5.
Figure 4-5   watsonx.ai utilities search box
c. Enter “watsonx.ai Studio” and choose the studio from the catalog, as shown in 
Figure 4-6.
Figure 4-6   wastonx.ai utilities list
d. Select a pricing plan ( Lite or Professional ), and then click Create .
Figure 4-7   watsonx.ai pricing plan selection
4. Generate a watsonx.ai API key:
a. Go to https://cloud.ibm.com/iam/apikeys .
b. Click Create , as shown in Figure 4-8.
Figure 4-8   Create API key menu option


Chapter 4. Building and using artificial intelligence models 31c. Enter the relevant information, and then click Create, as shown in Figure 4-9.
Figure 4-9   Create IBM Cloud API key
d. After the API key is successf ully created, copy or downlo ad the key and save it locally, 
as shown in Figure 4-10
Figure 4-10   API key creation
5. Open watsonx.ai Studio and associate the watsonx.ai service:
a. In the upper left of your screen, click the four horizontal lines, as shown in Figure 4-11
Figure 4-11   watsonx.ai studio menu icon


32 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aib. Select Resource list , as shown in Figure 4-12.
Figure 4-12   watsonx.ai studio Resource list option
c. Expand AI / Machine Learning , as shown in Figure 4-13.
Figure 4-13   AI / Machine Learning menu option
d. Click the watsonx.ai Studio  record, as shown in Figure 4-14.
Figure 4-14   watsonx.ai Studio record icon
e. Click View full details , as shown in Figure 4-15.
Figure 4-15   watsonx.ai studio details
f. Select Launch in → IBM watsonx , as shown in Figure 4-16.
Figure 4-16   watsonx.ai Launch in option
Figure 4-17 shows the Welcome to watsonx window.
Figure 4-17   Welcome to watsonx window


Chapter 4. Building and using artificial intelligence models 33g. Click the + in the  Projects table.
h. Enter a project name, for example, test_project , as shown in Figure 4-18.
Figure 4-18   Define details window
i. Select Storage (if required).
j. Click Create .
k. Select Chat and build prompts with foundations models .
l. Click Associate service  to associate a Watson ML servic e to the project, as shown in 
Figure 4-19.
Figure 4-19   Clicking Associate service 
m. Select the displayed Machine Learning  service and click Associate , as shown in 
Figure 4-20.
Figure 4-20   watsonx machine learning: Associate service


34 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ain. Go to the Assets  tab, and then click New asset , as shown in Figure 4-21.
Figure 4-21   watsonx projects: All assets
o. Select Chat and build prompts with foundation models , as shown in Figure 4-22. 
Figure 4-22   Chat and build pr ompts with foundation models tile
4.5  Data preparation and ingestion for AI model building
This section describes the following topics:
/SM590000Understanding the importance of data in AI
/SM590000Preparing and cleaning data: data quality considerations
/SM590000Handling missing data, outliers, and bias
/SM590000Ingesting data into watsonx.ai Studio
/SM590000Connecting to data repositories and cloud services
4.5.1  Understanding the importance of data in AI
Data is the backbone of AI. It shapes the accuracy, effectiveness, and reliability of AI models. 
High-quality data enables AI to learn patterns, make predictions, and deliver meaningful 
insights. Understanding the importance of data in AI goes beyond mere volume; it involves ensuring data accuracy, consistency, and fairness. Poor data quality, bias, or gaps can lead to flawed models, incorrect predictions, and poten tial ethical issues. Therefore, this section 
focuses on the robust data preparation, cleaning, and validation that is essential for building AI systems that are trustworthy, transparent,  and impactful in real-world applications.


Chapter 4. Building and using artificial intelligence models 354.5.2  Preparing and cleaning data: data quality considerations
Data quality is a critical factor in the success of  AI models because it directly influences their 
performance and accuracy. Here are some key considerations:
/SM590000Accuracy: Ensuring that data is correct, consistent, and error-free is vital for creating 
reliable AI models. Inaccurate data can lead to faulty predictions and flawed decision-making.
/SM590000Completeness: AI models rely on comprehensive datasets to learn effectively. Missing 
data can skew results, which cause incomplete or biased predictions.
/SM590000Consistency: Data must be consistent across different sources and formats to help ensure 
the AI model functions as expected.
/SM590000Relevance: Data should be pertinent to the problem the AI aims to solve. Irrelevant data 
might introduce noise and reduce model effectiveness.
/SM590000Bias and fairness: Addressing data biases and ensuring fairness are critical for building 
ethical and unbiased AI systems. Give careful attention to the diversity and representation in data to avoid discrimination and ensure balanced outcomes.
High data quality maximizes th e accuracy, transparency, an d applicability of AI systems, 
which ultimately enhances their value and impact.
4.5.3  Handling missing data, outliers, and bias
Handling missing data, outliers, and bias is cruc ial for ensuring the accuracy and fairness of 
AI models. Here is how each one is managed: 
/SM590000Handling missing data:
– Imputation techniques: Missing data can be filled by using st atistical techniques such 
as mean, median, or mode imputation, or more complex methods like k-nearest neighbors (KNNs) or predictive models.
– Data removal: Sometimes, records with significant missing values are removed if they 
are unlikely to add useful information.
– Domain knowledge: Input from domain experts can help determine whether missing 
data should be treated differently based on context. 
/SM590000Handling outliers:
– Detection: Outliers are identified by usi ng methods like Z-scores, Interquartile Range 
(IQR), or visualization techniques like boxplots.
– Treatment: Once detected, outliers can be managed by removal, transformation (for 
example, log transformation), or capping values based on acceptable thresholds.
– Contextual consideration: Not all outliers are problematic; they might represent 
genuine data points. Understanding their impact is crucial before deciding on a course of action.
/SM590000Addressing bias:
– Data auditing: Systematic review of datasets to identify sources of bias, such as 
underrepresentation of specific groups. 
– Data balancing: Techniques like oversampling, undersampling, or generating synthetic 
data (for example, SMOTE) can help balance datasets.

36 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai– Algorithmic bias mitigation: Algorithms can be fine-tuned by using fairness constraints 
or reweighting schemes to minimize bias during model training.
– Regular monitoring: Bias can emerge or c hange over time, which requires continuous 
assessment and model updates to maintain fairness and inclusivity.
By handling missing data, outliers, and bias, AI models can provide more accurate, reliable, 
and ethical outcomes, which ultimately increa se their utility and impact across diverse 
applications.
4.5.4  Ingesting data into watsonx.ai Studio
This section describes the following topics:
/SM590000Supported data formats and sources
/SM590000Manually uploading data to watsonx.ai Studio
Supported data formats and sources
watsonx.ai Studio supports seamless ingestion of diverse data formats and sources to 
facilitate efficient model devel opment and deployment. Suppor ted formats incl ude structured 
data (CSV, JSON, and Parquet), semi-structured data (XML and Avro), and unstructured data (plain text and PDF). Data can be sourced from cloud storage platforms (such as AWS S3 and IBM Cloud Object Storage), databases (such as PostgreSQL and MySQL), APIs, and on-premises file systems.
The ingestion process is op timized for scalability and ca n handle large datasets while 
ensuring data integrity and co mpatibility with downstream AI workflows. Integration with 
watsonx.data and watsonx.governance tools enables a secure, governed data pipeline, which enhances traceabilit y and compliance.
Manually uploading data to watsonx.ai Studio
You can upload files through the watsonx.ai Studio interface by dragging the files there, as 
shown in Figure 4-23. 
Figure 4-23   watsonx Studio data upload window
4.5.5  Connecting to data re positories and cloud services
watsonx.ai provides robust connectivity options to integrate with various data repositories and 
cloud services, which help ensure smooth data access for AI model development. Users can 
connect to cloud storage solutions such as IBM Cloud Object Storage, AWS S3, Azure Blob Storage, and Google Cloud Storage, and traditional databases like PostgreSQL, MySQL, and MongoDB.
Figure 4-24 on page 37 shows the watsonx cloud services connections window.


Chapter 4. Building and using artificial intelligence models 37Figure 4-24   watsonx cloud services connections 
At the time of publication, here are the re positories and services that are supported:
/SM590000Amazon Redshift
/SM590000Amazon S3
/SM590000Apache Cassandra
/SM590000Apache Derby
/SM590000Apache HDFS
/SM590000Apache Hive
/SM590000Apache Impala
/SM590000Apache Kafka
/SM590000Box
/SM590000DataStax Enterprise
/SM590000Denodo
/SM590000Dremio
/SM590000Dropbox
/SM590000Elasticsearch
/SM590000Google BigQuery
/SM590000Google Cloud Pub/Sub
/SM590000Google Cloud Storage
/SM590000Google Locker
/SM590000Greenplum Database
/SM590000IBM Cloud Data Engine
/SM590000IBM Cloud Object Storage
/SM590000IBM Cloudant®
/SM590000IBM Cognos® Analytics
/SM590000IBM Data Virtualization Manager for IBM z/OS®
/SM590000IBM DataStage® for Cloud Pak for Data
/SM590000IBM Db2®
/SM590000IBM Informix®
/SM590000IBM InfoSphere® DataStage
/SM590000IBM Match 360
/SM590000IBM MQ
/SM590000IBM Netezza® Performance Server
/SM590000IBM Planning Analytics
/SM590000IBM watsonx.data
/SM590000MariaDB
/SM590000Microsoft Azure Blob Storage
/SM590000Microsoft Azure Cosmos DB
/SM590000Microsoft Azure Data Lake Storage


38 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai/SM590000Microsoft Azure Databricks
/SM590000Microsoft Azure Files
/SM590000Microsoft Azure SQL Database
/SM590000Microsoft Azure Synapse Analytics
/SM590000Microsoft Power BI
/SM590000Microsoft SQL Server
/SM590000Milvus
/SM590000MongoDB
/SM590000MySQL
/SM590000Oracle Database
/SM590000PostgreSQL
/SM590000Presto
/SM590000Salesforce API
/SM590000SAP ASE
/SM590000SAP IQ
/SM590000SAP S/4HANA
/SM590000SingleStoreDB
/SM590000Snowflake
/SM590000Tableau
/SM590000Teradata database
/SM590000Vertica
watsonx.ai supports secure connection protoc ols, which include API-based integrations, 
JDBC/ODBC drivers, and file transfer mechanisms like SFTP. With built-in authentication and access controls, watsonx.ai helps ensure data  security while main taining flexibility for 
enterprise-scale data workflows.  By streamlining access to da ta repositories and services, 
the platform empowers teams to leverage their existing data infrastructure efficiently.
4.6  Building AI models in watsonx.ai
This section describes the following topics:
/SM590000Choosing the right model for your use case
/SM590000Model creation workflow
4.6.1  Choosing the right model for your use case
watsonx.ai supports many model types to meet diverse business needs: 
/SM590000Supervised models  for predictive tasks by using labeled data.
/SM590000Unsupervised models  for discovering patterns in unlabeled data.
/SM590000Reinforcement learning  (RL) models for decision-making through rewards and penalties.
/SM590000Large language models  (LLMs) for natural language understanding and generation. 
Pretrained LLMs streamline AI development, which enables faster implementation and powerful insights across applications.
In addition to its native capa bilities, watsonx.ai embraces flexibility with the IBM Bring Your 
Own Model (BYOM) feature, which enables users to integrate and fine-tune their own LLMs within the platform for customized solutions. Furthermore, watsonx.ai supports integration with Hugging Face, which enables access to a vast library of pretrained models and tools. This collaboration accelerates development by leveraging open-source innovations while maintaining watsonx.ai enterprise-grade secu rity and scalability.

Chapter 4. Building and using artificial intelligence models 394.6.2  Model creation workflow
This section describes the workflow of model creation.
Model selection and configuration
Selecting the appropriate AI model is key to achieving your business objectives, and 
watsonx.ai provides the flexib ility to support both LLM and non-LLM models. Whether your 
needs involve predictive analytics, pattern discovery, decision-making, or natural language 
processing (NLP), watsonx.ai helps ensure th at the correct tools are at your fingertips.
For non-LLM tasks, the platform accommodates models such as regression, classification, 
clustering, and RL, which enable a wide range of traditional ML applications.
When working with LLMs, watsonx.ai offers various pretrained models in different sizes, from 
lightweight options for tasks that require efficiency and speed to larger, more complex models that are ideal for nuanced language understanding and generation. Choosing the correct size depends on your specific use case, with sm aller models excelling in cost-effective, 
lower-latency scenarios and larger models delivering superior accuracy and depth for intricate applications.
With watsonx.ai, you can confidently match the model type and size to your project’s unique 
requirements to help ensure scalability, efficiency, and impact.
Training the model
Training your AI model is a crucial step in ta iloring it to your specific business needs. 
watsonx.ai provides powerful tools and workflows for training both non-LLM models and LLM models, which help ensure flexibility and pr ecision at every stage of development.
Non-LLM models
For traditional ML tasks, watson x.ai offers robust training ca pabilities that leverage tools like 
watsonx.ai Studio and AutoAI to streamline and enhance the development process.
watsonx.ai Studio
watson.ai Studio provides a collaborative env ironment for data scientists, developers, and 
analysts to prepare data, build, and train ML models. With features like Jupyter Notebooks, Python libraries, and model monitoring, wats on.ai Studio is desi gned for flexibility and 
scalability to accommodate pr ojects of any complexity.
For more information about watsonx.ai Studio, see IBM Watson Studio .
AutoAI
If you want o accelerate the development process, AutoAI automates key stages of ML, which 
include feature engineering, algorithm selection, and hyperparameter optimization. It simplifies the model-building process to make it  accessible to users with varying technical 
expertise while still deliverin g highly accurate results.
For more information about AutoAI, see IBM AutoAI .
LLM models
watsonx.ai provides advanced features for trai ning and fine-tuning LLMs to deliver seamless 
customization and performance.

40 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiPrompt Lab
Prompt Lab is an environment for creating and testing prompts that are tailored to specific 
tasks. With Prompt Lab, users can interact with  pretrained LLMs, evaluate their outputs, and 
refine instructions for optimal results, all without extensive coding expertise.
Within Prompt Lab, you can explore and train various training LLM training methodologies, 
such as Zero-Shot, Few-Shot, Multi-Shot, and Retrieval-Augmented Generation (RAG). 
For more information about Prompt Lab, see Prompt Lab .
Tuning Studio
For deeper customization, Tuning Studio enables fine-tuning of LLMs on your proprietary 
data, which helps ensure that the model adapts  to your specific dom ain while maintaining 
high performance. This feature is ideal for organizations seeking more targeted insights and applications from their AI. 
For more information about Tuning Studio, see Tuning Studio .
InstructLab
At the time of writing, InstructLab is not available. 
InstructLab will revolutionize the training proc ess by enabling users to craft task-specific 
instructions, which further enhance the prec ision of LLMs in generating accurate and 
actionable outputs. This tool simplifies the proc ess of aligning model behavior with unique 
business objectives.
With these comprehensive training tools, watsonx.ai empowers users to harness the full 
potential of their models, whether refining traditional ML algorithms or unleashing the power of cutting-edge LLMs.
4.7  Deploying AI models in watsonx.ai
This section explores the process of deploying AI models in the watsonx.ai platform. It provides a detailed overview of two major deployment options: Studio and Prompt Lab. From deploying models as APIs for real-time infe rence to batch processing workflows, the 
watsonx.ai platform helps ensure flexibility and scalability. Readers gain insights into selecting 
the most appropriate deployment strategy for thei r use case while also learning best practices 
to optimize performance and reliab ility in production environments.
4.7.1  watsonx.ai Studio deployments
Deploying models in watsonx.ai St udio involves several key steps to help ensure that your AI 
assets are effectively managed and operational. Here is a concise guide to help you through 
the process:
1. Create a deployment space: Begin by estab lishing a deployment space within the studio. 
This space serves as a collaborative environment where you can manage and deploy your AI assets. To set up a deployment space, go to the Deployment Space section in the Studio interface and follow the prompts to create a space (Figure 4-25 on page 41).

Chapter 4. Building and using artificial intelligence models 41Figure 4-25   watsonx.ai studio projects: Deployments
2. Promote or import your model: When your deployment space is ready, add your trained 
model to it. If your model is in a project, promote it to the deployment space. Alternatively, you can import models that are trained externally by uploading them directly into the deployment space. Ensure that the model files are in a compatible format and that any necessary dependencies are addressed. 
a. Click the three dots, and then click Promote to space  (Figure 4-26).
Figure 4-26   Model interface window
b. Enter deployment_test  in to the Name  field, select Production  under Deployment 
stage, and then click Create  (Figure 4-27).
Figure 4-27   Create a deployment spaceNote:  To help ensure that your trained model is in the right format and compressed, see 
Adding a model by using UI .


42 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai3. Create the deployment: With your model in the deployment space, initiate the deployment 
process:
a. Go to the Deployments window (Figure 4-28).
Figure 4-28   Deployments window
a. Select deployment_test
b. Click the generated model’s  service (Figure 4-29).
Figure 4-29   Services
c. Click New deployment , which opens the window that is shown in Figure 4-30 on 
page 43.


Chapter 4. Building and using artificial intelligence models 43Figure 4-30   Create a deployment
d. Under Deployment type, select either  Online or Batch , and enter 
regression_model_deployment  under Name  and regression_model_service  under 
Serving name. Click Create .
Online deployment is ideal for real-time processing, where the model handles input 
data and provides immediate predictions. Batch deployment is suitable for processing 
large datasets in bulk, which generate predictions for a collection of inputs at scheduled intervals or on-demand. 


44 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai4. Test the deployment: After deployment, validate the model’s functions by going to the 
newly created deployment and clicking regression_model_deployment , as shown in 
Figure 4-31. 
Figure 4-31   Deployment overview
watsonx.ai Studio provides multiple ways to test the deployment, such as the Test tab and 
code snippets (Figure 4-32).
Figure 4-32   Deployment testing


Chapter 4. Building and using artificial intelligence models 45By following these steps, you can effectively deploy and manage your AI models within 
watsonx.ai Studio, which helps ensure that the models are ready for production use and 
capable of delivering valuable insights. 
4.8  watsonx.ai LLM deployment
Deploying models in watsonx.ai St udio involves several key steps to help ensure that your AI 
assets are effectively managed and operational. This section provides a concise guide to help you through the process:
4.8.1  Model packaging and exporting
To package and export a model, complete the following steps:
1. Go to the Prompt Lab window, and if you do not already have a prompt set up, populate it 
by using the example that is shown in Figure 4-33. In this lab, you use a text classification prompt. 
Figure 4-33   watsonx Prompt Lab


46 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai2. Generate a response by the prompt. Keep the decoding method as Greedy , and set the 
max tokens to 5 to produce Positive and Negative text only (Figure 4-34).
Figure 4-34   Model parameters
3. Click Generate , which tests the prompt. Then, click the View code icon (Figure 4-35).
Figure 4-35   View code icon
4. Copy the code to a notepad application (Figure 4-36).
Figure 4-36   Code example


Chapter 4. Building and using artificial intelligence models 47The code (Figure 4-37) is an example of a REST call that starts the model. watsonx.ai 
also provides a Python API for model invoca tion, which you review later in this lab. 
The header of the REST request includes the URL where the model is hosted and a 
placeholder for the authentication token. At the time of writing, all users share a single model inference endpoint. In the future, IBM plans to provide dedicated model endpoints.
Security is managed by the IBM Cloud authentication token, which is described later in 
this section. 
The body of the request contains the entire prompt. 
Figure 4-37   Curl command example
5. At the end of the request, you specify the model parameters and the project ID, as shown 
in Figure 4-38 . 
Figure 4-38   Curl command details


48 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiTo look up the project ID, select Project → General → Manage  in the watsonx.ai project, 
as shown in Figure 4-39.
Figure 4-39   Projects Manage view
6. Save the newly configured prompt as a notebook. Select Standard notebook as the 
asset type and then select Save as , as shown in Figure 4-40.
Figure 4-40   Notebook save icon
7. You will now create an authentication token. Open the na vigation menu (four horizontal 
bars) in the upper left of the watsonx interface and select Access (IAM) , as shown in 
Figure 4-41 on page 49.


Chapter 4. Building and using artificial intelligence models 49Figure 4-41   Access (IAM) menu item
8. Select API Keys → Create . Give the token a name and save it in a notepad (Figure 4-42). 
You use the token in a Python notebook.
Figure 4-42   API keys
9. Go to your watsonx project and open the notebook that you saved in step 8 (Figure 4-43). 
Figure 4-43   watsonx Studio projects overview


50 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai10.Review the sample notebook. 
This notebook acts as a cli ent application that starts the deployed LLM with a Python SDK. 
You use the notebook as a client for simplicity of testing during this lab.
Enterprise client applications can be implemented in Python, Java, .NET, and many other 
programming languages. LLMs that are deployed in watsonx.ai can be started either with REST calls or the Python SDK.
Run the notebook to test the LLM with your prompts.
4.9  Operationalizing machine learning and LLM models
Now that you have a machine learning (ML) model or LLM built, you now enter the operational 
phase. Much like how traditional application development created the need for formal DevOps tools and systems, so too have AI models created the need for ModelOps. ModelOps is the practice of enabling the deployment and management of models throughout the application development and deploymen t lifecycle with the goal of operat ionalizing models in production. 
As a joint endeavor with traditional DevOps, ModelOps takes the feedback and measurements that are taken in the DevOps lifecycle to iterate on the training, testing and deploying stages of th e ModelOps lifecycle. 
Key stages in the ModelOps lifecycle include governance, monitoring, deployment of 
infrastructure, and m odel versioning. IBM offers variou s tools to help facilitate these 
processes, each of which has its own place in the lifecycle. Examples of such tools include 
the following ones: 
/SM590000IBM watsonx.gov  helps govern and monitor model key performance indicators (KPIs).
/SM590000IBM Instana ®™ helps monitor LLM performance, responsiveness, and throughput at the 
application level.
/SM590000IBM Turbonomic ® can help dynamically scale up and down infrastructure as the workload 
against your models changes.
/SM590000IBM API Connect ® provides a GUI wizard to create AI-aware APIs and products, plus 
integration with AI services to forward requests and manage responses.
To demonstrate how these models can be deployed into existing applicat ions, we briefly walk 
through how to call these models, and integrate them into the DevOps and ModelOps lifecycle. 
4.9.1  Calling ML mode ls by using API calls
When you have built a model, your ML model is live and ready to perform as an inference 
endpoint. This endpoint is your gateway to interact with the model so that you can send data and receive predictions in return. Here, we walk through how you can use it effectively. 
Securing your API key 
Before making any calls to your endpoint, yo u need an API key for se cure access. For more 
information about generating this key, see 4.8, “watsonx.ai LLM deployment” on page 45. Here is a quick overview: 
1. Go to your deployed model in watsonx.ai Studio.
2. Go to the Access  tab under the Deployment settings. 
3. Generate your API key and store it securely. (You use it to authenticate your requests.) 

Chapter 4. Building and using artificial intelligence models 51Making an API call
With your API key in hand, you are ready to communicate with your model. Figure 4-44 shows 
an example of a simple POST request (by using a cURL command) to send input data and 
retrieve predictions.
Figure 4-44   watsonx.ai regr ession model deployment tool
Note:  The watsonx.ai user interface (UI) prepopulates different ways to call the model, 
such as cURL, Java, JavaScript, Python, Scala, and others. 


52 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiVisualizing the process
Figure 4-45 shows a simple way to visua lize what occurs during an API call.
Figure 4-45   API call visualization
Your application sends input data to the endpoint; the model processes the data; and the 
results are sent back as predictions. Whether you are handling real-time data (through online deployment) or batch processing, this streamlined interaction helps ensure that you can make the most of your deployed model.
4.9.2  Calling Prompt Lab LLM  models by using API calls
Calling an LLM model from Prompt Lab is sim ilar to calling an LLM model from watsonx.ai’s 
Studio; the only difference is where to find the code within the UI to do so. To accomplish this task, complete the following steps:
1. Go to Prompt Lab, and then build your prompt in either the Chat , Structured , or Freeform  
tab. 
2. In the upper right, click the View code  icon (Figure 4-46).
Figure 4-46   View code icon
The Prompt Lab automatically creates everything that you need to copy and paste your 
prompt into the application of your choosing (in either cURL, Node.js, or Python), as shown in Figure 4-47 on page 53. 


Chapter 4. Building and using artificial intelligence models 53Figure 4-47   watsonx.ai Prompt Lab code window 
4.9.3  IBM watsonx Assistant
Operationalizing AI and ML models by using IBM technology helps ensure seamless 
deployment and management across enterprise environments while delivering measurable business outcomes. IBM watsonx.ai provides a robust platform for building, fine-tuning, and deploying AI models to enable data scientists to leverage pre-trained models or create custom solutions. Once models are developed, they can be containerized and deployed by using Red Hat OpenShift, which is the IBM enterprise Kubernetes platform, which helps ensure scalability, high availabilit y, and integration with an existi ng infrastructure. IBM Watson 
Studio simplifies model lifecycle management by providing end- to-end capabilities for version 
control, testing, and collaboration. Real-time monitoring is enabled through IBM Instana Observability so that teams can track KPIs, detect anomalies, and maintain model health in 
production environments.Note:  Include your bearer access token. For more information about this token within the 
IBM ecosystem, see Generating a bearer token .


54 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiFigure 4-48 shows an overview of IBM watsonx Assistant.
Figure 4-48   IBM watsonx Assistant overview
Ensuring the ongoing success of operational AI and ML solutions requires integrating 
governance, automation, and business alignment. IBM watsonx.governance enforces responsible AI principles by providing tool s for bias detection, lineage tracking, and 
compliance management, which help organizations meet regulatory requirements and ethical standards. Automated deployment pipelines with IBM DevOps for AI streamline continuous integration and continuous delivery (CI/CD), which enables rapid updates and retraining to address data drift or evolving business needs. Feedback loops that are powered by IBM Watson Discovery facilitate continuous improvement by analyzing real-world user 
interactions to enhance model performance. By leveraging IBM’s AI and hybrid cloud capabilities, organizations can operationalize AI and ML soluti ons effectively, which drive 
innovation while helpin g ensure reliability and trustworthiness.
For more information about tools that are related to operationalizing your models, see 4.10.3, 
“watsonx.ai data pipeline and orchestration” on page 55.
4.10  Additional information and where to go next
In this chapter, you le arned the following things:
/SM590000How to set up your environment, which includes IBM Cloud accounts and project 
configuration.
/SM590000The key features of watsonx.ai, such as FMs, Prompt Lab, and Tuning Studio.
/SM590000The importance of data quality, cleaning, and ingestion for AI model development.
/SM590000Building and training AI models, which include traditional ML and LLMs, by using tools like 
AutoAI, Tuning Studio, and InstructLab.
/SM590000Deploying AI models as APIs for real-time or batch processing and integrating them with 
enterprise systems.


Chapter 4. Building and using artificial intelligence models 554.10.1  Additional support and documentation
watsonx.ai has extensive support and documentat ion to help users maximize the platform's 
capabilities. The IBM watsonx Documentation Portal  offers a comprehensive collection of 
resources, which include detailed user guides, tutorials, API references, and best practices. Whether you are starting or looking to optimize your AI workflows, the portal helps ensure that you have the guidance that you need to succeed.
Highlights include th e following resources:
/SM590000Getting Started Guides: Step-by-step instructions for onboarding and initial setup.
/SM590000Model Development Resources: In-depth documentation about training, fine-tuning, and 
deploying both LLM and non-LLM models.
/SM590000Troubleshooting and FAQs: Solutions to common issues and tips for resolving challenges 
efficiently.
/SM590000Integration Guidance: Instructions for incorporating watsonx.ai into existing workflows and 
leveraging tools, such as Hugging Face and BYOM.
This rich repository of knowledge empowers users at every skill level to confidently build, 
deploy, and scale AI solutions with watsonx.ai.
4.10.2  watsonx.ai API reference
For comprehensive guidan ce about using watsonx. ai capabilities, the IBM watsonx API 
Documentation  offers detailed information about avail able APIs, including endpoints, request 
parameters, and response structures. This resource is essential for developers that want to integrate watsonx.ai into their applications by  providing clear instructions and examples to 
facilitate seamless implementa tion. Whether you are working with FMs, performing text 
inference, or managing deployments, this docum entation serves as a valuable reference to 
help ensure effective and efficient usage of watsonx.ai features.
4.10.3  watsonx.ai data pipeline and orchestration
For more information about data pipelining and orchestration, see the following resources:
/SM590000IBM Orchestration Pipelines:
https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-orchestrati
on-overview.html?context=wx
/SM590000IBM Seismic: Introducing AI Agent Orchestration (IBMid required):
https://ibm.seismic.com/Link/Content/DCbHf2RCFTPf3G9Cc2PBGggJWfGV
/SM590000Instana: 
https://www.ibm.com/products/instana/generative-ai-monitoring
/SM590000Turbonomic:
https://community.ibm.com/community/user/aiops/blogs/cheuk-hung-lam/2024/05/28/
turbonomic-tackles-gpus-for-genai-workloadshttps://www.ibm.com/case-studies/ibm-big-ai-models-turbonomic

56 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai

© Copyright IBM Corp. 2025. 57Chapter 5.Advanced capabilities of 
watsonx.ai
watsonx.ai embodies IBM’s extensiv e expertise in artificial in telligence (AI) and foundation 
models (FMs), which combine advanced AI research with practical tools to make large language models (LLMs) efficient and versatile across many applications. Underlying watsonx.ai is its integration of FMs that are tuned to accelerate and optimize business operations. These models are positioned to perform at the intersection of language understanding, structured data processing, and knowledge retrieval, which enhance the ability to extract, refine, and use va st amounts of unstructured data.
The platform's capabilities ex tend beyond simple text processing to include complex 
interactions between structured and unstructured data sources, which enable the model to draw relevant information and learn domain-specific knowledge. For example, watsonx.ai supports both general- purpose and highly specialized mode l architectures, which facilitate 
the design of task-optimized LLMs that serve nuanced business needs while ensuring data privacy and regulatory complia nce. Its multi-modal capabilit ies enable seamless handling of 
diverse data types (such as text, image, and audio inputs) and applications to traverse disparate data landscapes cohesively, which achi eves a high level of contextual relevance 
and adaptability.
The watsonx.ai platform has several capabilities to support advanced use cases such as 
prompt engineering, multi-task prompt tuning, and fine-tuning. 
The following topics are described in this chapter:
/SM5900005.1, “Prompt engineering” on page 58
/SM5900005.2, “Multitask prompt tuning” on page 61
/SM5900005.3, “Fine-tuning” on page 64
/SM5900005.4, “InstructLab” on page 675

58 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai5.1  Prompt engineering
Prompt engineering within the watsonx.ai ecosystem serves as an essential component in 
harnessing the full potential of language models. By precisely framing prompts, users can guide LLM responses to ward relevance and co herence, which greatly enhances the utility of 
generated outputs. The process of prompt engineering in watsonx.ai is highly nuanced, and it involves detailed adjustments to phrasing, context, and iterative feedback mechanisms to yield wanted outputs consistently. Prompt engineering plays a pivotal role in directing LLMs to perform specific tasks with high precision, a ta sk that requires linguistic adjustments and a 
deep understanding of the underlying model dynamics.
The reason why prompt engineering is important is because it is a way to make generalist 
models (LLMs) perform a specific task. Without quality informat ion, well-defin ed instructions, 
and a clear set of examples, the model might misbehave and hallucinate in various ways.
Prompt engineering is about writing something in a better, clearer, and cleaner form. It also 
involves using specific system tokens that are for only the model that is used and, if available, a series of examples that provide better information to the model so that it can perform as intended. This approach is explored more in-depth in this section.
5.1.1  Prompting techniques
There are three main prompting techniques: 
/SM590000Zero-shot prompting
/SM590000One-shot prompting
/SM590000Few-shot prompting
These techniques are not learning techniques, but prompting techniques only, which improve 
model performances at inference-time without modifying the original model.
These techniques leverage the model's existing capabilities with out requiring fine-tuning or 
parameter updates, which make them lightweight and adaptable solutions for various use cases. This section provides an explanation of each prompting technique, and when it is best to use each approach.
/SM590000Zero-shot prompting: Relies solely on the model's pre-trained knowledge to generate 
responses without providing any task-specific examples in the prompt. Instead, the input typically includes clear instructions or a well-defined query that guides the model to perform the task, for example, asking a model to summarize a paragraph or translate a sentence into another language without providi ng prior examples. The effectiveness of this 
approach hinges on the clarity and precision of the prompt and the model's inherent ability 
to generalize across diverse tasks.
/SM590000One-shot prompting: In this approach, a single example of the task is embedded within the 
prompt, alongside the query or instructions. This example serves as a reference for the 
model to infer the behavior. By including a single demonstration, one-shot prompting can 
enhance performance for tasks that require nuanced or domain-specific understanding because it provides a concre te context for the model to interpret the instructions.

Chapter 5. Advanced cap abilities of watsonx.ai 59/SM590000Few-shot prompting: Expands on this concept by incorporating multiple examples of the 
task in the prompt. The additional examples provide a richer context and help the model 
better understand complex patterns or subtle variations in the task. Few-shot prompting is useful for tasks that require multi-step reasoning, handling of ambiguous inputs, or understanding domain-specific jargon. However, it demands careful prompt construction to balance informativeness and brevity because excessive length can lead to token limitations or diminished performance.
Figure 5-1   LLM prompting method types overview
5.1.2  Importance of system tokens
In addition to zero-shot, one-shot, and few-shot prompting techniques, system tokens  (also 
known as system-level instructions  or control tokens) play a critical role in crafting effective 
prompts. These tokens provide metadata or guidance to steer the behavior of the language model at a higher level, often defining the co ntext, tone, or expected behavior of the model 
during inference. Importantly, the implementati on and interpretation of these tokens can vary 
across different models, making their effective use model specific.
System tokens enable users to establish a “role” or context for the model, shaping its 
responses beyond what is specified in the natural language prompt. 
By embedding these tokens, users can accomplish the following goals:
/SM590000Control output behavior: Ensure the order of prompt completions between the main 
prompt areas of System, Assistant, and User
/SM590000Reduce ambiguity: Guide the model's interpretation of the task, especially in contexts 
where instructions alone might be misinterpreted.
/SM590000Enhance few-shot learning: When combined with example-based prompting, system 
tokens can provide an overarching framework that amplifies the impact of the examples.
5.1.3  Model-specific peculiarities
Different language models interpret and use system tokens in unique ways due to their architecture and pre-training data. The best practices for incorporating system tokens are to understand model documentation. Because the behavior of system tokens is model-dependent, consulting the model's technical documentation is essential to understanding how tokens are implemented and what variations are supported.


60 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai5.1.4  How watsonx.ai suppo rts prompt engineering
Regarding prompt engineering, the simplest way to interact with LLMs is extensive but 
peculiar. Fortunately, the watsonx.ai platform enables prompt engineering by providing a series of tools for its usage. Figure 5-2 shows a series of these tools in the Prompt Lab section of watsonx.ai.
Figure 5-2   watsonx.ai Prompt Lab dashboard
Prompt Lab is the main area to access and interact with LLMs. Here, it is possible to interact with models and perform prompt engineering techniques. In Prompt Lab, users have three modes to select from to interact with LLMs:
/SM590000Chat mode: Provides a simplisti c, multimodal chatbot-like inte raction with capabilities such 
as memory and document understanding. It is good for model interaction and for simple system prompt definition and a zero-shot-prompting approach.
/SM590000Structured mode: Provides a way to set up your prompt to create a particular prompt 
engineering setting within the main model Instructions and Examples, where the examples are the input/output series that is provided in a few-shot-prompting setting. It automatically applies the system tokens for a specific model to best fit the one/few-shot-prompting setting.
/SM590000Freeform mode: Provides more advanced users with the option to be free of crafting their 
raw prompt by using all the possible prompt  engineering capabilitie s that leverage raw 
system tokens. Although this mode leverages the power of prompt engineering and freedom, it requires specific skills in unde rstanding system tokens, what they are for a 
specific model, and how to use them.
Although prompt engineering provides a faster way to the objective, it is not always the best 
tool to use or the most capable tool that is  available. As task difficulty and complexity 
increases, watsonx.ai can use more advanced model enhancing techniques. We explore these techniques in the following sections of this chapter. 


Chapter 5. Advanced cap abilities of watsonx.ai 615.2  Multitask prompt tuning
Multitask prompt tuning  within watsonx.ai builds on traditional prompt engineering by 
implementing adaptive mechanisms to refine the prompt’s interpretative accuracy over time. This approach differs fundamentally from prompt engineering because it modifies prompt content and continuously aligns the model’s interpretative layers with domain-specific expectations. In essence, prompt tuning enables models to retain learned adjustments across sessions, which support consistency and reduce  the need for extensive re-engineering.
Prompt tuning leverages techniques such as embedding adjustments and parameter scaling 
to influence the model’s internal state and guide responses within boundaries. Using the watsonx.ai dynamic configuration settings, de velopers can set up continuous tuning 
processes that adapt prompts based on evolving  business contexts, which lead to a finely 
calibrated model that re flects current operation al realities. This c apability enables rapid 
adaptation without costly retraining, and the watsonx.ai architecture permits this tuning to take place seamlessly, which enables real-time adjustments as new data is ingested or as user preferences change.
In this setting, it is not the LLM that is modified. Instead, a dedicated, smaller LLM is trained to 
generate the best possible prompt adjustment for each prompt in the input. The smaller LLM leverages the system tokens that are available for each LLM on watsonx.ai and produces new, compatible virtual tokens to enhance the performances. The smaller LLM is trained by using a loss function that accounts for the resulting response from the immutable (in this setting) generative LLM model that you want to improve, and adapts its weights to create better prompts through a 
tunable soft prompt , as shown in Figure 5-3.
Figure 5-3   Prompt tuning overview 
With prompt tuning, you create a model that automatizes the prompt engineering task, which makes it dynamically adaptive to new, incoming inputs over time. The key benefit of prompt tuning is performing tuning in ways that are better than what experts can do for certain tasks, that is, the best token leading to a successful completion of the input task. Prompt tuning can do this task because it leverages 100 - 10000 examples to learn whic h token is the best one 
to add to a starting pre-engineered prompt to minimize the loss of the generative model. 


62 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiThe watsonx.ai platform provides a simplistic way of using prompt tuning, as shown in 
Figure 5-4.
Figure 5-4   Prompt tuning in watsonx.ai Tuning Studio
5.2.1  Prompt tuning parameters
In watsonx.ai Tuning Studio, you can use multitask prompt tuning by leveraging various prompt tuning parameters. The process of optimizing hyperparameters for prompt tuning, such as batch size, the number of epochs, the learning rate, and accumulation steps, plays a critical role in achieving task-specific adaptation and helping ensure effective usage of LLMs. Each of these parameters impacts the training process in unique ways by influencing the generalization ability, st ability, computational ef ficiency, and pe rformance of th e fine-tuned 
prompts.
/SM590000
Batch size  refers to the number of training samples that are processed simultaneously 
during each forward and backward pass through the model. It is a fundamental factor in determining the balance between computational efficiency and the quality of gradient updates. Larger batch sizes tend to stabiliz e gradient updates by averaging over more 
samples, which enable faster convergence.  However, they often require significant 
computational resources and might overlook fine -grained variations in  the dataset, which 
might potentially limit the prompt's ability to address nuanced tasks.  Conversely, smaller 
batch sizes enable greater granularity in gradi ent computations, which is advantageous for 
small datasets or specific tasks. However, small batches introduce noisier gradient updates, which require more iterations to c onverge effectively. To optimize batch size, 
practitioners should aim for a balance that satisfies computat ional feasibility while meeting 
the requirements of the task. Dynamic batch sizing (adjusting the batch size during training) can further stabilize the learning process and e nhance overall effectiveness.
/SM590000The 
number of epochs  represents the total number of complete passes that the training 
algorithm makes through the dataset. This parameter directly influences how thoroughly the model explores the data to refine its prompt parameters. A higher number of epochs allows the model to capture in tricate patterns, which improv es task-specific adaptation. 
However, this approach  comes with the risk of overfitting, especially with smaller datasets, 
which reduce the generalization of the prompt s. Conversely, a lower number of epochs 
minimizes the risk of overfitting but might lead to underoptimized prompts that fail to leverage the model's full potential. To strike the right balance, monitor validation loss and apply early stopping criteria to help ensure that training halts before overfitting occurs. 
Using pre-trained checkpoints can also reduc e the need for extensive epochs because 
these starting points encapsulate foundatio nal knowledge that accelerates convergence.


Chapter 5. Advanced cap abilities of watsonx.ai 63/SM590000The learning rate  governs the size of the updates that are made to prompt parameters 
during each optimization step. It influences the speed and stability of th e training process. 
A high learning rate expedites convergenc e, which reduces training time but risks 
overshooting optimal solutions, and can lead to suboptimal performance or even divergence. Conversely, a low learning rate enables a more precise exploration of the 
parameter space, which increases the likelihood of finding an optimal solution at the cost of prolonged training. Effective strategies include employing learning rate schedules, such as cosine decay or step-based decay, which adjust the learning rate dynamically during training. Warm-up strategies, where the learning rate gradually increases at the start of training, can also mitigate in itial instability and improve overall training robustness.
/SM590000The concept of 
accumulation steps  addresses memory constraints by enabling gradient 
accumulation across several mini-batches before updating the model’s parameters. This approach effectively simulates larger batch sizes without exceeding hardware memory limits, which make it valuab le for memory-constrained environments. Accumulation steps 
smooth gradient updates by averaging across multiple mini-batches, which improve stability at the cost of increased training time . Optimizing this parame ter involves selecting 
an accumulation step size that balances memory efficiency with the effective batch size. Combining this approach with batch size tuning can further optimize resource usage and enhance performance.
5.2.2  Interdependencies an d holistic tuning strategies
These hyperparameters are interdependent be cause changes in one can influence the 
behavior of others. For example, increasing the number of epochs without modifying the 
learning rate might lead to overfitting, and combining a high batch size with too few epochs might result in undertrained prompts. To navigate these interdependencies, practitioners can employ regularization techniques such as data augmentation to counteract overfitting in high-epoch scenarios. Gradient c lipping can also be used to prev ent instability during training, 
particularly when high accumulation steps are involved.
Performance metrics, which include task-spec ific measures like accuracy or F1-scores, 
should guide the evaluation of prompt tuning effectiveness. Monitoring loss convergence and gradient stability help ensure that the chosen hy perparameters lead to tangible 
improvements.
Carefully calibrating batch size, the number of epochs, the learning rate, and accumulation 
steps enables precise optimization of prompt tuning, which unlocks the full potential of LLMs for specific tasks. By managing these para meters holistically, practitioners can achieve 
performance gains while balancing computational efficiency and resource constraints.

64 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai5.3  Fine-tuning
Fine-tuning within the watsonx.ai ecosys tem represents the next layer of model 
specialization, where foundation LLMs undergo retraining on domain-specific datasets to enhance accuracy and relevance for specific applications. Fine-tuning goes beyond prompt adjustments by modifying the model’s weights to encode new knowledge or adapt to complex industry-specific language structures, terminologies, and operational protocols. This process is beneficial for industries that require high precision in terminology and context, such as 
healthcare, finance, and legal services. Fi ne-tuning within the watsonx.ai ecosystem 
exemplifies a sophisticated approach to model specialization, which enables foundation LLMs 
to adapt to domain-specific needs through retr aining. Unlike prompt tuning, which focuses on 
lightweight modifications to steer model behavior, fine-tuning directly alters the model’s weights to encode new knowledge or align with complex industry-specific language structures and terminologies.
5.3.1  Challenges with fine-tuning
To fully appreciate the significan ce of watsonx.ai capabilities, it  is essential to understand the 
inherent complexity of fine-tuning in general. At its core, fine-tuning involves retraining a model’s internal weights on care fully curated datasets to refine its understanding of specific 
terminologies, language patterns, or operational protocols. This process differs from lightweight techniques like prompt tuning, wh ich adjusts model behavior externally without 
altering its core structure. Fine-tuning, by contrast, modifies the model itself, embedding new knowledge directly in to its architecture. 
While this approach enables unparalleled precis ion and customization, it introduces many 
challenges. Here are some common challenges with fine-tuning:
/SM590000Data management: One of the primary difficulties with fine-tuning. Fine-tuning demands 
datasets that are relevant and meticulously prepared, which includes ensuring that the data is formatted correctly, free from bias, and representative of the target domain. Even determining the appropriate size of the dataset requires careful consideration because too little data risks underfitting, and too much can lead to overfitting or unnecessary computational burdens. For example, in watsonx.ai, datasets are limited to 200 MB for JSON or JSONL files, or up to 10,000 examples when sourced from connected data stores. These constraints are carefully balanced to optimize efficiency without sacrificing 
performance, but managing these parameters manually would be daunting for most users.
/SM590000Precise calibration of numerous hyperparameters: These hyperparameters include the 
learning rate, batch size, number of training epochs, and strategies for regularization, among others. Each of these parameters is in terdependent, which means that altering one 
can have cascading effects on others. Achieving the optimal configuration often involves extensive trial-and-error or the usage of advanced hyperparameter optimization techniques like Bayesian search . This complexity is compo unded when working with large 
models, which can have billions of parameters, which requir e significant computational 
resources and expertise to manage effectively.

Chapter 5. Advanced cap abilities of watsonx.ai 65/SM590000Computationally demanding workloads requiring a high-performance architecture: Large 
models can have billions of  parameters, which require a significant amount of 
computational resources. Even with the right data and parameters in place, fine-tuning remains computationally demanding. Training these large LLMs requires access to high-performance hardware, such as multi-GPU, along with robust memory and storage capabilities. For organizations without dedicated AI infrastructure, thes e requirements are 
often prohibitive.
/SM590000Ensuring stability during the tr aining process: Large-scale op timization algorithms are 
prone to issues like exploding or vanishing gradients, so achieving convergence without diverging from the optimal solution requires careful tuning and monitoring.
5.3.2  How watsonx.ai addr esses fine-tuning challenges
By automating the complexities of fine-tuning, watsonx.ai transforms what was once a 
labor-intensive and technically demanding process into an accessible, streamlined experience. This approach lowers the barrier to entry for organizations looking to adopt AI and enables experienced practitioners to focus on higher-level strategic goals rather than 
getting bogged down in technical minutiae. With its combination of cutting-edge technologies, managed infrastructure, and user-centric design, watsonx.ai empowers businesses to harness the full potential of fine-tuning, which unlocks new levels of precision, efficiency, and innovation in AI-driven solutions. 
watsonx.ai provides the following features:
/SM590000Hardware and resource allocation automation: The platform’s automation begins with its 
ability to manage hardware and resource allocation seamle ssly. Users do not need to 
worry about provisioning servers, config uring GPUs, or scaling their setups to 
accommodate large datasets or models. Instead, watsonx.ai handles these tasks behind the scenes, helping ensure that every fine-tuning operation runs on optimized configurations. 
/SM590000Supervised Fine-Tuning Trainer (SFTTrainer): At the heart of watsonx.ai fine-tuning 
capabilities is the SFTTr ainer, which is a powerful tool that is developed in collaboration 
with Hugging Face. This framework simplifies the optimization of model weights by automating key aspects of the training process, which includes the application of advanced learning rate schedules and warm-u p strategies. These techniques are crucial 
for maintaining stab ility during training, particularl y when dealing with complex or 
high-dimensional data. By leveraging SFTTrainer, watsonx.ai helps ensure that models converge rapidly and reliably without the need for extensive manual intervention. 
/SM590000Low-rank adaptation (LoRA) and quantized low-rank adaptation (QLoRA): In addition to 
SFTTrainer, watsonx.ai incorporates cutting-edge techniques like LoRA and QLoRA. These methods represent a paradigm shift in fine-tuning efficiency. Rather than retraining all of a model’s parameters, LoRA focuses on fine-tuning small modula r blocks of weights 
while freezing most the model. This approach reduces the computational and memory requirements of the process, which makes fine-tuning accessible even on resource-constrained hardware. QLoRA goes a step further by lowering the precision of certain parameters during training, which further optimizes performance without compromising accuracy. These innovations enable watsonx.ai to deliver results faster and with fewer resources than traditional approaches. 
/SM590000Tuning Studio integration: Another key advantage of watsonx.ai is its integration with the 
Tuning Studio, which provides access to a library of pre-configured model templates. These templates enable users to build on pre-existing architectures that are optimized for specific tasks or domains. This approach eliminates the need to design custom models from scratch, which reduces the time and expertise that are required to initiate fine-tuning projects. 

66 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai/SM590000Custom FMs: For organizations with unique requirements, watsonx.ai also supports the 
import of custom FMs if they have fewer th an 20 billion parameters . This approach opens 
the possibility of automatically fine-tuning tons of available models on Hugging Face and 
on watsonx.ai. This flexibility helps ensure that the pla tform can accommodate a wide 
range of use cases and industries. 
/SM590000Monitoring and optimization: Throughout the fine-tuning process, watsonx.ai provides 
robust tools for monitoring and optimization.  Real-time performance tracking enables 
users to assess key metrics, such as validat ion loss and gradient stability, which helps 
ensure that the model improves as expect ed. The platform also employs advanced 
early-stopping mechanisms to prevent overfitting  by halting training when further iterations 
would yield diminishing returns. These features  enhance the efficiency of the process and 
improve the quality of the final model. 
Figure 5-5 shows the watsonx.ai fine-tuning process.
Figure 5-5   Prompt fine-t uning pipeline process overview
In conclusion, watsonx.ai revolutionizes the traditionally complex and resource-intensive 
process of fine-tuning LLMs by introducing a seamlessly automated, highly efficient, and 
scalable solution. By leveraging advanced tool s like the SFTTrainer, innovative techniques 
such as LoRA and QLoRA, and a robust Tuning Studio, watsonx.ai enables businesses to achieve unparalleled levels of cu stomization and precision in thei r AI solutions. Its ability to 
manage every aspect of the fine-tuning lifecycle (from data preparation and parameter optimization to resource allocation and model monitoring) removes significant technical barriers, which democratize access to AI specialization for organizations of all sizes. 
This comprehensive platform empowers businesses to tailor FMs to their unique 
domain-specific needs, whether in healthcare, fi nance, legal services, or other fields that 
require high precision. By doing so, the platform enhances the relevance and accuracy of AI applications and accelerates time-to-value while reducing the costs that are associated with traditional fine-tuning methods. watsonx.ai stands as a testament to IBM's commitment to innovation and accessibility by providing a robu st foundation for bu sinesses to unlock the 
transformative potential of AI with confidence and ease.


Chapter 5. Advanced cap abilities of watsonx.ai 675.4  InstructLab
InstructLab represents a groundbreaking shift in the way LLMs are fine-tuned by making the 
process more accessible, flexible, and efficient. At its core, InstructLab leverages a unique combination of community-driven input, synthetic data generation (SDG), and iterative training methodologies to refine LLMs in a way that dramatically lowers the barriers to entry for fine-tuning ta sks (see Figure 5-6). This process ma kes it simpler for developers and 
subject matter experts (SMEs) to improve model outputs. It also accelerates the fine-tuning cycle and reduces the computational overhead that is traditionally associated with customizing models.
Figure 5-6   The InstructLab large language  model development kit functions overview
The InstructLab approach to fine-tuning is heavily grounded in the concept of 
taxonomy-driven knowledge curation. Taxonomies , in this context, are structured frameworks 
of concepts and relationships that organize information into logical categories and subcategories. These taxonomies are built collaboratively by SMEs, and they serve as the foundation for the knowledge that is used to tune the model. For example, if a business wanted to fine-tune an LLM on customer support for a specific industry, an SME in that industry would work with a taxonomy that represents common questions, issues, and 
terminology that are relevant to the field. By formalizing domain knowledge in a taxonomy, InstructLab helps ensure that the model fine-tuning process is precise, efficient, and contextually relevant.
What makes this approach powerful is that the knowledge that is curated in these taxonomies 
is used to generate synthetic data. Unlike traditio nal fine-tuning methods that rely heavily on 
vast amounts of manually labeled training data, InstructLab uses SDG to produce the training examples that are needed to adjust model behavior. This task is accomplished by feeding the curated taxonomies into  the system (composed of Knowl edge and Skills taxo nomies), which 
contains question-answer pairs and other types of data. 


68 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiAn example of Skills taxonomies is provided in  Figure 5-7, where the Skill that is defined here 
is made to make a model learn to better interpret particular tables.
Figure 5-7   Skill Taxonomy example
This data generation process is not random: It follows predefined patterns based on the taxonomy’s structure, which helps ensure that the synthetic examples are highly relevant to 
the domain. This synthetic data serves as a proxy for real-world data, which enables InstructLab to adapt to niche topics or specialized knowledge areas without requiring the collection of large-scale, expensive datasets. The synthetic data can also be customized and controlled by the SME, which means that they can influence the generation process to help ensure that the model is trained  on the most important or crit ical examples. The ability to 
generate synthetic training data directly fr om taxonomies is what makes InstructLab so 
efficient: It drastically reduces the need for large-scale manual data curation and opens. 
Furthermore, it solves the ever-existing problem of not having enough data for fine-tuning a model that is tailored for a particular business need in the generative AI (gen AI) era. 
The InstructLab approach is built around iterative feedback and instruction training, which are 
two techniques that further streamline the fine-tuning process. Instruction training involves providing LLMs with explicit in structions (similar to how humans learn new tasks) about how 
to generate responses based on the synthetic data. This process is highly flexible because the SMEs can continually tweak the instructions and the knowledge base based on the evolving needs of the model and the domain it is being trained on. This process uses a 2-phase approach with a replay that serves the pur pose of ensuring high diversity and quality 
in the synthetically generated in struction-tuning data set while ensuring training stability. It 
also prevents catastrophic forgetting, which is a common situation that happens in the FM fine-tuning process when it is not well controlled. 
Once the initial synthetic data is generated from the curated taxonomy, the InstructLab 
system trains the model by providing it with a series of questions and answers that align with the knowledge base. The process is iterative, which means that the model does not undergo a single round of training and then stop. Instead, as new feedback is gathered from the model’s performance, the data is refined, and further training is conducted. This iterative feedback loop is crucial in guiding the model toward better understanding, more accurate outputs, and more aligned responses to specific use cases.


Chapter 5. Advanced cap abilities of watsonx.ai 69The power of iterative feedback lies in its ability to refine th e model's responses over time, 
which enhances its accuracy and applicability to real-world prob lems. SMEs can continuously 
assess the model’s performance in relation to specific tasks or topics, and helps ensure that the model becomes progressively better at understanding the subtleties of the domain and generating more precise, cont extually relevant responses.
Another key feature of InstructLab is its model-neutral and open-source nature. Unlike 
proprietary fine-tuning solutions that are often tightly coupled with specific models or platforms, InstructLab enables users to contribute to the fine-tuning of various LLMs regardless of their underlying architecture. InstructLab is built on the premise that fine-tuning should be an open, community-driven process. It supports a wide range of open-source LLMs from repositories like Hugging Face, which enables users to choose the model that best suits their needs, and even enabling them to experiment with models from different frameworks. The open-source nature of InstructLab also means that users have full transparency into the fine-tuning process and the abilit y to modify it as needed. An yone from hobbyists to industry 
experts can contribute to improving the system, whether by adding new taxonomies, adjusting training data, or developing new  techniques for SDG. This approach makes 
InstructLab a true community-driven initiative that is always evolving based on the needs and contributions of its users.
Figure 5-8 shows a community-driven InstructLab fine-tuning process example.
Figure 5-8   Community-driven Inst ructLab fine-tuning process example
Using InstructLab involves several key steps, each of which is designed to make the process 
as efficient and accessible as possible:
1. Users download a base model from a supported repository, such as Hugging Face, and 
initialize the InstructLab co mmand-line interface (CLI). 
2. Once the environment is set up, the user creates a knowledge base, which is stored in a 
structured directory that follows the taxonomy format. This knowledge base is populated with question-answer pairs, references to external documents (for example, PDFs or markdown files), and metadata to describe the knowledge, such as the domain and relevant attributes. 
3. Now, you generate synthetic data based on the knowledge base. This synthetic data is 
used to train the model, with the InstructLab system automatically generating examples that adhere to the structure of the taxonomy. The SDG process is designed to produce high-quality training samples by following the patterns and relationships that are defined in the taxonomy, which reduces the time and cost of manual data creation and helps ensure that the model receives high-quality, domain-specific examples. 


70 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai4. Once the synthetic data is generated, the model is trained by using the InstructLab training 
framework. This training can be done on a local machine or in a cloud environment, with the flexibility to use GPUs to accelerate the process. 
5. After training is complete, the model undergoes a testing phase to ensure that it performs 
as expected. InstructLab enables users to run tests that evaluate how well the model answers questions and generates responses, which help identify areas for further improvement. 
6. The final stage is deploying the trained model, which can be done through the InstructLab 
serving tools. Once the model is deployed, users can interact with it through the CLI or through an application interface, which e nables them to assess  how well it handles 
real-world queries and performs in live environments.
Figure 5-9 shows a high-level view of the InstructLab pipeline.
Figure 5-9   High-level overview of the InstructLab pipeline
5.4.1  Advantages of InstructLab
The InstructLab methodology provides numerous advantages over traditional fine-tuning 
techniques:
/SM590000Synthetic data generation (SDG):  Facing and solving an older pr oblem of data availability, 
InstructLab offers within its automatic capabilities a process to enhance, in cardinality and 
variability, a training dataset fo r LLM fine-tuning by using SDG, which is integr ated into its 
core. 
/SM590000Cost and complexity reduction: One of the most significant benefits of InstructLab is its 
ability to drastically reduce the cost and comp lexity of fine-tuning. By relying on SDG 
rather than manually labeled datasets, InstructLab eliminates one of the most time-consuming and expensive aspects of model customization. 
/SM590000Agile development: The iterative feedback loops and the ability to work collaboratively with 
SMEs enable the fine-tuning process to be far more agile, with models refined and improved continuously based on real-time insights. 


Chapter 5. Advanced cap abilities of watsonx.ai 71/SM590000Flexible model selection and customization: InstructLab is model-neutral, which means 
that users can select and fine-tune various LLMs based on their specific needs. Whether it is a general-purpose model like Granite or a more specialized compatible model that is found on Hugging Face for a particular domain, InstructLab enables users to adapt and improve the model that best fits their use case. 
/SM590000Broader audience participation: The open-sourc e nature and simplicity of the workflow in 
InstructLab make it possible for people without deep machine learning (ML) expertise to participate in model development and fine-tuning. This approach is a significant step toward democratizing AI and ensuring that more organizations, regardless of size or expertise, can harness the power of  LLMs for their specific needs.
The InstructLab innovative approach to mo del fine-tuning (leveraging taxonomy-driven 
knowledge curation, SDG, and iterative training) marks a transformative shift in how LLMs can be customized and applied across a wide ra nge of domains. By reducing the barriers to 
entry, lowering computational costs, and enabling highly specialized, domain-specific training, InstructLab is positioning itself as a key enabler of accessible, efficient, and scalable AI development. This open-source, community-driv en initiative is paving the way for a new 
generation of AI practitioners to fine-tune and enhance LLMs without requiring extensive 
technical expertise, which expands the scope and impact of AI in real-world applications.
5.4.2  How to use InstructLab
InstructLab is a model-neutral, open-source AI proj ect that facilitates contributions to LLMs. It 
is a new community-based approach to build truly open-source LLMs. InstructLab uses a synthetic-data-based alignment tuning method to train LLMs. The InstructLab tuning method is driven by manually created taxonomies. In structLab provides a process for optimizing and 
tuning LLMs by collecting knowledge and skills as par t of a taxonomy tree.
To start the InstructLab process, ilab must be installed. You can download it from its official 
repository .
ilab is a CLI tool that you can use to perform the following actions:
/SM590000Download a pre-trained LLM.
/SM590000Chat with the LLM.
/SM590000Add new knowledge and skills to the pre-train ed LLM by adding in formation to the 
companion taxonomy repository.
After you add kno wledge and skills to the taxonomy, yo u can perform the following actions:
/SM590000Use ilab to generate new synthetic training data based on the changes in your local 
taxonomy repository.
/SM590000Retrain the LLM with the new training data.
/SM590000Chat with the retrained LLM to see the results.

72 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiFigure 5-10 shows the ilab flow of commands, which show how to start processing data for 
synthetic data generation and the fine-tuning process.
Figure 5-10   The ilab flow of commands
Before you begin working with ilab, ensure that your system meets the following 
requirements:
/SM590000Operating system: Linux (tested on Fedora), or macOS with  Apple M1/M2/M3 chipsets.
/SM590000Disk space: Minimum of 250 GB. 500 GB is recommended for complete workflows.
/SM590000Python Version 3.10 or 3.11. At the time of writing, Python 3.12+ is unsupported due to 
dependency constraints.
/SM590000C++ compiler: Ensure that a modern GCC version or equivale nt is installed.
If you use Python environment management tools, ensure that they build libraries that are 
implemented in C by including flags during Python compilation. For example, when using 
pyenv , you use the following string:
PYTHON_CONFIGURE_OPTS="--enable-framework" pyenv install 3.11.5


Chapter 5. Advanced cap abilities of watsonx.ai 73To install the required tool s, run the following command:
sudo dnf install gcc gcc-c++ make git python3.11 python3.11-devel
ilab can be installed with various configurations, depending on your hardware and preferred 
accelerators. Different hardware setups often require specific steps to optimize performance and ensure compatibility with the chosen accelerators, such as  Apple Metal, AMD ROCm, or 
NVIDIA CUDA.
If a GPU is available, you can leverage more processing power by using the following 
commands for initialization:
python3.11 -m venv --upgrade-deps venv
source venv/bin/activatepip cache remove llama_cpp_pythonCMAKE_ARGS="-DGGML_CUDA=on -DGGML_NATIVE=off" pip install 'instructlab[cuda]'pip install vllm@git+https://github.com/opendatahub-io/vllm@v0.6.2
After you install ilab, proceed with the first initialization by running the following command:
Ilab config init
During initialization, ilab prompts you to perform specif ic tasks that influence how the 
environment is configured. By following these prompts, you can tailor ilab to meet your 
needs by setting up essential components lik e the taxonomy repository, model paths, and 
training profiles. For example, selecting a training profile helps ens ure compatibility with your 
hardware, whether it uses CPUs or GPUs, to provide the best performance and resource optimization for your setup:
1. Clone the taxonomy repository, either interactively or by specifying a path with the 
--taxonomy-path  flag.
2. Specify the path to your model. By default, it uses a quantized Granite model .
3. Select a training profile. For systems without dedicated GPUs, choose No Profile (CPU, 
Apple Metal, AMD ROCm) .
After initialization, the directories that are shown in Table 5-1 are created.
Table 5-1   ilab directory overview and details
Directory Description
~/.cache/instructlab/models/ Contains downloaded models.
~/.local/share/instructlab/datasets/ Stores the dataset outputs that are 
generated during workflows.
~/.local/share/instructlab/taxonomy/ Contains skill and knowledge data from the 
taxonomy repository.
~/.local/share/instructlab/checkpoint
s/Contains model checkpoints from the 
training process.
~/.config/instructlab/config.yaml The configuration file  that is generated 
during initialization.

74 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiAfter you install the InstructLab CLI on your syst em, start by downloading the base model that 
you want to train. The foundation of using InstructLab effectively is access to its models. The ilab CLI simplifies this process by offering robu st integration with repositories like Hugging 
Face or OCI. It provides authentication mechanisms, such as token-based access for Hugging Face, and features like repository specification and download acceleration. These capabilities help ensure se cure and efficient downlo ads of pre-tr ained models.
To quickly get started, download compact pre-trained versions of the following models:
/SM590000granite-7b-lab-GGUF
/SM590000merlinite-7b-lab-GGUF
/SM590000Mistral-7B-Instruct-v0.2-GGUF
To initiate the download of a m odel, can run the following command:
ilab model download –repository <MODEL-ID>
When this command runs, the ilab CLI interacts with the designated repositories to fetch the 
selected models. By default, the models are stored locally in the ~/.cache/instructlab/models/  directory, which helps ensure efficient reuse because the 
downloaded models do not need to be fetched again for future operations unless explicitly removed or updated.
You can download a non-default LLM from Hugging Face. If a Hugging Face token is required, 
can add it by running ilab model download –repository <MODEL-ID> , but add the token after 
the argument -hf-token .
You can use OCI-compliant repositories. To do so, log in to the registry and use the following 
command:
ilab model download -rp docker://<MODEL_ID> -rl latest
Once the models are downloaded, they can be served locally for inference. ilab supports 
serving both default and custom models if the system prerequisi tes are met. To serve models 
locally, ensure that the system has sufficient hardware resources, which include at least 8 GB of RAM and, for GPU-accelerated serving, an  NVIDIA GPU with CUDA support. If multiple 
ilab clients attempt to connect to the same server, the first client connects successfully, and 
the others create temporary servers, which require more resources. To prevent conflicts, manage the connections.
To serve the model, run the following command, which provides a URL for API interaction:
Ilab model serve –model-path <MODEL_PATH>
It is possible to interact with a served model directly within ilab by using the following 
command, with optional personalization of inference parameters, such as temperature:
Ilab model chat –model <MODEL_PATH> [-temperature <VALUE>]
Now, you can start personalizing the mo del by adding new skills and knowledge.
To train an open source  model with InstructLa b, create knowledge and skills in the taxonomy 
directory. When you initialized the ilab CLI, it automatically cloned the InstructLab taxonomy 
repository , which is the source of truth for your model training.
In the context of skill contributions, the requi red content is smaller in volume compared to 
knowledge contributions. A complete skill addition to the taxonomy tree can be represented by a few lines in a qna.yaml  file (short for “questions and answers”) and an attribution.txt  
file to cite sources. 

Chapter 5. Advanced cap abilities of watsonx.ai 75To make a valid skills contribution,  the pull request must include a qna.yaml  file with key-value 
entries that contain at least five question-and-answer pairs and an attribution.txt  file that 
lists the sources that are used. The taxonomy structure serves multiple purposes: selecting the relevant subset for data generation, ensu ring interpretability for contributors and 
maintainers, and forming part of the prompt for the LLM when generating synthetic samples.
Each qna.yaml  file must adhere to a standa rd structure with specific keys:
/SM590000version : Must be set to 2 (required).
/SM590000task_description : A description of the skill (required).
/SM590000created_by : The GitHub username of the contributor (required).
/SM590000seed_examples : A collection of key-value entries with at least five examples (required for 
new files, although older files may contain fewer examples).
/SM590000context : Provides relevant information for gr ounded skills, which guide the model’s 
processing (not used  for ungrounded skills).
/SM590000question : The model's input query (required).
/SM590000answer : The expected response (required).
The taxonomy tree also categorizes skills as  either grounded (req uiring context) or 
ungrounded (not requ iring context). For example,  a grounded skill might be 
grounded/linguistics/grammar, wh ile an ungrounded skill might be 
linguistics/writing/poetry/haiku. The qna.yaml  file is always in the fi nal node of the taxonomy 
path. Importantly, there is a limit on the content length in question-answer pairs to ensure model compatibility; co ntributions should not exceed appr oximately 2,300 words for these 
pairs. By adhering to these gui delines, contributors can maintain  consistency and utility within 
the skill taxonomy framework
.
To make the qna.yaml  files faster for humans to read, it is best practice to specify version  
first, which is followed by task_description , then created_by , and finally seed_examples . In 
seed_examples , it is a best practice to specify context  first (if applicable), followed by question  
and answer . 
Example 5-1 shows an example of a qna.yaml file.
Example 5-1   A qna.yaml file
version: 2
task_description: <string>created_by: <string>seed_examples:  - question: <string>    answer: |      <multi-line string>  - context: |      <multi-line string>    question: <string>    answer: |      <multi-line string>  ...
Create an attribution.txt  file that includes the sources of your information, which can be 
self-authored sources.

76 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiKnowledge contributions differ from skills by focusing on answering fa ctual, data-driven, or 
reference-based questions, which are often su pported by documents like textbooks, technical 
manuals, encyclopedias, journals, or maga zines. Although knowle dge and skills share 
similarities in their taxonomy structures, kno wledge nodes include additional elements to 
accommodate their document-based nature.
For contributors that use InstructLab 0.21.0 or later, knowledge contributions can include PDF 
files as valid document types, but earlier versions accept only markdown formats. Each knowledge node in the taxonomy tree contains a qna.yaml  file that is simila r in structure to the 
one that is used for skills, but with additional fields to support knowledge-specific attributes. Notably, all knowledge submissions must be in a Git repository, such as one hosted on GitHub, and the qna.yaml  file must reference this repository:
/SM590000Submit the most current version of the document.
/SM590000Contributions must be text-based. Images are ignored.
/SM590000Avoid using tables in your markdown freeform contributions.
The qna.yaml  file for knowledge contributions must follow a specific format and include the 
following fields:
/SM590000version : The version of the qna.yaml  file format, which is set to 3.
/SM590000created_by : The GitHub username of the contributor.
/SM590000domain : The category of the knowledge.
/SM590000seed_examples : A collection of key-value entries.
/SM590000context : A chunk of information from the knowledge document. Each qna.yaml  file must 
include at least five context blocks, with a maximum of 500 words per block.
/SM590000questions_and_answers : Holds the questions and answers based on the context. Each 
context block requires a minimum of three question-and-answer pairs, each with a maximum word count of 250 words.
–question : A question for the model.
–answer : The corresponding answer.
/SM590000document_outline : An overview of the document being submitted.
/SM590000document : The source document for the knowledge contribution.
/SM590000repo: The URL of the repository that contains the knowledge files.
/SM590000commit : The SHA of the commit in the repository for the knowledge files.
/SM590000patterns : A list of glob patterns specifying the files in the repository. Patterns starting with 
*, such as *.md, must be quoted (" *.md") to comply with YAML rules.
By adhering to these guidelines, knowledge c ontributions maintain a structured, accessible 
format that aligns with the taxonomy framework and supports efficient integration into the system.

Chapter 5. Advanced cap abilities of watsonx.ai 77When working with YAML files (f or both skills and knowledge), it is crucial to adhere to 
specific formatting rules to help ensure correctness and avoid parser errors. Indentation and spacing play a role, and YAML requires 
two spaces for each level of indentation ( tabs must not 
be used  under any circumstances). Also, avoid traili ng spaces at the end of lines because they 
can lead to issues during processing. For entries in seed_examples , each example begins with 
a - placed before the first field, such as question  or context . Subsequent keys within the 
same example should not include the -. Pay attention to sp ecial characters like " and ', which 
must be escaped by using a backslash ( \). To simplify handling these characters, YAML 
enables the use of the | character at the start of a value, which disables special character 
interpretation and supports multi-line strings. For example, lines that start with | are followed 
by an indented block that contains the string 's content. To avoid unexpected YAML parser 
behavior, it is a best practice to quote all values  by using double quotation marks ( "). This 
approach prevents values such as Yes or No from being interpreted as Boolean types ( True or 
False ). For more information about managing multi-line strings and YAML nuances, see the 
yaml-multiline.info  file.
Now, after creating a YAML file for skills and knowledge, as shown in Figure 5-7 on page 68, 
you can validate your new data. Use the ilab taxonomy diff  command to help ensure that 
ilab is registering your new knowledge or skills  and that your cont ributions are properly 
formatted. This command displays any new or modified YAML files within your taxonomy tree. You can also validate your entire taxonomy by performing a diff against an empty base by using the --taxonomy-base=empty  argument.
After validation, it is possible to start the Synthetic Data Generation (SDG) pipeline. To 
generate a synthetic da taset based on newly a dded knowledge or skill sets in the taxonomy 
repository, run the ilab data generate  command. Before proceeding, ensure the existing 
model to which you are adding skills or knowle dge is still running. Al ternatively, you can 
initiate the serv er by using the ilab data generate  command by specifyi ng a fully qualified 
model path with the --mode l flag. At the time of writing, the full CLI pipelin e supports only 
Mixtral and Mistral Instruct Family models as the teacher model. For the simple pipeline, Merlinite 7b Lab is the only supported teacher model due to the specific model prompt templates that it uses. There is  a plan to expand compatibility in  the future, and on watsonx.ai 
(as described in Chapter 6 , “Artificial intelligence  agents” on page 87).
To start generation, run the following command:
ilab data generate [--pipeline full --gpus <NUM_OF_GPUS> --model <MODEL_PATH>
Optionally, you can start SDG by using GPUs wh en they are available. You can specify the 
teacher model that is used (the default one for the ilab CLI is Merlinte-7B).
After generation finished, the synthetic dataset consists of two files in the 
~/.local/share/instructlab/datasets  directory: 
/SM590000skills_train_msgs_*.jsonl
/SM590000knowledge_train_msgs_*.jsonl
You can run the generate step against a different model through a compatible API, such as 
the one that is created by the ilab model serve or any remote or locally hosted LLM (through 
ollama, LM Studio, or others). Run the following command:
ilab data generate --endpoint-url http://localhost:8000/v1
Now that the curated dataset for a fine-tuning is ready, the fine-tuning process can be started.

78 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiThe InstructLab model train has three pipelines: simple , full, and accelerated . The default 
is full.
/SM590000simple  uses an SFTTrainer on Linux and MLX on MacOS. This type of training takes 
roughly an hour and produces the lowest fidelity model but should indicate whether your data is being picked up by the training process.
/SM590000full uses a custom training loop and data proc essing functions for the Granite family of 
models. This loop is optimized for CPU and MPS function. Use --pipeline=full  with 
--device=cpu  (Linux) or - -device=mps  (MacOS). You can also use --device=cpu  on a 
MacOS machine. However, MPS is optimized for better performance on these systems.
/SM590000accelerated  uses the instructlab-training  library, which suppor ts GPU-accelerated 
and distributed training. The full loop and data processing functions are either pulled directly from or based on of the work in this library.
To limit training time, you can adjust the num_epoch  parameter in the config.yaml  file. The 
maximum number of epochs for running the InstructLab end-to-end workkflow is 10.
The following command shows how to start the automatic fine-tuning process with the 
previously generated dataset. Furthermore, it can specify more than the pipeline, such as the 
device that you want the model to be trained on (CPU, MPS, or GPU).
Ilab model train [--pipeline <PIPE_ID> --device <DEVICE_ID> --data-path 
<DATA_PATH>]
This training step can potentially take from several minutes to several hours to complete, 
which depends on the available computing resources.
After the fine-tuning pipeline completes, it is possible to verify the quality of the new model 
and whether the genera ted dataset with the defined skills and k nowledge produced good 
results. To thoroughly test and evaluate a newly trained model by using InstructLab, first run a series of commands that are designed to assess the performance and accuracy of the model after training. The testing process involves using the ilab model test  command to obtain 
output from the model before and after the traini ng process. With this output, you can see how 
well the model performs based on its previous state and after it has undergone the enhancements from the training process. The results from this test show the effectiveness of your training and provide insight into areas where further improvement might be needed.
When the model is tested, you can use the ilab model evaluate  command to run the model 
through a set of predefined benchmarks to evaluate its performance across various categories. At the time of writing, there are four primary benchmarks that are supported by InstructLab: 
/SM590000Multitask Language Understanding (MMLU)
/SM590000MMLUBranch
/SM590000MTBench
/SM590000MTBenchBranch. 
These benchmarks assess different aspects of a model's capabilities, su ch as its knowledge 
and skills: 
/SM590000MMLU evaluates the model’s general knowledge on a wide range of topics. 
/SM590000MMLUBranch compares the model’s performance to the performance of a base model to 
identify improvemen ts in knowledge. 
/SM590000MTBench evaluates how well the model applies its knowledge in multi-turn conversations.
/SM590000MTBenchBranch assesses improvements or r egressions in specific skill areas when 
compared to a base model. 

Chapter 5. Advanced cap abilities of watsonx.ai 79For each benchmark, the evaluation generates detailed reports, which show scores and 
identify areas where the model performs well and areas that need further work. For example, the MMLU report provides a score for various s ubjects, such as abstract algebra, anatomy, 
and business ethics, which indicate how the model performs in each area. A typical output for MMLU looks like a series of subject categories with a score 0.0 - 1.0, with higher scores indicating better performance in the respective topics. 
Running MMLUBranch involves evaluating your model's contributions compared to a base 
model. The evaluation outputs a score for both the base model and the newly trained model, along with a report on the improvements or regressions that are observed. For example, you might see that the model improved in one area, like “tonsils,” from a score of 0.74 to 0.78, which indicates that your  training enhanced the model’s ab ility in that particular knowledge 
domain. 
MTBench and MTBenchBranch follow a similar structure, but they focus on testing the 
model’s skills rather th an just knowledge. MTBench evaluate s the model's ability to perform in 
multi-turn dialogs, which provide a score for each turn in the conversation, such as turn one and turn two. MTBenchBranch compares your model’s skill performance to  a base model, 
which provides a detailed breakdown of areas wh ere your model improved or regressed, and 
highlighting any skills that sh owed no significan t change. This deta iled feedback helps 
pinpoint specific areas where more fine-tuni ng might be necessary to enhance the model's 
abilities. For each benchmark, it is important to help ensure that  the model that is evaluated is 
in a supported format, either safetensors or GGUF. Using models directly from Hugging Face without downloading them is not supported.
Also, while using models for MMLU and MMLUBranch evaluations, GGUF models are not 
supported at the time of writing. When running MTBench and MTBenchBranch, it is a best practice to use the 
Prometheus-8x7b-v2.0  model as the judge model, but you can use a 
different judge model. You can download the Prometheus model by running the ilab model 
download  command for local use in these evaluations.
The entire process of running these evaluations can take from several minutes to several 
hours, which depend on the size of the model and the dataset that is used. Be prepared to allocate enough time for the evaluations to complete, especially wh en working with large 
datasets or multiple training epochs. The results from these evaluations provide a 
comprehensive view of the mode l's strengths and weak nesses, which offer valuable insights 
to guide further refinement and optimization of the model.
After the process ends and the results are good enough for the specified use case, a new 
fine-tuned model with new synthetic data in GGUF format is available in the ilab specified 
folder location in a GGUF format. Apart from the usage on ilab, it is possible to deploy it on 
hyperscalers such as watsonx.ai run time by using the Bring Your Own Model (BYOM) function, which fully enables a true, at-scale, enterprise-level fine-tuning process of FMs.
5.4.3  InstructLab on watsonx.ai Software-as-a-Service
At the time of writing, Inst ructLab has demonstr ated its usability pr imarily through the ilab 
CLI. This method enables users to interact with InstructLab features and functions in a straightforward and efficient manner. However, the development team is working on integrating InstructLa b with watsonx.ai. This upcoming integration will enh ance the user 
experience by providing a dedicated user interf ace (UI). This UI will streamline the entire 
process, which will make it mo re accessible and intuitive for users who might not be 
comfortable with CLI operations. This development is expected to open new possibilities for a 
broader range of user by facilitating simpler access to powerful InstructLab tools and features.

80 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiFigure 5-11 shows the interface for tuning a generative LLM by using InstructLab. This 
interface is designed to provide users with an intuitive platform for refining language models 
to meet specific needs and requ irements. It also illustrates the starting point of the overall 
process: the addition of  skills and knowledge. 
Figure 5-11   Tuning Studio interface
Users begin by selecting the FM , and then proceed to integrat e various skills and knowledge 
areas into the model. The interface guides users through detailed steps, which include specifying configurations. Throughout this process, users can monitor their progress and make necessary adjustments to ensure that the model's performance aligns with outcomes.
Moreover, InstructLab offers advanced features such as feedback, performance metrics, and 
troubleshooting tools to enhance the tuning ex perience. This comprehensive approach helps 
create an accurate and efficient LLM, with continuous improvement and adaptation to evolving linguistic patterns and user needs.
Figure 5-12 on page 81 illustrates a detailed tax onomy tree of skill and knowledge files that 
are managed within Tuning Studio on watsonx.ai for InstructLab. The UI shows a comprehensive and organized view of the hier archical structure of  skills and knowledge 
areas. Each node in the taxo nomy tree represents a distinct  category, which facilitates 
navigation and access to specific training data.


Chapter 5. Advanced cap abilities of watsonx.ai 81Figure 5-12   Taxonomy tree of skill and knowle dge files that are managed in Tuning Studio on 
watsonx.ai for InstructLab
One of the key functions of this UI is its abilit y to visualize and mainta in version history for 
each training run, which includes the train ed models, the associated skills and knowledge 
taxonomy, grounding data, and the generated synthetic data. By tracking the evolution of these elements over time, users can effectively monitor the progression and improvements that are made with each iteration. This version control mechanism is essential for helping ensure the reproducibility and relia bility of model training processe s. Also, it will be possible to 
add knowledge and ingest multiple data formats, such as PDFs, docx, HTML, MD, and more.
The Tuning Studio capability to  handle such a vast array of data types and their versions 
enables meticulous fine-tuning and enhances the overall model development lifecycle. Through this meticulous documentation and ma nagement, users can draw insights from past 
training runs, identify best practices, and apply learned lessons to future projects.


82 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiFigure 5-13 shows the progress map during the tuning phase of InstructLab on watsonx.ai.
Figure 5-13   Progress map during the tu ning phase of InstructLab on watsonx.ai
Before commencing the SDG and fine-tuning process, conduct a peer review of the submitted 
skills and knowledge files. This  review involves project mem bers and SMEs to help ensure 
comprehensive assessment and validation. If any modifications are identified as necessary during this review phase, new branches can be created within the version control system to incorporate these changes without disrupting the main development line.
After you incorporate the feedback and necessary adjustments, the fine-tuning process 
commences. This step involves leveraging pre-trained models and adjusting them to specific requirements by training on customized data sets. The goal is to enhance the model's 
performance in targeted areas to help ensure that it meets the specif ications and accuracy 
levels.
On successful fine-tuning, the newly optimized model can be exported in the GGUF format, 
which is a versatile and widely supported format  for deploying quantized FMs. Alternatively, 
the model can be directly deployed onto the watsonx.ai run time environment. This deployment establishes a seamless large language model operations (LLMOps) pipeline within watsonx.ai for InstructLab, which enables automated monitoring, maintenance, and iterative improvement of the model.
By integrating these processes within the watsonx.ai ecosystem, you help ensure continuous 
delivery and operational efficiency of AI solutions that align with best practices in modern ML workflows.
The fine-tuning process that is shown in Fi gure 5-14 on page 83 shows the intricate and 
methodical approach that is adopted by InstructLab within the watsonx.ai framework. This process is characterized by a 2-phase, fine-tuning methodology that is augmented with a replay buffer, which helps ensure enhanced performance and accuracy of the models.


Chapter 5. Advanced cap abilities of watsonx.ai 83Figure 5-14   InstructLab fine-tuning process with  real-time performance on the 2-phase fine-tuning 
process with replay buffer
In the initial phase, the pre-trained models undergo a rigorous training regimen by using 
targeted datasets that are pertinent to the specif ic knowledge areas that are identified within 
the taxonomy tree. This phase focuses on adapting the general-purpose models to the specialized requirements of the InstructLab projects, which hone their performance to meet the specifications. The second pha se uses the skills and continues  catastrophic forgetting by 
using a dedicated replay buffer while fi ne-tuning the skills in  the second phase. 
The replay buffer mechanism plays a crucial role in this fine-tuning process. By systematically 
storing and replaying past experiences (training data and model states) during the training sessions, the buffer helps ensure that the models continuously learn and adapt from the previous run. This approach mitigates catastroph ic forgetting and reinforces learning from 
high-value data points, which improves th e models' robustness and generalization 
capabilities. Figure 5-14 encapsu lates a holistic and dynamic ap proach to model fine-tuning 
within the watsonx.ai ecosystem, which highlight s the interplay of ad vanced methodologies 
and real-time performance monitoring to achieve superior gen AI solutions.
InstructLab will be available on watonx.ai as Software-as-a-Serv ice (SaaS), and it will be an 
important addition for gen AI on  an enterprise level. It will ena ble a pool of resources to 
fine-tune and improve generative LLMs.


84 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai5.4.4  InstructLab use case examples
Introducing InstructLab on watsonx.ai heralds a new era of advancements in gen AI, 
particularly in the realm of enterprise-level ap plications. This innovative platform integrates 
seamlessly within the watsonx.ai ecosystem , which enables a dynamic and robust 
environment for the fine-tuning and deployment of LLMs. As an SaaS offering, InstructLab provides an unparalleled suite of tools and methodologies to optimize model performance through a meticulously structured 2-phase, fine-tuning process.
The InstructLab capabilities ar e impactful across many use cases, and they empower 
organizations to tailor AI solutions to their unique requirements. By leveraging the advanced features of Instru ctLab, users can achieve superior accura cy, efficiency, and scalability in their 
AI-driven initiatives. 
Here are some of the first prominent examples of use cases where InstructLab has 
demonstrated significant improvements in its early life:
/SM590000Emergency medical services use case: A large hospital is looking to automate the 
processing of emergency medical records to  improve the efficiency and accuracy of 
critical tasks. The system should be capable of completing the following tasks:
– Case classification: Assigning priority le vels such as green (low urgency), orange 
(medium urgency), or red (high urgency) flags to cases based on their severity.
– Peer medical review recommendations: Identifying cases that might require further 
review by a medical peer to help ensu re proper oversight and decision-making.
– Clinical compliance and guideline deviation: Detecting discrepancies  in medical reports 
compared to established clinical guidelines, which enhance compliance and quality assurance.
– Knowledge: The system will rely  on hospital compliance da ta, which includes clinical 
guidelines and historical patient records to perform its tasks.
– Skill requirements:
 Case classification to prio ritize emergency scenarios.
 Identifying deviations from clinical guide lines to help ensure regulatory compliance.
/SM590000Call transcript processing use case: A large North American telecommunications 
company requires an automated system to process and summarize incoming customer support call transcripts. The system must extract and organize information based on a predefined set of 80 questions. Examples include “Did the customer want to upgrade their plan?” or “Did the customer report bandwidth issues?”
– Knowledge: The primar y data source will be call transcripts, which contain varied 
human expressions and conversational styles.
– Skill requirements: Interpreting and anal yzing natural languag e, which includes 
understanding diverse writing styles and linguistic nuances to extract relevant insights from conversations.

Chapter 5. Advanced cap abilities of watsonx.ai 85/SM590000Personalized retail recommendations use case: A retailer aims to deploy a personalized 
recommendation engine that suggests in-stock products that are tailored to a user’s dietary preferences, which include offering recommendations based on food allergies, nutritional goals, or ingredient restrictions.
– Knowledge: The engine must use detailed product nutritional information and inventory 
data to ensure accurate and timely recommendations.
– Skill requirements:
 Classification of ingredients and their alignment with dietary preferences. Recommending products that match user profiles while considering inventory 
availability.
– Agent capabilities:
 Understanding and analyzing inventory data in real time. Interpreting customer preferences and purchase history to generate meaningful 
suggestions.
/SM590000Intelligent auto claims processi ng use case: An insurance prov ider requires a solution to 
analyze images of auto accidents and sugges t insurance coverage recommendations that 
are based on the claimant's active policy. The system should improve the speed and accuracy of claims processing.
– Knowledge: The system needs access to policy details, which include terms, coverage 
limits, and exclusions.
– Skill requirements:
 Classification of accident severity by analyzing damage in submitted images.
 Matching severity with appropriate coverage based on the policy.
– Agent capabilities:
 Accessing and analyzing driver history to help ensure an accurate policy 
application.
 Interpreting active insurance policies to provide recommendations that are aligned 
with coverage terms.

86 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai

© Copyright IBM Corp. 2025. 87Chapter 6. Artificial intelligence agents
Artificial intelligence (AI) agents will soon become  pivotal in modern co mputing by serving as 
autonomous entities that can perceive their environment, reason about their goals, and 
perform actions to achieve wanted outcomes. These agents will be central to many AI 
systems, from personal assistants and autono mous vehicles to advanced simulations and 
decision-making tools. The vers atility and capability of AI agen ts arise from their ability to 
operate independently while adapting to dynamic and often unpredictable environments. They combine advanced algorithms with AI and generative AI (gen AI) models, which enable 
them to make intelligent decisions, ad apt to changes , and optimize outcomes. 
This chapter delves into the intricacies of AI agents by exploring their fundamental 
characteristics, the motivation behind their development, and their applications across various domains. Through a detailed ex ploration, readers will understa nd why AI agents represent a 
paradigm shift in how computational systems interact with and influence their surroundings.
The following topics are described in this chapter:
/SM5900006.1, “What makes an AI agent” on page 88
/SM5900006.2, “Why AI agents are needed” on page 94
/SM5900006.3, “Multiple AI agents” on page 95
/SM5900006.4, “AI agents on watsonx.ai” on page 100
/SM5900006.5, “AI agents use case examples” on page 1076

88 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai6.1  What makes an AI agent
An AI agent  can be formally defined as a computational  entity that is equipped with sensors to 
perceive its environment and use its actuators to interact with it. The core of an agent lies in its ability to reason and dec ide, which is driven by algorithms that enable it to  analyze inputs, 
predict outcomes, and choose actions that are aligned with specif ic objectives. Unlike 
traditional software systems, which operate on predefined instructions, AI agents exhibit autonomy and flexibility with dynam ic execution flows, which en able them to handle complex 
scenarios with minimal human intervention. This autonomy stems from their design 
principles, which often draw from cognitive sciences, game theory, and control systems, 
enabling agents to emulate human-like decision -making processes. Furthermore, each AI 
agent can be designed for differ ent levels of complexity and interaction. This adaptability 
makes AI agents indispensable in fields such as robotics and natural language processing (NLP), where complex interactions with dynamic environments are required.
Figure 6-1 shows a schematic view of an AI agent.
Figure 6-1   Schematic view of an AI agent
Agents, in the context of AI, can be conceptualized as sophisticated orchestrators of 
intelligence that can address intricate and multi-faceted problems. They achieve these goals by leveraging the reas oning capabilities of large languag e models (LLMs), formulating 
detailed plans to resolve challenges, and running these plans by using a diverse set of tools. 
An agent operates as a cohesive system, and its architecture can typically be broken down into four key modules:
/SM590000The agent core
/SM590000Search and memory modules
/SM590000Planning modules
/SM590000Tools
Each of these components plays a critical role in ensuring th e agent's function s, adaptability, 
and effectiveness.
The 
agent core  stands as the central coordination hub of the agent. It is often described as 
the “decision-making nucleus” due to its pivotal role in governing the agent's logic, behavior, and overall strategy. This module is responsible for synthesizing inputs, determining appropriate actions, and managing the interactions between other components. 


Chapter 6. Artificial intelligence agents 89To design a robust and efficient agent core, you must define several foundation aspects that 
serve as the blueprint for the agent’s behavior: 
/SM590000The general goals of the agent must be es tablished. These goals act as the guiding 
principles that dictate the agent’s actions  and responses, which help ensure that its 
operations align with the overarching objectives that it is designed to achieve. Without clear goals, the agent risks becoming unfocused or inefficient. 
/SM590000Another critical aspect of the agent core is the explicit definition of the tools that are 
available for execution, which involves cr eating a comprehensive “user manual” that 
counts and describes all tools at  the agent's disposal. Each t ool’s capabilities, limitations, 
and specific use cases should be outlined to enable the agent to allocate resources effectively and run tasks with precision. 
/SM590000Furthermore, the a gent core must provide detailed guidance about t he utilization of 
planning modules. These modules are instrumental in enabling the agent to adapt dynamically to varying scenarios by selecting the most suitable planning strategy based on the context. This adaptability is key to ensuring th e agent’s effectiv eness in complex 
and unpredictable environments.
Memory integration  is another cornerstone  of the agent core. The memory system is 
designed to maintain and use relevant information from prior interactions or external research, which enhances the agent’s ability to g enerate accurate and context-aware 
responses. This integration requires dynamic management of memory items to help ensure that only the most pertinent data is referenced during inference. Also, an optional persona definition can be incorporated into the agent core to influence the agent’s tone, preferences, and behavioral nuances. By defining a persona, the agent can be tailored to exhibit specific 
characteristics that align with its intended use case or audience, adding a layer of uniqueness to its interactions. 
Memory modules  are integral to the agent’s ability to maintain contextual 
awareness and continuity. These modules are responsible for storing and managing information that supports the agent’s operations. Memory can be categorized into two main types:
/SM590000Short-term memory
Short-term memory focuses on capturing the agent’s immediate actions, thoughts, and 
observations during ongoing interactions, which include data that is retrieved from vector searches, outputs from API calls, and results from database queries. Short-term memory 
is essential for helping ensure that the agent can respond effectively to the immediate 
context of a user’s query.
/SM590000Long-term memory
In contrast,  long-term memory serves as a repository for information that is accumulated 
over extended periods, which include summariz ed logs of prior interactions, personal 
details and preferences of the user, and other contextual information that might influence the agent’s behavior. Long-term memory enables the agent to maintain a consistent and personalized approach in its interactions, which enhance user satisfaction and engagement. For example, in a conversational agent, long-term memory enables the retention of user preferences and past conversations, which create a more seamless and intuitive experience.
When solving intricate problems, agents that are powered by LLMs are adept at navigating 
complexity by employing a combination of advanced methodologies that resembles planning an execution flow. One such methodology is 
task and question decomposition . This approach 
involves breaking down compound queries into smaller, more manageable sub-questions that can be addressed sequentially. 

90 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiFor example, to answer the question, “Will the te mperature tomorrow be higher or lower than 
the historical average?” the agent must decomp ose it into subquestions such as identifying 
the location, determining the forecasted temperature for the specified location, and retrieving the historical average temperature for comp arison. By addressing each sub-question 
individually, the agent can construct a comprehensive and accurate response. 
Another essential technique is the usage of 
reflection and critique frameworks . These 
methodologies, which include well-established prompting strategies such as ReAct , 
Reflexion , Chain of Thought , and Graph of Thought , are designed to enhance the reasoning 
capabilities of the agent. By  incorporating elements of evidence-based reasoning and 
iterative self-critique, these frameworks enable the agent to refine its execution plans and improve the quality of its responses. Reflection  techniques enable the agent to evaluate the 
plausibility and coherence of its answers, which help  ensures that its outputs meet high 
standards of reliab ility and relevance.
Classification and the impl ementation of guardrails  are extra mechanisms that enhance the 
agent’s decision-making process. By classifyi ng questions and queries, the agent can filter 
search results, identify relevant sub-agents, or even deny a response if necessary. Guardrails, which serve as a specialized form of classification , act as safeguards to help ensure that the 
agent’s outputs adhere to predefined ethical and operational guidelines. These mechanisms are valuable in scenarios where precision, safety, and compliance are paramount.
The 
tools  that are employed by an agent are another critical aspect of its function. These tools 
are executable workflows that enable the agent to perform specific tasks. Analogous to specialized third-pa rty APIs, tools provide the agent with targeted capabilities that extend its 
problem-solving abilities. Exam ples of tools include Retrie val-Augmented Generation (RAG) 
pipelines for generating context-aware answers, code interpreters for handling complex programming tasks,  and APIs for retrieving information from the internet. Also, utility APIs 
such as weather services or instant messaging platforms can be integrated to address domain-specific needs. The versatility and ef fectiveness of an agent are enhanced by its 
ability to leverage these specialized tools.
Going one step forward, agents that rely solely on text inputs face inher ent limitations in their 
ability to interact with and ana lyze diverse data formats. 
Multi-modal agents  address this 
limitation by incorporating the ability to process and reason ov er various input types, which 
include images, audio files, an d structured datasets. This capability expands the range of 
applications for agents, which enable them to ta ckle tasks that require visual analysis, speech 
processing, or combined reasoning across multiple modalities. For example, a multi-modal agent can analyze an image to extract relevant features, process an accompanying audio file for contextual information, and synthesize this  data with text-based inputs to deliver a 
comprehensive solution.
By following the main core modules of an AI agent and through thoughtful design and 
continuous refinement, agents are poised to become indispensable tools in the ever-expanding landscape of AI.
An AI agent, as illustrated in Fi gure 6-2 on page 91, consists of a centralized planning system 
that is supported by a general-purpose generative LLM. This LLM serves as the core orchestrator, which can devise a structured pl an of actions to address user queries. Its 
decision-making process is enhanced by a memory module, which maintains contextual information and is continuously updated based on past actions and outcomes, which help ensure adaptive and dynamic responses.

Chapter 6. Artificial intelligence agents 91Figure 6-2   High-level view at the core of an AI agent
The planning system interacts with an I/O communication layer to run the planned actions 
effectively. This layer acts as a bridge between the LLM and an extensive Tool Library, which enables the agent to pe rform specialized tasks beyond its intrinsic capabilities. The tools 
include functions such as code execution, do cument retrieval (RAG), calculations, weather 
queries, time and location services, and AI and gen AI models, among others. The module of communication is a crucial part of an agent because it sets the formatting of the I/O communication in a standardized way to enable fast and transparent invocation of tools and output comprehension.
The LLM at the core of this system is typically a large foundation model (FM) that is optimized 
for high performance across diverse benchmar ks and tasks. For example, in the context of 
the IBM watsonx.ai platform, this  role can be fulfilled by models  such as Mistral Large, Llama 
3.3 70B, or Llama 3.1 405B, which are know n for their robust capab ilities in generative 
reasoning and planning.
Behind the scenes, an agent has its own calle d system prompt, which is the usual system 
prompt that can be set for any generative LLM. It describes in detail the main task for which a generative LLM should adhere to. The scheme behind the main prompt solution is shown in Figure 6-3.
Figure 6-3   Logical prompt schema definition for agents


92 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiThis system prompt is a structured set of instructions that encapsulates the core task, and 
defines the role of the LLM, the expected output schema, and the instructions that the model should follow.
As shown in Figure 6-3 on page 91, the system prompt integrates multiple critical elements:
/SM590000Memory: Provides contextual data or historical  interactions that the LLM can leverage to 
maintain continuity and relevance in responses. This approach helps ensure that the agent adapts dynamically to ongoing tasks or user needs.
/SM590000Role definition: Specifies the persona or role that the agent assumes (for example, a 
customer support assistant, a data scientist, or a creative writer), and aligns the behavior and tone of the LLM with the intended use case. 
Example 6-1 shows an example of a role prompt definition:
Example 6-1   Defining a role for the agent
#Role
As a helpful assistant, your role is to provide users with actionable insights from their data files.
/SM590000Instructions: Detailed gu idelines about how the LLM should process inputs, interpret user 
queries, and generate outputs. These instructions help ensure that the model stays on task and adheres to the wanted operational framework.
Example 6-2 show an example of the instruction prompt definition.
Example 6-2   Outline instructions for the agent
# Instructions
The user can see only the Final Answer. All answers must be provided there. Functions must be used to retrieve factual or historical information to answer the message. If the user suggests using a function that is not available, answer that the function is not available.
/SM590000Output schema: A predefined structure that dictates how the LLM should format its 
responses, which may include JSON formats, bullet points, or other data representations 
that are required by downstream tools or workflows. This prompt is important for the 
overall communication and interaction in the multiple steps in an agentic execution flow.
Example 6-3 shows an example of the output schema prompt definition.
Example 6-3   Defining the output schema that the agent should follow
#Output schema (also known as the agent’s control logic)
You communicate only in instruction lines. The format is: "Instruction: expected output". Only use these instruction lines and must not enter empty lines or anything else between instruction lines. Skip the instruction lines Function Name, Function Input, Function Caption, and Function Output if no function calling is required. Message: User's message. You never use this instruction line. Thought: A single-line step-by-step plan of how to answer the user's message. You can use the available functions that are defined above. This instruction line must be immediately followed by Function Name if one of the available functions that are defined above needs to be called, or by Final Answer. Do not provide the answer here. Function Name: Name of the function. This instruction line must be immediately followed by Function Input. Function Input: Function parameters. An empty object is a valid parameter. 

Chapter 6. Artificial intelligence agents 93Function Output: Output of the function in JSON format. Thought: Continue your 
thinking process. Final Answer: Answer the user or ask for more information or clarification. It must always be preceded by Thought.
Also, the agent can enhance its functions with tools, which ex tend the LLM's capabilities by 
integrating external APIs or specialized modules. Each tool includes the following items:
/SM590000A tool description: An explanation of its purpose and function.
/SM590000A tool schema: Specifications for how the LLM should interact with the tool, which includes 
input and output formats.
Example 6-4 shows the example parsed output after a certain user request regarding an 
explanation of a certain dataset file occurs.
Example 6-4   Agent output example
Thought: The user wants to know what the file "CSV- bill of materials.csv" is about. I can use the Python tool to read the file and provide a summary. Function Name: Python Function Input: {"language":"python","code":"import pandas as pd\n\ndf = pd.read_csv('CSV-billmaterials.csv')\nprint(df.head()","inputFiles":{"file_670d56abb912d7771371652e":"CSV- bill of materials.csv"}} Function Output: The code ran successfully. Standard output: ‘’’QUANTITY ... PART 0 3700 ... 144EC8-14101-20 1 5500 ... 096EUF-T4101D20 2 45000 ... 004ZTF-41Z01M20 3 4334 ... NaN 4 1564 ... NaN 
[5 rows x 4 columns] ‘’’
By combining these components, the system prompt orchestrates the interaction between the 
LLM and external resources, which help ensure consistency, precision, and task alignment. 
This modular approach enables AI agents to handle complex workflows and adapt to diverse applications.
This standard agent architecture can generate and run plans, and iteratively refine its 
approach by leveraging feedback and memory updates. The modular design of agents, which is supported by a versatile Tool  Library, enables sca lability and adaptabilit y for a wide range of 
use cases, and with the usage of agents on watsonx.ai, the scaling to enterprise-grade solutions is once again possible.

94 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai6.2  Why AI agents are needed
The necessity for AI agents stems from the growing complexity of tasks and environments 
that surpass the capabilit ies of conventional software systems.  Traditional systems often falter 
in scenarios requiring adaptive, context-aware de cision-making, especially when dealing with 
vast datasets, uncertain outcomes, and real-time constraints. In domains like logistics, healthcare, and finance, where decision-making must balance multiple variables simultaneously, the utility of AI agents becomes evid ent. These agents excel in scenarios 
demanding real-time responses, such as aut onomous vehicles navigating dynamic traffic 
conditions or virtual assistants managing multifaceted user requests. 
Beyond efficiency, AI agents also bring about significant cost savings by automating repetitive 
tasks, reducing human error, and improving o perational accuracy. Thei r ability to adapt from 
experience by using a memory module and to adapt to novel situations is crucial in addressing problems where predefined solutions are inadequate. For example, reinforcement learning (RL) techniques enable AI agents to optimize behavior over time, which refines their 
decision-making capabilities with minimal hum an input. By acting as adaptive problem 
solvers, AI agents provide a bridge between theoretical AI research and practical, real-world enterprise-grade applications.
Figure 6-4 shows the gen AI journey for AI agents.
Figure 6-4   Generative AI journey for AI agents
Agents that are powered by LLMs represent the next frontier in driving productivity gains for 
enterprises. As businesses increasingly rely on AI to streamline operations and elevate 
customer experiences, agents offer a revolutionary step forward by automating complex, multi-step tasks that previously required human intervention. Unlike traditional LLMs, which excel at handling FAQs or supporting nuanced informational queries through RAG approaches, agents bring advanced capabilities to orches trate and run high-value workflows. 
This ability to handle complex sc enarios such as planning a marketing campaign, optimizing 
supply chain logistics, or conducting sophisticated data analysis, positions agents as essential tools for transforming enterprise productivity.


Chapter 6. Artificial intelligence agents 95The true potential of agents lies in their ca pacity to integrate seam lessly with existing 
systems, tools, and data sources, which enable th em to act as intelligent  intermediaries that 
connect disparate workflows. For enterprises, this approach means automating entire business processes rather than isolated tasks. For example, an agent can retrieve customer data, analyze purchasing patterns, generate  personalized recommendations, and trigger 
actions like sending tailored offers or updating customer relationship management systems. By eliminating manual handoffs and streamlining processes, agents enable employees to focus on strategic decision-making rather than repetitive or time-consuming tasks.
From a business perspective, agents are the key to unlocking the next wave of productivity 
gains. They enhance operational efficiency, reduce costs, and drive faster time-to-market for critical initiatives. Moreover, they enable businesses to scale their efforts without proportionally increasing resources, which are vital in today’s competitive and resource-constrained environments. Imagine an agent that can plan, run, and monitor an entire ad campaign or a product launch task in hours, which typically require weeks of human effort. This scalability im proves efficiency, and it  creates opport unities for innovation because 
teams can redirect their focus to higher-value, creative, and strategic activities. In this context, agents are a technological enhancement and a strategic imperative for the modern enterprise. They represent a shift from reactive to proactive operations, which enable businesses to anticipate needs, respond faster to  market dynamics, and drive growth in ways 
previously unimaginable. Enterp rises that adopt agent s will lead the charge in this new era of 
productivity, and set the benchmark for operational excellence and innovation in their industries.
6.3  Multiple AI agents
When you extend the concept of individual agents in software systems, you get multi-agent systems (MASs), which consist of multiple AI en tities working collaboratively or competitively 
to solve complex problems within shared digital environments. These systems are transformative when augmented by gen AI capabilities because th ey enable agents to 
engage in more sophisticated tasks, such as content generation, dynamic interaction with users, or collaborative reasoning. In a software-based MAS, each agent operates autonomously while adhering to predefined pr otocols for communication and collaboration. 
This autonomy enables agents to handle tasks like distributed resource management, 
adaptive problem-solving, and even creative endea vors, such as generating ideas, plans, or 
personalized user experiences. Gen AI further amplifies their function by enabling natural language generation, image synthesis, and decision-making support, which enhances the agents' ability to interpre t, reason, and produce outputs in real time. 
A key challenge in software-based MASs, particular ly ones that leverage gen AI, is achieving 
effective inter-agent communication and collaboration. Protocols that are based on message-passing, such as JSON over RESTful APIs or advanced graph-based communication models, help ensure that agents can share knowledge, negotiate responsibilities, and resolve conf licts. Generative AI enhances these interactions by enabling 
context-aware dialog and summa rization capabilities, which make inter-ag ent exchanges 
more natural and efficient. More over, coordination strategies within such systems are pivotal. 
In leader-followe r setups, generative agent s with advanced reasonin g capabilities may take 
on supervisory roles to craft high-level plans or  synthesize insights for the collective. Fully 
distributed MASs enable agents to collaborate dynamically by relying on peer-to-peer negotiations or RL-based decision policies to achieve shared objectives. For example, in a 
collaborative creative task such as automated marketing content generation, different agents in the system might specialize in headline creat ion, visual asset gener ation, and audience 
sentiment analysis. Together, these agents leverage gen AI to deliver cohesive, high-quality outputs that surpass the capab ilities of indivi dual components.

96 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiThe integration of a MAS with gen AI promises to unlock solutions for some of the most 
intricate software challenges, such as distri buted knowledge synthesis, large-scale content 
personalization, and dynamic adaptation to user behaviors. As these systems continue to evolve, they are poised to redefine how software systems interact, collaborate, and solve problems, which pave the way for a new era of  intelligent, cooperativ e, and gen AI-driven 
applications.
The decentralized nature of a MAS is critical in these addressed applications, which help 
scalability, robust ness, and fault tolerance. For example, a generative MAS that is deployed in 
customer support can assign tasks dynamically across agents, with some agents generating empathetic responses, other agents crafting visually appealing solutions, and yet other 
agents analyzing sentiment data in real time. This division of labor enables the system to manage workloads efficiently, respond quickly to changes, and help ensure seamless service continuity even if individual agents face disruptions. 
Being decentralized is one of the key advantag es of a MAS. Unlike centralized systems where 
a single entity controls decision-making and system-wide operations, MASs are designed to 
operate without a single point of failure. If one agent fails or is compromised, the rest of the system can continue to function, which makes it highly resilie nt. This decent ralized approach 
also allows MASs to scale efficiently because more agents can be added to the system without disrupting its overall performance. Furthermore, because the agents are distributed, they can operate in diverse environments, which include real-time scenarios, where traditional centralized systems might strugg le. The power of a MAS lies in  this collective intelligence, 
where the sum of  the parts is greater than th e individual agents' abilities.
As MASs continue to evolve, they are poised to unlock solutions for some of the most intricate 
challenges in fields such as distributed computing, environmental monitoring, and autonomous exploration. For example, in distributed computing, a MAS can optimize resource usage across a network of machines , which enable more efficient computation and 
data processing. In environmental monitoring,  you can use a MAS to deploy a network of 
sensors that autonomously gather and process data, which provides real-time insights into environmental conditions. In autonomous exploration, a MAS can enable a fleet of robotic vehicles or drones to work together to explor e unknown terrains, share information, and adapt 
to changes in the environment. 
A notable development in the realm of MASs is the advent of multi-agent orchestration 
frameworks. These frameworks provide a unified platform for coordinating conversations among multiple agents, serving as a high-level abstraction for using FMs. By integrating LLMs, tools, and human inputs, various framew orks enable the seamless orchestration of 
agent interactions in a way that enhances the capabilities of individual agents. These 
frameworks typically feature highly capable, customizable, and conversational agents, which 
can collaborate through automa ted agent chats, which improv e their collective ability to 
handle complex tasks. This integration enables MASs to function more efficiently by leveraging the power of language models, tools, and human expertise in a coordinated manner. As a result, MAS framew orks offer an exciting potenti al for creating more intelligent 
and adaptive systems that can address a wide range of challenges in both industrial and research contexts.
The ongoing evolution of MASs promises to push the boundaries of what is possible in 
autonomous systems by enabling new capabilit ies and driving innovation across various 
industries. As researchers continue to explore the potential of these systems, new frameworks, coordination strategies, and interaction protocols continue to emerge, further enhancing the power and flexibility of MASs in tackling the most co mplex and large-scale 
problems.

Chapter 6. Artificial intelligence agents 97The architectural diagram in Figure 6-5 outlines a sophisticated MAS framework that is 
designed to integrate diverse functions such as planning, memory management, 
communication, and task orchestr ation. The system employs a mo dular design that facilitates 
scalability, adaptability, and in teroperability, which caters to complex problem-solving 
scenarios across various domains. Let us delve into each component and their interplay within the architecture.
Figure 6-5   High-level view of a multi-agent syst em architecture
A MAS that is enhanced by gen AI represents a sophisticated and transformative paradigm in 
AI, where multiple autonomous agents collaborate or interact within a shared environment to achieve complex goals. These systems are built to harness the synergy of diverse agent capabilities by leveraging the powe rful reasoning, creativity, and  contextual un derstanding of 
LLMs to tackle problems beyond the scope of any single agent. Figure 6-5 lays out a comprehensive blueprint for such a system. Now, we will see a detailed, expert-level 
explanation of its various interconnected components and their interplay.
At the heart of this MAS architecture lies the concept of the agent, which operates as an 
autonomous unit that is equipped with distinct roles, behaviors, and tools. Each agent is imbued with a profile or persona that defines its domain expertise, operational boundaries, and response style. For example, an agent might  function as a scientific researcher that is 
skilled in computational chemistry or as a cust omer service representative with expertise in 
natural language understanding and resolution st rategies. This persona is not static because 
it evolves in response to feedback and enviro nmental changes, which help ensure that the 
agent remains relevant and effective in dynamic settings.
Central to an agent’s operation is its belief state, which is an internal model that encapsulates 
its understanding of the world, its knowledge of tasks at hand, and its memory of past interactions. This belief state is dynamic and constantly updated through observations, actions, and communications with other agents or external systems. The belief state also integrates inputs from memory. Memory is bifurcated into long-term and short-term storage, each serving distinct roles. Short-term memory  retains ephemeral inform ation that is crucial 
for immediate task execution and contextual reasoning. For example, an agent might use short-term memory to temporarily store user input or intermediate results of calculations. In contrast, long-term memory preserves knowledge that is accumulated over time, such as procedural expertise, domain-specific facts, or  historical interactions. The integration of 
memory with the agent’s belief state helps ensure that decisions are informed by both the immediate context and historical knowledge. This dual-layered memory structure enhances the system’s ability to handle co mplex, multi-step tasks that r equire contextual continuity and 
long-term planning.


98 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiDriving the agent’s reasonin g and interaction is its policy , a framework of decision-making 
rules and algorithms that govern how the agent interprets inputs, prioritizes tasks, and generates outputs. Policies can be simple, predefined heuristics or sophisticated, dynamically updated models that are informed by machine learning (ML) techniques, such as RL. These policies are operationalized through the agent’s LLM core, which is the generative engine that processes natural language inputs, synthesizes insights, generates context-aware responses, and even creates novel solutions to problems. The LLM also serves as the agent’s interface to interpret ambiguous or unstructured information, which effectively bridges the gap between human language and computational logic.
Agents in this system are equipped with advanc ed communication c apabilities that are 
defined through a structured I/O schema that enables them to interact with other agents, external tools, and the environment. Communication is about exchanging messages, negotiating shared understandings, aligning goals, and ensuring coordinated action. To accomplish these tasks, agents can rely on tools that are integrated within the system. These tools extend the agent’s functio nal repertoire and in clude utilities like API access for external 
data retrieval, calculators for mathematical computations, code interpreters for debugging and running programs, and multimodal models for handling complex data types, such as images or video. 
For domain-specific applications, tools like chemistry simulators or Robotic Process 
Automation (RPA) frameworks can be deployed, which enable agents to specialize in tasks that range from molecular modeling to workflow optimization. These tools are seamlessly integrated into the agent’s workflow, which enables it to run specialized tasks without requiring extra human intervention. 
There is no defined limit or minimum requirement regarding the number of tools that are 
available to a set of agents: it is something dependent on the specific use cases that are addressed. What is proposed here is just a pot ential set of tools that might be useful for 
various potential use cases in a specific set of industries. In a MAS, a set of tools might contain hundreds of tools, depending on what is the goal of the system.
At a higher level, the MAS orchestrates the activi ties of these agents to help ensure that they 
work collaboratively toward shar ed objectives. This orchestration is managed by several 
critical components. 
The first is the goal and task decomposition me chanism, which takes high-level goals and 
breaks them into smaller, ma nageable tasks and subtasks. Fo r example, if the system’s goal 
is to generate a comprehensive market analys is, this task might be decomposed into tasks 
like gathering data, analyzing trends, and generating a summary report, with each task that is further divided into specific steps like querying databases or visualizing data. 
To enable task execution, the system relies on a planner that assigns tasks to the most 
suitable agents based on th eir profiles, curr ent workloads, and skillsets. The 
planner  plays a 
pivotal role in the MAS by orchestrating the decomposition of high-level goals into granular tasks and subtasks. The planning  process leverages the agent’s  policy and LLM to identify 
the optimal sequence of actions that are required to achieve the outcome. The planner also monitors progress, adapting task sequences or agent roles as needed to accommodate changing conditions or unexpected challenges. For example, if one agent encounters a bottleneck in data retrieval, the planner can reassign related tasks to another agent with overlapping capabilities. 

Chapter 6. Artificial intelligence agents 99For critical use cases, th e architecture in corporates the ability to receive human feedback  
during the execution of the plan to help ensure that automation in high-stakes scenarios remains governable and aligned with human oversight. By enabling human intervention, the system can address unforeseen complexities, validate decisions, and maintain control over critical operations.
Task decomposition ensures that even complex ob jectives are approached methodically, with 
subtasks delegated to the appropriate agents or tools.
Collaboration within the MAS is further enhanced through sophisticated communication 
patterns, which define how agents interact and share information. The architecture supports multiple 
communication patterns :
/SM590000Layered communication: Hierarchical interactions where agents operate at different levels 
of abstraction by passing information up or down the chain.
/SM590000Decentralized communication: Peer-to-peer exchanges among agents to help ensure 
flexibility and redu ce bottlenecks.
/SM590000Centralized communication: A hub-and-spoke model where a central coordinator 
manages all interactions.
/SM590000Shared message pool: A collaborative mechanism where agents exchange messages 
through a shared repository.
Hybrid patterns, such as shared message pools or global workspaces, enable agents to post 
intermediate results or disco veries to a common repository , which enables asynchronous 
collaboration and emergent problem-solving. The blackboard pattern  or global workspace 
serves as a central knowledge-sharing hub within the MAS. Agents use this shared repository to post updates, intermediate calculations, or unresolved queries, which create a collaborative environment where other agents ca n contribute insights or take over pending 
tasks. For example, an agent working on a data analysis task might upload partial results to the blackboard, which another agent can use to generate visualizations or summaries. 
The MAS is supported by  an agents and skills repository, wh ich is a centralized directory that 
catalogs the capabilities, expert ise, and tools that are associ ated with each agent. This 
repository enables dynamic discovery and allocati on of agents for specific tasks, which helps 
ensure that the system can scale and adapt to  diverse challenges. It also facilitates the 
incorporation of new skills or agents so th at the system can evolve as new tools or 
requirements emerge.
The architecture helps ensure that the output that is generated by the system is coherent, 
contextually relevant, and correctly formatted. The 
I/O schema  governs the structure of inputs 
and outputs by standardizing interactions across agents, tools, and external systems. This schema helps ensure compatibilit y and consistency regardless of the task or domain. The 
system’s communication module also plays a role in tailoring outputs to the intended audience. For example, technical results might be presented in a concise, data-rich format for domain experts, but layperson-oriented outputs would emphasize clarity and simplicity. 

100 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiDespite its sophisticated design, the MAS must handle potential errors that can arise during 
execution:
/SM590000Failed API calls to tools: A tool might be te mporarily unavailable or malfunction, which can 
lead to incomplete or erroneous task execution. Human feedback can help decide whether to retry the tool, use an alternative tool, or modify the task parameters.
/SM590000Infinite loops: Erroneous task decomposition or planning might result in a loop where the 
system repeatedly runs the same actions without progress. Human intervention can identify and correct the root cause to prevent resource wastage.
/SM590000Rogue paths or wrong tool selection or input: The system might choose an inappropriate 
tool or misinterpret input data, which can lead to incorrect outputs. Human feedback can help realign the system’s actions to help ensure that the task remains on track.
By integrating human feedback into the loop, particularly for infinite loops or rogue paths, the 
MAS can maintain a high degree  of reliability and robustness by implementin g a fail-safe 
technique on human-in-the-loop interaction after deviant agents’ behavior is detected by the overall orchestration system. This hybrid approach of automation with human oversight is especially critical in high-stakes domains wh ere errors can have significant consequences.
The MAS operates within an environment, such as the external world or a digital context 
where tasks are performed and results are applied. Agents interact with the environment by observing its state, acting to modify it, and processing feedback to refine their belief states and policies. The environment is also where the whole MAS runs, and it is possible to have environments that are composed of multiple locations, such as cloud environments, virtual machines (VMs) and containers, or further proprietary software.
6.4  AI agents on watsonx.ai 
IBM watsonx.ai enhances the development of an enterprise-grade agentic technology stack (Figure 6-6) by providing powerful  tools, models, and middleware  capabilities th at are tailored 
for scalable, intelligent, and  adaptive operations. 
Figure 6-6   Enterprise agentic tech stack


Chapter 6. Artificial intelligence agents 101At the foundation level, watsonx.ai delivers robust LLMs optimized for specific enterprise use 
cases to enable agents to interpret and act on complex queries with high accuracy and relevance. These LLMs seamlessly integrate in to the Agentic Framework, which serves as 
the operational backbone for orchestrating tasks, planning workflows, and retaining memory for context-driven decision-making. 
IBM watsonx.ai also supports the Agentic Service Deployment & Operation Platform, which 
offers a streamlined way to de ploy and manage agentic services while  ensuring reliability, 
scalability, and security at an enterprise level. To ensure en terprise-grade observability and 
monitoring, watsonx.ai incorporat es observability mechanisms that  provide real-time insights 
into agent performance, operational health , and user interactions. These mechanisms 
facilitate continuous optimizati on and help ensure t hat agentic services  align with business 
goals. By bridging models, middleware, and applications, watsonx.ai creates a cohesive and modular enterprise tech stack that can address the evolving demands of modern businesses with precision, adaptab ility, and innovation.
watsonx.ai agents are a transformative innovation in the domain of AI. These agents are 
designed to provid e businesses with unparalle led capabilities in automa ting tasks, processes, 
and decision-making. Through a blend of inte rfaces, cutting-edge technologies, and robust 
integration options, watsonx.ai enables the dev elopment, deployment, and optimization of 
intelligent agents that cater to a diverse range of enterprise needs.
At the core of watsonx.ai's agent ic capabilities is the Agent Build er (Figure 6-7), which is an 
intuitive and powerful tool that accelerates the development lifecycle. The visual interface of the Agent Builder enables developers to construct agents with ease, which reduces the complexity that is typically associated with designing and managing such systems. Agents 
within watsonx.ai are defined through natural language instructions, and they can be equipped with various tools to expand their fu nctions. These tools act as modular building 
blocks to enable developers to create sophisticated workflows that are tailored to specific requirements.
Figure 6-7   Agent Builder view on the watsonx.ai UI


102 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiOne of the standout features of the Agent Builder is its seamless integration with multiple 
agent frameworks. In addition to IBM proprietary technologies, developers can also leverage popular open-source fr ameworks like LangChain and LangGr aph. This flexib ility helps ensure 
that businesses can use the best tools and methodologies that are available in the ecosystem, and adapt them to their unique operational need s. The ability to integrate 
open-source solutions with IBM advanced technologies provides a level of customization and extensibility that is critical for modern  enterprises, as shown in Figure 6-8.
Figure 6-8   Overview of customiz ations on watsonx.ai Agent Builder
Testing and debugging are essential components of the agent development process, and 
watsonx.ai Agent Builder excels in this area. Real-time te sting capabilities enable developers 
to identify and resolve issues as they arise,  which minimizes downtime  and iteration cycles. 
This feature is complemented by the “one-clic k” deployment mechanism,  which simplifies the 
process of making agents operational. Once developed, agents can be deployed as watsonx.ai AI services, which effectively turn them into API endpoints that can be accessed by various applications and systems. This streamlined workflow reduces time-to-market and helps ensure that agents can be quickly integrated into enterprise operations.
To further enhance the functions of agents, wats onx.ai offers an extensive Tool Library of 
enterprise-ready tools that are designed to a ugment the capabilities of agents. The tools are 
divided into the following categories:
/SM590000Web Search
/SM590000Document Search (RAG)
/SM590000Code Execution
/SM590000Data Connectors
/SM590000Custom Tool Builder
For example, the Web Search tool empowers age nts to perform real-time internet searches, 
which provide them with up-to-date information to enhance their decision-making and responses. The ability to access  fast and relevant search results helps ensure that agents 
remain informed and capable of handling dynamic queries.


Chapter 6. Artificial intelligence agents 103Another critical component of the Tool Library is the Document Search function, which uses 
RAG. This tool enables agents to efficiently index and retrieve documents from an organization’s knowledge base, which helps ensure that they can deliver accurate and context-aware responses. By leveraging RAG, agents can access vast amounts of information and distill it into  actionable insights, which makes them invaluable for 
knowledge-intensive tasks. 
The Tool Library also includes a Code Execution feature, which enables agents to run Python 
code in real time. This capability opens up a wide range of possibilit ies, from performing 
complex calculations to automati ng repetitive tasks. By integrat ing this feature, agents can 
operate as dynamic problem solvers that can adapt to various scenarios. 
Data accessibility is another co rnerstone of the watsonx.ai a gentic framework. With Data 
Connectors, agents can seamlessly interact with enterprise databases and data warehouses, which grant them access to critical organizatio nal data. This tool helps ensure that agents 
operate with a comprehensive understanding of the business context, which enables more informed and effective decision-making. 
The Tool Library supports the creation of custom tools so that organizations can extend the 
functions of their agents by integrating them with external services and unique enterprise systems. This level of customizat ion helps ensure that agents can meet the specific demands 
of any organization.
Deployment is a critical phase in the lifecycle of any AI system, and watsonx.ai provides a 
robust, framework-neutral solution for deploying agents. The deployment process is scalable, secure, and highly available, which helps ensure that agents can meet the demands of enterprise-scale operations. Whether an organiza tion requires a single agent for a specific 
task or a fleet of agents to ha ndle complex workflows, the wa tsonx.ai deployment capabilities 
can handle the challenge. Once deployed, th e performance and reliabilit y of agents must be 
monitored to ensure that they operate as intended. watsonx.ai includes comprehensive 
monitoring tools that track key performance in dicators (KPIs) and analyze logs. These tools 
provide valuable insights into agent behavior, wh ich enable developers and administrators to 
identify areas for improvement and help ensure that agents deliver optimal results. 
watsonx.ai emphasizes transparency  and explainability, which are crucial for building trust in 
AI systems. By offering detailed explanations of agent decisions and actions, the platform 
helps organizations maintain compliance with regulatory requirements and ethical standards.
Looking to the future, watsonx.ai is poised to introduce the Flows Engine, which is a 
lightweight agentic framework and tool-building pl atform that will fu rther enhance the 
capabilities of the platform. The Flows Engine will enable developers to rapidly build custom 
tools and integrate them with en terprise IT systems, which will provide unparalleled flexibility. 
This framework is designed to facilitate reason ing and complex decision-making, which will 
make agents more effect ive in handling intricat e tasks. Also, the Flows Engine will include a 
chat UI widget that can be easily inte grated into thir d-party applications, which will enable 
seamless AI-mediated interactions.

104 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiComplementing watsonx.ai is IBM watsonx Orchestrate, which is a platform that focuses on 
starting AI assistants and agents for business processes and task automation (Figure 6-9). By combining the capab ilities of watsonx.ai and watsonx Orchestrate, organizations can 
achieve end-to-end automation to streamline operations and drive efficiency across their 
workflows. This synergy highlights IBM’s commitment to providing comprehensive AI solutions that address the divers e needs of modern enterprises.
Figure 6-9   Overview of watsonx  Orchestrate for Agents capabilities
The integration of watsonx Orchestrate and watsonx.ai offers an advanced, enterprise-grade 
framework that enhances agentic support by uniting robust autom ation, intelligent workflows, 
and conversational AI capabilitie s. watsonx Orchestrate function s as a supervisory agent that 
leverages LLMs to coordinate interactions ac ross customers, employees, subject matter 
experts (SMEs), and applications. By employin g Skills Studio, watsonx Orchestrate enables 
the discovery, creation, and management of gen AI and digital automations by combining tasks, workflows, and skills to drive seamless operational efficiency. Skills and automations 
can be trained and published in to a comprehensive skills catalo g, which includes prebuilt 
integrations with enterprise so lutions such as SAP, Salesfor ce, and ServiceNow, to help 
ensure compatibility wit h diverse enterprise systems. Si multaneously, watsonx.ai powers 
intelligent AI assistants, enabling conversational experiences that feature advanced functions like slot filling, LLM rout ing, disambiguation, and digression s. This combinat ion helps ensure 
personalized and context-aware interactions. Together, watsonx Orchestrate and watsonx.ai establish a scalable ecosystem to enable enterprises to simplify complex processes, reduce operational bottlenecks, and deliver intuitive, guided experiences that align AI-driven solutions with business objectives in a dyna mic, flexible, and secure manner.
Because of these agentic frameworks on watsonx.ai that are combined with watsonx 
Orchestrate, you can use advanced assistants that can perform many types of tasks.
Figure 6-10 on page 105 provides a comprehens ive depiction of Assistants with Agents on 
watsonx, which shows the integration of wats onx.ai and watsonx Orchestrate to enable 
sophisticated AI-driven solutions for enterprise workflows.


Chapter 6. Artificial intelligence agents 105Figure 6-10   Assistant with Agents that use wats onx.ai and watsonx Orchestr ate for agentic use cases
At the core of this design is the concept of a unified, intelligent a ssistant framework that 
combines modularity, scalabilit y, and precision to deliver seamless experiences across 
diverse use cases and tasks. Each component in Figure 6-10 plays a critical role in orchestrating complex interactions between users, agents, tools, and workflows, which create a robust and adaptable ecosystem for AI-powered operations. 
At the top of the architecture is the Unified As sistant, which serves as the single, user-facing 
interface. This layer is designed to provide a consistent and coherent interaction experience 
by simplifying user engagement and abstracting the complexities of the underlying system. The Unified Assistant is supported by the Supervisory AI Meta-Agent, which is a central coordinator that facilitates all workflows and he lps ensure that user requests ar e processed 
accurately and efficiently. The meta-agent acts as the brain of the system by orchestrating the activities of multiple subordi nate agents and tools to  fulfill user intents.  Its responsibilities 
include routing tasks to the appropriate agents, resolving ambiguities in user input, starting the necessary tools, reasoning through multi-step problems, and planning actions to achieve outcomes. These capabilities are made possible by the integratio n of LLMs that  are powered 
by watsonx.ai, which provides the advanced natural language understanding, contextual awareness, and reasoning ab ilities that are needed for high-quality interactions.


106 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiBeneath the Supervisory AI Meta-Agent lies a network of specialized assistants that are 
labeled as Assistant 1, Assistant 2, and so on, which represents a modular and scalable approach to task execution. Each assistant is tailored to handle specific domains or functions, 
and they operate autonomously while contributing to the overall system. These assistants are designed to run actions independently, and they leverage the power of LLMs for NLP, multi-turn conversations, and decision-making.  This autonomy enables them to reduce 
manual intervention so that organizations can achieve higher levels of efficiency and 
productivity. The assistants also facilitate user  interactions by maintaining context, 
understanding intent, and responding dy namically to evolving requirements. 
A defining feature of this architecture is its reliance on tool calling, which forms the backbone 
of the system’s operational fl exibility and extensibility. Figu re 6-10 on page 105 emphasizes 
the integration of various categories of tools that the agents can call to complete tasks. These tools include fixed flows for handling repetitive and standardized operations; knowledge repositories for answering questions and providing insights; APIs for interacting with external systems; business automation modules for streamlining enterprise processes; and multi-agent frameworks for coordinating complex tasks that require collaboration among several AI agents. The usage of LLM-based tool calling helps ensure that the system can adapt to various workflows, which enables interoperability with existing IT infrastructures and third-party applicat ions. watsonx.ai enhances th is capability through its extensive Tool Library, 
which includes features such as RAG for knowledge discovery; real-time Python code execution for computational tasks; and seamless data connectors for integrating with enterprise databases and services. Custom tools can also be developed and incorporated, which enable organizations to tailor the system to their unique needs and challenges.
The bottom part of Figure 6-10 on page 105 im plicitly connects to watsonx Orchestrate, 
which is the IBM platform for deploying and managing AI-driven workflows. By leveraging 
watsonx Orchestrate, this archit ecture gains enterprise-grade ca pabilities for task automation, 
governance, and monitoring. This integration enables organizations to deploy AI assistants quickly and securely, with the ab ility to scale the system as the number of tasks , agents, and 
integrations grows. Security and compliance ar e also ensured, which addresses the stringent 
requirements of enterprise environments. The orchestration layer further optimizes performance by streamlining the deployment and management of agents, which minimizes operational overhead while maximizing system reliability.
The overall interaction flow in Figure 6-10 on page 105 begins with the user engaging with the 
Unified Assistant. The user’s input is processed by the Supervisory AI Meta-Agent, which applies its routing, reasoning, and tool-calling capabilities to determine the be st course of 
action. Then, tasks are delegated to the appropriate assistants, which run them autonomously by using the integrated tools and workflows. The outputs and actions from these assistants are aggregated by the meta-agent and presented to the user, which helps ensure a cohesive and intuitive experience. This flow highlight s the system’s ability to handle 
diverse and complex tasks while maintaining a si mple interface.
In conclusion, watsonx.ai agents represent a revolutionary approach to enterprise AI by 
offering a powerful combination of flexibility, function, and scala bility. From the intuitive Agent 
Builder to the expansive Tool Library and fo rthcoming innovations like the Flows Engine, 
watsonx.ai empowers businesses to create intelligent agents that drive productivity and 
innovation. By leveraging th ese advanced capabilit ies, organizations can harness the full 
potential of AI to achieve their strategic goals and maintain a competitive edge in an increasingly digital world.

Chapter 6. Artificial intelligence agents 1076.5  AI agents use case examples
AI agents are redefining the landscape of business operations and service delivery by 
unlocking unprecedented efficiencies and enabling more personalized, data-driven decision-making. Across  industries, these inte lligent systems ha ve found applications in 
areas such as customer service and support, sales and marketing automation, operational efficiency, financial advisory, healthcare, and supply chain management. 
The following sections provide a detailed exploration of select use cases, and highlight their 
impact and capabilities.
Customer service and support agents AI agents
AI agents are revolutionizing customer service by offering continuous assistance. By using 
tools like watsonx.ai Web Search and Document Search (powered by RAG), these agents provide timely and accurate responses to customer inquiries. By leveraging NLP and ML, they can understand context, resolve issues, and e scalate complex cases to human agents when 
necessary. For example, chatbots that are built by using watsonx.ai Agent Builder can be deployed as API endpoints to handle FAQs, help with troubleshooting, and manage order tracking. These agents help ensure instant support while reducing operational costs, improving customer satisfaction, and fostering loyalty.
Sales and marketing automation
AI agents are indispensable tools in sales and marketing, where personalization and timely interactions drive success. Powered by watsonx.ai Data Connectors, agents can analyze customer behavior, qualify leads, and deliver tailored recommendations. By automating follow-ups and dynamically adjusting strategies  by using real-time insights, these agents 
enhance engagement and drive conversions. For example, e-commerce platforms can use watsonx.ai agents to suggest personalized items, send promotional offers, and re-engage customers who abandon carts. Such integrations help businesses maximize revenue potential while delivering highly customized customer experiences.
Operational efficiency and process automation
Automating repetitive and rout ine tasks is a hallmark of AI a gents, and watsonx.ai provides 
the tools to optimize these processes. By using the Code Execution capa bility, agents can run 
Python scripts in real time to process data, verify documents, or automate workflows. Beyond task automation, these agents can monitor work flows, identify bottle necks, and recommend 
improvements. For example, administrative tasks like employee onboarding can be fully automated with watsonx.ai by processing documentation, setting up accounts, and scheduling orientation sessions, which free employees to focus on more strategic responsibilities.
Healthcare assistants and patient care
In healthcare, AI agents improve patient care an d operational efficiency by acting as virtual 
assistants. By integrating with watsonx.a Tool Library and leveraging custom tools, these agents can manage schedules, send medication reminders, and provide basic medical guidance. For example, telemedicine platforms can deploy agents to conduct preliminary symptom checks, monitor health metrics such as heart rate or blood pressure, and alert providers to abnormalities. These capabilities reduce the workload on healthcare staff while ensuring proactive and timely patient care, which enhances outcomes and satisfaction.

108 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiSupply chain and logistics optimization
AI agents are transforming supply chain management by combining real-time data with 
predictive analytics. Tools like watsonx.ai Data Connectors enable agents to forecast demand, manage inventory, and plan delivery routes effectively. Logistics companies can leverage these capabilities to analyze historical  patterns, predict future needs, and maintain 
optimal stock levels. Also, AI-driven route optimization, when informed by traffic and weather data, helps ensure timely deliveries and reduces costs. With watsonx.ai, organizations can build scalable, secure agents that streamline supply chain operations, which enhance both 
efficiency and customer satisfaction.

© Copyright IBM Corp. 2025. 109Chapter 7. Use cases
This chapter describes two separate use cases and shows what problems IBM watsox.ai 
tools can solve. It also describes a framework that outlines how companies who are trying to 
prepare for the future are thinking about use cases of the future to keep ahead of the curve.
The following topics are described in this chapter:
/SM5900007.1, “Using RAG to aid a medical school admissions office” on page 110
/SM5900007.2, “Embedding workflow automation to streamline recommendations” on page 1117

110 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai7.1  Using RAG to aid a medi cal school admissions office
As a quick refresher, Retrieval-Augmented Generation (RAG) is a technique that combines 
information retrieval and language model generation to provide precise and contextually relevant responses to user queries. It works by first retrieving relevant documents or 
passages from a large corpus of documents by using a retrieval step, and then feeds these passages along with the original query into a large language model (LLM) to generate a response.
RAG is one of the most commonly used techniques  that are used in production AI workloads. 
It is used in various ways, such as in question-answering systems, chatbots, and digital workers. One powerful tool that can be used in these systems is a summary of ingested documents. This section describes how a lead ing medical school in th e US turned to the 
watsonx.ai platform to enable it to accomplish its goals.
7.1.1  The challenge
A leading medical school in the US decides to offer tuition-free education to its admitted students. They anticipated a surge in applications. To help manage the expected increase in applications, the institution turned to wats onx.ai. They hoped that IBM could provide a 
technology solution to help the admissions committee efficiently process and review the incoming applications. 
7.1.2  The solution
Working with the medical school, the IBM team developed an innovative solution by using watsonx.ai to generate one-page abstracts that summarized the incoming 50 - 70-page applications. The incoming applications included essays, with each application containing 5 - 8 essays that varied from several paragraphs to several pages. The IBM granite-13b-chat-v2 model within the watsonx.ai platform was used to generate a 1 - 2 paragraph summary of each of the essays, which was included with the application abstracts.
Figure 7-1 shows the watsonx.ai workflow that accomplished this task.
Figure 7-1   watsonx.ai workflow example


Chapter 7. Use cases 1117.1.3  Special considerations
From the start of this project, both IBM and the medical school recognized the importance of 
developing a solution that met the institution's needs and aligned with IBM AI Ethics. The usage of AI in the admissions process raised important ethical considerations, such as helping ensure fairness , transparency, and acc ountability. The goal was to create a system 
that would augment human decision-making rather than replace it, and provide the admissions committee with the tools that they  needed to make informed decisions.
The project incorporated a range of both technical and non-technical guardrails to address AI 
ethics considerations throughout the project lifecycle:
/SM590000Augmenting human decision making: The solution was designed to support human 
decision-makers, and not replace them. The AI-powered pipeline generated summaries and identified key information, but all decisions remained in the hands of humans.
/SM590000Education and training: The IBM team provided ongoing education and training on AI to 
both technical and business users to help ensure that everyone that was involved in the project understood the ca pabilities and limitations  of the technology.
/SM590000Thresholds and AI notices: The team im plemented technical guardrails by using 
watsonx.governance to detect and prevent potential biases or errors in the system.
/SM590000Feedback mechanism: The IBM team establishe d a continuous feedback loop with the 
client to refine and improve the solution over time.
By considering these guardrails from the beginning, the project was able to meet the 
institutions needs while aligning to their governance frameworks regarding fairness, transparency, and accountability. It also limited the school’s risk exposure and reduced the 
likelihood of having to redesign the system to incorporate new safeguards because decision points were integrated from the start.
7.2  Embedding workflow automation to streamline 
recommendations
This use case describes at how workflow automation can lead to directly addressing the 
wants and needs of a financial institution, and act as a potential opportunity pipeline for the banks serving their customers.
7.2.1  The challenge
Small local and regional banks, especially ones serving customers in more rural settings, 
often face different challenges than the large banks that service most of the total addressable market. Some of these challenges are self-e vident, such as having comparatively limited 
access to capital, but other challenges are less apparent, such as an increased need to rely on low- and no-code solutions to maintain technological parity with the custom-built 
applications that are designed and managed by large centralized IT teams that are staffed by larger finance institutions.
For one of these banks servicing rural clients in  the Midwest region of the US, they wanted to 
address some of the market trends they read about in 5 banking customer experience trends 
to consider for 2024 , with a specific focus on providing customers with immediate service and 
personalized recommendations. 

112 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiIn these rural settings, it is often difficult fo r a bank’s customer base to travel to the nearest 
branch, and at specific times of the year, the journey takes time away from their responsibilities at farms, ranche s, and processing centers, whic h directly impacts their annual 
income. 
7.2.2  The solution
By leveraging a collection of tools, this regional bank was able to build a solution that incorporated three IBM tools to reduce friction with its users while leveraging largely pre-built skills and solutions. The soluti on centered on a watsonx Orc hestrate business automation 
application that takes in loan applications and responds to the loan applicant with near-real-time approval or rejection notices based on thresholds that are set by the bank.
Figure 7-2 shows the watsonx Orchestrate workflow that was used in this use case.
Figure 7-2   watsonx Orchestrate workflow example
Customers connected to the bank through IBM watsonx Assistant, which leveraged an IBM 
LLM to initiate a natural language dialog with the client and collect various loan application documents, which included custom forms that are specific to this bank. These documents were passed from watsonx Assistant to watsonx Orchestrate, which called on several of its prebuilt skills, and custom skills  that were developed in the Skills Studio feature to access 
additional services outside of th e bank (such as credit reports). 
After running through a decision tree based on the inputs, the client received an approval or 
rejection notification. In either case, the notice was sent back to the applicant through a custom-generated response that was tailored by watsonx.ai and specific to that customer, with an explanation of the decision based on that customer’s specific application. Furthermore, the bank leveraged its own client knowledge to augment the loan decision with additional offers or suggestions to the customer based on their specific customer profile. For those customers that applied for an automotive loan and had a mortgage and business account with the bank, the bank suggested that they sign up for a wealth management account that was serviced by the bank. For th ose customers who applied for a small business 
loan who did not have a checking or savings account that was associated with the EIN on the application, the bank suggested that they open a full suite of business accounts with the loan. These examples pu lled on watsonx.ai gene rative capabilities to create personalized 
recommendations based on individual customers rather than “one-size-fits-all” templates that are applied as a blanket policy to all applicatio ns. Holistically, this entire solution can be 
summarized as an AI agent. 


Chapter 7. Use cases 1137.2.3  Special considerations
With a smaller workforce to dedicate to this solution and limited previous AI experience, the 
bank leveraged existing tools to reduce the workload and expertise requirements on the bank’s workforce. This approach was one of the key reasons IBM and the client focused on 
leveraging watsonx Orchestrate over designing a custom application that integrated AI tools 
through API calls. Instead of building a solution, the bank relied on built-in AI functions that automatically combined pre- packaged skills dynamically  and in-context based on 
organizational knowledge and prior interactions to help workers design the workflow of their application. Users prov ided natural language in puts to select and sequ ence the required skills 
for a task, and watsonx Orchestrate connected them with the associated applications, tools, data, and historical details. This approach enabled the team to automate processes without needing highly specialized IT skills or ex pert knowledge of business processes and 
applications.

114 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai

© Copyright IBM Corp. 2025.  115AI artificial intelligence
AIaaS AI as a Service
BYOM Bring Your Own Model
CLI command-line interface
CNN convolutional neural network
DL deep learning
DQN Deep Q-Network
ESG environmental, social, and 
governance
FM foundation model
gen AI Generative AI
IBM International Business Machines 
Corporation
KNN k-nearest neighbor
KPI key performance indicator
LLM large language model
LLMOps large language model operations 
LoRA low-rank adaptation
MAS multi-agent system
MDP Markov decision processes
ML machine learning
MLOps machine learning operations
MMLU Massive Multitask Language 
Understanding
NLP natural language processing
PCA Principal Component Analysis
QLoRA quantized low-rank adaptation
RAG Retrieval-Augmented Generation
RL reinforcement learning
RNN recurrent neural network
RPA Robotic Process Automation
SaaS Software-as-a-Service
SDG synthetic data generation
SME subject matter expert
UI user interfaceAbbreviations and acronyms

116 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai

© Copyright IBM Corp. 2025. 117Related publications
The publications that are listed in this section are considered suitable for a more detailed 
description of the topics that are covered in this book.
IBM Redbooks
The following IBM Redbooks publications provid e additional information about the topics in 
this document. Some publications that are refere nced in this list might be available in softcopy 
only. 
/SM590000Simplify Your AI Journey: Ensuring Trustworthy AI with IBM watsonx.governance , 
SG24-8573
/SM590000Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.data , SG24-8570
You can search for, view, download, or order these documents and other Redbooks, 
Redpapers, web docs, drafts, and addition al materials, at the following website: 
ibm.com/redbooks
Online resources
These websites are also relevant as further information sources:
/SM590000Code samples of common machine learning scenarios:
https://github.com/IBM/watson-machine-learning-samples
/SM590000Examples of using Instructlab and AI agents:
https://github.com/IBM/watsonx-ai-platform-demos
/SM590000IBM AI risk atlas
https://www.ibm.com/docs/en/watsonx/w-and-w/2.1.x?topic=ai-risk-atlas
/SM590000IBM watsonx documentation (Includes links to all watsonx products)
https://www.ibm.com/docs/en/watsonx
/SM590000IBM watsonx.governance product
https://www.ibm.com/products/watsonx-governance
/SM590000IBM watsonx product portfolio
https://www.ibm.com/watsonx

118 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.aiHelp from IBM
IBM Support and downloads
ibm.com/support
IBM Global Services
ibm.com/services

(0.2”spine)
0.17”<->0.473”
90<->249 pagesSimplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai

ibm.com /redbooksPrinted in U.S.A .Back cover
ISBN 0738461989SG24-8574-00
®

